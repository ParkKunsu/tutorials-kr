
<!DOCTYPE html>


<html lang="ko" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="단어 임베딩: 어휘의 의미(Lexical semantics)를 인코딩하기" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/beginner/nlp/word_embeddings_tutorial.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="번역: 임성연 단어 임베딩(word embedding)이란 말뭉치(혹은 코퍼스, corpus) 내 각 단어에 일대일로 대응하는 밀집된 실수 벡터(dense vector)의 집합, 혹은 이 벡터를 구하는 행위를 가리킵니다. 주로 단어를 피처(feature)로 사용하는 자연어 처리 분야에서는 단어를 컴퓨터 친화적인 형태로 바꾸어 주는 작업이 필수적입니다. 컴퓨터가 단어를 바로 이해하기는 상당히 어렵기 때문이죠. 그렇다면, 단어를 어떻게 표현하는 것이 좋을까요? 물론 각 문자에 해당하는 ASCII코드를 사용할 수 있겠지만, ASCI..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="번역: 임성연 단어 임베딩(word embedding)이란 말뭉치(혹은 코퍼스, corpus) 내 각 단어에 일대일로 대응하는 밀집된 실수 벡터(dense vector)의 집합, 혹은 이 벡터를 구하는 행위를 가리킵니다. 주로 단어를 피처(feature)로 사용하는 자연어 처리 분야에서는 단어를 컴퓨터 친화적인 형태로 바꾸어 주는 작업이 필수적입니다. 컴퓨터가 단어를 바로 이해하기는 상당히 어렵기 때문이죠. 그렇다면, 단어를 어떻게 표현하는 것이 좋을까요? 물론 각 문자에 해당하는 ASCII코드를 사용할 수 있겠지만, ASCI..." />
<meta property="og:ignore_canonical" content="true" />

    <title>단어 임베딩: 어휘의 의미(Lexical semantics)를 인코딩하기 &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../../_static/doctools.js?v=92e14aea"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/translations.js?v=b5f768d8"></script>
    <script src="../../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'beginner/nlp/word_embeddings_tutorial';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/beginner/nlp/word_embeddings_tutorial.html" />
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../../genindex.html" />
    <link rel="search" title="검색" href="../../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../_static/js/theme.js"></script>
<script type="text/javascript" src="../../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"></div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">단어 임베딩: 어휘의...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="단어 임베딩: 어휘의 의미(Lexical semantics)를 인코딩하기">
        <meta itemprop="position" content="1">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">beginner/nlp/word_embeddings_tutorial</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-beginner-nlp-word-embeddings-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="lexical-semantics">
<span id="sphx-glr-beginner-nlp-word-embeddings-tutorial-py"></span><h1>단어 임베딩: 어휘의 의미(Lexical semantics)를 인코딩하기<a class="headerlink" href="#lexical-semantics" title="Link to this heading">#</a></h1>
<p><strong>번역</strong>: <a class="reference external" href="http://github.com/sylim2357">임성연</a></p>
<p>단어 임베딩(word embedding)이란 말뭉치(혹은 코퍼스, corpus) 내 각 단어에 일대일로 대응하는 밀집된 실수 벡터(dense vector)의 집합, 혹은 이 벡터를
구하는 행위를 가리킵니다. 주로 단어를 피처(feature)로 사용하는 자연어 처리 분야에서는 단어를 컴퓨터 친화적인
형태로 바꾸어 주는 작업이 필수적입니다. 컴퓨터가 단어를 바로 이해하기는 상당히 어렵기 때문이죠.
그렇다면, 단어를 어떻게 표현하는 것이 좋을까요? 물론 각 문자에 해당하는 ASCII코드를 사용할 수 있겠지만,
ASCII코드는 이 단어가 <em>무엇</em> 인지를 알려줄 뿐, 단어가 어떤 <em>의미</em> 를 가지는지는 알려주지 않습니다.
(룰베이스로 어미 등 문법적 특징을 활용하거나 영어의 경우 대문자를 사용할 수 있겠지만 충분하지 않습니다.)
단어를 어떻게 표현할지 뿐 아니라, 이 표현법을 어떠한 방식으로 연산해야 할지 또한 큰 문제입니다.
보통 이러한 밀도 높은 벡터를 얻기 위해 사용하는 뉴럴넷 모델은 <span class="math">\(|V|\)</span> (말뭉치의 단어 개수)의
큰 입력 차원과 몇 안되는 (텍스를 분류하는 문제라고 할 경우) 작은 출력 차원을 가집니다.
즉, 단어들 간의 연산이 필수입니다. 어떻게 이 큰 차원의 공간을 작은 공간으로 변형시킬 수 있을까요?</p>
<p>먼저, 상기한 ASCII코드 대신 원핫 인코딩(one-hot encoding)을 사용해보는 것은 어떨까요? 원핫 인코딩이란
하나의 단어 <span class="math">\(w\)</span> 를 아래의 벡터로 표현하는 것을 말합니다.</p>
<div class="math">
\[\overbrace{\left[ 0, 0, \dots, 1, \dots, 0, 0 \right]}^\text{|V| elements}

\]</div>
<p>여기서 1은 해당 벡터가 표현하고자 하는 단어에 해당하는 위치 1곳에 자리합니다. (나머지는 전부
0입니다.) 다른 단어를 나타내는 벡터에선 1이 다른 곳에 위치해 있겠죠.</p>
<p>원핫 인코딩은 만들기가 쉽다는 장점이 있지만, 단순한 만큼 단점도 있습니다. 일단 단어 벡터 한 개는
모든 단어를 표현할 수 있을 만한 크기가 되어야 합니다. 우리가 얼마나 많은 종류의 단어를
사용하는지를 생각 한다면 어마어마하게 큰 벡터라는 것을 알 수 있죠. 이 뿐만이 아닙니다.
원핫 벡터는 모든 단어를 독립적인 개체로 가정하는 것을 볼 수 있습니다. 즉, 공간상에서
완전히 다른 축에 위치해 있어서 단어간의 관계를 나타낼 수가 없습니다. 하지만 우리는 단어
사이의 <em>유사도</em> 를 어떻게든 계산하고 싶은거죠. 왜 유사도가 중요하냐구요? 다음 예제를 봅시다.</p>
<p>우리의 목표가 언어 모델을 만드는 것이라고 가정하고 다음의 문장이 학습 데이터로써 주어졌다고 해봅시다.</p>
<ul class="simple">
<li><p>수학자가 가게로 뛰어갔다.</p></li>
<li><p>물리학자가 가게로 뛰어갔다.</p></li>
<li><p>수학자가 리만 가설을 증명했다.</p></li>
</ul>
<p>또한 학습 데이터에는 없는 아래 문장이 있다고 생각해봅시다.</p>
<ul class="simple">
<li><p>물리학자가 리만 가설을 증명했다.</p></li>
</ul>
<p>ASCII 코드나 원핫 인코딩 기반 언어 모델은 위 문장을 어느정도 다룰 수 있겠지만, 개선의 여지가 있지 않을까요?
먼저 아래의 두 사실을 생각해봅시다.</p>
<ul class="simple">
<li><p>‘수학자’와 ‘물리학자’가 문장 내에서 같은 역할을 맡고 있습니다. 이 두 단어는 어떻게든 의미적인 연관성이 있을 겁니다.</p></li>
<li><p>새로운 문장에서 ‘물리학자’가 맡은 역할을 ‘수학자’가 맡는 것을 학습 데이터에서 본 적이 있습니다.</p></li>
</ul>
<p>우리 모델이 위의 사실을 통해 ‘물리학자’가 새 문장에 잘 들어 맞는다는 것을 추론할 수
있다면 참 좋을 것입니다. 이것이 위에서 언급한 유사도의 의미입니다. 철자적 유사도 뿐
아니라 <em>의미적 유사도</em> 인 것입니다. 이것이야말로 언어 데이터에 내재하는 희박성(sparsity)에
대한 처방이 될 것입니다. 우리가 본 것과 아직 보지 않은 것 사이를 이어주는 것이죠.
앞으로는 다음의 언어학적 기본 명제를 가정하도록 합시다. 바로 비슷한 맥락에서 등장하는
단어들은 서로 의미적 연관성을 가진다는 것입니다. 언어학적으로는 <a class="reference external" href="https://en.wikipedia.org/wiki/Distributional_semantics">분산 의미 가설(distributional
hypothesis)</a> 이라고도 합니다.</p>
<section id="id2">
<h2>밀집된 단어 임베딩 구하기<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>어떻게 단어의 의미적 유사도를 인코딩 할 수 있을까요? 다시 말해, 어떻게 해야 단어의 유사도를
단어 벡터에 반영할 수 있을까요? 단어 데이터에 의미적 속성(attribute)을 부여하는 건 어떤가요?
예를 들어 ‘수학자’와 ‘물리학자’가 모두 뛸 수 있다면, 해당 단어의 ‘뛸 수 있음’ 속성에 높은 점수를 주는 겁니다.
계속 해봅시다. 다른 단어들에 대해서는 어떠한 속성을 만들 수 있을지 생각해봅시다.</p>
<p>만약 각 속성을 하나의 차원이라고 본다면 하나의 단어에 아래와 같은 벡터를 배정할 수 있을겁니다.</p>
<div class="math">
\[ q_\text{수학자} = \left[ \overbrace{2.3}^\text{뛸 수 있음},
\overbrace{9.4}^\text{커피를 좋아함}, \overbrace{-5.5}^\text{물리 전공임}, \dots \right]\]</div>
<div class="math">
\[ q_\text{물리학자} = \left[ \overbrace{2.5}^\text{뛸 수 있음},
\overbrace{9.1}^\text{커피를 좋아함}, \overbrace{6.4}^\text{물리 전공임}, \dots \right]\]</div>
<p>그러면 아래와 같이 두 단어 사이의 유사도를 구할 수 있습니다. (‘유사도’라는 함수를 정의하는 겁니다)</p>
<div class="math">
\[\text{유사도}(\text{물리학자}, \text{수학자}) = q_\text{물리학자} \cdot q_\text{수학자}

\]</div>
<p>물론 보통은 이렇게 벡터의 길이로 나눠주지만요.</p>
<div class="math">
\[ \text{유사도}(\text{물리학자}, \text{수학자}) = \frac{q_\text{물리학자} \cdot q_\text{수학자}}
{\| q_\text{물리학자} \| \| q_\text{수학자} \|} = \cos (\phi)\]</div>
<p><span class="math">\(\phi\)</span> 는 두 벡터 사이의 각입니다. 이런 식이면 정말 비슷한 단어는 유사도 1을 갖고,
정말 다른 단어는 유사도 -1을 갖겠죠. 비슷한 의미를 가질수록 같은 방향을 가리키고 있을 테니까요.</p>
<p>이 글 초반에 나온 희박한 원핫 벡터가 사실은 우리가 방금 정의한 의미 벡터의
특이 케이스라는 것을 금방 알 수 있습니다. 단어 벡터의 각 원소는 그 단어의 의미적 속성을
표현하고, 모든 단어 쌍의 유사도는 0이기 때문이죠. 위에서 정의한 의미 벡터는 <em>밀집</em> 되어 있습니다.
즉, 원핫 벡터에 비해 0 원소의 수가 적다고 할 수 있습니다.</p>
<p>하지만 이 벡터들은 구하기가 진짜 어렵습니다. 단어간의 유사도를 결정 지을 만한
의미적 속성은 어떻게 결정할 것이며, 속성을 결정했다고 하더라도 각 속성에
해당하는 값은 도대체 어떠한 기준으로 정해야 할까요? 속성과 값을 데이터에 기반해
만들고 자동으로 단어 벡터를 만들 수는 없을까요? 있습니다. 딥러닝을 사용하면 말이죠.
딥러닝은 인공신경망을 이용하여 사람의 개입 없이 속성의 표현 방법을 자동으로 학습합니다.
이를 이용해 단어 벡터를 모델 모수로 설정하고 모델 학습시에 단어 벡터도 함께 업데이트 하면
될 것입니다. 이렇게 우리 신경망 모델은 적어도 이론상으로는 충분히 학습할 수 있는
<em>잠재 의미 속성</em> 을 찾을 것입니다. 여기서 말하는 잠재 의미 속성으로 이루어진 벡터는 사람이
해석하기 상당히 어렵다는 점을 기억해 두세요. 위에서 수학자와 물리학자에게 커피를 좋아한다는
등 사람이 임의적으로 단어에 부여한 속성과는 달리, 인공신경망이 자동으로 단어의 속성을 찾는다면
그 속성과 값이 의미하는 바를 알기가 어려울 것입니다. 예를 들어서 신경망 모델이 찾은 ‘수학자’와
‘물리학자’의 표현 벡터 둘 다 두번째 원소가 크다고 가정해 봅시다. 둘이 비슷하다는 건 알겠지만,
도대체 두번째 원소가 무엇을 의미하는지는 알기가 매우 힘든 것입니다. 표현 벡터 공간상에서
비슷하다는 정보 외에는 아마 많은 정보를 주긴 어려울 것입니다.</p>
<p>요약하자면, <strong>단어 임베딩은 단어의 *의미* 를 표현하는 방법이며, 차후에 임베딩을 사용해서
풀고자 하는 문제에 유용할 의미 정보를 효율적으로 인코딩한 것입니다.</strong> 품사 태그, 파스 트리(parse tree) 등
단어의 의미 외에 다른 것도 인코딩 할 수 있습니다! 피처 임베딩의 개념을 잡는 것이 중요합니다.</p>
</section>
<section id="id3">
<h2>파이토치에서 단어 임베딩 하기<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>실제로 코드와 예시를 보기 전에, 파이토치를 비롯해 딥러닝 관련 프로그래밍을 할 때
단어 임베딩을 어떻게 사용하는지에 대해 조금 알아봅시다. 맨 위에서 원핫 벡터를
정의했던 것 처럼, 단어 임베딩을 사용할 때에도 각 단어에 인덱스를 부여해야 합니다.
이 인덱스를 참조 테이블(look-up table)에서 사용할 것입니다. 즉, <span class="math">\(|V| \times D\)</span> 크기의 행렬에
단어 임베딩을 저장하는데, <span class="math">\(D\)</span> 차원의 임베딩 벡터가 행렬의 <span class="math">\(i\)</span> 번째 행에
저장되어있어 <span class="math">\(i\)</span> 를 인덱스로 활용해 임베딩 벡터를 참조하는 것입니다.
아래의 모든 코드에서는 단어와 인덱스를 매핑해주는 딕셔너리를 word_to_ix라 칭합니다.</p>
<p>파이토치는 임베딩을 손쉽게 사용할 수 있게 torch.nn.Embedding에 위에서 설명한 참조 테이블
기능을 지원합니다. 이 모듈은 단어의 개수와 임베딩의 차원, 총 2개의 변수를 입력 변수로 받습니다.</p>
<p>torch.nn.Embedding 테이블의 임베딩을 참조하기 위해선 torch.LongTensor 타입의 인덱스 변수를
꼭 사용해야 합니다. (인덱스는 실수가 아닌 정수이기 때문입니다.)</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Author: Robert Guthrie</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;torch._C.Generator object at 0x7f0e60159890&gt;
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">word_to_ix</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;hello&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;world&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="n">embeds</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>  <span class="c1"># 2 words in vocab, 5 dimensional embeddings</span>
<span class="n">lookup_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">word_to_ix</span><span class="p">[</span><span class="s2">&quot;hello&quot;</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="n">hello_embed</span> <span class="o">=</span> <span class="n">embeds</span><span class="p">(</span><span class="n">lookup_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">hello_embed</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],
       grad_fn=&lt;EmbeddingBackward0&gt;)
</pre></div>
</div>
</section>
<section id="n">
<h2>예시: N그램 언어 모델링<a class="headerlink" href="#n" title="Link to this heading">#</a></h2>
<p>N그램 언어 모델링에선 단어 시퀀스 <span class="math">\(w\)</span> 가 주어졌을 때 아래의 것을 얻고자
함을 상기해 봅시다.</p>
<div class="math">
\[P(w_i | w_{i-1}, w_{i-2}, \dots, w_{i-n+1} )

\]</div>
<p><span class="math">\(w_i\)</span> 는 시퀀스에서 i번째 단어입니다.</p>
<p>이 예시에서는 학습 데이터를 바탕으로 손실 함수를 계산하고 역전파를 통해
모수를 업데이트 해보겠습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">CONTEXT_SIZE</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">10</span>
<span class="c1"># 셰익스피어 소네트(Sonnet) 2를 사용하겠습니다.</span>
<span class="n">test_sentence</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;When forty winters shall besiege thy brow,</span>
<span class="s2">And dig deep trenches in thy beauty&#39;s field,</span>
<span class="s2">Thy youth&#39;s proud livery so gazed on now,</span>
<span class="s2">Will be a totter&#39;d weed of small worth held:</span>
<span class="s2">Then being asked, where all thy beauty lies,</span>
<span class="s2">Where all the treasure of thy lusty days;</span>
<span class="s2">To say, within thine own deep sunken eyes,</span>
<span class="s2">Were an all-eating shame, and thriftless praise.</span>
<span class="s2">How much more praise deserv&#39;d thy beauty&#39;s use,</span>
<span class="s2">If thou couldst answer &#39;This fair child of mine</span>
<span class="s2">Shall sum my count, and make my old excuse,&#39;</span>
<span class="s2">Proving his beauty by succession thine!</span>
<span class="s2">This were to be new made when thou art old,</span>
<span class="s2">And see thy blood warm when thou feel&#39;st it cold.&quot;&quot;&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="c1"># 원래는 입력을 제대로 토큰화(tokenize) 해야하지만 이번엔 간소화하여 진행하겠습니다.</span>
<span class="c1"># 튜플로 이루어진 리스트를 만들겠습니다. 각 튜플은 ([ i-CONTEXT_SIZE 번째 단어, ..., i-1 번째 단어 ], 목표 단어)입니다.</span>
<span class="n">ngrams</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span>
        <span class="p">[</span><span class="n">test_sentence</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">CONTEXT_SIZE</span><span class="p">)],</span>
        <span class="n">test_sentence</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">CONTEXT_SIZE</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_sentence</span><span class="p">))</span>
<span class="p">]</span>
<span class="c1"># 첫 3개의 튜플을 출력하여 데이터가 어떻게 생겼는지 보겠습니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ngrams</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">test_sentence</span><span class="p">)</span>
<span class="n">word_to_ix</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>


<span class="k">class</span><span class="w"> </span><span class="nc">NGramLanguageModeler</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">context_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NGramLanguageModeler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">context_size</span> <span class="o">*</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">embeds</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">log_probs</span>


<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">NGramLanguageModeler</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">EMBEDDING_DIM</span><span class="p">,</span> <span class="n">CONTEXT_SIZE</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">context</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">:</span>

        <span class="c1"># 첫번째. 모델에 넣어줄 입력값을 준비합니다. (i.e, 단어를 정수 인덱스로</span>
        <span class="c1"># 바꾸고 파이토치 텐서로 감싸줍시다.)</span>
        <span class="n">context_idxs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">word_to_ix</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">context</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

        <span class="c1"># 두번째. 토치는 기울기가 *누적* 됩니다. 새 인스턴스를 넣어주기 전에</span>
        <span class="c1"># 기울기를 초기화합니다.</span>
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># 세번째. 순전파를 통해 다음에 올 단어에 대한 로그 확률을 구합니다.</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">context_idxs</span><span class="p">)</span>

        <span class="c1"># 네번째. 손실함수를 계산합니다. (파이토치에서는 목표 단어를 텐서로 감싸줘야 합니다.)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">word_to_ix</span><span class="p">[</span><span class="n">target</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">))</span>

        <span class="c1"># 다섯번째. 역전파를 통해 기울기를 업데이트 해줍니다.</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># tensor.item()을 호출하여 단일원소 텐서에서 숫자를 반환받습니다.</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>  <span class="c1"># 반복할 때마다 손실이 줄어드는 것을 봅시다!</span>

<span class="c1"># &quot;beauty&quot;와 같이 특정 단어에 대한 임베딩을 확인하려면,</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">word_to_ix</span><span class="p">[</span><span class="s2">&quot;beauty&quot;</span><span class="p">]])</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[([&#39;forty&#39;, &#39;When&#39;], &#39;winters&#39;), ([&#39;winters&#39;, &#39;forty&#39;], &#39;shall&#39;), ([&#39;shall&#39;, &#39;winters&#39;], &#39;besiege&#39;)]
[516.8885982036591, 514.2938964366913, 511.71493124961853, 509.1507501602173, 506.60002613067627, 504.0616383552551, 501.53501987457275, 499.0196087360382, 496.51370453834534, 494.0152516365051]
tensor([-0.5678,  0.7198,  0.1921,  0.2479, -0.3110,  0.0452, -1.5457, -0.3738,
         0.7146,  1.9276], grad_fn=&lt;SelectBackward0&gt;)
</pre></div>
</div>
</section>
<section id="continuous-bag-of-words">
<h2>예시: 단어 임베딩 계산하기: Continuous Bag-of-Words<a class="headerlink" href="#continuous-bag-of-words" title="Link to this heading">#</a></h2>
<p>The Continuous Bag-of-Words (CBOW) 모델은 NLP 딥러닝에서 많이 쓰입니다.
이 모델은 문장 내에서 주변 단어, 즉 앞 몇 단어와 뒤 몇 단어를 보고 특정
단어를 예측하는데, 언어 모델링과는 달리 순차적이지도 않고 확률적이지도 않습니다.
주로 CBOW는 복잡한 모델의 초기 입력값으로 쓰일 단어 임베딩을 빠르게 학습하는
데에 쓰입니다. 이것을 <em>사전 훈련된(pre-trained) 임베딩</em> 이라고 부르죠.
몇 퍼센트 정도의 성능 향상을 기대할 수 있는 기법입니다.</p>
<p>CBOW 모델은 다음과 같습니다. 목표 단어 <span class="math">\(w_i\)</span> 와 그 양쪽에 <span class="math">\(N\)</span> 개의
문맥 단어 <span class="math">\(w_{i-1}, \dots, w_{i-N}\)</span> 와 <span class="math">\(w_{i+1}, \dots, w_{i+N}\)</span>
가 주어졌을 때, (문맥 단어를 총칭해 <span class="math">\(C\)</span> 라고 합시다.)</p>
<div class="math">
\[-\log p(w_i | C) = -\log \text{Softmax}\left(A(\sum_{w \in C} q_w) + b\right)

\]</div>
<p>위 식을 최소화하는 것이 CBOW의 목적입니다. 여기서 <span class="math">\(q_w\)</span> 는 단어 <span class="math">\(w\)</span> 의
임베딩 입니다.</p>
<p>아래의 클래스 템플릿을 보고 파이토치로 CBOW를 구현해 보세요. 힌트는 다음과 같습니다.</p>
<ul class="simple">
<li><p>어떤 모수를 정의해야 하는지 생각해보세요.</p></li>
<li><p>각 작업에서 다루어지는 변수의 차원이 어떤지 꼭 생각해보세요.
텐서의 모양을 바꿔야 한다면 .view()를 사용하세요.</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">CONTEXT_SIZE</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 왼쪽으로 2단어, 오른쪽으로 2단어</span>
<span class="n">raw_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;We are about to study the idea of a computational process.</span>
<span class="s2">Computational processes are abstract beings that inhabit computers.</span>
<span class="s2">As they evolve, processes manipulate other abstract things called data.</span>
<span class="s2">The evolution of a process is directed by a pattern of rules</span>
<span class="s2">called a program. People create programs to direct processes. In effect,</span>
<span class="s2">we conjure the spirits of the computer with our spells.&quot;&quot;&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

<span class="c1"># 중복된 단어를 제거하기 위해 `raw_text` 를 집합(set) 자료형으로 바꿔줍니다.</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">raw_text</span><span class="p">)</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

<span class="n">word_to_ix</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">CONTEXT_SIZE</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">raw_text</span><span class="p">)</span> <span class="o">-</span> <span class="n">CONTEXT_SIZE</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">[</span><span class="n">raw_text</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">CONTEXT_SIZE</span><span class="p">)]</span>
        <span class="o">+</span> <span class="p">[</span><span class="n">raw_text</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">CONTEXT_SIZE</span><span class="p">)]</span>
    <span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">raw_text</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">context</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CBOW</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="k">pass</span>

<span class="c1"># 모델을 만들고 학습해 보세요.</span>
<span class="c1"># 아래는 데이터 준비를 원활하게 돕기 위한 함수입니다.</span>


<span class="k">def</span><span class="w"> </span><span class="nf">make_context_vector</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_to_ix</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">context</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">idxs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>


<span class="n">make_context_vector</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">word_to_ix</span><span class="p">)</span>  <span class="c1"># 예시</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[([&#39;are&#39;, &#39;We&#39;, &#39;to&#39;, &#39;study&#39;], &#39;about&#39;), ([&#39;about&#39;, &#39;are&#39;, &#39;study&#39;, &#39;the&#39;], &#39;to&#39;), ([&#39;to&#39;, &#39;about&#39;, &#39;the&#39;, &#39;idea&#39;], &#39;study&#39;), ([&#39;study&#39;, &#39;to&#39;, &#39;idea&#39;, &#39;of&#39;], &#39;the&#39;), ([&#39;the&#39;, &#39;study&#39;, &#39;of&#39;, &#39;a&#39;], &#39;idea&#39;)]

tensor([48,  0, 38, 28])
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 4.404 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-nlp-word-embeddings-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/363afc3b7c522e4e56981679c22f1ad6/word_embeddings_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">word_embeddings_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/58bdf45884d385ba7031225104b471d3/word_embeddings_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">word_embeddings_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/81ac1e09d57b4487bbbafee5f7e669b8/word_embeddings_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">word_embeddings_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">밀집된 단어 임베딩 구하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">파이토치에서 단어 임베딩 하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n">예시: N그램 언어 모델링</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-bag-of-words">예시: 단어 임베딩 계산하기: Continuous Bag-of-Words</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "\ub2e8\uc5b4 \uc784\ubca0\ub529: \uc5b4\ud718\uc758 \uc758\ubbf8(Lexical semantics)\ub97c \uc778\ucf54\ub529\ud558\uae30",
       "headline": "\ub2e8\uc5b4 \uc784\ubca0\ub529: \uc5b4\ud718\uc758 \uc758\ubbf8(Lexical semantics)\ub97c \uc778\ucf54\ub529\ud558\uae30",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/beginner/nlp/word_embeddings_tutorial.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. \ub2e8\uc5b4 \uc784\ubca0\ub529: \uc5b4\ud718\uc758 \uc758\ubbf8(Lexical semantics)\ub97c \uc778\ucf54\ub529\ud558\uae30# \ubc88\uc5ed: \uc784\uc131\uc5f0 \ub2e8\uc5b4 \uc784\ubca0\ub529(word embedding)\uc774\ub780 \ub9d0\ubb49\uce58(\ud639\uc740 \ucf54\ud37c\uc2a4, corpus) \ub0b4 \uac01 \ub2e8\uc5b4\uc5d0 \uc77c\ub300\uc77c\ub85c \ub300\uc751\ud558\ub294 \ubc00\uc9d1\ub41c \uc2e4\uc218 \ubca1\ud130(dense vector)\uc758 \uc9d1\ud569, \ud639\uc740 \uc774 \ubca1\ud130\ub97c \uad6c\ud558\ub294 \ud589\uc704\ub97c \uac00\ub9ac\ud0b5\ub2c8\ub2e4. \uc8fc\ub85c \ub2e8\uc5b4\ub97c \ud53c\ucc98(feature)\ub85c \uc0ac\uc6a9\ud558\ub294 \uc790\uc5f0\uc5b4 \ucc98\ub9ac \ubd84\uc57c\uc5d0\uc11c\ub294 \ub2e8\uc5b4\ub97c \ucef4\ud4e8\ud130 \uce5c\ud654\uc801\uc778 \ud615\ud0dc\ub85c \ubc14\uafb8\uc5b4 \uc8fc\ub294 \uc791\uc5c5\uc774 \ud544\uc218\uc801\uc785\ub2c8\ub2e4. \ucef4\ud4e8\ud130\uac00 \ub2e8\uc5b4\ub97c \ubc14\ub85c \uc774\ud574\ud558\uae30\ub294 \uc0c1\ub2f9\ud788 \uc5b4\ub835\uae30 \ub54c\ubb38\uc774\uc8e0. \uadf8\ub807\ub2e4\uba74, \ub2e8\uc5b4\ub97c \uc5b4\ub5bb\uac8c \ud45c\ud604\ud558\ub294 \uac83\uc774 \uc88b\uc744\uae4c\uc694? \ubb3c\ub860 \uac01 \ubb38\uc790\uc5d0 \ud574\ub2f9\ud558\ub294 ASCII\ucf54\ub4dc\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uaca0\uc9c0\ub9cc, ASCII\ucf54\ub4dc\ub294 \uc774 \ub2e8\uc5b4\uac00 \ubb34\uc5c7 \uc778\uc9c0\ub97c \uc54c\ub824\uc904 \ubfd0, \ub2e8\uc5b4\uac00 \uc5b4\ub5a4 \uc758\ubbf8 \ub97c \uac00\uc9c0\ub294\uc9c0\ub294 \uc54c\ub824\uc8fc\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. (\ub8f0\ubca0\uc774\uc2a4\ub85c \uc5b4\ubbf8 \ub4f1 \ubb38\ubc95\uc801 \ud2b9\uc9d5\uc744 \ud65c\uc6a9\ud558\uac70\ub098 \uc601\uc5b4\uc758 \uacbd\uc6b0 \ub300\ubb38\uc790\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uaca0\uc9c0\ub9cc \ucda9\ubd84\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.) \ub2e8\uc5b4\ub97c \uc5b4\ub5bb\uac8c \ud45c\ud604\ud560\uc9c0 \ubfd0 \uc544\ub2c8\ub77c, \uc774 \ud45c\ud604\ubc95\uc744 \uc5b4\ub5a0\ud55c \ubc29\uc2dd\uc73c\ub85c \uc5f0\uc0b0\ud574\uc57c \ud560\uc9c0 \ub610\ud55c \ud070 \ubb38\uc81c\uc785\ub2c8\ub2e4. \ubcf4\ud1b5 \uc774\ub7ec\ud55c \ubc00\ub3c4 \ub192\uc740 \ubca1\ud130\ub97c \uc5bb\uae30 \uc704\ud574 \uc0ac\uc6a9\ud558\ub294 \ub274\ub7f4\ub137 \ubaa8\ub378\uc740 \\(|V|\\) (\ub9d0\ubb49\uce58\uc758 \ub2e8\uc5b4 \uac1c\uc218)\uc758 \ud070 \uc785\ub825 \ucc28\uc6d0\uacfc \uba87 \uc548\ub418\ub294 (\ud14d\uc2a4\ub97c \ubd84\ub958\ud558\ub294 \ubb38\uc81c\ub77c\uace0 \ud560 \uacbd\uc6b0) \uc791\uc740 \ucd9c\ub825 \ucc28\uc6d0\uc744 \uac00\uc9d1\ub2c8\ub2e4. \uc989, \ub2e8\uc5b4\ub4e4 \uac04\uc758 \uc5f0\uc0b0\uc774 \ud544\uc218\uc785\ub2c8\ub2e4. \uc5b4\ub5bb\uac8c \uc774 \ud070 \ucc28\uc6d0\uc758 \uacf5\uac04\uc744 \uc791\uc740 \uacf5\uac04\uc73c\ub85c \ubcc0\ud615\uc2dc\ud0ac \uc218 \uc788\uc744\uae4c\uc694? \uba3c\uc800, \uc0c1\uae30\ud55c ASCII\ucf54\ub4dc \ub300\uc2e0 \uc6d0\ud56b \uc778\ucf54\ub529(one-hot encoding)\uc744 \uc0ac\uc6a9\ud574\ubcf4\ub294 \uac83\uc740 \uc5b4\ub5a8\uae4c\uc694? \uc6d0\ud56b \uc778\ucf54\ub529\uc774\ub780 \ud558\ub098\uc758 \ub2e8\uc5b4 \\(w\\) \ub97c \uc544\ub798\uc758 \ubca1\ud130\ub85c \ud45c\ud604\ud558\ub294 \uac83\uc744 \ub9d0\ud569\ub2c8\ub2e4. \\[\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements} \\] \uc5ec\uae30\uc11c 1\uc740 \ud574\ub2f9 \ubca1\ud130\uac00 \ud45c\ud604\ud558\uace0\uc790 \ud558\ub294 \ub2e8\uc5b4\uc5d0 \ud574\ub2f9\ud558\ub294 \uc704\uce58 1\uacf3\uc5d0 \uc790\ub9ac\ud569\ub2c8\ub2e4. (\ub098\uba38\uc9c0\ub294 \uc804\ubd80 0\uc785\ub2c8\ub2e4.) \ub2e4\ub978 \ub2e8\uc5b4\ub97c \ub098\ud0c0\ub0b4\ub294 \ubca1\ud130\uc5d0\uc120 1\uc774 \ub2e4\ub978 \uacf3\uc5d0 \uc704\uce58\ud574 \uc788\uaca0\uc8e0. \uc6d0\ud56b \uc778\ucf54\ub529\uc740 \ub9cc\ub4e4\uae30\uac00 \uc27d\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\uc9c0\ub9cc, \ub2e8\uc21c\ud55c \ub9cc\ud07c \ub2e8\uc810\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc77c\ub2e8 \ub2e8\uc5b4 \ubca1\ud130 \ud55c \uac1c\ub294 \ubaa8\ub4e0 \ub2e8\uc5b4\ub97c \ud45c\ud604\ud560 \uc218 \uc788\uc744 \ub9cc\ud55c \ud06c\uae30\uac00 \ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4. \uc6b0\ub9ac\uac00 \uc5bc\ub9c8\ub098 \ub9ce\uc740 \uc885\ub958\uc758 \ub2e8\uc5b4\ub97c \uc0ac\uc6a9\ud558\ub294\uc9c0\ub97c \uc0dd\uac01 \ud55c\ub2e4\uba74 \uc5b4\ub9c8\uc5b4\ub9c8\ud558\uac8c \ud070 \ubca1\ud130\ub77c\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc8e0. \uc774 \ubfd0\ub9cc\uc774 \uc544\ub2d9\ub2c8\ub2e4. \uc6d0\ud56b \ubca1\ud130\ub294 \ubaa8\ub4e0 \ub2e8\uc5b4\ub97c \ub3c5\ub9bd\uc801\uc778 \uac1c\uccb4\ub85c \uac00\uc815\ud558\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc989, \uacf5\uac04\uc0c1\uc5d0\uc11c \uc644\uc804\ud788 \ub2e4\ub978 \ucd95\uc5d0 \uc704\uce58\ud574 \uc788\uc5b4\uc11c \ub2e8\uc5b4\uac04\uc758 \uad00\uacc4\ub97c \ub098\ud0c0\ub0bc \uc218\uac00 \uc5c6\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac\ub294 \ub2e8\uc5b4 \uc0ac\uc774\uc758 \uc720\uc0ac\ub3c4 \ub97c \uc5b4\ub5bb\uac8c\ub4e0 \uacc4\uc0b0\ud558\uace0 \uc2f6\uc740\uac70\uc8e0. \uc65c \uc720\uc0ac\ub3c4\uac00 \uc911\uc694\ud558\ub0d0\uad6c\uc694? \ub2e4\uc74c \uc608\uc81c\ub97c \ubd05\uc2dc\ub2e4. \uc6b0\ub9ac\uc758 \ubaa9\ud45c\uac00 \uc5b8\uc5b4 \ubaa8\ub378\uc744 \ub9cc\ub4dc\ub294 \uac83\uc774\ub77c\uace0 \uac00\uc815\ud558\uace0 \ub2e4\uc74c\uc758 \ubb38\uc7a5\uc774 \ud559\uc2b5 \ub370\uc774\ud130\ub85c\uc368 \uc8fc\uc5b4\uc84c\ub2e4\uace0 \ud574\ubd05\uc2dc\ub2e4. \uc218\ud559\uc790\uac00 \uac00\uac8c\ub85c \ub6f0\uc5b4\uac14\ub2e4. \ubb3c\ub9ac\ud559\uc790\uac00 \uac00\uac8c\ub85c \ub6f0\uc5b4\uac14\ub2e4. \uc218\ud559\uc790\uac00 \ub9ac\ub9cc \uac00\uc124\uc744 \uc99d\uba85\ud588\ub2e4. \ub610\ud55c \ud559\uc2b5 \ub370\uc774\ud130\uc5d0\ub294 \uc5c6\ub294 \uc544\ub798 \ubb38\uc7a5\uc774 \uc788\ub2e4\uace0 \uc0dd\uac01\ud574\ubd05\uc2dc\ub2e4. \ubb3c\ub9ac\ud559\uc790\uac00 \ub9ac\ub9cc \uac00\uc124\uc744 \uc99d\uba85\ud588\ub2e4. ASCII \ucf54\ub4dc\ub098 \uc6d0\ud56b \uc778\ucf54\ub529 \uae30\ubc18 \uc5b8\uc5b4 \ubaa8\ub378\uc740 \uc704 \ubb38\uc7a5\uc744 \uc5b4\ub290\uc815\ub3c4 \ub2e4\ub8f0 \uc218 \uc788\uaca0\uc9c0\ub9cc, \uac1c\uc120\uc758 \uc5ec\uc9c0\uac00 \uc788\uc9c0 \uc54a\uc744\uae4c\uc694? \uba3c\uc800 \uc544\ub798\uc758 \ub450 \uc0ac\uc2e4\uc744 \uc0dd\uac01\ud574\ubd05\uc2dc\ub2e4. \u2018\uc218\ud559\uc790\u2019\uc640 \u2018\ubb3c\ub9ac\ud559\uc790\u2019\uac00 \ubb38\uc7a5 \ub0b4\uc5d0\uc11c \uac19\uc740 \uc5ed\ud560\uc744 \ub9e1\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub450 \ub2e8\uc5b4\ub294 \uc5b4\ub5bb\uac8c\ub4e0 \uc758\ubbf8\uc801\uc778 \uc5f0\uad00\uc131\uc774 \uc788\uc744 \uac81\ub2c8\ub2e4. \uc0c8\ub85c\uc6b4 \ubb38\uc7a5\uc5d0\uc11c \u2018\ubb3c\ub9ac\ud559\uc790\u2019\uac00 \ub9e1\uc740 \uc5ed\ud560\uc744 \u2018\uc218\ud559\uc790\u2019\uac00 \ub9e1\ub294 \uac83\uc744 \ud559\uc2b5 \ub370\uc774\ud130\uc5d0\uc11c \ubcf8 \uc801\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac \ubaa8\ub378\uc774 \uc704\uc758 \uc0ac\uc2e4\uc744 \ud1b5\ud574 \u2018\ubb3c\ub9ac\ud559\uc790\u2019\uac00 \uc0c8 \ubb38\uc7a5\uc5d0 \uc798 \ub4e4\uc5b4 \ub9de\ub294\ub2e4\ub294 \uac83\uc744 \ucd94\ub860\ud560 \uc218 \uc788\ub2e4\uba74 \ucc38 \uc88b\uc744 \uac83\uc785\ub2c8\ub2e4. \uc774\uac83\uc774 \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c \uc720\uc0ac\ub3c4\uc758 \uc758\ubbf8\uc785\ub2c8\ub2e4. \ucca0\uc790\uc801 \uc720\uc0ac\ub3c4 \ubfd0 \uc544\ub2c8\ub77c \uc758\ubbf8\uc801 \uc720\uc0ac\ub3c4 \uc778 \uac83\uc785\ub2c8\ub2e4. \uc774\uac83\uc774\uc57c\ub9d0\ub85c \uc5b8\uc5b4 \ub370\uc774\ud130\uc5d0 \ub0b4\uc7ac\ud558\ub294 \ud76c\ubc15\uc131(sparsity)\uc5d0 \ub300\ud55c \ucc98\ubc29\uc774 \ub420 \uac83\uc785\ub2c8\ub2e4. \uc6b0\ub9ac\uac00 \ubcf8 \uac83\uacfc \uc544\uc9c1 \ubcf4\uc9c0 \uc54a\uc740 \uac83 \uc0ac\uc774\ub97c \uc774\uc5b4\uc8fc\ub294 \uac83\uc774\uc8e0. \uc55e\uc73c\ub85c\ub294 \ub2e4\uc74c\uc758 \uc5b8\uc5b4\ud559\uc801 \uae30\ubcf8 \uba85\uc81c\ub97c \uac00\uc815\ud558\ub3c4\ub85d \ud569\uc2dc\ub2e4. \ubc14\ub85c \ube44\uc2b7\ud55c \ub9e5\ub77d\uc5d0\uc11c \ub4f1\uc7a5\ud558\ub294 \ub2e8\uc5b4\ub4e4\uc740 \uc11c\ub85c \uc758\ubbf8\uc801 \uc5f0\uad00\uc131\uc744 \uac00\uc9c4\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \uc5b8\uc5b4\ud559\uc801\uc73c\ub85c\ub294 \ubd84\uc0b0 \uc758\ubbf8 \uac00\uc124(distributional hypothesis) \uc774\ub77c\uace0\ub3c4 \ud569\ub2c8\ub2e4. \ubc00\uc9d1\ub41c \ub2e8\uc5b4 \uc784\ubca0\ub529 \uad6c\ud558\uae30# \uc5b4\ub5bb\uac8c \ub2e8\uc5b4\uc758 \uc758\ubbf8\uc801 \uc720\uc0ac\ub3c4\ub97c \uc778\ucf54\ub529 \ud560 \uc218 \uc788\uc744\uae4c\uc694? \ub2e4\uc2dc \ub9d0\ud574, \uc5b4\ub5bb\uac8c \ud574\uc57c \ub2e8\uc5b4\uc758 \uc720\uc0ac\ub3c4\ub97c \ub2e8\uc5b4 \ubca1\ud130\uc5d0 \ubc18\uc601\ud560 \uc218 \uc788\uc744\uae4c\uc694? \ub2e8\uc5b4 \ub370\uc774\ud130\uc5d0 \uc758\ubbf8\uc801 \uc18d\uc131(attribute)\uc744 \ubd80\uc5ec\ud558\ub294 \uac74 \uc5b4\ub5a4\uac00\uc694? \uc608\ub97c \ub4e4\uc5b4 \u2018\uc218\ud559\uc790\u2019\uc640 \u2018\ubb3c\ub9ac\ud559\uc790\u2019\uac00 \ubaa8\ub450 \ub6f8 \uc218 \uc788\ub2e4\uba74, \ud574\ub2f9 \ub2e8\uc5b4\uc758 \u2018\ub6f8 \uc218 \uc788\uc74c\u2019 \uc18d\uc131\uc5d0 \ub192\uc740 \uc810\uc218\ub97c \uc8fc\ub294 \uac81\ub2c8\ub2e4. \uacc4\uc18d \ud574\ubd05\uc2dc\ub2e4. \ub2e4\ub978 \ub2e8\uc5b4\ub4e4\uc5d0 \ub300\ud574\uc11c\ub294 \uc5b4\ub5a0\ud55c \uc18d\uc131\uc744 \ub9cc\ub4e4 \uc218 \uc788\uc744\uc9c0 \uc0dd\uac01\ud574\ubd05\uc2dc\ub2e4. \ub9cc\uc57d \uac01 \uc18d\uc131\uc744 \ud558\ub098\uc758 \ucc28\uc6d0\uc774\ub77c\uace0 \ubcf8\ub2e4\uba74 \ud558\ub098\uc758 \ub2e8\uc5b4\uc5d0 \uc544\ub798\uc640 \uac19\uc740 \ubca1\ud130\ub97c \ubc30\uc815\ud560 \uc218 \uc788\uc744\uac81\ub2c8\ub2e4. \\[ q_\\text{\uc218\ud559\uc790} = \\left[ \\overbrace{2.3}^\\text{\ub6f8 \uc218 \uc788\uc74c}, \\overbrace{9.4}^\\text{\ucee4\ud53c\ub97c \uc88b\uc544\ud568}, \\overbrace{-5.5}^\\text{\ubb3c\ub9ac \uc804\uacf5\uc784}, \\dots \\right]\\] \\[ q_\\text{\ubb3c\ub9ac\ud559\uc790} = \\left[ \\overbrace{2.5}^\\text{\ub6f8 \uc218 \uc788\uc74c}, \\overbrace{9.1}^\\text{\ucee4\ud53c\ub97c \uc88b\uc544\ud568}, \\overbrace{6.4}^\\text{\ubb3c\ub9ac \uc804\uacf5\uc784}, \\dots \\right]\\] \uadf8\ub7ec\uba74 \uc544\ub798\uc640 \uac19\uc774 \ub450 \ub2e8\uc5b4 \uc0ac\uc774\uc758 \uc720\uc0ac\ub3c4\ub97c \uad6c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\u2018\uc720\uc0ac\ub3c4\u2019\ub77c\ub294 \ud568\uc218\ub97c \uc815\uc758\ud558\ub294 \uac81\ub2c8\ub2e4) \\[\\text{\uc720\uc0ac\ub3c4}(\\text{\ubb3c\ub9ac\ud559\uc790}, \\text{\uc218\ud559\uc790}) = q_\\text{\ubb3c\ub9ac\ud559\uc790} \\cdot q_\\text{\uc218\ud559\uc790} \\] \ubb3c\ub860 \ubcf4\ud1b5\uc740 \uc774\ub807\uac8c \ubca1\ud130\uc758 \uae38\uc774\ub85c \ub098\ub220\uc8fc\uc9c0\ub9cc\uc694. \\[ \\text{\uc720\uc0ac\ub3c4}(\\text{\ubb3c\ub9ac\ud559\uc790}, \\text{\uc218\ud559\uc790}) = \\frac{q_\\text{\ubb3c\ub9ac\ud559\uc790} \\cdot q_\\text{\uc218\ud559\uc790}} {\\| q_\\text{\ubb3c\ub9ac\ud559\uc790} \\| \\| q_\\text{\uc218\ud559\uc790} \\|} = \\cos (\\phi)\\] \\(\\phi\\) \ub294 \ub450 \ubca1\ud130 \uc0ac\uc774\uc758 \uac01\uc785\ub2c8\ub2e4. \uc774\ub7f0 \uc2dd\uc774\uba74 \uc815\ub9d0 \ube44\uc2b7\ud55c \ub2e8\uc5b4\ub294 \uc720\uc0ac\ub3c4 1\uc744 \uac16\uace0, \uc815\ub9d0 \ub2e4\ub978 \ub2e8\uc5b4\ub294 \uc720\uc0ac\ub3c4 -1\uc744 \uac16\uaca0\uc8e0. \ube44\uc2b7\ud55c \uc758\ubbf8\ub97c \uac00\uc9c8\uc218\ub85d \uac19\uc740 \ubc29\ud5a5\uc744 \uac00\ub9ac\ud0a4\uace0 \uc788\uc744 \ud14c\ub2c8\uae4c\uc694. \uc774 \uae00 \ucd08\ubc18\uc5d0 \ub098\uc628 \ud76c\ubc15\ud55c \uc6d0\ud56b \ubca1\ud130\uac00 \uc0ac\uc2e4\uc740 \uc6b0\ub9ac\uac00 \ubc29\uae08 \uc815\uc758\ud55c \uc758\ubbf8 \ubca1\ud130\uc758 \ud2b9\uc774 \ucf00\uc774\uc2a4\ub77c\ub294 \uac83\uc744 \uae08\ubc29 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e8\uc5b4 \ubca1\ud130\uc758 \uac01 \uc6d0\uc18c\ub294 \uadf8 \ub2e8\uc5b4\uc758 \uc758\ubbf8\uc801 \uc18d\uc131\uc744 \ud45c\ud604\ud558\uace0, \ubaa8\ub4e0 \ub2e8\uc5b4 \uc30d\uc758 \uc720\uc0ac\ub3c4\ub294 0\uc774\uae30 \ub54c\ubb38\uc774\uc8e0. \uc704\uc5d0\uc11c \uc815\uc758\ud55c \uc758\ubbf8 \ubca1\ud130\ub294 \ubc00\uc9d1 \ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc989, \uc6d0\ud56b \ubca1\ud130\uc5d0 \ube44\ud574 0 \uc6d0\uc18c\uc758 \uc218\uac00 \uc801\ub2e4\uace0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc774 \ubca1\ud130\ub4e4\uc740 \uad6c\ud558\uae30\uac00 \uc9c4\uc9dc \uc5b4\ub835\uc2b5\ub2c8\ub2e4. \ub2e8\uc5b4\uac04\uc758 \uc720\uc0ac\ub3c4\ub97c \uacb0\uc815 \uc9c0\uc744 \ub9cc\ud55c \uc758\ubbf8\uc801 \uc18d\uc131\uc740 \uc5b4\ub5bb\uac8c \uacb0\uc815\ud560 \uac83\uc774\uba70, \uc18d\uc131\uc744 \uacb0\uc815\ud588\ub2e4\uace0 \ud558\ub354\ub77c\ub3c4 \uac01 \uc18d\uc131\uc5d0 \ud574\ub2f9\ud558\ub294 \uac12\uc740 \ub3c4\ub300\uccb4 \uc5b4\ub5a0\ud55c \uae30\uc900\uc73c\ub85c \uc815\ud574\uc57c \ud560\uae4c\uc694? \uc18d\uc131\uacfc \uac12\uc744 \ub370\uc774\ud130\uc5d0 \uae30\ubc18\ud574 \ub9cc\ub4e4\uace0 \uc790\ub3d9\uc73c\ub85c \ub2e8\uc5b4 \ubca1\ud130\ub97c \ub9cc\ub4e4 \uc218\ub294 \uc5c6\uc744\uae4c\uc694? \uc788\uc2b5\ub2c8\ub2e4. \ub525\ub7ec\ub2dd\uc744 \uc0ac\uc6a9\ud558\uba74 \ub9d0\uc774\uc8e0. \ub525\ub7ec\ub2dd\uc740 \uc778\uacf5\uc2e0\uacbd\ub9dd\uc744 \uc774\uc6a9\ud558\uc5ec \uc0ac\ub78c\uc758 \uac1c\uc785 \uc5c6\uc774 \uc18d\uc131\uc758 \ud45c\ud604 \ubc29\ubc95\uc744 \uc790\ub3d9\uc73c\ub85c \ud559\uc2b5\ud569\ub2c8\ub2e4. \uc774\ub97c \uc774\uc6a9\ud574 \ub2e8\uc5b4 \ubca1\ud130\ub97c \ubaa8\ub378 \ubaa8\uc218\ub85c \uc124\uc815\ud558\uace0 \ubaa8\ub378 \ud559\uc2b5\uc2dc\uc5d0 \ub2e8\uc5b4 \ubca1\ud130\ub3c4 \ud568\uaed8 \uc5c5\ub370\uc774\ud2b8 \ud558\uba74 \ub420 \uac83\uc785\ub2c8\ub2e4. \uc774\ub807\uac8c \uc6b0\ub9ac \uc2e0\uacbd\ub9dd \ubaa8\ub378\uc740 \uc801\uc5b4\ub3c4 \uc774\ub860\uc0c1\uc73c\ub85c\ub294 \ucda9\ubd84\ud788 \ud559\uc2b5\ud560 \uc218 \uc788\ub294 \uc7a0\uc7ac \uc758\ubbf8 \uc18d\uc131 \uc744 \ucc3e\uc744 \uac83\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c \ub9d0\ud558\ub294 \uc7a0\uc7ac \uc758\ubbf8 \uc18d\uc131\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ubca1\ud130\ub294 \uc0ac\ub78c\uc774 \ud574\uc11d\ud558\uae30 \uc0c1\ub2f9\ud788 \uc5b4\ub835\ub2e4\ub294 \uc810\uc744 \uae30\uc5b5\ud574 \ub450\uc138\uc694. \uc704\uc5d0\uc11c \uc218\ud559\uc790\uc640 \ubb3c\ub9ac\ud559\uc790\uc5d0\uac8c \ucee4\ud53c\ub97c \uc88b\uc544\ud55c\ub2e4\ub294 \ub4f1 \uc0ac\ub78c\uc774 \uc784\uc758\uc801\uc73c\ub85c \ub2e8\uc5b4\uc5d0 \ubd80\uc5ec\ud55c \uc18d\uc131\uacfc\ub294 \ub2ec\ub9ac, \uc778\uacf5\uc2e0\uacbd\ub9dd\uc774 \uc790\ub3d9\uc73c\ub85c \ub2e8\uc5b4\uc758 \uc18d\uc131\uc744 \ucc3e\ub294\ub2e4\uba74 \uadf8 \uc18d\uc131\uacfc \uac12\uc774 \uc758\ubbf8\ud558\ub294 \ubc14\ub97c \uc54c\uae30\uac00 \uc5b4\ub824\uc6b8 \uac83\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4\uc11c \uc2e0\uacbd\ub9dd \ubaa8\ub378\uc774 \ucc3e\uc740 \u2018\uc218\ud559\uc790\u2019\uc640 \u2018\ubb3c\ub9ac\ud559\uc790\u2019\uc758 \ud45c\ud604 \ubca1\ud130 \ub458 \ub2e4 \ub450\ubc88\uc9f8 \uc6d0\uc18c\uac00 \ud06c\ub2e4\uace0 \uac00\uc815\ud574 \ubd05\uc2dc\ub2e4. \ub458\uc774 \ube44\uc2b7\ud558\ub2e4\ub294 \uac74 \uc54c\uaca0\uc9c0\ub9cc, \ub3c4\ub300\uccb4 \ub450\ubc88\uc9f8 \uc6d0\uc18c\uac00 \ubb34\uc5c7\uc744 \uc758\ubbf8\ud558\ub294\uc9c0\ub294 \uc54c\uae30\uac00 \ub9e4\uc6b0 \ud798\ub4e0 \uac83\uc785\ub2c8\ub2e4. \ud45c\ud604 \ubca1\ud130 \uacf5\uac04\uc0c1\uc5d0\uc11c \ube44\uc2b7\ud558\ub2e4\ub294 \uc815\ubcf4 \uc678\uc5d0\ub294 \uc544\ub9c8 \ub9ce\uc740 \uc815\ubcf4\ub97c \uc8fc\uae34 \uc5b4\ub824\uc6b8 \uac83\uc785\ub2c8\ub2e4. \uc694\uc57d\ud558\uc790\uba74, \ub2e8\uc5b4 \uc784\ubca0\ub529\uc740 \ub2e8\uc5b4\uc758 *\uc758\ubbf8* \ub97c \ud45c\ud604\ud558\ub294 \ubc29\ubc95\uc774\uba70, \ucc28\ud6c4\uc5d0 \uc784\ubca0\ub529\uc744 \uc0ac\uc6a9\ud574\uc11c \ud480\uace0\uc790 \ud558\ub294 \ubb38\uc81c\uc5d0 \uc720\uc6a9\ud560 \uc758\ubbf8 \uc815\ubcf4\ub97c \ud6a8\uc728\uc801\uc73c\ub85c \uc778\ucf54\ub529\ud55c \uac83\uc785\ub2c8\ub2e4. \ud488\uc0ac \ud0dc\uadf8, \ud30c\uc2a4 \ud2b8\ub9ac(parse tree) \ub4f1 \ub2e8\uc5b4\uc758 \uc758\ubbf8 \uc678\uc5d0 \ub2e4\ub978 \uac83\ub3c4 \uc778\ucf54\ub529 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4! \ud53c\ucc98 \uc784\ubca0\ub529\uc758 \uac1c\ub150\uc744 \uc7a1\ub294 \uac83\uc774 \uc911\uc694\ud569\ub2c8\ub2e4. \ud30c\uc774\ud1a0\uce58\uc5d0\uc11c \ub2e8\uc5b4 \uc784\ubca0\ub529 \ud558\uae30# \uc2e4\uc81c\ub85c \ucf54\ub4dc\uc640 \uc608\uc2dc\ub97c \ubcf4\uae30 \uc804\uc5d0, \ud30c\uc774\ud1a0\uce58\ub97c \ube44\ub86f\ud574 \ub525\ub7ec\ub2dd \uad00\ub828 \ud504\ub85c\uadf8\ub798\ubc0d\uc744 \ud560 \ub54c \ub2e8\uc5b4 \uc784\ubca0\ub529\uc744 \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud558\ub294\uc9c0\uc5d0 \ub300\ud574 \uc870\uae08 \uc54c\uc544\ubd05\uc2dc\ub2e4. \ub9e8 \uc704\uc5d0\uc11c \uc6d0\ud56b \ubca1\ud130\ub97c \uc815\uc758\ud588\ub358 \uac83 \ucc98\ub7fc, \ub2e8\uc5b4 \uc784\ubca0\ub529\uc744 \uc0ac\uc6a9\ud560 \ub54c\uc5d0\ub3c4 \uac01 \ub2e8\uc5b4\uc5d0 \uc778\ub371\uc2a4\ub97c \ubd80\uc5ec\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774 \uc778\ub371\uc2a4\ub97c \ucc38\uc870 \ud14c\uc774\ube14(look-up table)\uc5d0\uc11c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \uc989, \\(|V| \\times D\\) \ud06c\uae30\uc758 \ud589\ub82c\uc5d0 \ub2e8\uc5b4 \uc784\ubca0\ub529\uc744 \uc800\uc7a5\ud558\ub294\ub370, \\(D\\) \ucc28\uc6d0\uc758 \uc784\ubca0\ub529 \ubca1\ud130\uac00 \ud589\ub82c\uc758 \\(i\\) \ubc88\uc9f8 \ud589\uc5d0 \uc800\uc7a5\ub418\uc5b4\uc788\uc5b4 \\(i\\) \ub97c \uc778\ub371\uc2a4\ub85c \ud65c\uc6a9\ud574 \uc784\ubca0\ub529 \ubca1\ud130\ub97c \ucc38\uc870\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc544\ub798\uc758 \ubaa8\ub4e0 \ucf54\ub4dc\uc5d0\uc11c\ub294 \ub2e8\uc5b4\uc640 \uc778\ub371\uc2a4\ub97c \ub9e4\ud551\ud574\uc8fc\ub294 \ub515\uc154\ub108\ub9ac\ub97c word_to_ix\ub77c \uce6d\ud569\ub2c8\ub2e4. \ud30c\uc774\ud1a0\uce58\ub294 \uc784\ubca0\ub529\uc744 \uc190\uc27d\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\uac8c torch.nn.Embedding\uc5d0 \uc704\uc5d0\uc11c \uc124\uba85\ud55c \ucc38\uc870 \ud14c\uc774\ube14 \uae30\ub2a5\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \uc774 \ubaa8\ub4c8\uc740 \ub2e8\uc5b4\uc758 \uac1c\uc218\uc640 \uc784\ubca0\ub529\uc758 \ucc28\uc6d0, \ucd1d 2\uac1c\uc758 \ubcc0\uc218\ub97c \uc785\ub825 \ubcc0\uc218\ub85c \ubc1b\uc2b5\ub2c8\ub2e4. torch.nn.Embedding \ud14c\uc774\ube14\uc758 \uc784\ubca0\ub529\uc744 \ucc38\uc870\ud558\uae30 \uc704\ud574\uc120 torch.LongTensor \ud0c0\uc785\uc758 \uc778\ub371\uc2a4 \ubcc0\uc218\ub97c \uaf2d \uc0ac\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4. (\uc778\ub371\uc2a4\ub294 \uc2e4\uc218\uac00 \uc544\ub2cc \uc815\uc218\uc774\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.) # Author: Robert Guthrie import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim torch.manual_seed(1) \u003ctorch._C.Generator object at 0x7f0e60159890\u003e word_to_ix = {\"hello\": 0, \"world\": 1} embeds = nn.Embedding(2, 5) # 2 words in vocab, 5 dimensional embeddings lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long) hello_embed = embeds(lookup_tensor) print(hello_embed) tensor([[ 0.6614, 0.2669, 0.0617, 0.6213, -0.4519]], grad_fn=\u003cEmbeddingBackward0\u003e) \uc608\uc2dc: N\uadf8\ub7a8 \uc5b8\uc5b4 \ubaa8\ub378\ub9c1# N\uadf8\ub7a8 \uc5b8\uc5b4 \ubaa8\ub378\ub9c1\uc5d0\uc120 \ub2e8\uc5b4 \uc2dc\ud000\uc2a4 \\(w\\) \uac00 \uc8fc\uc5b4\uc84c\uc744 \ub54c \uc544\ub798\uc758 \uac83\uc744 \uc5bb\uace0\uc790 \ud568\uc744 \uc0c1\uae30\ud574 \ubd05\uc2dc\ub2e4. \\[P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} ) \\] \\(w_i\\) \ub294 \uc2dc\ud000\uc2a4\uc5d0\uc11c i\ubc88\uc9f8 \ub2e8\uc5b4\uc785\ub2c8\ub2e4. \uc774 \uc608\uc2dc\uc5d0\uc11c\ub294 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \ubc14\ud0d5\uc73c\ub85c \uc190\uc2e4 \ud568\uc218\ub97c \uacc4\uc0b0\ud558\uace0 \uc5ed\uc804\ud30c\ub97c \ud1b5\ud574 \ubaa8\uc218\ub97c \uc5c5\ub370\uc774\ud2b8 \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. CONTEXT_SIZE = 2 EMBEDDING_DIM = 10 # \uc170\uc775\uc2a4\ud53c\uc5b4 \uc18c\ub124\ud2b8(Sonnet) 2\ub97c \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4. test_sentence = \"\"\"When forty winters shall besiege thy brow, And dig deep trenches in thy beauty\u0027s field, Thy youth\u0027s proud livery so gazed on now, Will be a totter\u0027d weed of small worth held: Then being asked, where all thy beauty lies, Where all the treasure of thy lusty days; To say, within thine own deep sunken eyes, Were an all-eating shame, and thriftless praise. How much more praise deserv\u0027d thy beauty\u0027s use, If thou couldst answer \u0027This fair child of mine Shall sum my count, and make my old excuse,\u0027 Proving his beauty by succession thine! This were to be new made when thou art old, And see thy blood warm when thou feel\u0027st it cold.\"\"\".split() # \uc6d0\ub798\ub294 \uc785\ub825\uc744 \uc81c\ub300\ub85c \ud1a0\ud070\ud654(tokenize) \ud574\uc57c\ud558\uc9c0\ub9cc \uc774\ubc88\uc5d4 \uac04\uc18c\ud654\ud558\uc5ec \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4. # \ud29c\ud50c\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ub9ac\uc2a4\ud2b8\ub97c \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4. \uac01 \ud29c\ud50c\uc740 ([ i-CONTEXT_SIZE \ubc88\uc9f8 \ub2e8\uc5b4, ..., i-1 \ubc88\uc9f8 \ub2e8\uc5b4 ], \ubaa9\ud45c \ub2e8\uc5b4)\uc785\ub2c8\ub2e4. ngrams = [ ( [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)], test_sentence[i] ) for i in range(CONTEXT_SIZE, len(test_sentence)) ] # \uccab 3\uac1c\uc758 \ud29c\ud50c\uc744 \ucd9c\ub825\ud558\uc5ec \ub370\uc774\ud130\uac00 \uc5b4\ub5bb\uac8c \uc0dd\uacbc\ub294\uc9c0 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. print(ngrams[:3]) vocab = set(test_sentence) word_to_ix = {word: i for i, word in enumerate(vocab)} class NGramLanguageModeler(nn.Module): def __init__(self, vocab_size, embedding_dim, context_size): super(NGramLanguageModeler, self).__init__() self.embeddings = nn.Embedding(vocab_size, embedding_dim) self.linear1 = nn.Linear(context_size * embedding_dim, 128) self.linear2 = nn.Linear(128, vocab_size) def forward(self, inputs): embeds = self.embeddings(inputs).view((1, -1)) out = F.relu(self.linear1(embeds)) out = self.linear2(out) log_probs = F.log_softmax(out, dim=1) return log_probs losses = [] loss_function = nn.NLLLoss() model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE) optimizer = optim.SGD(model.parameters(), lr=0.001) for epoch in range(10): total_loss = 0 for context, target in ngrams: # \uccab\ubc88\uc9f8. \ubaa8\ub378\uc5d0 \ub123\uc5b4\uc904 \uc785\ub825\uac12\uc744 \uc900\ube44\ud569\ub2c8\ub2e4. (i.e, \ub2e8\uc5b4\ub97c \uc815\uc218 \uc778\ub371\uc2a4\ub85c # \ubc14\uafb8\uace0 \ud30c\uc774\ud1a0\uce58 \ud150\uc11c\ub85c \uac10\uc2f8\uc90d\uc2dc\ub2e4.) context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long) # \ub450\ubc88\uc9f8. \ud1a0\uce58\ub294 \uae30\uc6b8\uae30\uac00 *\ub204\uc801* \ub429\ub2c8\ub2e4. \uc0c8 \uc778\uc2a4\ud134\uc2a4\ub97c \ub123\uc5b4\uc8fc\uae30 \uc804\uc5d0 # \uae30\uc6b8\uae30\ub97c \ucd08\uae30\ud654\ud569\ub2c8\ub2e4. model.zero_grad() # \uc138\ubc88\uc9f8. \uc21c\uc804\ud30c\ub97c \ud1b5\ud574 \ub2e4\uc74c\uc5d0 \uc62c \ub2e8\uc5b4\uc5d0 \ub300\ud55c \ub85c\uadf8 \ud655\ub960\uc744 \uad6c\ud569\ub2c8\ub2e4. log_probs = model(context_idxs) # \ub124\ubc88\uc9f8. \uc190\uc2e4\ud568\uc218\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. (\ud30c\uc774\ud1a0\uce58\uc5d0\uc11c\ub294 \ubaa9\ud45c \ub2e8\uc5b4\ub97c \ud150\uc11c\ub85c \uac10\uc2f8\uc918\uc57c \ud569\ub2c8\ub2e4.) loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long)) # \ub2e4\uc12f\ubc88\uc9f8. \uc5ed\uc804\ud30c\ub97c \ud1b5\ud574 \uae30\uc6b8\uae30\ub97c \uc5c5\ub370\uc774\ud2b8 \ud574\uc90d\ub2c8\ub2e4. loss.backward() optimizer.step() # tensor.item()\uc744 \ud638\ucd9c\ud558\uc5ec \ub2e8\uc77c\uc6d0\uc18c \ud150\uc11c\uc5d0\uc11c \uc22b\uc790\ub97c \ubc18\ud658\ubc1b\uc2b5\ub2c8\ub2e4. total_loss += loss.item() losses.append(total_loss) print(losses) # \ubc18\ubcf5\ud560 \ub54c\ub9c8\ub2e4 \uc190\uc2e4\uc774 \uc904\uc5b4\ub4dc\ub294 \uac83\uc744 \ubd05\uc2dc\ub2e4! # \"beauty\"\uc640 \uac19\uc774 \ud2b9\uc815 \ub2e8\uc5b4\uc5d0 \ub300\ud55c \uc784\ubca0\ub529\uc744 \ud655\uc778\ud558\ub824\uba74, print(model.embeddings.weight[word_to_ix[\"beauty\"]]) [([\u0027forty\u0027, \u0027When\u0027], \u0027winters\u0027), ([\u0027winters\u0027, \u0027forty\u0027], \u0027shall\u0027), ([\u0027shall\u0027, \u0027winters\u0027], \u0027besiege\u0027)] [516.8885982036591, 514.2938964366913, 511.71493124961853, 509.1507501602173, 506.60002613067627, 504.0616383552551, 501.53501987457275, 499.0196087360382, 496.51370453834534, 494.0152516365051] tensor([-0.5678, 0.7198, 0.1921, 0.2479, -0.3110, 0.0452, -1.5457, -0.3738, 0.7146, 1.9276], grad_fn=\u003cSelectBackward0\u003e) \uc608\uc2dc: \ub2e8\uc5b4 \uc784\ubca0\ub529 \uacc4\uc0b0\ud558\uae30: Continuous Bag-of-Words# The Continuous Bag-of-Words (CBOW) \ubaa8\ub378\uc740 NLP \ub525\ub7ec\ub2dd\uc5d0\uc11c \ub9ce\uc774 \uc4f0\uc785\ub2c8\ub2e4. \uc774 \ubaa8\ub378\uc740 \ubb38\uc7a5 \ub0b4\uc5d0\uc11c \uc8fc\ubcc0 \ub2e8\uc5b4, \uc989 \uc55e \uba87 \ub2e8\uc5b4\uc640 \ub4a4 \uba87 \ub2e8\uc5b4\ub97c \ubcf4\uace0 \ud2b9\uc815 \ub2e8\uc5b4\ub97c \uc608\uce21\ud558\ub294\ub370, \uc5b8\uc5b4 \ubaa8\ub378\ub9c1\uacfc\ub294 \ub2ec\ub9ac \uc21c\ucc28\uc801\uc774\uc9c0\ub3c4 \uc54a\uace0 \ud655\ub960\uc801\uc774\uc9c0\ub3c4 \uc54a\uc2b5\ub2c8\ub2e4. \uc8fc\ub85c CBOW\ub294 \ubcf5\uc7a1\ud55c \ubaa8\ub378\uc758 \ucd08\uae30 \uc785\ub825\uac12\uc73c\ub85c \uc4f0\uc77c \ub2e8\uc5b4 \uc784\ubca0\ub529\uc744 \ube60\ub974\uac8c \ud559\uc2b5\ud558\ub294 \ub370\uc5d0 \uc4f0\uc785\ub2c8\ub2e4. \uc774\uac83\uc744 \uc0ac\uc804 \ud6c8\ub828\ub41c(pre-trained) \uc784\ubca0\ub529 \uc774\ub77c\uace0 \ubd80\ub974\uc8e0. \uba87 \ud37c\uc13c\ud2b8 \uc815\ub3c4\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uae30\ub300\ud560 \uc218 \uc788\ub294 \uae30\ubc95\uc785\ub2c8\ub2e4. CBOW \ubaa8\ub378\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. \ubaa9\ud45c \ub2e8\uc5b4 \\(w_i\\) \uc640 \uadf8 \uc591\ucabd\uc5d0 \\(N\\) \uac1c\uc758 \ubb38\ub9e5 \ub2e8\uc5b4 \\(w_{i-1}, \\dots, w_{i-N}\\) \uc640 \\(w_{i+1}, \\dots, w_{i+N}\\) \uac00 \uc8fc\uc5b4\uc84c\uc744 \ub54c, (\ubb38\ub9e5 \ub2e8\uc5b4\ub97c \ucd1d\uce6d\ud574 \\(C\\) \ub77c\uace0 \ud569\uc2dc\ub2e4.) \\[-\\log p(w_i | C) = -\\log \\text{Softmax}\\left(A(\\sum_{w \\in C} q_w) + b\\right) \\] \uc704 \uc2dd\uc744 \ucd5c\uc18c\ud654\ud558\ub294 \uac83\uc774 CBOW\uc758 \ubaa9\uc801\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c \\(q_w\\) \ub294 \ub2e8\uc5b4 \\(w\\) \uc758 \uc784\ubca0\ub529 \uc785\ub2c8\ub2e4. \uc544\ub798\uc758 \ud074\ub798\uc2a4 \ud15c\ud50c\ub9bf\uc744 \ubcf4\uace0 \ud30c\uc774\ud1a0\uce58\ub85c CBOW\ub97c \uad6c\ud604\ud574 \ubcf4\uc138\uc694. \ud78c\ud2b8\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. \uc5b4\ub5a4 \ubaa8\uc218\ub97c \uc815\uc758\ud574\uc57c \ud558\ub294\uc9c0 \uc0dd\uac01\ud574\ubcf4\uc138\uc694. \uac01 \uc791\uc5c5\uc5d0\uc11c \ub2e4\ub8e8\uc5b4\uc9c0\ub294 \ubcc0\uc218\uc758 \ucc28\uc6d0\uc774 \uc5b4\ub5a4\uc9c0 \uaf2d \uc0dd\uac01\ud574\ubcf4\uc138\uc694. \ud150\uc11c\uc758 \ubaa8\uc591\uc744 \ubc14\uafd4\uc57c \ud55c\ub2e4\uba74 .view()\ub97c \uc0ac\uc6a9\ud558\uc138\uc694. CONTEXT_SIZE = 2 # \uc67c\ucabd\uc73c\ub85c 2\ub2e8\uc5b4, \uc624\ub978\ucabd\uc73c\ub85c 2\ub2e8\uc5b4 raw_text = \"\"\"We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\"\"\".split() # \uc911\ubcf5\ub41c \ub2e8\uc5b4\ub97c \uc81c\uac70\ud558\uae30 \uc704\ud574 `raw_text` \ub97c \uc9d1\ud569(set) \uc790\ub8cc\ud615\uc73c\ub85c \ubc14\uafd4\uc90d\ub2c8\ub2e4. vocab = set(raw_text) vocab_size = len(vocab) word_to_ix = {word: i for i, word in enumerate(vocab)} data = [] for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE): context = ( [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)] + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)] ) target = raw_text[i] data.append((context, target)) print(data[:5]) class CBOW(nn.Module): def __init__(self): pass def forward(self, inputs): pass # \ubaa8\ub378\uc744 \ub9cc\ub4e4\uace0 \ud559\uc2b5\ud574 \ubcf4\uc138\uc694. # \uc544\ub798\ub294 \ub370\uc774\ud130 \uc900\ube44\ub97c \uc6d0\ud65c\ud558\uac8c \ub3d5\uae30 \uc704\ud55c \ud568\uc218\uc785\ub2c8\ub2e4. def make_context_vector(context, word_to_ix): idxs = [word_to_ix[w] for w in context] return torch.tensor(idxs, dtype=torch.long) make_context_vector(data[0][0], word_to_ix) # \uc608\uc2dc [([\u0027are\u0027, \u0027We\u0027, \u0027to\u0027, \u0027study\u0027], \u0027about\u0027), ([\u0027about\u0027, \u0027are\u0027, \u0027study\u0027, \u0027the\u0027], \u0027to\u0027), ([\u0027to\u0027, \u0027about\u0027, \u0027the\u0027, \u0027idea\u0027], \u0027study\u0027), ([\u0027study\u0027, \u0027to\u0027, \u0027idea\u0027, \u0027of\u0027], \u0027the\u0027), ([\u0027the\u0027, \u0027study\u0027, \u0027of\u0027, \u0027a\u0027], \u0027idea\u0027)] tensor([48, 0, 38, 28]) Total running time of the script: (0 minutes 4.404 seconds) Download Jupyter notebook: word_embeddings_tutorial.ipynb Download Python source code: word_embeddings_tutorial.py Download zipped: word_embeddings_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/beginner/nlp/word_embeddings_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>