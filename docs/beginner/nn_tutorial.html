
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="torch.nn 이 실제로 무엇인가요?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/beginner/nn_tutorial.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="저자: Jeremy Howard, fast.ai. Rachel Thomas, Francisco Ingham에 감사합니다. 번역: 남상호 이 튜토리얼을 스크립트가 아닌 노트북으로 실행하기를 권장합니다. 노트북 (.ipynb) 파일을 다운 받으시려면, 페이지 상단에 있는 링크를 클릭해 주세요. PyTorch는 여러분이 신경망(neural network)를 생성하고 학습시키는 것을 도와주기 위해서 torch.nn, torch.optim, Dataset, 그리고 DataLoader 와 같은 잘 디자인된 모듈과 클래스들을 제공합니다. ..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="저자: Jeremy Howard, fast.ai. Rachel Thomas, Francisco Ingham에 감사합니다. 번역: 남상호 이 튜토리얼을 스크립트가 아닌 노트북으로 실행하기를 권장합니다. 노트북 (.ipynb) 파일을 다운 받으시려면, 페이지 상단에 있는 링크를 클릭해 주세요. PyTorch는 여러분이 신경망(neural network)를 생성하고 학습시키는 것을 도와주기 위해서 torch.nn, torch.optim, Dataset, 그리고 DataLoader 와 같은 잘 디자인된 모듈과 클래스들을 제공합니다. ..." />
<meta property="og:ignore_canonical" content="true" />

    <title>torch.nn 이 실제로 무엇인가요? &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'beginner/nn_tutorial';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/beginner/nn_tutorial.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors" href="understanding_leaf_vs_nonleaf_tutorial.html" />
    <link rel="prev" title="PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)" href="examples_nn/dynamic_net.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="basics/intro.html">파이토치(PyTorch) 기본 익히기</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="introyt/introyt_index.html">PyTorch 소개 - YouTube 시리즈</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="introyt/introyt1_tutorial.html">PyTorch 소개</a></li>
<li class="toctree-l2"><a class="reference internal" href="introyt/tensors_deeper_tutorial.html">Pytorch Tensor 소개</a></li>
<li class="toctree-l2"><a class="reference internal" href="introyt/autogradyt_tutorial.html">The Fundamentals of Autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="introyt/modelsyt_tutorial.html">Building Models with PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="introyt/tensorboardyt_tutorial.html">PyTorch TensorBoard 지원</a></li>
<li class="toctree-l2"><a class="reference internal" href="introyt/trainingyt.html">Training with PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="introyt/captumyt.html">Model Understanding with Captum</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning PyTorch</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="deep_learning_60min_blitz.html">PyTorch로 딥러닝하기: 60분만에 끝장내기</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="blitz/tensor_tutorial.html">텐서(Tensor)</a></li>
<li class="toctree-l2"><a class="reference internal" href="blitz/autograd_tutorial.html"><code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code> 에 대한 간단한 소개</a></li>
<li class="toctree-l2"><a class="reference internal" href="blitz/neural_networks_tutorial.html">신경망(Neural Networks)</a></li>
<li class="toctree-l2"><a class="reference internal" href="blitz/cifar10_tutorial.html">분류기(Classifier) 학습하기</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pytorch_with_examples.html">예제로 배우는 파이토치(PyTorch)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="examples_tensor/polynomial_numpy.html">준비 운동: NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_tensor/polynomial_tensor.html">파이토치(PyTorch): 텐서(Tensor)</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_autograd/polynomial_autograd.html">PyTorch: 텐서(Tensor)와 autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_autograd/polynomial_custom_function.html">PyTorch: 새 autograd Function 정의하기</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_nn/polynomial_nn.html">PyTorch: nn</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_nn/polynomial_optim.html">PyTorch: optim</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_nn/polynomial_module.html">PyTorch: 사용자 정의 nn.Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_nn/dynamic_net.html">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#"><cite>torch.nn</cite> 이 <em>실제로</em> 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="understanding_leaf_vs_nonleaf_tutorial.html">Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/nlp_from_scratch_index.html">NLP from Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/tensorboard_tutorial.html">TensorBoard로 모델, 데이터, 학습 시각화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/pinmem_nonblock.html">A guide on good usage of <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> and <code class="docutils literal notranslate"><span class="pre">pin_memory()</span></code> in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/visualizing_gradients_tutorial.html">Visualizing Gradients</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../intro.html" class="nav-link">Intro</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><cite>torch....</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../intro.html">
        <meta itemprop="name" content="Intro">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="<cite>torch.nn</cite> 이 <em>실제로</em> 무엇인가요?">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">beginner/nn_tutorial</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-beginner-nn-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="torch-nn">
<span id="sphx-glr-beginner-nn-tutorial-py"></span><h1><cite>torch.nn</cite> 이 <em>실제로</em> 무엇인가요?<a class="headerlink" href="#torch-nn" title="Link to this heading">#</a></h1>
<p><strong>저자</strong>: Jeremy Howard, <a class="reference external" href="https://www.fast.ai">fast.ai</a>.  Rachel Thomas, Francisco Ingham에 감사합니다.</p>
<p><strong>번역</strong>: <a class="reference external" href="https://github.com/namdori61">남상호</a></p>
<p>이 튜토리얼을 스크립트가 아닌 노트북으로 실행하기를 권장합니다. 노트북 (<code class="docutils literal notranslate"><span class="pre">.ipynb</span></code>) 파일을 다운 받으시려면,
페이지 상단에 있는 링크를 클릭해 주세요.</p>
<p>PyTorch는 여러분이 신경망(neural network)를 생성하고 학습시키는 것을 도와주기 위해서
<a class="reference external" href="https://pytorch.org/docs/stable/nn.html">torch.nn</a> ,
<a class="reference external" href="https://pytorch.org/docs/stable/optim.html">torch.optim</a> ,
<a class="reference external" href="https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset">Dataset</a> ,
그리고 <a class="reference external" href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader">DataLoader</a>
와 같은 잘 디자인된 모듈과 클래스들을 제공합니다.
이들의 성능을 최대한 활용하고 여러분의 문제에 맞게 커스터마이즈하기 위해서,
정확히 이들이 어떤 작업을 수행하는지 이해할 필요가 있습니다.
이해를 증진하기 위해서, 우리는 먼저 이들 모델들로부터 아무 특징도 사용하지 않고
MNIST 데이터셋에 대해 기초적인 신경망을 학습시킬 것입니다;
우리는 처음에는 가장 기초적인 PyTorch 텐서(tensor) 기능만을 사용할 것입니다.
그러고 나서 우리는 점차적으로 <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code>, <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>, 또는
<code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 로부터 한 번에 하나씩 특징을 추가하면서, 정확히 각 부분이 어떤 일을 하는지 그리고
이것이 어떻게 코드를 더 간결하고 유연하게 만드는지 보여줄 것입니다.</p>
<p><strong>이 튜토리얼은 여러분이 이미 PyTorch를 설치하였고, 그리고 텐서 연산의 기초에 대해 익숙하다고 가정합니다.</strong>
(만약 여러분이 NumPy 배열(array) 연산에 익숙하다면, 여기에서 사용되는 PyTorch 텐서 연산도
거의 동일하다는 것을 알게 될 것입니다).</p>
<section id="mnist">
<h2>MNIST 데이터 준비<a class="headerlink" href="#mnist" title="Link to this heading">#</a></h2>
<p>우리는 손으로 쓴 숫자(0에서 9 사이)의 흑백 이미지로 구성된 클래식
<a class="reference external" href="https://yann.lecun.com/exdb/mnist/index.html">MNIST</a> 데이터셋을 사용할 것입니다.</p>
<p>우리는 경로 설정을 담당하는 (Python3 표준 라이브러리의 일부인)
<a class="reference external" href="https://docs.python.org/3/library/pathlib.html">pathlib</a> 을 사용할 것이고,
<a class="reference external" href="http://docs.python-requests.org/en/master/">requests</a> 를 이용하여
데이터셋을 다운로드 할 것입니다. 우리는 모듈을 사용할 때만 임포트(import) 할 것이므로,
여러분은 매 포인트마다 정확히 어떤 것이 사용되는지 확인할 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>

<span class="n">DATA_PATH</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
<span class="n">PATH</span> <span class="o">=</span> <span class="n">DATA_PATH</span> <span class="o">/</span> <span class="s2">&quot;mnist&quot;</span>

<span class="n">PATH</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">URL</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/pytorch/tutorials/raw/main/_static/&quot;</span>
<span class="n">FILENAME</span> <span class="o">=</span> <span class="s2">&quot;mnist.pkl.gz&quot;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">PATH</span> <span class="o">/</span> <span class="n">FILENAME</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">URL</span> <span class="o">+</span> <span class="n">FILENAME</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
        <span class="p">(</span><span class="n">PATH</span> <span class="o">/</span> <span class="n">FILENAME</span><span class="p">)</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;wb&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
<p>이 데이터셋은 NumPy 배열 포맷이고, 데이터를 직렬화하기 위한
python 전용 포맷 pickle을 이용하여 저장되어 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">gzip</span>

<span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="n">open</span><span class="p">((</span><span class="n">PATH</span> <span class="o">/</span> <span class="n">FILENAME</span><span class="p">)</span><span class="o">.</span><span class="n">as_posix</span><span class="p">(),</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="p">((</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">),</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;latin-1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>각 이미지는 28 x 28 형태이고, 784 (=28x28) 크기를 가진 하나의 행으로 저장되어 있습니다.
한 장을 살펴봅시다; 먼저 우리는 이 이미지를 2d로 재구성해야 합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="c1"># Colab이 아닌 경우에만 ``pyplot.show()``</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">google.colab</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_nn_tutorial_001.png" srcset="../_images/sphx_glr_nn_tutorial_001.png" alt="nn tutorial" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(50000, 784)
</pre></div>
</div>
<p>PyTorch는 NumPy 배열보다는 <code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code> 를 사용하므로, 데이터를 변환해야 합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">n</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])
torch.Size([50000, 784])
tensor(0) tensor(9)
</pre></div>
</div>
</section>
<section id="id4">
<h2>(<code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> 없이) 밑바닥부터 신경망 만들기<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<p>PyTorch 텐서 연산만으로 첫 모델을 만들어봅시다.
여러분이 신경망의 기초에 대해서 이미 익숙하다고 가정합니다.
(만약 익숙하지 않다면 <a class="reference external" href="https://course.fast.ai">course.fast.ai</a> 에서 학습할 수 있습니다).</p>
<p>PyTorch는 랜덤 또는 0으로만 이루어진 텐서를 생성하는 메소드를 제공하고,
우리는 간단한 선형 모델의 가중치(weights)와 절편(bias)을 생성하기 위해서 이것을 사용할 것입니다.
이들은 일반적인 텐서에 매우 특별한 한 가지가 추가된 것입니다: 우리는 PyTorch에게 이들이
기울기(gradient)가 필요하다고 알려줍니다.
이를 통해 PyTorch는 텐서에 행해지는 모든 연산을 기록하게 하고,
따라서 <em>자동적으로</em> 역전파(back-propagation) 동안에 기울기를 계산할 수 있습니다!</p>
<p>가중치에 대해서는 <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 를 초기화(initialization) <strong>다음에</strong> 설정합니다,
왜냐하면 우리는 해당 단계가 기울기에 포함되는 것을 원치 않기 때문입니다.
(PyTorch에서 <code class="docutils literal notranslate"><span class="pre">_</span></code> 다음에 오는 메소드 이름은 연산이 인플레이스(in-place)로 수행되는 것을 의미합니다.)</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference external" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Xavier initialisation</a>
기법을 이용하여 가중치를 초기화 합니다. (<code class="docutils literal notranslate"><span class="pre">1/sqrt(n)</span></code> 을 곱해서 초기화).</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">784</span><span class="p">)</span>
<span class="n">weights</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>PyTorch의 기울기를 자동으로 계산 해주는 기능 덕분에, Python 표준 함수
(또는 호출 가능한 객체)를 모델로 사용할 수 있습니다!
그러므로 간단한 선형 모델을 만들기 위해서 단순한 행렬 곱셈과 브로드캐스트(broadcast)
덧셈을 사용하여 보겠습니다. 또한, 우리는 활성화 함수(activation function)가 필요하므로,
<cite>log_softmax</cite> 를 구현하고 사용할 것입니다.
PyTorch에서 많은 사전 구현된 손실 함수(loss function), 활성화 함수들이 제공되지만,
일반적인 python을 사용하여 자신만의 함수를 쉽게 작성할 수 있음을 기억해 주세요.
PyTorch는 심지어 여러분의 함수를 위해서 빠른 가속기(accelerator) 또는 벡터화된 CPU 코드를 만들어줄 것입니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">model</span><span class="p">(</span><span class="n">xb</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">xb</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
<p>위에서, <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> 기호는 행렬 곱셈(matrix multiplication) 연산을 나타냅니다.
우리는 하나의 배치(batch) 데이터(이 경우에는 64개의 이미지들)에 대하여 함수를 호출할 것입니다.
이것은 하나의 <em>포워드 전달(forward pass)</em> 입니다. 이 단계에서 우리는 무작위(random) 가중치로
시작했기 때문에 우리의 예측이 무작위 예측보다 전혀 나은 점이 없을 것입니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># 배치 크기</span>

<span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">bs</span><span class="p">]</span>  <span class="c1"># x로부터 미니배치(mini-batch) 추출</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>  <span class="c1"># 예측</span>
<span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">preds</span><span class="o">.</span><span class="n">shape</span>
<span class="nb">print</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">preds</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([-2.4194, -2.6655, -2.7124, -2.0198, -2.1675, -2.3091, -1.8263, -2.6058,
        -2.6682, -2.0846], grad_fn=&lt;SelectBackward0&gt;) torch.Size([64, 10])
</pre></div>
</div>
<p>여러분이 보시듯이, <code class="docutils literal notranslate"><span class="pre">preds</span></code> 텐서(tensor)는 텐서 값 외에도, 또한
기울기 함수(gradient function)를 담고 있습니다.
우리는 나중에 이것을 역전파(backpropagation)를 위해 사용할 것입니다.
이제 손실함수(loss function)로 사용하기 위한 음의 로그 우도(negative log-likelihood)를
구현합시다. (다시 말하지만, 우리는 표준 Python을 사용할 수 있습니다.):</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">nll</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="nb">input</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">target</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">loss_func</span> <span class="o">=</span> <span class="n">nll</span>
</pre></div>
</div>
<p>우리의 무작위 모델에 대한 손실을 점검해 봄으로써 역전파 이후에 개선이 있는지 나중에
확인할 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">bs</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(2.3103, grad_fn=&lt;NegBackward0&gt;)
</pre></div>
</div>
<p>또한, 우리 모델의 정확도(accuracy)를 계산하기 위한 함수를 구현합시다.
매 예측마다, 만약 가장 큰 값의 인덱스가 목푯값(target value)과 동일하다면,
그 예측은 올바른 것입니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">accuracy</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">yb</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<p>우리의 무작위 모델의 정확도를 점검해 봅시다, 그럼으로써 손실이 개선됨에 따라서 정확도가 개선되는지
확인할 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(0.1562)
</pre></div>
</div>
<p>이제 우리는 훈련 루프(training loop)를 실행할 수 있습니다. 매 반복마다, 다음을 수행할 것입니다:</p>
<ul class="simple">
<li><p>데이터의 미니배치를 선택 (<code class="docutils literal notranslate"><span class="pre">bs</span></code> 크기)</p></li>
<li><p>모델을 이용하여 예측 수행</p></li>
<li><p>손실 계산</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> 를 이용하여 모델의 기울기 업데이트, 이 경우에는, <code class="docutils literal notranslate"><span class="pre">weights</span></code> 와 <code class="docutils literal notranslate"><span class="pre">bias</span></code>.</p></li>
</ul>
<p>이제 우리는 이 기울기들을 이용하여 가중치와 절편을 업데이트 합니다.
우리는 이것을 <code class="docutils literal notranslate"><span class="pre">torch.no_grad()</span></code> 컨텍스트 매니저(context manager) 내에서 실행합니다,
왜냐하면 이러한 실행이 다음 기울기의 계산에 기록되지 않기를 원하기 때문입니다.
PyTorch의 자동 기울기(Autograd)가 어떻게 연산을 기록하는지
<a class="reference external" href="https://pytorch.org/docs/stable/notes/autograd.html">여기</a>에서 더 알아볼 수 있습니다.</p>
<p>그러고 나서 기울기를 0으로 설정합니다, 그럼으로써 다음 루프(loop)에 준비하게 됩니다.
그렇지 않으면, 우리의 기울기들은 일어난 모든 연산의 누적 집계를 기록하게 되어버립니다.
(즉, <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> 가 이미 저장된 것을 대체하기보단, 기존 값에 기울기를 <em>더하게</em> 됩니다).</p>
<div class="admonition tip">
<p class="admonition-title">팁</p>
<p>여러분들은 PyTorch 코드에 대하여 표준 python 디버거(debugger)를 사용할 수 있으므로,
매 단계마다 다양한 변수 값을 점검할 수 있습니다.
아래에서 <code class="docutils literal notranslate"><span class="pre">set_trace()</span></code> 를 주석 해제하여 사용해 보세요.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.core.debugger</span><span class="w"> </span><span class="kn">import</span> <span class="n">set_trace</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># 학습률(learning rate)</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 훈련에 사용할 에폭(epoch) 수</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1">#         set_trace()</span>
        <span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span>
        <span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span> <span class="o">+</span> <span class="n">bs</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">weights</span> <span class="o">-=</span> <span class="n">weights</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
            <span class="n">bias</span> <span class="o">-=</span> <span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
            <span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>
</div>
<p>이제 다 됐습니다: 제일 간단한 신경망(neural network)의 모든 것을 밑바닥부터 생성하고
훈련하였습니다! (이번에는 은닉층(hidden layer)이 없기 때문에,
로지스틱 회귀(logistic regression)입니다).</p>
<p>이제 손실과 정확도를 이전 값들과 비교하면서 확인해봅시다.
우리는 손실은 감소하고, 정확도는 증가하기를 기대할 것이고, 그들은 아래와 같습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">),</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(0.0820, grad_fn=&lt;NegBackward0&gt;) tensor(1.)
</pre></div>
</div>
</section>
<section id="torch-nn-functional">
<h2><code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span></code> 사용하기<a class="headerlink" href="#torch-nn-functional" title="Link to this heading">#</a></h2>
<p>이제 코드를 리팩토링(refactoring) 하겠습니다. 그로써 이전과 동일하지만,
PyTorch의 <code class="docutils literal notranslate"><span class="pre">nn</span></code> 클래스의 장점을 활용하여 더 간결하고 유연하게 만들 것입니다.
지금부터 매 단계에서, 우리는 코드를 더 짧고, 이해하기 쉽고, 유연하게 만들어야 합니다.</p>
<p>처음이면서 우리의 코드를 짧게 만들기 가장 쉬운 단계는 직접 작성한 활성화, 손실 함수를
<code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span></code> 의 함수로 대체하는 것입니다
(관례에 따라, 일반적으로 <code class="docutils literal notranslate"><span class="pre">F</span></code> 네임스페이스(namespace)를 통해 임포트(import) 합니다).
이 모듈에는 <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> 라이브러리의 모든 함수가 포함되어 있습니다
(라이브러리의 다른 부분에는 클래스가 포함되어 있습니다.)
다양한 손실 및 활성화 함수뿐만 아니라, 풀링(pooling) 함수와 같이 신경망을 만드는데
편리한 몇 가지 함수도 여기에서 찾을 수 있습니다.
(컨볼루션(convolution) 연산, 선형(linear) 레이어, 등을 수행하는 함수도 있지만,
앞으로 보시겠지만 대개는 라이브러리의 다른 부분을 사용하여 더 잘 처리할 수 있습니다.)</p>
<p>만약 여러분들이 음의 로그 우도 손실과 로그 소프트맥스 (log softmax) 활성화 함수를 사용하는 경우,
PyTorch는 이 둘을 결합하는 단일 함수인 <code class="docutils literal notranslate"><span class="pre">F.cross_entropy</span></code> 를 제공합니다.
따라서 모델에서 활성화 함수를 제거할 수도 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="n">loss_func</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span>

<span class="k">def</span><span class="w"> </span><span class="nf">model</span><span class="p">(</span><span class="n">xb</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">xb</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span>
</pre></div>
</div>
<p>더 이상 <code class="docutils literal notranslate"><span class="pre">model</span></code> 함수에서 <code class="docutils literal notranslate"><span class="pre">log_softmax</span></code> 를 호출하지 않고 있습니다.
손실과 정확도과 이전과 동일한지 확인해 봅시다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">),</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(0.0820, grad_fn=&lt;NllLossBackward0&gt;) tensor(1.)
</pre></div>
</div>
</section>
<section id="nn-module">
<h2><code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 을 이용하여 리팩토링 하기<a class="headerlink" href="#nn-module" title="Link to this heading">#</a></h2>
<p>다음으로, 더 명확하고 간결한 훈련 루프를 위해 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 및 <code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code> 를 사용합니다.
우리는 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> (자체가 클래스이고 상태를 추적할 수 있는) 하위 클래스(subclass)를 만듭니다.
이 경우에는, 포워드(forward) 단계에 대한 가중치, 절편, 그리고 메소드(method) 등을 유지하는
클래스를 만들고자 합니다.
<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 은 우리가 사용할 몇 가지 속성(attribute)과 메소드를 (<code class="docutils literal notranslate"><span class="pre">.parameters()</span></code> 와
<code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code> 같은) 가지고 있습니다.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p><code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> (대문자 M) 은 PyTorch의 특정 개념이고, 우리는 이 클래스를
많이 사용할 것입니다. <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 를 Python의 코드를 임포트하기 위한 코드 파일인
<a class="reference external" href="https://docs.python.org/3/tutorial/modules.html">module</a> (소문자 <code class="docutils literal notranslate"><span class="pre">m</span></code>)
의 개념과 헷갈리지 말아주세요.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Mnist_Logistic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">784</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">xb</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</pre></div>
</div>
<p>함수를 사용하는 대신에 이제는 오브젝트(object) 를 사용하기 때문에,
먼저 모델을 인스턴스화(instantiate) 해야 합니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_Logistic</span><span class="p">()</span>
</pre></div>
</div>
<p>이제 우리는 이전과 동일한 방식으로 손실을 계산할 수 있습니다.
여기서 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 오브젝트들은 마치 함수처럼 사용됩니다 (즉, 이들은 <em>호출가능</em> 합니다),
그러나 배후에서 PyTorch는 우리의 <code class="docutils literal notranslate"><span class="pre">forward</span></code> 메소드를 자동으로 호출합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(2.5184, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
<p>이전에는 훈련 루프를 위해 이름 별로 각 매개변수(parameter)의 값을 업데이트하고 다음과 같이
각 매개 변수에 대한 기울기들을 개별적으로 수동으로 0으로 제거해야 했습니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">weights</span> <span class="o">-=</span> <span class="n">weights</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
    <span class="n">bias</span> <span class="o">-=</span> <span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
    <span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></div>
</div>
<p>이제 우리는 model.parameters() 및 model.zero_grad() (모두
<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 에 대해 PyTorch에 의해 정의됨)를 활용하여 이러한 단계를 더 간결하게
만들고, 특히 더 복잡한 모델에 대해서 일부 매개변수를 잊어 버리는 오류를 덜 발생시킬 수 있습니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">p</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
<p>이제 이것을 나중에 다시 실행할 수 있도록 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 함수로 작은 훈련 루프를 감쌀 것입니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span>
            <span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span> <span class="o">+</span> <span class="n">bs</span>
            <span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
            <span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                    <span class="n">p</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
                <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>손실이 줄어들었는지 다시 한 번 확인합시다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(0.0827, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
</section>
<section id="nn-linear">
<h2><code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> 를 사용하여 리팩토링 하기<a class="headerlink" href="#nn-linear" title="Link to this heading">#</a></h2>
<p>계속해서 코드를 리팩토링 합니다. <code class="docutils literal notranslate"><span class="pre">self.weights</span></code> 및 <code class="docutils literal notranslate"><span class="pre">self.bias</span></code> 를 수동으로 정의 및
초기화하고, <code class="docutils literal notranslate"><span class="pre">xb</span>&#160; <span class="pre">&#64;</span> <span class="pre">self.weights</span> <span class="pre">+</span> <span class="pre">self.bias</span></code> 를 계산하는 대신에,
위의 모든 것을 해줄 PyTorch 클래스인
<a class="reference external" href="https://pytorch.org/docs/stable/nn.html#linear-layers">nn.Linear</a> 를 선형
레이어로 사용합니다.
PyTorch 에는 다양한 유형의 코드를 크게 단순화 할 수 있는 미리 정의된 레이어가 있고 이는 또한
종종 기존 코드보다 속도를 빠르게 합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Mnist_Logistic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
</pre></div>
</div>
<p>이전과 같은 방식으로 모델을 인스턴스화하고 손실을 계산합니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_Logistic</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(2.3160, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
<p>우리는 여전히 이전과 동일한 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 메소드를 사용할 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">fit</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(0.0814, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
</section>
<section id="id7">
<h2><code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> 을 이용하여 리팩토링 하기<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<p>PyTorch에는 다양한 최적화(optimization) 알고리즘을 가진 패키지인 <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> 도 있습니다.
각 매개변수를 수동으로 업데이트 하는 대신, 옵티마이저(optimizer)의 <code class="docutils literal notranslate"><span class="pre">step</span></code> 메소드를 사용하여
업데이트를 진행할 수 있습니다.</p>
<p>이렇게 하면 이전에 수동으로 코딩한 최적화 단계를 대체할 수 있습니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">p</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
<p>대신에 이렇게 말이죠:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
<p>(<code class="docutils literal notranslate"><span class="pre">optim.zero_grad()</span></code> 는 기울기를 0으로 재설정 해줍니다. 다음 미니 배치에 대한
기울기를 계산하기 전에 호출해야 합니다.)</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">optim</span>
</pre></div>
</div>
<p>나중에 다시 사용할 수 있도록 모델과 옵티마이져를 만드는 작은 함수를 정의합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_Logistic</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span>
        <span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span> <span class="o">+</span> <span class="n">bs</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(2.3094, grad_fn=&lt;NllLossBackward0&gt;)
tensor(0.0807, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
</section>
<section id="id8">
<h2>Dataset 을 이용하여 리팩토링하기<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<p>PyTorch 에는 추상 Dataset 클래스가 있습니다. Dataset 은
<code class="docutils literal notranslate"><span class="pre">__len__</span></code> 함수 (Python의 표준 <code class="docutils literal notranslate"><span class="pre">len</span></code> 함수에 의해 호출됨) 및
<code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> 함수를 가진 어떤 것이라도 될 수 있으며, 이 함수들을 인덱싱(indexing)하기
위한 방법으로 사용합니다.
<a class="reference external" href="https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html">이 튜토리얼</a>
은 <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> 의 하위 클래스로써, 사용자 지정 <code class="docutils literal notranslate"><span class="pre">FacialLandmarkDataset</span></code> 클래스를 만드는
좋은 예를 제시합니다.</p>
<p>PyTorch 의 <a class="reference external" href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset">TensorDataset</a>
은 텐서를 감싸는(wrapping) Dataset 입니다.
길이와 인덱싱 방식을 정의함으로써 텐서의 첫 번째 차원을 따라 반복, 인덱싱 및 슬라이스(slice)하는 방법도 제공합니다.
이렇게하면 훈련 할 때 동일한 라인에서 독립(independent) 변수와 종속(dependent) 변수에 쉽게 액세스 할 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDataset</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">x_train</span></code> 및 <code class="docutils literal notranslate"><span class="pre">y_train</span></code> 모두 하나의 <code class="docutils literal notranslate"><span class="pre">TensorDataset</span></code> 에 합쳐질 수 있습니다,
따라서 반복시키고 슬라이스 하기 편리합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>이전에는 <code class="docutils literal notranslate"><span class="pre">x</span></code> 및 <code class="docutils literal notranslate"><span class="pre">y</span></code> 값의 미니 배치를 별도로 반복해야 했습니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
<span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
</pre></div>
</div>
<p>이제 이 두 단계를 함께 수행할 수 있습니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="o">=</span> <span class="n">train_ds</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">bs</span> <span class="p">:</span> <span class="n">i</span><span class="o">*</span><span class="n">bs</span><span class="o">+</span><span class="n">bs</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">train_ds</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">bs</span><span class="p">:</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bs</span> <span class="o">+</span> <span class="n">bs</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(0.0811, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
</section>
<section id="id10">
<h2><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 를 사용하여 리팩토링하기<a class="headerlink" href="#id10" title="Link to this heading">#</a></h2>
<p>PyTorch 의 <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 는 배치 관리를 담당합니다.
여러분들은 모든 <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> 으로부터 <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 를 생성할 수 있습니다.
<code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 는 배치들에 대해서 반복하기 쉽게 만들어줍니다.
<code class="docutils literal notranslate"><span class="pre">train_ds[i*bs</span> <span class="pre">:</span> <span class="pre">i*bs+bs]</span></code> 를 사용하는 대신,
<code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 는 매 미니배치를 자동적으로 제공합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">)</span>
</pre></div>
</div>
<p>이전에는 루프가 다음과 같이 배치 <code class="docutils literal notranslate"><span class="pre">(xb,</span> <span class="pre">yb)</span></code> 를 반복했습니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="o">=</span> <span class="n">train_ds</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">bs</span> <span class="p">:</span> <span class="n">i</span><span class="o">*</span><span class="n">bs</span><span class="o">+</span><span class="n">bs</span><span class="p">]</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
</pre></div>
</div>
<p>이제 (xb, yb)가 DataLoader 에서 자동으로 로드되므로 루프가 훨씬 깨끗해졌습니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor(0.0807, grad_fn=&lt;NllLossBackward0&gt;)
</pre></div>
</div>
<p>PyTorch의 nn.Module, nn.Parameter, Dataset 및 DataLoader 덕분에 이제 훈련 루프가
훨씬 더 작아지고 이해하기 쉬워졌습니다.
이제 실제로 효과적인 모델을 만드는 데 필요한 기본 기능을 추가해 보겠습니다.</p>
</section>
<section id="validation">
<h2>검증(validation) 추가하기<a class="headerlink" href="#validation" title="Link to this heading">#</a></h2>
<p>섹션 1에서, 우리는 훈련 데이터에 사용하기 위해 합리적인 훈련 루프를 설정하려고했습니다.
실전에서, 여러분들은 과적합(overfitting)을 확인하기 위해서 <strong>항상</strong>
<a class="reference external" href="https://www.fast.ai/2017/11/13/validation-sets/">검증 데이터셋(validation set)</a> 이
있어야 합니다.</p>
<p>훈련 데이터를 섞는(shuffling) 것은 배치와 과적합 사이의 상관관계를 방지하기 위해
<a class="reference external" href="https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks">중요합니다.</a>
반면에, 검증 손실(validation loss)은 검증 데이터셋을 섞든 안섞든 동일합니다.
데이터를 섞는 것은 추가 시간이 걸리므로, 검증 데이터를 섞는 것은 의미가 없습니다.</p>
<p>검증 데이터셋에 대한 배치 크기는 학습 데이터셋 배치 크기의 2배를 사용할 것입니다.
이는 검증 데이터셋에 대해서는 역전파(backpropagation)가 필요하지 않으므로 메모리를
덜 사용하기 때문입니다 (기울기를 저장할 필요가 없음).
더 큰 배치 크기를 사용하여 손실을 더 빨리 계산하기 위해 이렇게 합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">valid_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>각 에폭이 끝날 때 검증 손실을 계산하고 프린트 할 것입니다.</p>
<p>(훈련 전에 항상 <code class="docutils literal notranslate"><span class="pre">model.train()</span></code> 을 호출하고, 추론(inference) 전에 <code class="docutils literal notranslate"><span class="pre">model.eval()</span></code>
을 호출합니다, 이는 <code class="docutils literal notranslate"><span class="pre">nn.BatchNorm2d</span></code> 및 <code class="docutils literal notranslate"><span class="pre">nn.Dropout</span></code> 과 같은 레이어에서
이러한 다른 단계(훈련, 추론) 에 대한 적절한 동작이 일어나게 하기 위함입니다.)</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">valid_loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span> <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">valid_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0 tensor(0.3160)
1 tensor(0.2796)
</pre></div>
</div>
</section>
<section id="fit-get-data">
<h2>fit() 와 get_data() 생성하기<a class="headerlink" href="#fit-get-data" title="Link to this heading">#</a></h2>
<p>이제 우리는 우리만의 작은 리팩토링을 수행할 것입니다.
훈련 데이터셋과 검증 데이터셋 모두에 대한 손실을 계산하는 유사한 프로세스를 두 번 거치므로,
이를 하나의 배치에 대한 손실을 계산하는 자체 함수 <code class="docutils literal notranslate"><span class="pre">loss_batch</span></code> 로 만들어보겠습니다.</p>
<p>훈련 데이터셋에 대한 옵티마이저를 전달하고 이를 사용하여 역전파를 수행합니다.
검증 데이터셋의 경우 옵티마이저를 전달하지 않으므로 메소드가 역전파를 수행하지 않습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">loss_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">opt</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">opt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="nb">len</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">fit</span></code> 은 모델을 훈련하고 각 에폭에 대한 훈련 및 검증 손실을 계산하는 작업을 수행합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
            <span class="n">loss_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">losses</span><span class="p">,</span> <span class="n">nums</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="o">*</span><span class="p">[</span><span class="n">loss_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span> <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">nums</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">nums</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">get_data</span></code> 는 학습 및 검증 데이터셋에 대한 dataloader 를 출력합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_data</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>이제 dataloader를 가져오고 모델을 훈련하는 전체 프로세스를 3 줄의 코드로 실행할 수 있습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">()</span>
<span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0 0.3630067733645439
1 0.2930187728047371
</pre></div>
</div>
<p>이러한 기본 3줄의 코드를 사용하여 다양한 모델을 훈련할 수 있습니다.
컨볼루션 신경망(CNN)을 훈련하는 데 사용할 수 있는지 살펴 보겠습니다!</p>
</section>
<section id="cnn">
<h2>CNN 으로 넘어가기<a class="headerlink" href="#cnn" title="Link to this heading">#</a></h2>
<p>이제 3개의 컨볼루션 레이어로 신경망을 구축할 것입니다.
이전 섹션의 어떤 함수도 모델의 형식에 대해 가정하지 않기 때문에,
별도의 수정없이 CNN을 학습하는 데 사용할 수 있습니다.</p>
<p>Pytorch의 사전정의된
<a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d">Conv2d</a> 클래스를
컨볼루션 레이어로 사용합니다. 3개의 컨볼루션 레이어로 CNN을 정의합니다.
각 컨볼루션 뒤에는 ReLU가 있습니다. 마지막으로 평균 풀링(average pooling)을 수행합니다.
(<code class="docutils literal notranslate"><span class="pre">view</span></code> 는 PyTorch의 NumPy <code class="docutils literal notranslate"><span class="pre">reshape</span></code> 버전입니다.)</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Mnist_CNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">):</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">xb</span><span class="p">))</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">xb</span><span class="p">))</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">xb</span><span class="p">))</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">xb</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">xb</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.1</span>
</pre></div>
</div>
<p><a class="reference external" href="https://cs231n.github.io/neural-networks-3/#sgd">모멘텀(Momentum)</a> 은
이전 업데이트도 고려하고 일반적으로 더 빠른 훈련으로 이어지는 확률적 경사하강법(stochastic gradient descent)
의 변형입니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Mnist_CNN</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0 0.34925633783340454
1 0.26948165959119796
</pre></div>
</div>
</section>
<section id="nn-sequential">
<h2><code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code> 사용하기<a class="headerlink" href="#nn-sequential" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> 에는 코드를 간단히 사용할 수 있는 또 다른 편리한 클래스인
<a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential">Sequential</a>
이 있습니다..
<code class="docutils literal notranslate"><span class="pre">Sequential</span></code> 객체는 그 안에 포함된 각 모듈을 순차적으로 실행합니다.
이것은 우리의 신경망을 작성하는 더 간단한 방법입니다.</p>
<p>이를 활용하려면 주어진 함수에서 <strong>사용자정의 레이어(custom layer)</strong> 를 쉽게
정의할 수 있어야 합니다.
예를 들어, PyTorch에는 <cite>view</cite> 레이어가 없으므로 우리의 신경망 용으로 만들어야 합니다.
<code class="docutils literal notranslate"><span class="pre">Lambda</span></code> 는 <code class="docutils literal notranslate"><span class="pre">Sequential</span></code> 로 신경망을 정의할 때 사용할 수 있는 레이어를 생성할 것입니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Lambda</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">func</span> <span class="o">=</span> <span class="n">func</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Sequential</span></code> 로 생성된 모들은 간단하게 아래와 같습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">Lambda</span><span class="p">(</span><span class="n">preprocess</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
<span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0 0.33789493412971494
1 0.2209822019934654
</pre></div>
</div>
</section>
<section id="id12">
<h2><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 감싸기<a class="headerlink" href="#id12" title="Link to this heading">#</a></h2>
<dl class="simple">
<dt>우리의 CNN은 상당히 간결하지만, MNIST에서만 작동합니다, 왜냐하면:</dt><dd><ul class="simple">
<li><p>입력이 28*28의 긴 벡터라고 가정합니다.</p></li>
<li><p>최종적으로 CNN 그리드 크기는 4*4 라고 가정합니다. (이것은 우리가 사용한 평균 풀링 커널 크기 때문입니다.)</p></li>
</ul>
</dd>
</dl>
<p>이 두 가지 가정을 제거하여 모델이 모든 2d 단일 채널(channel) 이미지에서 작동하도록 하겠습니다.
먼저 초기 Lambda 레이어를 제거하고 데이터 전처리를 제네레이터(generator)로 이동시킬 수 있습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">y</span>


<span class="k">class</span><span class="w"> </span><span class="nc">WrappedDataLoader</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dl</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dl</span> <span class="o">=</span> <span class="n">dl</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">func</span> <span class="o">=</span> <span class="n">func</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dl</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dl</span><span class="p">:</span>
            <span class="k">yield</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">b</span><span class="p">))</span>

<span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">WrappedDataLoader</span><span class="p">(</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">WrappedDataLoader</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
</pre></div>
</div>
<p>다음으로 <code class="docutils literal notranslate"><span class="pre">nn.AvgPool2d</span></code> 를 <code class="docutils literal notranslate"><span class="pre">nn.AdaptiveAvgPool2d</span></code> 로 대체하여 우리가 가진
<em>입력</em> 텐서가 아니라 원하는 <em>출력</em> 텐서의 크기를 정의할 수 있습니다.
결과적으로 우리 모델은 모든 크기의 입력과 함께 작동합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
<span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<p>한번 실행해 봅시다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0 0.37546271090507505
1 0.2893157567501068
</pre></div>
</div>
</section>
<section id="accelerator">
<h2><a class="reference external" href="https://pytorch.org/docs/stable/torch.html#accelerators">가속기(Accelerator)</a> 사용하기<a class="headerlink" href="#accelerator" title="Link to this heading">#</a></h2>
<p>만약 여러분들이 운이 좋아서 CUDA와 같은 가속기(accelerator)를 사용할 수 있다면(대부분의 클라우드 제공 업체에서
시간당 약 $0.50 에 이용할 수 있습니다), 코드 실행 속도를 높일 수 있습니다.
먼저 가속기(accelerator)가 PyTorch에서 작동하는지 확인합니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 현재 사용 가능한 가속기가 있다면 사용합니다. 아니라면 CPU를 사용합니다.</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">current_accelerator</span><span class="p">()</span><span class="o">.</span><span class="n">type</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2"> device&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Using cuda device
</pre></div>
</div>
<p>가속기로 배치를 옮기도록 <code class="docutils literal notranslate"><span class="pre">preprocess</span></code> 를 업데이트 합시다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">preprocess</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


<span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">WrappedDataLoader</span><span class="p">(</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">WrappedDataLoader</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
</pre></div>
</div>
<p>마지막으로 모델을 가속기로 이동시킬 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<p>이제 더 빨리 실행됩니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0 0.22098514339923858
1 0.19568672462701797
</pre></div>
</div>
</section>
<section id="id13">
<h2>마치면서<a class="headerlink" href="#id13" title="Link to this heading">#</a></h2>
<p>이제 PyTorch를 사용하여 다양한 유형의 모델을 학습하는 데 사용할 수 있는 일반 데이터 파이프 라인과
훈련 루프가 있습니다.
이제 모델 학습이 얼마나 간단한지 확인하려면 <a class="reference external" href="https://github.com/fastai/fastai_dev/blob/master/dev_nb/mnist_sample.ipynb">mnist_sample 노트북</a> 을 살펴보세요.</p>
<p>물론 데이터 증강(data augmentation), 초매개변수 조정(hyperparameter tuning),
훈련과정 모니터링(monitoring training), 전이 학습(transfer learning) 등과 같이
추가하고 싶은 항목들이 많이 있을 것입니다.
이러한 기능들은 이 튜토리얼에 표시된 것과 동일한 설계 접근 방식을 사용하여 개발된 fastai 라이브러리에서
사용할 수 있으며, 모델을 더욱 발전시키려는 실무자에게 자연스러운 다음 단계를 제공합니다.</p>
<p>이 튜토리얼의 시작 부분에서 <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code>, <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>,
그리고 <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 의 각 예제를 통해 설명하겠다고 이야기했었습니다.
이제 위의 내용들을 요약해보겠습니다:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Module</span></code>: 함수처럼 동작하지만, 또한 상태(state) (예를 들어, 신경망의 레이어 가중치)를
포함할 수 있는 호출 가능한 오브젝트를 생성합니다.
이는 포함된 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> (들)가 어떤 것인지 알고, 모든 기울기를 0으로 설정하고 가중치
업데이트 등을 위해 반복할 수 있습니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Parameter</span></code>: <code class="docutils literal notranslate"><span class="pre">Module</span></code> 에 역전파 동안 업데이트가 필요한 가중치가 있음을 알려주는
텐서용 래퍼입니다. <cite>requires_grad</cite> 속성이 설정된 텐서만 업데이트 됩니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">functional</span></code>: 활성화 함수, 손실 함수 등을 포함하는 모듈 (관례에 따라 일반적으로
<code class="docutils literal notranslate"><span class="pre">F</span></code> 네임스페이스로 임포트 됩니다) 이고, 물론 컨볼루션 및 선형 레이어 등에 대해서
상태를 저장하지않는(non-stateful) 버전의 레이어를 포함합니다.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.optim</span></code>: 역전파 단계에서 <code class="docutils literal notranslate"><span class="pre">Parameter</span></code> 의 가중치를 업데이트하는,
<code class="docutils literal notranslate"><span class="pre">SGD</span></code> 와 같은 옵티마이저를 포함합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Dataset</span></code>: <code class="docutils literal notranslate"><span class="pre">TensorDataset</span></code> 과 같이 PyTorch와 함께 제공되는 클래스를 포함하여 <code class="docutils literal notranslate"><span class="pre">__len__</span></code> 및
<code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> 이 있는 객체의 추상 인터페이스</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code>: 모든 종류의 <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> 을 기반으로 데이터의 배치들을 출력하는 반복자(iterator)를 생성합니다.</p></li>
</ul>
</div></blockquote>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (1 minutes 38.650 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-nn-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/d9398fce39ca80dc4bb8b8ea55b575a8/nn_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">nn_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/f16255c783f9e487235b8eff6c8792b9/nn_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">nn_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/15164824fff70e07a432631c2b8dbaae/nn_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">nn_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="examples_nn/dynamic_net.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</p>
      </div>
    </a>
    <a class="right-next"
       href="understanding_leaf_vs_nonleaf_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="examples_nn/dynamic_net.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</p>
      </div>
    </a>
    <a class="right-next"
       href="understanding_leaf_vs_nonleaf_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mnist">MNIST 데이터 준비</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">(<code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> 없이) 밑바닥부터 신경망 만들기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-nn-functional"><code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span></code> 사용하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-module"><code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 을 이용하여 리팩토링 하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-linear"><code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> 를 사용하여 리팩토링 하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7"><code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> 을 이용하여 리팩토링 하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Dataset 을 이용하여 리팩토링하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10"><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 를 사용하여 리팩토링하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#validation">검증(validation) 추가하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fit-get-data">fit() 와 get_data() 생성하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn">CNN 으로 넘어가기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nn-sequential"><code class="docutils literal notranslate"><span class="pre">nn.Sequential</span></code> 사용하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12"><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 감싸기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accelerator">가속기(Accelerator) 사용하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">마치면서</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "torch.nn \uc774 \uc2e4\uc81c\ub85c \ubb34\uc5c7\uc778\uac00\uc694?",
       "headline": "torch.nn \uc774 \uc2e4\uc81c\ub85c \ubb34\uc5c7\uc778\uac00\uc694?",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/beginner/nn_tutorial.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. torch.nn \uc774 \uc2e4\uc81c\ub85c \ubb34\uc5c7\uc778\uac00\uc694?# \uc800\uc790: Jeremy Howard, fast.ai. Rachel Thomas, Francisco Ingham\uc5d0 \uac10\uc0ac\ud569\ub2c8\ub2e4. \ubc88\uc5ed: \ub0a8\uc0c1\ud638 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc2a4\ud06c\ub9bd\ud2b8\uac00 \uc544\ub2cc \ub178\ud2b8\ubd81\uc73c\ub85c \uc2e4\ud589\ud558\uae30\ub97c \uad8c\uc7a5\ud569\ub2c8\ub2e4. \ub178\ud2b8\ubd81 (.ipynb) \ud30c\uc77c\uc744 \ub2e4\uc6b4 \ubc1b\uc73c\uc2dc\ub824\uba74, \ud398\uc774\uc9c0 \uc0c1\ub2e8\uc5d0 \uc788\ub294 \ub9c1\ud06c\ub97c \ud074\ub9ad\ud574 \uc8fc\uc138\uc694. PyTorch\ub294 \uc5ec\ub7ec\ubd84\uc774 \uc2e0\uacbd\ub9dd(neural network)\ub97c \uc0dd\uc131\ud558\uace0 \ud559\uc2b5\uc2dc\ud0a4\ub294 \uac83\uc744 \ub3c4\uc640\uc8fc\uae30 \uc704\ud574\uc11c torch.nn , torch.optim , Dataset , \uadf8\ub9ac\uace0 DataLoader \uc640 \uac19\uc740 \uc798 \ub514\uc790\uc778\ub41c \ubaa8\ub4c8\uacfc \ud074\ub798\uc2a4\ub4e4\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774\ub4e4\uc758 \uc131\ub2a5\uc744 \ucd5c\ub300\ud55c \ud65c\uc6a9\ud558\uace0 \uc5ec\ub7ec\ubd84\uc758 \ubb38\uc81c\uc5d0 \ub9de\uac8c \ucee4\uc2a4\ud130\ub9c8\uc774\uc988\ud558\uae30 \uc704\ud574\uc11c, \uc815\ud655\ud788 \uc774\ub4e4\uc774 \uc5b4\ub5a4 \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub294\uc9c0 \uc774\ud574\ud560 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ud574\ub97c \uc99d\uc9c4\ud558\uae30 \uc704\ud574\uc11c, \uc6b0\ub9ac\ub294 \uba3c\uc800 \uc774\ub4e4 \ubaa8\ub378\ub4e4\ub85c\ubd80\ud130 \uc544\ubb34 \ud2b9\uc9d5\ub3c4 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 MNIST \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574 \uae30\ucd08\uc801\uc778 \uc2e0\uacbd\ub9dd\uc744 \ud559\uc2b5\uc2dc\ud0ac \uac83\uc785\ub2c8\ub2e4; \uc6b0\ub9ac\ub294 \ucc98\uc74c\uc5d0\ub294 \uac00\uc7a5 \uae30\ucd08\uc801\uc778 PyTorch \ud150\uc11c(tensor) \uae30\ub2a5\ub9cc\uc744 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \uadf8\ub7ec\uace0 \ub098\uc11c \uc6b0\ub9ac\ub294 \uc810\ucc28\uc801\uc73c\ub85c torch.nn, torch.optim, Dataset, \ub610\ub294 DataLoader \ub85c\ubd80\ud130 \ud55c \ubc88\uc5d0 \ud558\ub098\uc529 \ud2b9\uc9d5\uc744 \ucd94\uac00\ud558\uba74\uc11c, \uc815\ud655\ud788 \uac01 \ubd80\ubd84\uc774 \uc5b4\ub5a4 \uc77c\uc744 \ud558\ub294\uc9c0 \uadf8\ub9ac\uace0 \uc774\uac83\uc774 \uc5b4\ub5bb\uac8c \ucf54\ub4dc\ub97c \ub354 \uac04\uacb0\ud558\uace0 \uc720\uc5f0\ud558\uac8c \ub9cc\ub4dc\ub294\uc9c0 \ubcf4\uc5ec\uc904 \uac83\uc785\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 \uc5ec\ub7ec\ubd84\uc774 \uc774\ubbf8 PyTorch\ub97c \uc124\uce58\ud558\uc600\uace0, \uadf8\ub9ac\uace0 \ud150\uc11c \uc5f0\uc0b0\uc758 \uae30\ucd08\uc5d0 \ub300\ud574 \uc775\uc219\ud558\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4. (\ub9cc\uc57d \uc5ec\ub7ec\ubd84\uc774 NumPy \ubc30\uc5f4(array) \uc5f0\uc0b0\uc5d0 \uc775\uc219\ud558\ub2e4\uba74, \uc5ec\uae30\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 PyTorch \ud150\uc11c \uc5f0\uc0b0\ub3c4 \uac70\uc758 \ub3d9\uc77c\ud558\ub2e4\ub294 \uac83\uc744 \uc54c\uac8c \ub420 \uac83\uc785\ub2c8\ub2e4). MNIST \ub370\uc774\ud130 \uc900\ube44# \uc6b0\ub9ac\ub294 \uc190\uc73c\ub85c \uc4f4 \uc22b\uc790(0\uc5d0\uc11c 9 \uc0ac\uc774)\uc758 \ud751\ubc31 \uc774\ubbf8\uc9c0\ub85c \uad6c\uc131\ub41c \ud074\ub798\uc2dd MNIST \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uacbd\ub85c \uc124\uc815\uc744 \ub2f4\ub2f9\ud558\ub294 (Python3 \ud45c\uc900 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \uc77c\ubd80\uc778) pathlib \uc744 \uc0ac\uc6a9\ud560 \uac83\uc774\uace0, requests \ub97c \uc774\uc6a9\ud558\uc5ec \ub370\uc774\ud130\uc14b\uc744 \ub2e4\uc6b4\ub85c\ub4dc \ud560 \uac83\uc785\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud560 \ub54c\ub9cc \uc784\ud3ec\ud2b8(import) \ud560 \uac83\uc774\ubbc0\ub85c, \uc5ec\ub7ec\ubd84\uc740 \ub9e4 \ud3ec\uc778\ud2b8\ub9c8\ub2e4 \uc815\ud655\ud788 \uc5b4\ub5a4 \uac83\uc774 \uc0ac\uc6a9\ub418\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. from pathlib import Path import requests DATA_PATH = Path(\"data\") PATH = DATA_PATH / \"mnist\" PATH.mkdir(parents=True, exist_ok=True) URL = \"https://github.com/pytorch/tutorials/raw/main/_static/\" FILENAME = \"mnist.pkl.gz\" if not (PATH / FILENAME).exists(): content = requests.get(URL + FILENAME).content (PATH / FILENAME).open(\"wb\").write(content) \uc774 \ub370\uc774\ud130\uc14b\uc740 NumPy \ubc30\uc5f4 \ud3ec\ub9f7\uc774\uace0, \ub370\uc774\ud130\ub97c \uc9c1\ub82c\ud654\ud558\uae30 \uc704\ud55c python \uc804\uc6a9 \ud3ec\ub9f7 pickle\uc744 \uc774\uc6a9\ud558\uc5ec \uc800\uc7a5\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. import pickle import gzip with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\") \uac01 \uc774\ubbf8\uc9c0\ub294 28 x 28 \ud615\ud0dc\uc774\uace0, 784 (=28x28) \ud06c\uae30\ub97c \uac00\uc9c4 \ud558\ub098\uc758 \ud589\uc73c\ub85c \uc800\uc7a5\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ud55c \uc7a5\uc744 \uc0b4\ud3b4\ubd05\uc2dc\ub2e4; \uba3c\uc800 \uc6b0\ub9ac\ub294 \uc774 \uc774\ubbf8\uc9c0\ub97c 2d\ub85c \uc7ac\uad6c\uc131\ud574\uc57c \ud569\ub2c8\ub2e4. from matplotlib import pyplot import numpy as np pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\") # Colab\uc774 \uc544\ub2cc \uacbd\uc6b0\uc5d0\ub9cc ``pyplot.show()`` try: import google.colab except ImportError: pyplot.show() print(x_train.shape) (50000, 784) PyTorch\ub294 NumPy \ubc30\uc5f4\ubcf4\ub2e4\ub294 torch.tensor \ub97c \uc0ac\uc6a9\ud558\ubbc0\ub85c, \ub370\uc774\ud130\ub97c \ubcc0\ud658\ud574\uc57c \ud569\ub2c8\ub2e4. import torch x_train, y_train, x_valid, y_valid = map( torch.tensor, (x_train, y_train, x_valid, y_valid) ) n, c = x_train.shape print(x_train, y_train) print(x_train.shape) print(y_train.min(), y_train.max()) tensor([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) tensor([5, 0, 4, ..., 8, 4, 8]) torch.Size([50000, 784]) tensor(0) tensor(9) (torch.nn \uc5c6\uc774) \ubc11\ubc14\ub2e5\ubd80\ud130 \uc2e0\uacbd\ub9dd \ub9cc\ub4e4\uae30# PyTorch \ud150\uc11c \uc5f0\uc0b0\ub9cc\uc73c\ub85c \uccab \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4. \uc5ec\ub7ec\ubd84\uc774 \uc2e0\uacbd\ub9dd\uc758 \uae30\ucd08\uc5d0 \ub300\ud574\uc11c \uc774\ubbf8 \uc775\uc219\ud558\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4. (\ub9cc\uc57d \uc775\uc219\ud558\uc9c0 \uc54a\ub2e4\uba74 course.fast.ai \uc5d0\uc11c \ud559\uc2b5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4). PyTorch\ub294 \ub79c\ub364 \ub610\ub294 0\uc73c\ub85c\ub9cc \uc774\ub8e8\uc5b4\uc9c4 \ud150\uc11c\ub97c \uc0dd\uc131\ud558\ub294 \uba54\uc18c\ub4dc\ub97c \uc81c\uacf5\ud558\uace0, \uc6b0\ub9ac\ub294 \uac04\ub2e8\ud55c \uc120\ud615 \ubaa8\ub378\uc758 \uac00\uc911\uce58(weights)\uc640 \uc808\ud3b8(bias)\uc744 \uc0dd\uc131\ud558\uae30 \uc704\ud574\uc11c \uc774\uac83\uc744 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \uc774\ub4e4\uc740 \uc77c\ubc18\uc801\uc778 \ud150\uc11c\uc5d0 \ub9e4\uc6b0 \ud2b9\ubcc4\ud55c \ud55c \uac00\uc9c0\uac00 \ucd94\uac00\ub41c \uac83\uc785\ub2c8\ub2e4: \uc6b0\ub9ac\ub294 PyTorch\uc5d0\uac8c \uc774\ub4e4\uc774 \uae30\uc6b8\uae30(gradient)\uac00 \ud544\uc694\ud558\ub2e4\uace0 \uc54c\ub824\uc90d\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 PyTorch\ub294 \ud150\uc11c\uc5d0 \ud589\ud574\uc9c0\ub294 \ubaa8\ub4e0 \uc5f0\uc0b0\uc744 \uae30\ub85d\ud558\uac8c \ud558\uace0, \ub530\ub77c\uc11c \uc790\ub3d9\uc801\uc73c\ub85c \uc5ed\uc804\ud30c(back-propagation) \ub3d9\uc548\uc5d0 \uae30\uc6b8\uae30\ub97c \uacc4\uc0b0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4! \uac00\uc911\uce58\uc5d0 \ub300\ud574\uc11c\ub294 requires_grad \ub97c \ucd08\uae30\ud654(initialization) \ub2e4\uc74c\uc5d0 \uc124\uc815\ud569\ub2c8\ub2e4, \uc65c\ub0d0\ud558\uba74 \uc6b0\ub9ac\ub294 \ud574\ub2f9 \ub2e8\uacc4\uac00 \uae30\uc6b8\uae30\uc5d0 \ud3ec\ud568\ub418\ub294 \uac83\uc744 \uc6d0\uce58 \uc54a\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. (PyTorch\uc5d0\uc11c _ \ub2e4\uc74c\uc5d0 \uc624\ub294 \uba54\uc18c\ub4dc \uc774\ub984\uc740 \uc5f0\uc0b0\uc774 \uc778\ud50c\ub808\uc774\uc2a4(in-place)\ub85c \uc218\ud589\ub418\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.) \ucc38\uace0 Xavier initialisation \uae30\ubc95\uc744 \uc774\uc6a9\ud558\uc5ec \uac00\uc911\uce58\ub97c \ucd08\uae30\ud654 \ud569\ub2c8\ub2e4. (1/sqrt(n) \uc744 \uacf1\ud574\uc11c \ucd08\uae30\ud654). import math weights = torch.randn(784, 10) / math.sqrt(784) weights.requires_grad_() bias = torch.zeros(10, requires_grad=True) PyTorch\uc758 \uae30\uc6b8\uae30\ub97c \uc790\ub3d9\uc73c\ub85c \uacc4\uc0b0 \ud574\uc8fc\ub294 \uae30\ub2a5 \ub355\ubd84\uc5d0, Python \ud45c\uc900 \ud568\uc218 (\ub610\ub294 \ud638\ucd9c \uac00\ub2a5\ud55c \uac1d\uccb4)\ub97c \ubaa8\ub378\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4! \uadf8\ub7ec\ubbc0\ub85c \uac04\ub2e8\ud55c \uc120\ud615 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uae30 \uc704\ud574\uc11c \ub2e8\uc21c\ud55c \ud589\ub82c \uacf1\uc148\uacfc \ube0c\ub85c\ub4dc\uce90\uc2a4\ud2b8(broadcast) \ub367\uc148\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uc6b0\ub9ac\ub294 \ud65c\uc131\ud654 \ud568\uc218(activation function)\uac00 \ud544\uc694\ud558\ubbc0\ub85c, log_softmax \ub97c \uad6c\ud604\ud558\uace0 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. PyTorch\uc5d0\uc11c \ub9ce\uc740 \uc0ac\uc804 \uad6c\ud604\ub41c \uc190\uc2e4 \ud568\uc218(loss function), \ud65c\uc131\ud654 \ud568\uc218\ub4e4\uc774 \uc81c\uacf5\ub418\uc9c0\ub9cc, \uc77c\ubc18\uc801\uc778 python\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc790\uc2e0\ub9cc\uc758 \ud568\uc218\ub97c \uc27d\uac8c \uc791\uc131\ud560 \uc218 \uc788\uc74c\uc744 \uae30\uc5b5\ud574 \uc8fc\uc138\uc694. PyTorch\ub294 \uc2ec\uc9c0\uc5b4 \uc5ec\ub7ec\ubd84\uc758 \ud568\uc218\ub97c \uc704\ud574\uc11c \ube60\ub978 \uac00\uc18d\uae30(accelerator) \ub610\ub294 \ubca1\ud130\ud654\ub41c CPU \ucf54\ub4dc\ub97c \ub9cc\ub4e4\uc5b4\uc904 \uac83\uc785\ub2c8\ub2e4. def log_softmax(x): return x - x.exp().sum(-1).log().unsqueeze(-1) def model(xb): return log_softmax(xb @ weights + bias) \uc704\uc5d0\uc11c, @ \uae30\ud638\ub294 \ud589\ub82c \uacf1\uc148(matrix multiplication) \uc5f0\uc0b0\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \ud558\ub098\uc758 \ubc30\uce58(batch) \ub370\uc774\ud130(\uc774 \uacbd\uc6b0\uc5d0\ub294 64\uac1c\uc758 \uc774\ubbf8\uc9c0\ub4e4)\uc5d0 \ub300\ud558\uc5ec \ud568\uc218\ub97c \ud638\ucd9c\ud560 \uac83\uc785\ub2c8\ub2e4. \uc774\uac83\uc740 \ud558\ub098\uc758 \ud3ec\uc6cc\ub4dc \uc804\ub2ec(forward pass) \uc785\ub2c8\ub2e4. \uc774 \ub2e8\uacc4\uc5d0\uc11c \uc6b0\ub9ac\ub294 \ubb34\uc791\uc704(random) \uac00\uc911\uce58\ub85c \uc2dc\uc791\ud588\uae30 \ub54c\ubb38\uc5d0 \uc6b0\ub9ac\uc758 \uc608\uce21\uc774 \ubb34\uc791\uc704 \uc608\uce21\ubcf4\ub2e4 \uc804\ud600 \ub098\uc740 \uc810\uc774 \uc5c6\uc744 \uac83\uc785\ub2c8\ub2e4. bs = 64 # \ubc30\uce58 \ud06c\uae30 xb = x_train[0:bs] # x\ub85c\ubd80\ud130 \ubbf8\ub2c8\ubc30\uce58(mini-batch) \ucd94\ucd9c preds = model(xb) # \uc608\uce21 preds[0], preds.shape print(preds[0], preds.shape) tensor([-2.4194, -2.6655, -2.7124, -2.0198, -2.1675, -2.3091, -1.8263, -2.6058, -2.6682, -2.0846], grad_fn=\u003cSelectBackward0\u003e) torch.Size([64, 10]) \uc5ec\ub7ec\ubd84\uc774 \ubcf4\uc2dc\ub4ef\uc774, preds \ud150\uc11c(tensor)\ub294 \ud150\uc11c \uac12 \uc678\uc5d0\ub3c4, \ub610\ud55c \uae30\uc6b8\uae30 \ud568\uc218(gradient function)\ub97c \ub2f4\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \ub098\uc911\uc5d0 \uc774\uac83\uc744 \uc5ed\uc804\ud30c(backpropagation)\ub97c \uc704\ud574 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \uc774\uc81c \uc190\uc2e4\ud568\uc218(loss function)\ub85c \uc0ac\uc6a9\ud558\uae30 \uc704\ud55c \uc74c\uc758 \ub85c\uadf8 \uc6b0\ub3c4(negative log-likelihood)\ub97c \uad6c\ud604\ud569\uc2dc\ub2e4. (\ub2e4\uc2dc \ub9d0\ud558\uc9c0\ub9cc, \uc6b0\ub9ac\ub294 \ud45c\uc900 Python\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.): def nll(input, target): return -input[range(target.shape[0]), target].mean() loss_func = nll \uc6b0\ub9ac\uc758 \ubb34\uc791\uc704 \ubaa8\ub378\uc5d0 \ub300\ud55c \uc190\uc2e4\uc744 \uc810\uac80\ud574 \ubd04\uc73c\ub85c\uc368 \uc5ed\uc804\ud30c \uc774\ud6c4\uc5d0 \uac1c\uc120\uc774 \uc788\ub294\uc9c0 \ub098\uc911\uc5d0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. yb = y_train[0:bs] print(loss_func(preds, yb)) tensor(2.3103, grad_fn=\u003cNegBackward0\u003e) \ub610\ud55c, \uc6b0\ub9ac \ubaa8\ub378\uc758 \uc815\ud655\ub3c4(accuracy)\ub97c \uacc4\uc0b0\ud558\uae30 \uc704\ud55c \ud568\uc218\ub97c \uad6c\ud604\ud569\uc2dc\ub2e4. \ub9e4 \uc608\uce21\ub9c8\ub2e4, \ub9cc\uc57d \uac00\uc7a5 \ud070 \uac12\uc758 \uc778\ub371\uc2a4\uac00 \ubaa9\ud46f\uac12(target value)\uacfc \ub3d9\uc77c\ud558\ub2e4\uba74, \uadf8 \uc608\uce21\uc740 \uc62c\ubc14\ub978 \uac83\uc785\ub2c8\ub2e4. def accuracy(out, yb): preds = torch.argmax(out, dim=1) return (preds == yb).float().mean() \uc6b0\ub9ac\uc758 \ubb34\uc791\uc704 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \uc810\uac80\ud574 \ubd05\uc2dc\ub2e4, \uadf8\ub7fc\uc73c\ub85c\uc368 \uc190\uc2e4\uc774 \uac1c\uc120\ub428\uc5d0 \ub530\ub77c\uc11c \uc815\ud655\ub3c4\uac00 \uac1c\uc120\ub418\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. print(accuracy(preds, yb)) tensor(0.1562) \uc774\uc81c \uc6b0\ub9ac\ub294 \ud6c8\ub828 \ub8e8\ud504(training loop)\ub97c \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9e4 \ubc18\ubcf5\ub9c8\ub2e4, \ub2e4\uc74c\uc744 \uc218\ud589\ud560 \uac83\uc785\ub2c8\ub2e4: \ub370\uc774\ud130\uc758 \ubbf8\ub2c8\ubc30\uce58\ub97c \uc120\ud0dd (bs \ud06c\uae30) \ubaa8\ub378\uc744 \uc774\uc6a9\ud558\uc5ec \uc608\uce21 \uc218\ud589 \uc190\uc2e4 \uacc4\uc0b0 loss.backward() \ub97c \uc774\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uae30\uc6b8\uae30 \uc5c5\ub370\uc774\ud2b8, \uc774 \uacbd\uc6b0\uc5d0\ub294, weights \uc640 bias. \uc774\uc81c \uc6b0\ub9ac\ub294 \uc774 \uae30\uc6b8\uae30\ub4e4\uc744 \uc774\uc6a9\ud558\uc5ec \uac00\uc911\uce58\uc640 \uc808\ud3b8\uc744 \uc5c5\ub370\uc774\ud2b8 \ud569\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc774\uac83\uc744 torch.no_grad() \ucee8\ud14d\uc2a4\ud2b8 \ub9e4\ub2c8\uc800(context manager) \ub0b4\uc5d0\uc11c \uc2e4\ud589\ud569\ub2c8\ub2e4, \uc65c\ub0d0\ud558\uba74 \uc774\ub7ec\ud55c \uc2e4\ud589\uc774 \ub2e4\uc74c \uae30\uc6b8\uae30\uc758 \uacc4\uc0b0\uc5d0 \uae30\ub85d\ub418\uc9c0 \uc54a\uae30\ub97c \uc6d0\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. PyTorch\uc758 \uc790\ub3d9 \uae30\uc6b8\uae30(Autograd)\uac00 \uc5b4\ub5bb\uac8c \uc5f0\uc0b0\uc744 \uae30\ub85d\ud558\ub294\uc9c0 \uc5ec\uae30\uc5d0\uc11c \ub354 \uc54c\uc544\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\uace0 \ub098\uc11c \uae30\uc6b8\uae30\ub97c 0\uc73c\ub85c \uc124\uc815\ud569\ub2c8\ub2e4, \uadf8\ub7fc\uc73c\ub85c\uc368 \ub2e4\uc74c \ub8e8\ud504(loop)\uc5d0 \uc900\ube44\ud558\uac8c \ub429\ub2c8\ub2e4. \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74, \uc6b0\ub9ac\uc758 \uae30\uc6b8\uae30\ub4e4\uc740 \uc77c\uc5b4\ub09c \ubaa8\ub4e0 \uc5f0\uc0b0\uc758 \ub204\uc801 \uc9d1\uacc4\ub97c \uae30\ub85d\ud558\uac8c \ub418\uc5b4\ubc84\ub9bd\ub2c8\ub2e4. (\uc989, loss.backward() \uac00 \uc774\ubbf8 \uc800\uc7a5\ub41c \uac83\uc744 \ub300\uccb4\ud558\uae30\ubcf4\ub2e8, \uae30\uc874 \uac12\uc5d0 \uae30\uc6b8\uae30\ub97c \ub354\ud558\uac8c \ub429\ub2c8\ub2e4). \ud301 \uc5ec\ub7ec\ubd84\ub4e4\uc740 PyTorch \ucf54\ub4dc\uc5d0 \ub300\ud558\uc5ec \ud45c\uc900 python \ub514\ubc84\uac70(debugger)\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc73c\ubbc0\ub85c, \ub9e4 \ub2e8\uacc4\ub9c8\ub2e4 \ub2e4\uc591\ud55c \ubcc0\uc218 \uac12\uc744 \uc810\uac80\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc544\ub798\uc5d0\uc11c set_trace() \ub97c \uc8fc\uc11d \ud574\uc81c\ud558\uc5ec \uc0ac\uc6a9\ud574 \ubcf4\uc138\uc694. from IPython.core.debugger import set_trace lr = 0.5 # \ud559\uc2b5\ub960(learning rate) epochs = 2 # \ud6c8\ub828\uc5d0 \uc0ac\uc6a9\ud560 \uc5d0\ud3ed(epoch) \uc218 for epoch in range(epochs): for i in range((n - 1) // bs + 1): # set_trace() start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() with torch.no_grad(): weights -= weights.grad * lr bias -= bias.grad * lr weights.grad.zero_() bias.grad.zero_() \uc774\uc81c \ub2e4 \ub410\uc2b5\ub2c8\ub2e4: \uc81c\uc77c \uac04\ub2e8\ud55c \uc2e0\uacbd\ub9dd(neural network)\uc758 \ubaa8\ub4e0 \uac83\uc744 \ubc11\ubc14\ub2e5\ubd80\ud130 \uc0dd\uc131\ud558\uace0 \ud6c8\ub828\ud558\uc600\uc2b5\ub2c8\ub2e4! (\uc774\ubc88\uc5d0\ub294 \uc740\ub2c9\uce35(hidden layer)\uc774 \uc5c6\uae30 \ub54c\ubb38\uc5d0, \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0(logistic regression)\uc785\ub2c8\ub2e4). \uc774\uc81c \uc190\uc2e4\uacfc \uc815\ud655\ub3c4\ub97c \uc774\uc804 \uac12\ub4e4\uacfc \ube44\uad50\ud558\uba74\uc11c \ud655\uc778\ud574\ubd05\uc2dc\ub2e4. \uc6b0\ub9ac\ub294 \uc190\uc2e4\uc740 \uac10\uc18c\ud558\uace0, \uc815\ud655\ub3c4\ub294 \uc99d\uac00\ud558\uae30\ub97c \uae30\ub300\ud560 \uac83\uc774\uace0, \uadf8\ub4e4\uc740 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4. print(loss_func(model(xb), yb), accuracy(model(xb), yb)) tensor(0.0820, grad_fn=\u003cNegBackward0\u003e) tensor(1.) torch.nn.functional \uc0ac\uc6a9\ud558\uae30# \uc774\uc81c \ucf54\ub4dc\ub97c \ub9ac\ud329\ud1a0\ub9c1(refactoring) \ud558\uaca0\uc2b5\ub2c8\ub2e4. \uadf8\ub85c\uc368 \uc774\uc804\uacfc \ub3d9\uc77c\ud558\uc9c0\ub9cc, PyTorch\uc758 nn \ud074\ub798\uc2a4\uc758 \uc7a5\uc810\uc744 \ud65c\uc6a9\ud558\uc5ec \ub354 \uac04\uacb0\ud558\uace0 \uc720\uc5f0\ud558\uac8c \ub9cc\ub4e4 \uac83\uc785\ub2c8\ub2e4. \uc9c0\uae08\ubd80\ud130 \ub9e4 \ub2e8\uacc4\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \ucf54\ub4dc\ub97c \ub354 \uc9e7\uace0, \uc774\ud574\ud558\uae30 \uc27d\uace0, \uc720\uc5f0\ud558\uac8c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4. \ucc98\uc74c\uc774\uba74\uc11c \uc6b0\ub9ac\uc758 \ucf54\ub4dc\ub97c \uc9e7\uac8c \ub9cc\ub4e4\uae30 \uac00\uc7a5 \uc26c\uc6b4 \ub2e8\uacc4\ub294 \uc9c1\uc811 \uc791\uc131\ud55c \ud65c\uc131\ud654, \uc190\uc2e4 \ud568\uc218\ub97c torch.nn.functional \uc758 \ud568\uc218\ub85c \ub300\uccb4\ud558\ub294 \uac83\uc785\ub2c8\ub2e4 (\uad00\ub840\uc5d0 \ub530\ub77c, \uc77c\ubc18\uc801\uc73c\ub85c F \ub124\uc784\uc2a4\ud398\uc774\uc2a4(namespace)\ub97c \ud1b5\ud574 \uc784\ud3ec\ud2b8(import) \ud569\ub2c8\ub2e4). \uc774 \ubaa8\ub4c8\uc5d0\ub294 torch.nn \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \ubaa8\ub4e0 \ud568\uc218\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4 (\ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \ub2e4\ub978 \ubd80\ubd84\uc5d0\ub294 \ud074\ub798\uc2a4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.) \ub2e4\uc591\ud55c \uc190\uc2e4 \ubc0f \ud65c\uc131\ud654 \ud568\uc218\ubfd0\ub9cc \uc544\ub2c8\ub77c, \ud480\ub9c1(pooling) \ud568\uc218\uc640 \uac19\uc774 \uc2e0\uacbd\ub9dd\uc744 \ub9cc\ub4dc\ub294\ub370 \ud3b8\ub9ac\ud55c \uba87 \uac00\uc9c0 \ud568\uc218\ub3c4 \uc5ec\uae30\uc5d0\uc11c \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\ucee8\ubcfc\ub8e8\uc158(convolution) \uc5f0\uc0b0, \uc120\ud615(linear) \ub808\uc774\uc5b4, \ub4f1\uc744 \uc218\ud589\ud558\ub294 \ud568\uc218\ub3c4 \uc788\uc9c0\ub9cc, \uc55e\uc73c\ub85c \ubcf4\uc2dc\uaca0\uc9c0\ub9cc \ub300\uac1c\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \ub2e4\ub978 \ubd80\ubd84\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub354 \uc798 \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.) \ub9cc\uc57d \uc5ec\ub7ec\ubd84\ub4e4\uc774 \uc74c\uc758 \ub85c\uadf8 \uc6b0\ub3c4 \uc190\uc2e4\uacfc \ub85c\uadf8 \uc18c\ud504\ud2b8\ub9e5\uc2a4 (log softmax) \ud65c\uc131\ud654 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0, PyTorch\ub294 \uc774 \ub458\uc744 \uacb0\ud569\ud558\ub294 \ub2e8\uc77c \ud568\uc218\uc778 F.cross_entropy \ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c \ubaa8\ub378\uc5d0\uc11c \ud65c\uc131\ud654 \ud568\uc218\ub97c \uc81c\uac70\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. import torch.nn.functional as F loss_func = F.cross_entropy def model(xb): return xb @ weights + bias \ub354 \uc774\uc0c1 model \ud568\uc218\uc5d0\uc11c log_softmax \ub97c \ud638\ucd9c\ud558\uc9c0 \uc54a\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc190\uc2e4\uacfc \uc815\ud655\ub3c4\uacfc \uc774\uc804\uacfc \ub3d9\uc77c\ud55c\uc9c0 \ud655\uc778\ud574 \ubd05\uc2dc\ub2e4: print(loss_func(model(xb), yb), accuracy(model(xb), yb)) tensor(0.0820, grad_fn=\u003cNllLossBackward0\u003e) tensor(1.) nn.Module \uc744 \uc774\uc6a9\ud558\uc5ec \ub9ac\ud329\ud1a0\ub9c1 \ud558\uae30# \ub2e4\uc74c\uc73c\ub85c, \ub354 \uba85\ud655\ud558\uace0 \uac04\uacb0\ud55c \ud6c8\ub828 \ub8e8\ud504\ub97c \uc704\ud574 nn.Module \ubc0f nn.Parameter \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 nn.Module (\uc790\uccb4\uac00 \ud074\ub798\uc2a4\uc774\uace0 \uc0c1\ud0dc\ub97c \ucd94\uc801\ud560 \uc218 \uc788\ub294) \ud558\uc704 \ud074\ub798\uc2a4(subclass)\ub97c \ub9cc\ub4ed\ub2c8\ub2e4. \uc774 \uacbd\uc6b0\uc5d0\ub294, \ud3ec\uc6cc\ub4dc(forward) \ub2e8\uacc4\uc5d0 \ub300\ud55c \uac00\uc911\uce58, \uc808\ud3b8, \uadf8\ub9ac\uace0 \uba54\uc18c\ub4dc(method) \ub4f1\uc744 \uc720\uc9c0\ud558\ub294 \ud074\ub798\uc2a4\ub97c \ub9cc\ub4e4\uace0\uc790 \ud569\ub2c8\ub2e4. nn.Module \uc740 \uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud560 \uba87 \uac00\uc9c0 \uc18d\uc131(attribute)\uacfc \uba54\uc18c\ub4dc\ub97c (.parameters() \uc640 .zero_grad() \uac19\uc740) \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ucc38\uace0 nn.Module (\ub300\ubb38\uc790 M) \uc740 PyTorch\uc758 \ud2b9\uc815 \uac1c\ub150\uc774\uace0, \uc6b0\ub9ac\ub294 \uc774 \ud074\ub798\uc2a4\ub97c \ub9ce\uc774 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. nn.Module \ub97c Python\uc758 \ucf54\ub4dc\ub97c \uc784\ud3ec\ud2b8\ud558\uae30 \uc704\ud55c \ucf54\ub4dc \ud30c\uc77c\uc778 module (\uc18c\ubb38\uc790 m) \uc758 \uac1c\ub150\uacfc \ud5f7\uac08\ub9ac\uc9c0 \ub9d0\uc544\uc8fc\uc138\uc694. from torch import nn class Mnist_Logistic(nn.Module): def __init__(self): super().__init__() self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784)) self.bias = nn.Parameter(torch.zeros(10)) def forward(self, xb): return xb @ self.weights + self.bias \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\ub294 \ub300\uc2e0\uc5d0 \uc774\uc81c\ub294 \uc624\ube0c\uc81d\ud2b8(object) \ub97c \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0, \uba3c\uc800 \ubaa8\ub378\uc744 \uc778\uc2a4\ud134\uc2a4\ud654(instantiate) \ud574\uc57c \ud569\ub2c8\ub2e4: model = Mnist_Logistic() \uc774\uc81c \uc6b0\ub9ac\ub294 \uc774\uc804\uacfc \ub3d9\uc77c\ud55c \ubc29\uc2dd\uc73c\ub85c \uc190\uc2e4\uc744 \uacc4\uc0b0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c nn.Module \uc624\ube0c\uc81d\ud2b8\ub4e4\uc740 \ub9c8\uce58 \ud568\uc218\ucc98\ub7fc \uc0ac\uc6a9\ub429\ub2c8\ub2e4 (\uc989, \uc774\ub4e4\uc740 \ud638\ucd9c\uac00\ub2a5 \ud569\ub2c8\ub2e4), \uadf8\ub7ec\ub098 \ubc30\ud6c4\uc5d0\uc11c PyTorch\ub294 \uc6b0\ub9ac\uc758 forward \uba54\uc18c\ub4dc\ub97c \uc790\ub3d9\uc73c\ub85c \ud638\ucd9c\ud569\ub2c8\ub2e4. print(loss_func(model(xb), yb)) tensor(2.5184, grad_fn=\u003cNllLossBackward0\u003e) \uc774\uc804\uc5d0\ub294 \ud6c8\ub828 \ub8e8\ud504\ub97c \uc704\ud574 \uc774\ub984 \ubcc4\ub85c \uac01 \ub9e4\uac1c\ubcc0\uc218(parameter)\uc758 \uac12\uc744 \uc5c5\ub370\uc774\ud2b8\ud558\uace0 \ub2e4\uc74c\uacfc \uac19\uc774 \uac01 \ub9e4\uac1c \ubcc0\uc218\uc5d0 \ub300\ud55c \uae30\uc6b8\uae30\ub4e4\uc744 \uac1c\ubcc4\uc801\uc73c\ub85c \uc218\ub3d9\uc73c\ub85c 0\uc73c\ub85c \uc81c\uac70\ud574\uc57c \ud588\uc2b5\ub2c8\ub2e4: with torch.no_grad(): weights -= weights.grad * lr bias -= bias.grad * lr weights.grad.zero_() bias.grad.zero_() \uc774\uc81c \uc6b0\ub9ac\ub294 model.parameters() \ubc0f model.zero_grad() (\ubaa8\ub450 nn.Module \uc5d0 \ub300\ud574 PyTorch\uc5d0 \uc758\ud574 \uc815\uc758\ub428)\ub97c \ud65c\uc6a9\ud558\uc5ec \uc774\ub7ec\ud55c \ub2e8\uacc4\ub97c \ub354 \uac04\uacb0\ud558\uac8c \ub9cc\ub4e4\uace0, \ud2b9\ud788 \ub354 \ubcf5\uc7a1\ud55c \ubaa8\ub378\uc5d0 \ub300\ud574\uc11c \uc77c\ubd80 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc78a\uc5b4 \ubc84\ub9ac\ub294 \uc624\ub958\ub97c \ub35c \ubc1c\uc0dd\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4: with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() \uc774\uc81c \uc774\uac83\uc744 \ub098\uc911\uc5d0 \ub2e4\uc2dc \uc2e4\ud589\ud560 \uc218 \uc788\ub3c4\ub85d fit \ud568\uc218\ub85c \uc791\uc740 \ud6c8\ub828 \ub8e8\ud504\ub97c \uac10\uc300 \uac83\uc785\ub2c8\ub2e4. def fit(): for epoch in range(epochs): for i in range((n - 1) // bs + 1): start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() fit() \uc190\uc2e4\uc774 \uc904\uc5b4\ub4e4\uc5c8\ub294\uc9c0 \ub2e4\uc2dc \ud55c \ubc88 \ud655\uc778\ud569\uc2dc\ub2e4: print(loss_func(model(xb), yb)) tensor(0.0827, grad_fn=\u003cNllLossBackward0\u003e) nn.Linear \ub97c \uc0ac\uc6a9\ud558\uc5ec \ub9ac\ud329\ud1a0\ub9c1 \ud558\uae30# \uacc4\uc18d\ud574\uc11c \ucf54\ub4dc\ub97c \ub9ac\ud329\ud1a0\ub9c1 \ud569\ub2c8\ub2e4. self.weights \ubc0f self.bias \ub97c \uc218\ub3d9\uc73c\ub85c \uc815\uc758 \ubc0f \ucd08\uae30\ud654\ud558\uace0, xb\u00a0 @ self.weights + self.bias \ub97c \uacc4\uc0b0\ud558\ub294 \ub300\uc2e0\uc5d0, \uc704\uc758 \ubaa8\ub4e0 \uac83\uc744 \ud574\uc904 PyTorch \ud074\ub798\uc2a4\uc778 nn.Linear \ub97c \uc120\ud615 \ub808\uc774\uc5b4\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. PyTorch \uc5d0\ub294 \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ucf54\ub4dc\ub97c \ud06c\uac8c \ub2e8\uc21c\ud654 \ud560 \uc218 \uc788\ub294 \ubbf8\ub9ac \uc815\uc758\ub41c \ub808\uc774\uc5b4\uac00 \uc788\uace0 \uc774\ub294 \ub610\ud55c \uc885\uc885 \uae30\uc874 \ucf54\ub4dc\ubcf4\ub2e4 \uc18d\ub3c4\ub97c \ube60\ub974\uac8c \ud569\ub2c8\ub2e4. class Mnist_Logistic(nn.Module): def __init__(self): super().__init__() self.lin = nn.Linear(784, 10) def forward(self, xb): return self.lin(xb) \uc774\uc804\uacfc \uac19\uc740 \ubc29\uc2dd\uc73c\ub85c \ubaa8\ub378\uc744 \uc778\uc2a4\ud134\uc2a4\ud654\ud558\uace0 \uc190\uc2e4\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4: model = Mnist_Logistic() print(loss_func(model(xb), yb)) tensor(2.3160, grad_fn=\u003cNllLossBackward0\u003e) \uc6b0\ub9ac\ub294 \uc5ec\uc804\ud788 \uc774\uc804\uacfc \ub3d9\uc77c\ud55c fit \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. fit() print(loss_func(model(xb), yb)) tensor(0.0814, grad_fn=\u003cNllLossBackward0\u003e) torch.optim \uc744 \uc774\uc6a9\ud558\uc5ec \ub9ac\ud329\ud1a0\ub9c1 \ud558\uae30# PyTorch\uc5d0\ub294 \ub2e4\uc591\ud55c \ucd5c\uc801\ud654(optimization) \uc54c\uace0\ub9ac\uc998\uc744 \uac00\uc9c4 \ud328\ud0a4\uc9c0\uc778 torch.optim \ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc218\ub3d9\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8 \ud558\ub294 \ub300\uc2e0, \uc635\ud2f0\ub9c8\uc774\uc800(optimizer)\uc758 step \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5c5\ub370\uc774\ud2b8\ub97c \uc9c4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub807\uac8c \ud558\uba74 \uc774\uc804\uc5d0 \uc218\ub3d9\uc73c\ub85c \ucf54\ub529\ud55c \ucd5c\uc801\ud654 \ub2e8\uacc4\ub97c \ub300\uccb4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() \ub300\uc2e0\uc5d0 \uc774\ub807\uac8c \ub9d0\uc774\uc8e0: opt.step() opt.zero_grad() (optim.zero_grad() \ub294 \uae30\uc6b8\uae30\ub97c 0\uc73c\ub85c \uc7ac\uc124\uc815 \ud574\uc90d\ub2c8\ub2e4. \ub2e4\uc74c \ubbf8\ub2c8 \ubc30\uce58\uc5d0 \ub300\ud55c \uae30\uc6b8\uae30\ub97c \uacc4\uc0b0\ud558\uae30 \uc804\uc5d0 \ud638\ucd9c\ud574\uc57c \ud569\ub2c8\ub2e4.) from torch import optim \ub098\uc911\uc5d0 \ub2e4\uc2dc \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ubaa8\ub378\uacfc \uc635\ud2f0\ub9c8\uc774\uc838\ub97c \ub9cc\ub4dc\ub294 \uc791\uc740 \ud568\uc218\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. def get_model(): model = Mnist_Logistic() return model, optim.SGD(model.parameters(), lr=lr) model, opt = get_model() print(loss_func(model(xb), yb)) for epoch in range(epochs): for i in range((n - 1) // bs + 1): start_i = i * bs end_i = start_i + bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() print(loss_func(model(xb), yb)) tensor(2.3094, grad_fn=\u003cNllLossBackward0\u003e) tensor(0.0807, grad_fn=\u003cNllLossBackward0\u003e) Dataset \uc744 \uc774\uc6a9\ud558\uc5ec \ub9ac\ud329\ud1a0\ub9c1\ud558\uae30# PyTorch \uc5d0\ub294 \ucd94\uc0c1 Dataset \ud074\ub798\uc2a4\uac00 \uc788\uc2b5\ub2c8\ub2e4. Dataset \uc740 __len__ \ud568\uc218 (Python\uc758 \ud45c\uc900 len \ud568\uc218\uc5d0 \uc758\ud574 \ud638\ucd9c\ub428) \ubc0f __getitem__ \ud568\uc218\ub97c \uac00\uc9c4 \uc5b4\ub5a4 \uac83\uc774\ub77c\ub3c4 \ub420 \uc218 \uc788\uc73c\uba70, \uc774 \ud568\uc218\ub4e4\uc744 \uc778\ub371\uc2f1(indexing)\ud558\uae30 \uc704\ud55c \ubc29\ubc95\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc \uc740 Dataset \uc758 \ud558\uc704 \ud074\ub798\uc2a4\ub85c\uc368, \uc0ac\uc6a9\uc790 \uc9c0\uc815 FacialLandmarkDataset \ud074\ub798\uc2a4\ub97c \ub9cc\ub4dc\ub294 \uc88b\uc740 \uc608\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4. PyTorch \uc758 TensorDataset \uc740 \ud150\uc11c\ub97c \uac10\uc2f8\ub294(wrapping) Dataset \uc785\ub2c8\ub2e4. \uae38\uc774\uc640 \uc778\ub371\uc2f1 \ubc29\uc2dd\uc744 \uc815\uc758\ud568\uc73c\ub85c\uc368 \ud150\uc11c\uc758 \uccab \ubc88\uc9f8 \ucc28\uc6d0\uc744 \ub530\ub77c \ubc18\ubcf5, \uc778\ub371\uc2f1 \ubc0f \uc2ac\ub77c\uc774\uc2a4(slice)\ud558\ub294 \ubc29\ubc95\ub3c4 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774\ub807\uac8c\ud558\uba74 \ud6c8\ub828 \ud560 \ub54c \ub3d9\uc77c\ud55c \ub77c\uc778\uc5d0\uc11c \ub3c5\ub9bd(independent) \ubcc0\uc218\uc640 \uc885\uc18d(dependent) \ubcc0\uc218\uc5d0 \uc27d\uac8c \uc561\uc138\uc2a4 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. from torch.utils.data import TensorDataset x_train \ubc0f y_train \ubaa8\ub450 \ud558\ub098\uc758 TensorDataset \uc5d0 \ud569\uccd0\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4, \ub530\ub77c\uc11c \ubc18\ubcf5\uc2dc\ud0a4\uace0 \uc2ac\ub77c\uc774\uc2a4 \ud558\uae30 \ud3b8\ub9ac\ud569\ub2c8\ub2e4. train_ds = TensorDataset(x_train, y_train) \uc774\uc804\uc5d0\ub294 x \ubc0f y \uac12\uc758 \ubbf8\ub2c8 \ubc30\uce58\ub97c \ubcc4\ub3c4\ub85c \ubc18\ubcf5\ud574\uc57c \ud588\uc2b5\ub2c8\ub2e4: xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] \uc774\uc81c \uc774 \ub450 \ub2e8\uacc4\ub97c \ud568\uaed8 \uc218\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: xb,yb = train_ds[i*bs : i*bs+bs] model, opt = get_model() for epoch in range(epochs): for i in range((n - 1) // bs + 1): xb, yb = train_ds[i * bs: i * bs + bs] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() print(loss_func(model(xb), yb)) tensor(0.0811, grad_fn=\u003cNllLossBackward0\u003e) DataLoader \ub97c \uc0ac\uc6a9\ud558\uc5ec \ub9ac\ud329\ud1a0\ub9c1\ud558\uae30# PyTorch \uc758 DataLoader \ub294 \ubc30\uce58 \uad00\ub9ac\ub97c \ub2f4\ub2f9\ud569\ub2c8\ub2e4. \uc5ec\ub7ec\ubd84\ub4e4\uc740 \ubaa8\ub4e0 Dataset \uc73c\ub85c\ubd80\ud130 DataLoader \ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. DataLoader \ub294 \ubc30\uce58\ub4e4\uc5d0 \ub300\ud574\uc11c \ubc18\ubcf5\ud558\uae30 \uc27d\uac8c \ub9cc\ub4e4\uc5b4\uc90d\ub2c8\ub2e4. train_ds[i*bs : i*bs+bs] \ub97c \uc0ac\uc6a9\ud558\ub294 \ub300\uc2e0, DataLoader \ub294 \ub9e4 \ubbf8\ub2c8\ubc30\uce58\ub97c \uc790\ub3d9\uc801\uc73c\ub85c \uc81c\uacf5\ud569\ub2c8\ub2e4. from torch.utils.data import DataLoader train_ds = TensorDataset(x_train, y_train) train_dl = DataLoader(train_ds, batch_size=bs) \uc774\uc804\uc5d0\ub294 \ub8e8\ud504\uac00 \ub2e4\uc74c\uacfc \uac19\uc774 \ubc30\uce58 (xb, yb) \ub97c \ubc18\ubcf5\ud588\uc2b5\ub2c8\ub2e4: for i in range((n-1)//bs + 1): xb,yb = train_ds[i*bs : i*bs+bs] pred = model(xb) \uc774\uc81c (xb, yb)\uac00 DataLoader \uc5d0\uc11c \uc790\ub3d9\uc73c\ub85c \ub85c\ub4dc\ub418\ubbc0\ub85c \ub8e8\ud504\uac00 \ud6e8\uc52c \uae68\ub057\ud574\uc84c\uc2b5\ub2c8\ub2e4: for xb,yb in train_dl: pred = model(xb) model, opt = get_model() for epoch in range(epochs): for xb, yb in train_dl: pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() print(loss_func(model(xb), yb)) tensor(0.0807, grad_fn=\u003cNllLossBackward0\u003e) PyTorch\uc758 nn.Module, nn.Parameter, Dataset \ubc0f DataLoader \ub355\ubd84\uc5d0 \uc774\uc81c \ud6c8\ub828 \ub8e8\ud504\uac00 \ud6e8\uc52c \ub354 \uc791\uc544\uc9c0\uace0 \uc774\ud574\ud558\uae30 \uc26c\uc6cc\uc84c\uc2b5\ub2c8\ub2e4. \uc774\uc81c \uc2e4\uc81c\ub85c \ud6a8\uacfc\uc801\uc778 \ubaa8\ub378\uc744 \ub9cc\ub4dc\ub294 \ub370 \ud544\uc694\ud55c \uae30\ubcf8 \uae30\ub2a5\uc744 \ucd94\uac00\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uac80\uc99d(validation) \ucd94\uac00\ud558\uae30# \uc139\uc158 1\uc5d0\uc11c, \uc6b0\ub9ac\ub294 \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 \ud569\ub9ac\uc801\uc778 \ud6c8\ub828 \ub8e8\ud504\ub97c \uc124\uc815\ud558\ub824\uace0\ud588\uc2b5\ub2c8\ub2e4. \uc2e4\uc804\uc5d0\uc11c, \uc5ec\ub7ec\ubd84\ub4e4\uc740 \uacfc\uc801\ud569(overfitting)\uc744 \ud655\uc778\ud558\uae30 \uc704\ud574\uc11c \ud56d\uc0c1 \uac80\uc99d \ub370\uc774\ud130\uc14b(validation set) \uc774 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4. \ud6c8\ub828 \ub370\uc774\ud130\ub97c \uc11e\ub294(shuffling) \uac83\uc740 \ubc30\uce58\uc640 \uacfc\uc801\ud569 \uc0ac\uc774\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubc29\uc9c0\ud558\uae30 \uc704\ud574 \uc911\uc694\ud569\ub2c8\ub2e4. \ubc18\uba74\uc5d0, \uac80\uc99d \uc190\uc2e4(validation loss)\uc740 \uac80\uc99d \ub370\uc774\ud130\uc14b\uc744 \uc11e\ub4e0 \uc548\uc11e\ub4e0 \ub3d9\uc77c\ud569\ub2c8\ub2e4. \ub370\uc774\ud130\ub97c \uc11e\ub294 \uac83\uc740 \ucd94\uac00 \uc2dc\uac04\uc774 \uac78\ub9ac\ubbc0\ub85c, \uac80\uc99d \ub370\uc774\ud130\ub97c \uc11e\ub294 \uac83\uc740 \uc758\ubbf8\uac00 \uc5c6\uc2b5\ub2c8\ub2e4. \uac80\uc99d \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \ubc30\uce58 \ud06c\uae30\ub294 \ud559\uc2b5 \ub370\uc774\ud130\uc14b \ubc30\uce58 \ud06c\uae30\uc758 2\ubc30\ub97c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \uc774\ub294 \uac80\uc99d \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud574\uc11c\ub294 \uc5ed\uc804\ud30c(backpropagation)\uac00 \ud544\uc694\ud558\uc9c0 \uc54a\uc73c\ubbc0\ub85c \uba54\ubaa8\ub9ac\ub97c \ub35c \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4 (\uae30\uc6b8\uae30\ub97c \uc800\uc7a5\ud560 \ud544\uc694\uac00 \uc5c6\uc74c). \ub354 \ud070 \ubc30\uce58 \ud06c\uae30\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc190\uc2e4\uc744 \ub354 \ube68\ub9ac \uacc4\uc0b0\ud558\uae30 \uc704\ud574 \uc774\ub807\uac8c \ud569\ub2c8\ub2e4. train_ds = TensorDataset(x_train, y_train) train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True) valid_ds = TensorDataset(x_valid, y_valid) valid_dl = DataLoader(valid_ds, batch_size=bs * 2) \uac01 \uc5d0\ud3ed\uc774 \ub05d\ub0a0 \ub54c \uac80\uc99d \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uace0 \ud504\ub9b0\ud2b8 \ud560 \uac83\uc785\ub2c8\ub2e4. (\ud6c8\ub828 \uc804\uc5d0 \ud56d\uc0c1 model.train() \uc744 \ud638\ucd9c\ud558\uace0, \ucd94\ub860(inference) \uc804\uc5d0 model.eval() \uc744 \ud638\ucd9c\ud569\ub2c8\ub2e4, \uc774\ub294 nn.BatchNorm2d \ubc0f nn.Dropout \uacfc \uac19\uc740 \ub808\uc774\uc5b4\uc5d0\uc11c \uc774\ub7ec\ud55c \ub2e4\ub978 \ub2e8\uacc4(\ud6c8\ub828, \ucd94\ub860) \uc5d0 \ub300\ud55c \uc801\uc808\ud55c \ub3d9\uc791\uc774 \uc77c\uc5b4\ub098\uac8c \ud558\uae30 \uc704\ud568\uc785\ub2c8\ub2e4.) model, opt = get_model() for epoch in range(epochs): model.train() for xb, yb in train_dl: pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() model.eval() with torch.no_grad(): valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl) print(epoch, valid_loss / len(valid_dl)) 0 tensor(0.3160) 1 tensor(0.2796) fit() \uc640 get_data() \uc0dd\uc131\ud558\uae30# \uc774\uc81c \uc6b0\ub9ac\ub294 \uc6b0\ub9ac\ub9cc\uc758 \uc791\uc740 \ub9ac\ud329\ud1a0\ub9c1\uc744 \uc218\ud589\ud560 \uac83\uc785\ub2c8\ub2e4. \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uacfc \uac80\uc99d \ub370\uc774\ud130\uc14b \ubaa8\ub450\uc5d0 \ub300\ud55c \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\ub294 \uc720\uc0ac\ud55c \ud504\ub85c\uc138\uc2a4\ub97c \ub450 \ubc88 \uac70\uce58\ubbc0\ub85c, \uc774\ub97c \ud558\ub098\uc758 \ubc30\uce58\uc5d0 \ub300\ud55c \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\ub294 \uc790\uccb4 \ud568\uc218 loss_batch \ub85c \ub9cc\ub4e4\uc5b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \uc804\ub2ec\ud558\uace0 \uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5ed\uc804\ud30c\ub97c \uc218\ud589\ud569\ub2c8\ub2e4. \uac80\uc99d \ub370\uc774\ud130\uc14b\uc758 \uacbd\uc6b0 \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \uc804\ub2ec\ud558\uc9c0 \uc54a\uc73c\ubbc0\ub85c \uba54\uc18c\ub4dc\uac00 \uc5ed\uc804\ud30c\ub97c \uc218\ud589\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. def loss_batch(model, loss_func, xb, yb, opt=None): loss = loss_func(model(xb), yb) if opt is not None: loss.backward() opt.step() opt.zero_grad() return loss.item(), len(xb) fit \uc740 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uace0 \uac01 \uc5d0\ud3ed\uc5d0 \ub300\ud55c \ud6c8\ub828 \ubc0f \uac80\uc99d \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\ub294 \uc791\uc5c5\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. import numpy as np def fit(epochs, model, loss_func, opt, train_dl, valid_dl): for epoch in range(epochs): model.train() for xb, yb in train_dl: loss_batch(model, loss_func, xb, yb, opt) model.eval() with torch.no_grad(): losses, nums = zip( *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl] ) val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums) print(epoch, val_loss) get_data \ub294 \ud559\uc2b5 \ubc0f \uac80\uc99d \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c dataloader \ub97c \ucd9c\ub825\ud569\ub2c8\ub2e4. def get_data(train_ds, valid_ds, bs): return ( DataLoader(train_ds, batch_size=bs, shuffle=True), DataLoader(valid_ds, batch_size=bs * 2), ) \uc774\uc81c dataloader\ub97c \uac00\uc838\uc624\uace0 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\ub294 \uc804\uccb4 \ud504\ub85c\uc138\uc2a4\ub97c 3 \uc904\uc758 \ucf54\ub4dc\ub85c \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: train_dl, valid_dl = get_data(train_ds, valid_ds, bs) model, opt = get_model() fit(epochs, model, loss_func, opt, train_dl, valid_dl) 0 0.3630067733645439 1 0.2930187728047371 \uc774\ub7ec\ud55c \uae30\ubcf8 3\uc904\uc758 \ucf54\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \ubaa8\ub378\uc744 \ud6c8\ub828\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ucee8\ubcfc\ub8e8\uc158 \uc2e0\uacbd\ub9dd(CNN)\uc744 \ud6c8\ub828\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294\uc9c0 \uc0b4\ud3b4 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4! CNN \uc73c\ub85c \ub118\uc5b4\uac00\uae30# \uc774\uc81c 3\uac1c\uc758 \ucee8\ubcfc\ub8e8\uc158 \ub808\uc774\uc5b4\ub85c \uc2e0\uacbd\ub9dd\uc744 \uad6c\ucd95\ud560 \uac83\uc785\ub2c8\ub2e4. \uc774\uc804 \uc139\uc158\uc758 \uc5b4\ub5a4 \ud568\uc218\ub3c4 \ubaa8\ub378\uc758 \ud615\uc2dd\uc5d0 \ub300\ud574 \uac00\uc815\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0, \ubcc4\ub3c4\uc758 \uc218\uc815\uc5c6\uc774 CNN\uc744 \ud559\uc2b5\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Pytorch\uc758 \uc0ac\uc804\uc815\uc758\ub41c Conv2d \ud074\ub798\uc2a4\ub97c \ucee8\ubcfc\ub8e8\uc158 \ub808\uc774\uc5b4\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. 3\uac1c\uc758 \ucee8\ubcfc\ub8e8\uc158 \ub808\uc774\uc5b4\ub85c CNN\uc744 \uc815\uc758\ud569\ub2c8\ub2e4. \uac01 \ucee8\ubcfc\ub8e8\uc158 \ub4a4\uc5d0\ub294 ReLU\uac00 \uc788\uc2b5\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c \ud3c9\uade0 \ud480\ub9c1(average pooling)\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. (view \ub294 PyTorch\uc758 NumPy reshape \ubc84\uc804\uc785\ub2c8\ub2e4.) class Mnist_CNN(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1) self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1) self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1) def forward(self, xb): xb = xb.view(-1, 1, 28, 28) xb = F.relu(self.conv1(xb)) xb = F.relu(self.conv2(xb)) xb = F.relu(self.conv3(xb)) xb = F.avg_pool2d(xb, 4) return xb.view(-1, xb.size(1)) lr = 0.1 \ubaa8\uba58\ud140(Momentum) \uc740 \uc774\uc804 \uc5c5\ub370\uc774\ud2b8\ub3c4 \uace0\ub824\ud558\uace0 \uc77c\ubc18\uc801\uc73c\ub85c \ub354 \ube60\ub978 \ud6c8\ub828\uc73c\ub85c \uc774\uc5b4\uc9c0\ub294 \ud655\ub960\uc801 \uacbd\uc0ac\ud558\uac15\ubc95(stochastic gradient descent) \uc758 \ubcc0\ud615\uc785\ub2c8\ub2e4. model = Mnist_CNN() opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) fit(epochs, model, loss_func, opt, train_dl, valid_dl) 0 0.34925633783340454 1 0.26948165959119796 nn.Sequential \uc0ac\uc6a9\ud558\uae30# torch.nn \uc5d0\ub294 \ucf54\ub4dc\ub97c \uac04\ub2e8\ud788 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ub610 \ub2e4\ub978 \ud3b8\ub9ac\ud55c \ud074\ub798\uc2a4\uc778 Sequential \uc774 \uc788\uc2b5\ub2c8\ub2e4.. Sequential \uac1d\uccb4\ub294 \uadf8 \uc548\uc5d0 \ud3ec\ud568\ub41c \uac01 \ubaa8\ub4c8\uc744 \uc21c\ucc28\uc801\uc73c\ub85c \uc2e4\ud589\ud569\ub2c8\ub2e4. \uc774\uac83\uc740 \uc6b0\ub9ac\uc758 \uc2e0\uacbd\ub9dd\uc744 \uc791\uc131\ud558\ub294 \ub354 \uac04\ub2e8\ud55c \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc774\ub97c \ud65c\uc6a9\ud558\ub824\uba74 \uc8fc\uc5b4\uc9c4 \ud568\uc218\uc5d0\uc11c \uc0ac\uc6a9\uc790\uc815\uc758 \ub808\uc774\uc5b4(custom layer) \ub97c \uc27d\uac8c \uc815\uc758\ud560 \uc218 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, PyTorch\uc5d0\ub294 view \ub808\uc774\uc5b4\uac00 \uc5c6\uc73c\ubbc0\ub85c \uc6b0\ub9ac\uc758 \uc2e0\uacbd\ub9dd \uc6a9\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4. Lambda \ub294 Sequential \ub85c \uc2e0\uacbd\ub9dd\uc744 \uc815\uc758\ud560 \ub54c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ub808\uc774\uc5b4\ub97c \uc0dd\uc131\ud560 \uac83\uc785\ub2c8\ub2e4. class Lambda(nn.Module): def __init__(self, func): super().__init__() self.func = func def forward(self, x): return self.func(x) def preprocess(x): return x.view(-1, 1, 28, 28) Sequential \ub85c \uc0dd\uc131\ub41c \ubaa8\ub4e4\uc740 \uac04\ub2e8\ud558\uac8c \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4: model = nn.Sequential( Lambda(preprocess), nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.AvgPool2d(4), Lambda(lambda x: x.view(x.size(0), -1)), ) opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) fit(epochs, model, loss_func, opt, train_dl, valid_dl) 0 0.33789493412971494 1 0.2209822019934654 DataLoader \uac10\uc2f8\uae30# \uc6b0\ub9ac\uc758 CNN\uc740 \uc0c1\ub2f9\ud788 \uac04\uacb0\ud558\uc9c0\ub9cc, MNIST\uc5d0\uc11c\ub9cc \uc791\ub3d9\ud569\ub2c8\ub2e4, \uc65c\ub0d0\ud558\uba74: \uc785\ub825\uc774 28*28\uc758 \uae34 \ubca1\ud130\ub77c\uace0 \uac00\uc815\ud569\ub2c8\ub2e4. \ucd5c\uc885\uc801\uc73c\ub85c CNN \uadf8\ub9ac\ub4dc \ud06c\uae30\ub294 4*4 \ub77c\uace0 \uac00\uc815\ud569\ub2c8\ub2e4. (\uc774\uac83\uc740 \uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud55c \ud3c9\uade0 \ud480\ub9c1 \ucee4\ub110 \ud06c\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.) \uc774 \ub450 \uac00\uc9c0 \uac00\uc815\uc744 \uc81c\uac70\ud558\uc5ec \ubaa8\ub378\uc774 \ubaa8\ub4e0 2d \ub2e8\uc77c \ucc44\ub110(channel) \uc774\ubbf8\uc9c0\uc5d0\uc11c \uc791\ub3d9\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. \uba3c\uc800 \ucd08\uae30 Lambda \ub808\uc774\uc5b4\ub97c \uc81c\uac70\ud558\uace0 \ub370\uc774\ud130 \uc804\ucc98\ub9ac\ub97c \uc81c\ub124\ub808\uc774\ud130(generator)\ub85c \uc774\ub3d9\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4: def preprocess(x, y): return x.view(-1, 1, 28, 28), y class WrappedDataLoader: def __init__(self, dl, func): self.dl = dl self.func = func def __len__(self): return len(self.dl) def __iter__(self): for b in self.dl: yield (self.func(*b)) train_dl, valid_dl = get_data(train_ds, valid_ds, bs) train_dl = WrappedDataLoader(train_dl, preprocess) valid_dl = WrappedDataLoader(valid_dl, preprocess) \ub2e4\uc74c\uc73c\ub85c nn.AvgPool2d \ub97c nn.AdaptiveAvgPool2d \ub85c \ub300\uccb4\ud558\uc5ec \uc6b0\ub9ac\uac00 \uac00\uc9c4 \uc785\ub825 \ud150\uc11c\uac00 \uc544\ub2c8\ub77c \uc6d0\ud558\ub294 \ucd9c\ub825 \ud150\uc11c\uc758 \ud06c\uae30\ub97c \uc815\uc758\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c \uc6b0\ub9ac \ubaa8\ub378\uc740 \ubaa8\ub4e0 \ud06c\uae30\uc758 \uc785\ub825\uacfc \ud568\uaed8 \uc791\ub3d9\ud569\ub2c8\ub2e4. model = nn.Sequential( nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1), Lambda(lambda x: x.view(x.size(0), -1)), ) opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) \ud55c\ubc88 \uc2e4\ud589\ud574 \ubd05\uc2dc\ub2e4: fit(epochs, model, loss_func, opt, train_dl, valid_dl) 0 0.37546271090507505 1 0.2893157567501068 \uac00\uc18d\uae30(Accelerator) \uc0ac\uc6a9\ud558\uae30# \ub9cc\uc57d \uc5ec\ub7ec\ubd84\ub4e4\uc774 \uc6b4\uc774 \uc88b\uc544\uc11c CUDA\uc640 \uac19\uc740 \uac00\uc18d\uae30(accelerator)\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4\uba74(\ub300\ubd80\ubd84\uc758 \ud074\ub77c\uc6b0\ub4dc \uc81c\uacf5 \uc5c5\uccb4\uc5d0\uc11c \uc2dc\uac04\ub2f9 \uc57d $0.50 \uc5d0 \uc774\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4), \ucf54\ub4dc \uc2e4\ud589 \uc18d\ub3c4\ub97c \ub192\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uba3c\uc800 \uac00\uc18d\uae30(accelerator)\uac00 PyTorch\uc5d0\uc11c \uc791\ub3d9\ud558\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4: # \ud604\uc7ac \uc0ac\uc6a9 \uac00\ub2a5\ud55c \uac00\uc18d\uae30\uac00 \uc788\ub2e4\uba74 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc544\ub2c8\ub77c\uba74 CPU\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\" print(f\"Using {device} device\") Using cuda device \uac00\uc18d\uae30\ub85c \ubc30\uce58\ub97c \uc62e\uae30\ub3c4\ub85d preprocess \ub97c \uc5c5\ub370\uc774\ud2b8 \ud569\uc2dc\ub2e4: def preprocess(x, y): return x.view(-1, 1, 28, 28).to(device), y.to(device) train_dl, valid_dl = get_data(train_ds, valid_ds, bs) train_dl = WrappedDataLoader(train_dl, preprocess) valid_dl = WrappedDataLoader(valid_dl, preprocess) \ub9c8\uc9c0\ub9c9\uc73c\ub85c \ubaa8\ub378\uc744 \uac00\uc18d\uae30\ub85c \uc774\ub3d9\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4. model.to(device) opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9) \uc774\uc81c \ub354 \ube68\ub9ac \uc2e4\ud589\ub429\ub2c8\ub2e4: fit(epochs, model, loss_func, opt, train_dl, valid_dl) 0 0.22098514339923858 1 0.19568672462701797 \ub9c8\uce58\uba74\uc11c# \uc774\uc81c PyTorch\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc591\ud55c \uc720\ud615\uc758 \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uc77c\ubc18 \ub370\uc774\ud130 \ud30c\uc774\ud504 \ub77c\uc778\uacfc \ud6c8\ub828 \ub8e8\ud504\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uc81c \ubaa8\ub378 \ud559\uc2b5\uc774 \uc5bc\ub9c8\ub098 \uac04\ub2e8\ud55c\uc9c0 \ud655\uc778\ud558\ub824\uba74 mnist_sample \ub178\ud2b8\ubd81 \uc744 \uc0b4\ud3b4\ubcf4\uc138\uc694. \ubb3c\ub860 \ub370\uc774\ud130 \uc99d\uac15(data augmentation), \ucd08\ub9e4\uac1c\ubcc0\uc218 \uc870\uc815(hyperparameter tuning), \ud6c8\ub828\uacfc\uc815 \ubaa8\ub2c8\ud130\ub9c1(monitoring training), \uc804\uc774 \ud559\uc2b5(transfer learning) \ub4f1\uacfc \uac19\uc774 \ucd94\uac00\ud558\uace0 \uc2f6\uc740 \ud56d\ubaa9\ub4e4\uc774 \ub9ce\uc774 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uae30\ub2a5\ub4e4\uc740 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0 \ud45c\uc2dc\ub41c \uac83\uacfc \ub3d9\uc77c\ud55c \uc124\uacc4 \uc811\uadfc \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac1c\ubc1c\ub41c fastai \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, \ubaa8\ub378\uc744 \ub354\uc6b1 \ubc1c\uc804\uc2dc\ud0a4\ub824\ub294 \uc2e4\ubb34\uc790\uc5d0\uac8c \uc790\uc5f0\uc2a4\ub7ec\uc6b4 \ub2e4\uc74c \ub2e8\uacc4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \uc2dc\uc791 \ubd80\ubd84\uc5d0\uc11c torch.nn, torch.optim, Dataset, \uadf8\ub9ac\uace0 DataLoader \uc758 \uac01 \uc608\uc81c\ub97c \ud1b5\ud574 \uc124\uba85\ud558\uaca0\ub2e4\uace0 \uc774\uc57c\uae30\ud588\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\uc81c \uc704\uc758 \ub0b4\uc6a9\ub4e4\uc744 \uc694\uc57d\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4: torch.nn: Module: \ud568\uc218\ucc98\ub7fc \ub3d9\uc791\ud558\uc9c0\ub9cc, \ub610\ud55c \uc0c1\ud0dc(state) (\uc608\ub97c \ub4e4\uc5b4, \uc2e0\uacbd\ub9dd\uc758 \ub808\uc774\uc5b4 \uac00\uc911\uce58)\ub97c \ud3ec\ud568\ud560 \uc218 \uc788\ub294 \ud638\ucd9c \uac00\ub2a5\ud55c \uc624\ube0c\uc81d\ud2b8\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774\ub294 \ud3ec\ud568\ub41c Parameter (\ub4e4)\uac00 \uc5b4\ub5a4 \uac83\uc778\uc9c0 \uc54c\uace0, \ubaa8\ub4e0 \uae30\uc6b8\uae30\ub97c 0\uc73c\ub85c \uc124\uc815\ud558\uace0 \uac00\uc911\uce58 \uc5c5\ub370\uc774\ud2b8 \ub4f1\uc744 \uc704\ud574 \ubc18\ubcf5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Parameter: Module \uc5d0 \uc5ed\uc804\ud30c \ub3d9\uc548 \uc5c5\ub370\uc774\ud2b8\uac00 \ud544\uc694\ud55c \uac00\uc911\uce58\uac00 \uc788\uc74c\uc744 \uc54c\ub824\uc8fc\ub294 \ud150\uc11c\uc6a9 \ub798\ud37c\uc785\ub2c8\ub2e4. requires_grad \uc18d\uc131\uc774 \uc124\uc815\ub41c \ud150\uc11c\ub9cc \uc5c5\ub370\uc774\ud2b8 \ub429\ub2c8\ub2e4. functional: \ud65c\uc131\ud654 \ud568\uc218, \uc190\uc2e4 \ud568\uc218 \ub4f1\uc744 \ud3ec\ud568\ud558\ub294 \ubaa8\ub4c8 (\uad00\ub840\uc5d0 \ub530\ub77c \uc77c\ubc18\uc801\uc73c\ub85c F \ub124\uc784\uc2a4\ud398\uc774\uc2a4\ub85c \uc784\ud3ec\ud2b8 \ub429\ub2c8\ub2e4) \uc774\uace0, \ubb3c\ub860 \ucee8\ubcfc\ub8e8\uc158 \ubc0f \uc120\ud615 \ub808\uc774\uc5b4 \ub4f1\uc5d0 \ub300\ud574\uc11c \uc0c1\ud0dc\ub97c \uc800\uc7a5\ud558\uc9c0\uc54a\ub294(non-stateful) \ubc84\uc804\uc758 \ub808\uc774\uc5b4\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. torch.optim: \uc5ed\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c Parameter \uc758 \uac00\uc911\uce58\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\ub294, SGD \uc640 \uac19\uc740 \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. Dataset: TensorDataset \uacfc \uac19\uc774 PyTorch\uc640 \ud568\uaed8 \uc81c\uacf5\ub418\ub294 \ud074\ub798\uc2a4\ub97c \ud3ec\ud568\ud558\uc5ec __len__ \ubc0f __getitem__ \uc774 \uc788\ub294 \uac1d\uccb4\uc758 \ucd94\uc0c1 \uc778\ud130\ud398\uc774\uc2a4 DataLoader: \ubaa8\ub4e0 \uc885\ub958\uc758 Dataset \uc744 \uae30\ubc18\uc73c\ub85c \ub370\uc774\ud130\uc758 \ubc30\uce58\ub4e4\uc744 \ucd9c\ub825\ud558\ub294 \ubc18\ubcf5\uc790(iterator)\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. Total running time of the script: (1 minutes 38.650 seconds) Download Jupyter notebook: nn_tutorial.ipynb Download Python source code: nn_tutorial.py Download zipped: nn_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/beginner/nn_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>