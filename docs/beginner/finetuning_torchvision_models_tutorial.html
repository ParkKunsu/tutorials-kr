
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="Torchvision 모델의 미세 조정(Finetuning)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/beginner/finetuning_torchvision_models_tutorial.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Author: Nathan Inkawhich 번역: 송채영 이 튜토리얼에서는 1000개의 클래스의 ImageNet 데이터셋에서 사전 학습된 torchvision 모델, 을 미세 조정하고 특징을 추출하는 방법에 대해 자세히 살펴보겠습니다. 여러 최신 CNN 아키텍처로 작업하는 방법을 심도 있게 살펴보고, PyTorch 모델을 미세 조정할 수 있는 직관력을 키울 것입니다. 각 모델의 아키텍처가 다르기 때문에 모든 시나리오에서 작동하는 상용구 형식의 미세 조정 코드는 없습니다. 오히려, 연구자가 기존의 아키텍처를 살펴보고 각 모델에..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="Author: Nathan Inkawhich 번역: 송채영 이 튜토리얼에서는 1000개의 클래스의 ImageNet 데이터셋에서 사전 학습된 torchvision 모델, 을 미세 조정하고 특징을 추출하는 방법에 대해 자세히 살펴보겠습니다. 여러 최신 CNN 아키텍처로 작업하는 방법을 심도 있게 살펴보고, PyTorch 모델을 미세 조정할 수 있는 직관력을 키울 것입니다. 각 모델의 아키텍처가 다르기 때문에 모든 시나리오에서 작동하는 상용구 형식의 미세 조정 코드는 없습니다. 오히려, 연구자가 기존의 아키텍처를 살펴보고 각 모델에..." />
<meta property="og:ignore_canonical" content="true" />

    <title>Torchvision 모델의 미세 조정(Finetuning) &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'beginner/finetuning_torchvision_models_tutorial';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/beginner/finetuning_torchvision_models_tutorial.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Torchvision...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Torchvision 모델의 미세 조정(Finetuning)">
        <meta itemprop="position" content="1">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">beginner/finetuning_torchvision_models_tutorial</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-beginner-finetuning-torchvision-models-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="torchvision-finetuning">
<span id="sphx-glr-beginner-finetuning-torchvision-models-tutorial-py"></span><h1>Torchvision 모델의 미세 조정(Finetuning)<a class="headerlink" href="#torchvision-finetuning" title="Link to this heading">#</a></h1>
<p><strong>Author:</strong> <a class="reference external" href="https://github.com/inkawhich">Nathan Inkawhich</a>
<strong>번역</strong>: <a class="reference external" href="https://github.com/dudtheheaven">송채영</a></p>
<p>이 튜토리얼에서는 1000개의 클래스의 ImageNet 데이터셋에서
사전 학습된 <a class="reference external" href="https://pytorch.org/docs/stable/torchvision/models.html">torchvision 모델</a>, 을 미세 조정하고
특징을 추출하는 방법에 대해 자세히 살펴보겠습니다.
여러 최신 CNN 아키텍처로 작업하는 방법을 심도 있게 살펴보고,
PyTorch 모델을 미세 조정할 수 있는 직관력을 키울 것입니다.
각 모델의 아키텍처가 다르기 때문에 모든 시나리오에서 작동하는
상용구 형식의 미세 조정 코드는 없습니다.
오히려, 연구자가 기존의 아키텍처를 살펴보고 각 모델에 맞게 커스텀 조정을 해야합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1">#</span>
<span class="c1"># 이 문서에서는 두 가지 유형의 전이 학습을 수행합니다. 미세 조정과 특징 추출입니다.</span>
<span class="c1"># **미세 조정** 에서는, 사전 학습된 모델로 시작해</span>
<span class="c1"># 새로운 작업을 위해 모델의 매개변수 *모두* 를 업데이트 하여 본질적으로 전체 모델을 재학습합니다.</span>
<span class="c1"># **특징 추출**에서는, 사전 학습된 모델로 시작해</span>
<span class="c1"># 예측을 도출하는 최종 레이어의 가중치만 업데이트합니다.</span>
<span class="c1"># 사전 학습된 CNN을 고정된 특징 추출기(feature-extractor)로 사용하고</span>
<span class="c1"># 출력 레이어만 변경하기 때문에 이를 특징 추출이라고 합니다.</span>
<span class="c1"># 전송(transfer)에 대한 자세한 기술 정보는</span>
<span class="c1">#  `여기 &lt;https://cs231n.github.io/transfer-learning/&gt;`__ 와</span>
<span class="c1"># `여기 &lt;https://ruder.io/transfer-learning/&gt;`__ 를 재구성합니다.</span>
<span class="c1">#</span>
<span class="c1"># 일반적으로 두 전이 학습 방법 모두 몇 가지 단계를 동일하게 따릅니다.</span>
<span class="c1">#</span>
<span class="c1"># - 사전 훈련된 모델을 초기화합니다.</span>
<span class="c1"># - 최종 레이어를 재구성하여 새 데이터 집합의 클래스 수와 동일한 수의 출력을 갖도록 합니다.</span>
<span class="c1"># - 새 데이터셋의 클래스 수와 동일한 출력 수를 갖도록 최종 레이어를 재구성합니다.</span>
<span class="c1"># - 훈련 중에 업데이트할 매개변수를 최적화 알고리즘에 맞게 정의합니다.</span>
<span class="c1"># - 학습 단계를 실행합니다.</span>
<span class="c1">#</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">division</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;PyTorch Version: &quot;</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Torchvision Version: &quot;</span><span class="p">,</span><span class="n">torchvision</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>PyTorch Version:  2.8.0+cu128
Torchvision Version:  0.23.0+cu128
</pre></div>
</div>
<section id="id1">
<h2>입력<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>실행을 위해 변경할 모든 매개변수는 다음과 같습니다.
<em>hymenoptera_data</em> 데이터셋을 <a class="reference external" href="https://download.pytorch.org/tutorial/hymenoptera_data.zip">여기</a> 에서
다운받아 사용하겠습니다. 이 데이터셋에는
<strong>벌</strong> 과 <strong>개미</strong> 라는 두 개의 클래스가 포함되어 있으며
사용자 정의 데이터셋을 직접 작성하지 않고
<a class="reference external" href="https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder">ImageFolder</a>
데이터셋을 사용할 수 있도록 구조화되어 있습니다.
데이터를 다운로드하고 <code class="docutils literal notranslate"><span class="pre">data_dir</span></code> 입력을 데이터셋의 루트(root) 디렉토리로 설정합니다.
<code class="docutils literal notranslate"><span class="pre">model_name</span></code> 입력은 사용하려는 모델의 이름이며
아래의 목록에서 선택해야 합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">resnet</span><span class="p">,</span> <span class="n">alexnet</span><span class="p">,</span> <span class="n">vgg</span><span class="p">,</span> <span class="n">squeezenet</span><span class="p">,</span> <span class="n">densenet</span><span class="p">,</span> <span class="n">inception</span><span class="p">]</span>
</pre></div>
</div>
<p>다른 입력은 다음과 같습니다. <code class="docutils literal notranslate"><span class="pre">num_classes</span></code> 은 데이터셋의 클래스 수,
<code class="docutils literal notranslate"><span class="pre">batch_size</span></code> 는 훈련에 사용되는 배치 크기로
모델의 성능에 따라 조정할 수 있으며,
<code class="docutils literal notranslate"><span class="pre">num_epochs</span></code> 는 실행하려는 훈련 에폭 수,
<code class="docutils literal notranslate"><span class="pre">feature_extract</span></code> 는 미세 조정 또는 특징 추출 여부를 정의하는 부울(boolean)입니다.
<a href="#id2"><span class="problematic" id="id3">``</span></a>feature_extract = False``이면 모델이 미세 조정되고
모든 모델의 매개변수가 업데이트됩니다.
<a href="#id4"><span class="problematic" id="id5">``</span></a>feature_extract = True``인 경우 마지막 레이어의 매개변수만 업데이트되고
다른 매개변수는 고정된 상태로 유지됩니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 최상위 데이터 디렉토리입니다. 여기서는 디렉토리 형식이</span>
<span class="c1"># ImageFolder 구조를 따른다고 가정합니다.</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="s2">&quot;./data/hymenoptera_data&quot;</span>

<span class="c1"># [resnet, alexnet, vgg, squeezenet, densenet, inception] 이 중 모델을 선택합니다.</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;squeezenet&quot;</span>

<span class="c1"># 데이터 집합의 클래스 수</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># 훈련을 위한 배치 크기 (메모리 용량에 따라 변경됩니다.)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>

<span class="c1"># 훈련할 에폭 수</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">15</span>

<span class="c1"># 특징 추출을 위한 플래그(flag)입니다. False일 경우, 전체 모델을 미세 조정하고</span>
<span class="c1"># True일 경우 재형성된 레이어어의 매개변수만 업데이트합니다.</span>
<span class="n">feature_extract</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</section>
<section id="helper-functions">
<h2>도우미 함수(Helper Functions)<a class="headerlink" href="#helper-functions" title="Link to this heading">#</a></h2>
<p>모델을 조정하는 코드를 작성하기 전에
몇 가지 도우미 함수(Helper Functions)를 정의해 보겠습니다.</p>
<section id="id6">
<h3>모델 훈련 및 검증 코드<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">train_model</span></code> 함수는 주어진 모델의 학습과 검증을 처리합니다.
이 함수는 PyTorch 모델, 데이터로더(dataloader) 딕셔너리, 손실 함수,
옵티마이저, 훈련 및 검증을 위해 정해진 에폭 수,
그리고 Inception 모델일 때를 나타내는 부울 플래그(boolean flag)를 입력으로 받습니다.
이 아키텍처는 보조(auxiliary) 출력을 사용하고, 전체 모델 손실은
<a class="reference external" href="https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958">여기</a> 에 설명된 대로
보조(auxiliary) 출력과 최종 출력을 모두 존중하므로
<em>is_inception</em> 플래그(flag)는 <em>Inception v3</em> 모델을 수용하는 데 사용됩니다.
이 함수는 지정된 에폭 수 동안 학습하고
각 에폭이 끝난 후 전체 검증 단계를 실행합니다.
또한, 검증 정확도 측면에서 가장 성능이 좋은 모델을 추적하고
학습이 끝나면 해당 모델을 반환합니다.
각 에폭이 끝나면 훈련 및 검증 정확도를 볼 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloaders</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">is_inception</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">since</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="n">val_acc_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">best_model_wts</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
    <span class="n">best_acc</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">{}</span><span class="s1">/</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-&#39;</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>

        <span class="c1"># 각 에폭은 학습 단계와 검증 단계를 갖습니다.</span>
        <span class="k">for</span> <span class="n">phase</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">:</span>
                <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># 학습 모드로 모델 설정</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>   <span class="c1"># 평가 모드로 모델 설정</span>

            <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">running_corrects</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># 데이터를 반복</span>
            <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataloaders</span><span class="p">[</span><span class="n">phase</span><span class="p">]:</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="c1"># 매개변수 경사도를 0으로 설정</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

                <span class="c1"># 순방향</span>
                <span class="c1"># 훈련 하는 동안만 기록을 추적합니다.</span>
                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">):</span>
                    <span class="c1"># 모델의 출력을 가져오고 손실을 계산합니다.</span>
                    <span class="c1"># 학습 시 보조(auxiliary) 출력이 있는 inception의 특별한 경우입니다.</span>
                    <span class="c1">#   학습 모드에서는 최종 출력과 보조(auxiliary) 출력을 합산해 손실을 계산하지만</span>
                    <span class="c1">#   테스트에서는 최종 출력만 고려합니다.</span>
                    <span class="k">if</span> <span class="n">is_inception</span> <span class="ow">and</span> <span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">:</span>
                        <span class="c1"># https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958 에서</span>
                        <span class="n">outputs</span><span class="p">,</span> <span class="n">aux_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                        <span class="n">loss1</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                        <span class="n">loss2</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">aux_outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss1</span> <span class="o">+</span> <span class="mf">0.4</span><span class="o">*</span><span class="n">loss2</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

                    <span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                    <span class="c1"># 학습 단계인 경우 역전파 + 최적화</span>
                    <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;train&#39;</span><span class="p">:</span>
                        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                <span class="c1"># 통계</span>
                <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">running_corrects</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">preds</span> <span class="o">==</span> <span class="n">labels</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

            <span class="n">epoch_loss</span> <span class="o">=</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="n">phase</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
            <span class="n">epoch_acc</span> <span class="o">=</span> <span class="n">running_corrects</span><span class="o">.</span><span class="n">double</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="n">phase</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>

            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> Loss: </span><span class="si">{:.4f}</span><span class="s1"> Acc: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">phase</span><span class="p">,</span> <span class="n">epoch_loss</span><span class="p">,</span> <span class="n">epoch_acc</span><span class="p">))</span>

            <span class="c1"># 모델을 깊은 복사(deep copy)함</span>
            <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;val&#39;</span> <span class="ow">and</span> <span class="n">epoch_acc</span> <span class="o">&gt;</span> <span class="n">best_acc</span><span class="p">:</span>
                <span class="n">best_acc</span> <span class="o">=</span> <span class="n">epoch_acc</span>
                <span class="n">best_model_wts</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">phase</span> <span class="o">==</span> <span class="s1">&#39;val&#39;</span><span class="p">:</span>
                <span class="n">val_acc_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_acc</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">()</span>

    <span class="n">time_elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">since</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training complete in </span><span class="si">{:.0f}</span><span class="s1">m </span><span class="si">{:.0f}</span><span class="s1">s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time_elapsed</span> <span class="o">//</span> <span class="mi">60</span><span class="p">,</span> <span class="n">time_elapsed</span> <span class="o">%</span> <span class="mi">60</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best val Acc: </span><span class="si">{:4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best_acc</span><span class="p">))</span>

    <span class="c1"># 최고의 모델 가중치 불러오기</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">best_model_wts</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">val_acc_history</span>
</pre></div>
</div>
</section>
<section id="requires-grad">
<h3>모델 매개변수의 .requires_grad 속성 설정<a class="headerlink" href="#requires-grad" title="Link to this heading">#</a></h3>
<p>이 도우미 함수(Helper Functions)는 특징 추출 시
모델에 있는 매개변수의 <code class="docutils literal notranslate"><span class="pre">.requires_grad</span></code> 속성을 False로 설정합니다.
기본적으로, 사전 학습된 모델을 읽어 들일 때 모든 매개변수가
<a href="#id7"><span class="problematic" id="id8">``</span></a>.requires_grad=True``로 설정되어 있으므로
처음부터 학습하거나 미세 조정하는 경우라면 괜찮습니다.
그러나 특징 추출 중이고 새로 초기화된 레이어에 대한 경사도만 계산하려는 경우
다른 모든 매개변수에는 경사도가 필요하지 않아야 합니다.
이것은 나중에 더 이해를 할 수 있을 것입니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_parameter_requires_grad</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">feature_extracting</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">feature_extracting</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</section>
</section>
<section id="id9">
<h2>네트워크 초기화 및 재구성하기<a class="headerlink" href="#id9" title="Link to this heading">#</a></h2>
<p>이제 가장 흥미로운 부분입니다.
여기서는 각 네트워크의 재구성을 처리합니다.
이 절차는 자동 절차가 아니며 각 모델마다 고유합니다.
CNN 모델의 최종 레이어(FC layer라고도 불림)는
데이터셋의 출력 클래스 수와 동일한 수의 노드를 가지고 있습니다.
모든 모델은 이미 ImageNet에서 사전 학습 되었기 때문에
각 클래스당 하나의 노드씩 1000 크기의 출력 레이어를 가지고 있습니다.
여기서의 목표는 이전과 동일한 수의 입력을 갖고,
데이터셋의 클래스 수와 동일한 수의 출력을 갖도록 마지막 레이어를 재구성하는 것입니다.
다음 섹션에서는 각 모델의 아키텍처를
개별적으로 변경하는 방법에 대해 설명하겠습니다.
하지만 먼저, 미세 조정과 특징 추출의 차이점에 대한
한 가지 중요한 세부 사항이 있습니다.</p>
<p>특징 추출 시 마지막 레이어의 매개변수만 업데이트 하고 싶을 때
다시 말해, 재구성하는 레이어의 매개변수만 업데이트를 하고 싶은 경우가 있습니다.
이런 경우에는 변경하지 않는 매개변수의 경사도를 계산할 필요가 없으므로
효율성을 위해 .requires_grad 속성을 False로 설정합니다.
기본적으로 이 속성은 True로 설정되어 있기 때문에 이 설정은 중요합니다.
그런 다음 새 레이어를 초기화할 때 기본적으로 새 매개변수에는 <a href="#id10"><span class="problematic" id="id11">``</span></a>.requires_grad=True``가 있으므로
새 레이어의 매개변수만 업데이트됩니다.
미세 조정할 때는 모든 .requires_grad를 기본값인 True로 설정할 수 있습니다.</p>
<p>마지막으로, inception_v3는 입력 크기를 (299,299)로 요구하지만,
다른 모든 모델은 (224,224)를 기대한다는 점을 기억하세요.</p>
<section id="resnet">
<h3>Resnet<a class="headerlink" href="#resnet" title="Link to this heading">#</a></h3>
<p>Resnet은 ‘Deep Residual Learning for Image Recognition
&lt;<a class="reference external" href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a>&gt;`__ 논문에서 소개되었습니다.
Resnet18, Resnet34, Resnet50, Resnet101, and Resnet152 등 다양한 크기의 여러 가지 변형이 있으며
모두 torchvision 모델에서 사용할 수 있습니다.
여기서는 데이터셋이 작고 클래스가 두 개 뿐인 Resnet18을 사용합니다.
모델을 출력하면 아래 그림과 같이
마지막 레이어가 완전히 연결된 레이어임을 알 수 있습니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">fc</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>입력 특징이 512개, 출력 특징이 2개인 선형 레이어가 되도록
<a href="#id12"><span class="problematic" id="id13">``</span></a>model.fc``를 다시 초기화해야 합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="alexnet">
<h3>Alexnet<a class="headerlink" href="#alexnet" title="Link to this heading">#</a></h3>
<p>Alexnet은 <a class="reference external" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep
Convolutional Neural
Networks</a> 논문에 소개된 바 있으며
ImageNet 데이터셋에서 최초로 매우 성공적인 CNN을 구현한 바 있습니다.
모델의 아키텍처를 출력하면 모델 출력이
분류기(classifier)의 6번째 레이어에서 나오는 것을 볼 수 있습니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">classifier</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="o">...</span>
    <span class="p">(</span><span class="mi">6</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
 <span class="p">)</span>
</pre></div>
</div>
<p>데이터셋과 함께 모델을 사용하려면 이 레이어를 다음과 같이 다시 초기화합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="vgg">
<h3>VGG<a class="headerlink" href="#vgg" title="Link to this heading">#</a></h3>
<p>VGG는 <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">Very Deep Convolutional Networks for
Large-Scale Image Recognition</a> 논문에서 소개되었습니다.
Torchvision 다양한 길이와 배치 정규화 레이어가 있는
8가지 버전의 VGG를 제공합니다.
여기서는 배치 정규화 기능이 있는 VGG-11을 사용합니다.
출력 레이어는 Alexnet과 유사합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">classifier</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="o">...</span>
    <span class="p">(</span><span class="mi">6</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
 <span class="p">)</span>
</pre></div>
</div>
<p>따라서 동일한 기술을 사용해 출력 레이어를 수정합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="squeezenet">
<h3>Squeezenet<a class="headerlink" href="#squeezenet" title="Link to this heading">#</a></h3>
<p>Squeeznet 아키텍처는 <a class="reference external" href="https://arxiv.org/abs/1602.07360">SqueezeNet:
AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model
size</a> 논문에 설명되어 있고,
AlexNet 수준의 정확도를 제공하면서
여기에 표시된 모델들과는 다른 출력 구조를 사용합니다.
Torchvision에는 두 가지 버전의 Squeezenet이 있고 여기서는 1.0 버전을 사용합니다.
출력은 분류기(classifier)의 첫 번째 레이어인
1x1 합성곱 레이어에서 나옵니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">classifier</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
 <span class="p">)</span>
</pre></div>
</div>
<p>네트워크를 수정하기 위해 Conv2d 레이어를 다시 초기화하여
깊이 2의 특징 맵을 다음과 같이 출력합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="densenet">
<h3>Densenet<a class="headerlink" href="#densenet" title="Link to this heading">#</a></h3>
<p>Densenet <a class="reference external" href="https://arxiv.org/abs/1608.06993">Densely Connected Convolutional
Networks</a> 논문에서 소개되었습니다.
Torchvision에는 4가지의 변형 Densenet이 있지만
여기서는 Densenet-121만 사용합니다.
출력 레이는 1024개의 입력 특징을 가진 선형 레이어 입니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">classifier</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>네트워크를 재구성하기 위해 분류기(classifier)의 선형 레이어를
다음과 같이 다시 초기화합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="inception-v3">
<h3>Inception v3<a class="headerlink" href="#inception-v3" title="Link to this heading">#</a></h3>
<p>마지막으로 Inception v3은 <a class="reference external" href="https://arxiv.org/pdf/1512.00567v1.pdf">Rethinking the Inception
Architecture for Computer
Vision</a> 에서 처음 설명했습니다.
이 네트워크는 학습 시 두 개의 출력 레이어가 있다는 점이 독특합니다.
두 번째 출력은 보조(axuiliary) 출력으로 알려져 있으며
네트워크의 AuxLogits 부분에 포함되어 있습니다.
기본 출력은 네트워크 끝에 있는 선형 레이어이며
테스트할 때는 기본 출력만 고려합니다.
읽어 들인 모델의 보조(auxiliary) 출력과 기본 출력은 다음과 같이 출력됩니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">AuxLogits</span><span class="p">):</span> <span class="n">InceptionAux</span><span class="p">(</span>
    <span class="o">...</span>
    <span class="p">(</span><span class="n">fc</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
 <span class="p">)</span>
 <span class="o">...</span>
<span class="p">(</span><span class="n">fc</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>이 모델을 미세 조정하려면 두 레이어를
다음과 같이 모두 재구성해야 합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">AuxLogits</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<p>많은 모델이 비슷한 출력 구조를 가지고 있지만,
각각은 약간 다르게 처리되어야 합니다.
또한, 재구성된 네트워크의 출력 모델 구조를 확인하고
출력 기능의 수가 데이터셋의 클래스 수와 동일한지 확인해야 합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">initialize_model</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">feature_extract</span><span class="p">,</span> <span class="n">use_pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="c1"># 이 if 문에서 설정할 변수를 초기화 합니다.</span>
    <span class="c1"># 각 변수는 모델에 따라 다릅니다.</span>
    <span class="n">model_ft</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s2">&quot;resnet&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Resnet18</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_ft</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="n">use_pretrained</span><span class="p">)</span>
        <span class="n">set_parameter_requires_grad</span><span class="p">(</span><span class="n">model_ft</span><span class="p">,</span> <span class="n">feature_extract</span><span class="p">)</span>
        <span class="n">num_ftrs</span> <span class="o">=</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span>
        <span class="n">model_ft</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_ftrs</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="mi">224</span>

    <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s2">&quot;alexnet&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Alexnet</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_ft</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">alexnet</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="n">use_pretrained</span><span class="p">)</span>
        <span class="n">set_parameter_requires_grad</span><span class="p">(</span><span class="n">model_ft</span><span class="p">,</span> <span class="n">feature_extract</span><span class="p">)</span>
        <span class="n">num_ftrs</span> <span class="o">=</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">in_features</span>
        <span class="n">model_ft</span><span class="o">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_ftrs</span><span class="p">,</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="mi">224</span>

    <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s2">&quot;vgg&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; VGG11_bn</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_ft</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">vgg11_bn</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="n">use_pretrained</span><span class="p">)</span>
        <span class="n">set_parameter_requires_grad</span><span class="p">(</span><span class="n">model_ft</span><span class="p">,</span> <span class="n">feature_extract</span><span class="p">)</span>
        <span class="n">num_ftrs</span> <span class="o">=</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">in_features</span>
        <span class="n">model_ft</span><span class="o">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_ftrs</span><span class="p">,</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="mi">224</span>

    <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s2">&quot;squeezenet&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Squeezenet</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_ft</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">squeezenet1_0</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="n">use_pretrained</span><span class="p">)</span>
        <span class="n">set_parameter_requires_grad</span><span class="p">(</span><span class="n">model_ft</span><span class="p">,</span> <span class="n">feature_extract</span><span class="p">)</span>
        <span class="n">model_ft</span><span class="o">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">model_ft</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="mi">224</span>

    <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s2">&quot;densenet&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Densenet</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_ft</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">densenet121</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="n">use_pretrained</span><span class="p">)</span>
        <span class="n">set_parameter_requires_grad</span><span class="p">(</span><span class="n">model_ft</span><span class="p">,</span> <span class="n">feature_extract</span><span class="p">)</span>
        <span class="n">num_ftrs</span> <span class="o">=</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">in_features</span>
        <span class="n">model_ft</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_ftrs</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="mi">224</span>

    <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s2">&quot;inception&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Inception v3</span>
<span class="sd">        Be careful, expects (299,299) sized images and has auxiliary output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">model_ft</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">inception_v3</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="n">use_pretrained</span><span class="p">)</span>
        <span class="n">set_parameter_requires_grad</span><span class="p">(</span><span class="n">model_ft</span><span class="p">,</span> <span class="n">feature_extract</span><span class="p">)</span>
        <span class="c1"># 보조 네트워크(auxilary net) 처리</span>
        <span class="n">num_ftrs</span> <span class="o">=</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">AuxLogits</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span>
        <span class="n">model_ft</span><span class="o">.</span><span class="n">AuxLogits</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_ftrs</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="c1"># 주 네트워크(primary net) 처리</span>
        <span class="n">num_ftrs</span> <span class="o">=</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span>
        <span class="n">model_ft</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_ftrs</span><span class="p">,</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="mi">299</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Invalid model name, exiting...&quot;</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">model_ft</span><span class="p">,</span> <span class="n">input_size</span>

<span class="c1"># 실행을 위한 모델 초기화</span>
<span class="n">model_ft</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">feature_extract</span><span class="p">,</span> <span class="n">use_pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 방금 인스턴스화한 모델 출력</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_ft</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning:

The parameter &#39;pretrained&#39; is deprecated since 0.13 and may be removed in the future, please use &#39;weights&#39; instead.

/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning:

Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_0_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_0_Weights.DEFAULT` to get the most up-to-date weights.

Downloading: &quot;https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth&quot; to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth

  0%|          | 0.00/4.78M [00:00&lt;?, ?B/s]
100%|██████████| 4.78M/4.78M [00:00&lt;00:00, 79.8MB/s]
SqueezeNet(
  (features): Sequential(
    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    (3): Fire(
      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (4): Fire(
      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (5): Fire(
      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    (7): Fire(
      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (8): Fire(
      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (9): Fire(
      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (10): Fire(
      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
    (12): Fire(
      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
      (squeeze_activation): ReLU(inplace=True)
      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      (expand1x1_activation): ReLU(inplace=True)
      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (expand3x3_activation): ReLU(inplace=True)
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))
    (2): ReLU(inplace=True)
    (3): AdaptiveAvgPool2d(output_size=(1, 1))
  )
)
</pre></div>
</div>
</section>
</section>
<section id="id14">
<h2>데이터 읽어 들이기<a class="headerlink" href="#id14" title="Link to this heading">#</a></h2>
<p>입력 크기를 알았으니 이제 데이터 전이(transform), 이미지 데이터셋,
그리고 데이터로더(dataloader)를 초기화할 수 있습니다.
<a class="reference external" href="https://pytorch.org/docs/master/torchvision/models.html">여기</a> 에서 설명된 대로
모델은 하드코딩(hard-coded)된 정규화 값으로 사전 학습되었습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 학습을 위한 데이터 증강 및 정규화</span>
<span class="c1"># 검증을 위한 정규화만 수행</span>
<span class="n">data_transforms</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="n">input_size</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
    <span class="p">]),</span>
    <span class="s1">&#39;val&#39;</span><span class="p">:</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">input_size</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="n">input_size</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
    <span class="p">]),</span>
<span class="p">}</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing Datasets and Dataloaders...&quot;</span><span class="p">)</span>

<span class="c1"># 학습 및 검증 데이터셋 생성</span>
<span class="n">image_datasets</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">data_transforms</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">]}</span>
<span class="c1"># 학습 및 검증 데이터로더(dataloader) 생성</span>
<span class="n">dataloaders_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">image_datasets</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">]}</span>

<span class="c1"># 사용 가능한 GPU 탐지</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Initializing Datasets and Dataloaders...
</pre></div>
</div>
</section>
<section id="id15">
<h2>옵티마이저 생성<a class="headerlink" href="#id15" title="Link to this heading">#</a></h2>
<p>이제 모델 구조가 정확해졌으니, 미세 조정 및 특징 추출을 위한 마지막 단계는
원하는 매개변수만 업데이트하는 옵티마이저를 생성하는 것입니다.
사전 학습된 모델을 읽어 들인 후 구조를 재조정하기 전에
<code class="docutils literal notranslate"><span class="pre">feature_extract=True``인</span> <span class="pre">경우</span> <span class="pre">매개변수의</span>
<span class="pre">모든</span> <span class="pre">``.requires_grad</span></code> 속성을 일일이 False로 설정한 것을 기억하세요.
그러면 재초기화된 레이어의 파라미터는
기본적으로 <a href="#id16"><span class="problematic" id="id17">``</span></a>.requires_grad=True``를 갖습니다.
이제 <em>.requires_grad=True인 모든 매개변수가
최적화되어야 한다는 것을 알았습니다.</em>
다음으로 이러한 매개변수 목록을 만들고
이 목록을 SGD 알고리즘 생성자(constructor)에 입력합니다.</p>
<p>이를 확인하려면 출력된 매개변수를 확인하여 학습하세요.
미세 조정할 때 이 목록은 길어야 하며
모든 모델의 매개변수를 포함해야 합니다.
하지만, 특징을 추출할 때는 이 목록이 짧아야 하며
재구성된 레이어의 가중치와 편향(bias)만 포함해야 합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># GPU로 모델 전송</span>
<span class="n">model_ft</span> <span class="o">=</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 이 실행에서 최적화/업데이트할 매개변수를 수집합니다.</span>
<span class="c1">#  미세 조정을 하는 경우 모든 매개변수를 업데이트합니다.</span>
<span class="c1">#  하지만, 특징 추출 방법을 사용하는 경우에는</span>
<span class="c1">#  방금 초기화한 매개변수, 즉 requires_grad가 Ture인 매개변수만 업데이트합니다.</span>

<span class="n">params_to_update</span> <span class="o">=</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Params to learn:&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">feature_extract</span><span class="p">:</span>
    <span class="n">params_to_update</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span><span class="n">param</span> <span class="ow">in</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">params_to_update</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">name</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span><span class="n">param</span> <span class="ow">in</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">name</span><span class="p">)</span>

<span class="c1"># 모든 매개변수가 최적화되고 있는지 확인합니다.</span>
<span class="n">optimizer_ft</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params_to_update</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Params to learn:
         classifier.1.weight
         classifier.1.bias
</pre></div>
</div>
</section>
<section id="id18">
<h2>학습 및 검증 단계 실행<a class="headerlink" href="#id18" title="Link to this heading">#</a></h2>
<p>마지막 단계는 모델에 대한 손실을 설정한 다음
설정된 에폭 수에 대해 학습 및 검증 함수(validation function)를 실행하는 것입니다.
이 단계는 에폭 수에 따라 CPU에서는 시간이 걸릴 수 있습니다.
또한, 기본 학습률은 모든 모델에 최적이 아니므로
최대 정확도를 얻으려면 각 모델에 대해 개별적으로 조정해야 합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 손실 함수 설정</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># 학습 및 평가</span>
<span class="n">model_ft</span><span class="p">,</span> <span class="n">hist</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">model_ft</span><span class="p">,</span> <span class="n">dataloaders_dict</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer_ft</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">is_inception</span><span class="o">=</span><span class="p">(</span><span class="n">model_name</span><span class="o">==</span><span class="s2">&quot;inception&quot;</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 0/14
----------
train Loss: 0.5197 Acc: 0.7377
val Loss: 0.3218 Acc: 0.8889

Epoch 1/14
----------
train Loss: 0.3047 Acc: 0.8852
val Loss: 0.2838 Acc: 0.8954

Epoch 2/14
----------
train Loss: 0.2320 Acc: 0.8893
val Loss: 0.2880 Acc: 0.9150

Epoch 3/14
----------
train Loss: 0.2131 Acc: 0.9262
val Loss: 0.2946 Acc: 0.9216

Epoch 4/14
----------
train Loss: 0.1815 Acc: 0.9262
val Loss: 0.3670 Acc: 0.9150

Epoch 5/14
----------
train Loss: 0.1677 Acc: 0.9221
val Loss: 0.3186 Acc: 0.9216

Epoch 6/14
----------
train Loss: 0.1863 Acc: 0.9262
val Loss: 0.3685 Acc: 0.8954

Epoch 7/14
----------
train Loss: 0.1349 Acc: 0.9385
val Loss: 0.3709 Acc: 0.9020

Epoch 8/14
----------
train Loss: 0.1751 Acc: 0.9098
val Loss: 0.5201 Acc: 0.8497

Epoch 9/14
----------
train Loss: 0.2799 Acc: 0.8730
val Loss: 0.3116 Acc: 0.9150

Epoch 10/14
----------
train Loss: 0.1438 Acc: 0.9549
val Loss: 0.3341 Acc: 0.9281

Epoch 11/14
----------
train Loss: 0.1545 Acc: 0.9303
val Loss: 0.3258 Acc: 0.9085

Epoch 12/14
----------
train Loss: 0.1401 Acc: 0.9344
val Loss: 0.3512 Acc: 0.9020

Epoch 13/14
----------
train Loss: 0.1455 Acc: 0.9303
val Loss: 0.3301 Acc: 0.9216

Epoch 14/14
----------
train Loss: 0.1322 Acc: 0.9426
val Loss: 0.3269 Acc: 0.9346

Training complete in 0m 11s
Best val Acc: 0.934641
</pre></div>
</div>
</section>
<section id="id19">
<h2>처음부터 학습된 모델과의 비교<a class="headerlink" href="#id19" title="Link to this heading">#</a></h2>
<p>재미로, 전이 학습을 사용하지 않을 경우
모델이 어떻게 학습하는지 살펴봅시다.
미세 조정과 특징 추출의 성능은 데이터셋에 따라 크게 다르지만
일반적으로 두 전이 학습 방법은 처음부터 학습한 모델에 비해
학습 시간 및 전반적인 정확도 측면에서 유리한 결과를 제공합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 이 실행에 사용된 모델의 사전 학습되지 않은 버전을 초기화합니다.</span>
<span class="n">scratch_model</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">feature_extract</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">scratch_model</span> <span class="o">=</span> <span class="n">scratch_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">scratch_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">scratch_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scratch_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">_</span><span class="p">,</span><span class="n">scratch_hist</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">scratch_model</span><span class="p">,</span> <span class="n">dataloaders_dict</span><span class="p">,</span> <span class="n">scratch_criterion</span><span class="p">,</span> <span class="n">scratch_optimizer</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">is_inception</span><span class="o">=</span><span class="p">(</span><span class="n">model_name</span><span class="o">==</span><span class="s2">&quot;inception&quot;</span><span class="p">))</span>

<span class="c1"># 전이 학습 방법과 처음부터 학습된 모델에 대한</span>
<span class="c1"># 검증 정확도 vs. 학습 에폭 수에 대한 학습 곡선을 표시합니다.</span>
<span class="n">ohist</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">shist</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">ohist</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">hist</span><span class="p">]</span>
<span class="n">shist</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">scratch_hist</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Validation Accuracy vs. Number of Training Epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Training Epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Validation Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">ohist</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Pretrained&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">shist</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Scratch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_finetuning_torchvision_models_tutorial_001.png" srcset="../_images/sphx_glr_finetuning_torchvision_models_tutorial_001.png" alt="Validation Accuracy vs. Number of Training Epochs" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning:

Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.

Epoch 0/14
----------
train Loss: 0.7011 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 1/14
----------
train Loss: 0.6936 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 2/14
----------
train Loss: 0.6951 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 3/14
----------
train Loss: 0.6930 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 4/14
----------
train Loss: 0.6931 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 5/14
----------
train Loss: 0.6931 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 6/14
----------
train Loss: 0.6931 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 7/14
----------
train Loss: 0.6931 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 8/14
----------
train Loss: 0.6931 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 9/14
----------
train Loss: 0.6931 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 10/14
----------
train Loss: 0.6931 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 11/14
----------
train Loss: 0.6931 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 12/14
----------
train Loss: 0.6931 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 13/14
----------
train Loss: 0.6931 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Epoch 14/14
----------
train Loss: 0.6931 Acc: 0.5041
val Loss: 0.6931 Acc: 0.4575

Training complete in 0m 13s
Best val Acc: 0.457516
</pre></div>
</div>
</section>
<section id="id20">
<h2>최종 생각과 앞으로의 방향<a class="headerlink" href="#id20" title="Link to this heading">#</a></h2>
<p>다른 모델 몇 가지를 실행해보고 정확도가 얼마나 좋아지는지 확인해 보세요.
또한, 역방향 패스에서는 대부분의 변화도를 계산할 필요가 없기 때문에
특징 추출에 시간이 덜 걸린다는 점에 주목하세요.
여기에서 할 수 있는 것은 많으며 다음과 같이 할 수 있습니다.</p>
<ul class="simple">
<li><p>더 어려운 데이터 집합으로 이 코드를 실행하고
전이 학습의 몇 가지 이점을 더 확인해 보세요.</p></li>
<li><p>여기에 설명된 방법을 사용하거나 전이 학습을 사용하여
새로운 domain(예: NLP, 오디오 등)에서 다른 모델을 업데이트합니다.</p></li>
<li><p>모델이 만족하면 ONNX 모델로 내보내거나 하이브리드 프론트엔드를 사용해
더 빠른 속도와 최적화 기회를 얻을 수 있습니다.</p></li>
</ul>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 28.330 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-finetuning-torchvision-models-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/a578ec7a4897bae05854d7036ad0ad1d/finetuning_torchvision_models_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">finetuning_torchvision_models_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/6a05c032f283b687694821232af906a7/finetuning_torchvision_models_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">finetuning_torchvision_models_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/a81f470ffd2c9ea33796069dd179b5c1/finetuning_torchvision_models_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">finetuning_torchvision_models_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">입력</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#helper-functions">도우미 함수(Helper Functions)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">모델 훈련 및 검증 코드</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#requires-grad">모델 매개변수의 .requires_grad 속성 설정</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">네트워크 초기화 및 재구성하기</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet">Resnet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet">Alexnet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vgg">VGG</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#squeezenet">Squeezenet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#densenet">Densenet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inception-v3">Inception v3</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">데이터 읽어 들이기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">옵티마이저 생성</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">학습 및 검증 단계 실행</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">처음부터 학습된 모델과의 비교</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">최종 생각과 앞으로의 방향</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Torchvision \ubaa8\ub378\uc758 \ubbf8\uc138 \uc870\uc815(Finetuning)",
       "headline": "Torchvision \ubaa8\ub378\uc758 \ubbf8\uc138 \uc870\uc815(Finetuning)",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/beginner/finetuning_torchvision_models_tutorial.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. Torchvision \ubaa8\ub378\uc758 \ubbf8\uc138 \uc870\uc815(Finetuning)# Author: Nathan Inkawhich \ubc88\uc5ed: \uc1a1\ucc44\uc601 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 1000\uac1c\uc758 \ud074\ub798\uc2a4\uc758 ImageNet \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc0ac\uc804 \ud559\uc2b5\ub41c torchvision \ubaa8\ub378, \uc744 \ubbf8\uc138 \uc870\uc815\ud558\uace0 \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574 \uc790\uc138\ud788 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc5ec\ub7ec \ucd5c\uc2e0 CNN \uc544\ud0a4\ud14d\ucc98\ub85c \uc791\uc5c5\ud558\ub294 \ubc29\ubc95\uc744 \uc2ec\ub3c4 \uc788\uac8c \uc0b4\ud3b4\ubcf4\uace0, PyTorch \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud560 \uc218 \uc788\ub294 \uc9c1\uad00\ub825\uc744 \ud0a4\uc6b8 \uac83\uc785\ub2c8\ub2e4. \uac01 \ubaa8\ub378\uc758 \uc544\ud0a4\ud14d\ucc98\uac00 \ub2e4\ub974\uae30 \ub54c\ubb38\uc5d0 \ubaa8\ub4e0 \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uc791\ub3d9\ud558\ub294 \uc0c1\uc6a9\uad6c \ud615\uc2dd\uc758 \ubbf8\uc138 \uc870\uc815 \ucf54\ub4dc\ub294 \uc5c6\uc2b5\ub2c8\ub2e4. \uc624\ud788\ub824, \uc5f0\uad6c\uc790\uac00 \uae30\uc874\uc758 \uc544\ud0a4\ud14d\ucc98\ub97c \uc0b4\ud3b4\ubcf4\uace0 \uac01 \ubaa8\ub378\uc5d0 \ub9de\uac8c \ucee4\uc2a4\ud140 \uc870\uc815\uc744 \ud574\uc57c\ud569\ub2c8\ub2e4. # # \uc774 \ubb38\uc11c\uc5d0\uc11c\ub294 \ub450 \uac00\uc9c0 \uc720\ud615\uc758 \uc804\uc774 \ud559\uc2b5\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \ubbf8\uc138 \uc870\uc815\uacfc \ud2b9\uc9d5 \ucd94\ucd9c\uc785\ub2c8\ub2e4. # **\ubbf8\uc138 \uc870\uc815** \uc5d0\uc11c\ub294, \uc0ac\uc804 \ud559\uc2b5\ub41c \ubaa8\ub378\ub85c \uc2dc\uc791\ud574 # \uc0c8\ub85c\uc6b4 \uc791\uc5c5\uc744 \uc704\ud574 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218 *\ubaa8\ub450* \ub97c \uc5c5\ub370\uc774\ud2b8 \ud558\uc5ec \ubcf8\uc9c8\uc801\uc73c\ub85c \uc804\uccb4 \ubaa8\ub378\uc744 \uc7ac\ud559\uc2b5\ud569\ub2c8\ub2e4. # **\ud2b9\uc9d5 \ucd94\ucd9c**\uc5d0\uc11c\ub294, \uc0ac\uc804 \ud559\uc2b5\ub41c \ubaa8\ub378\ub85c \uc2dc\uc791\ud574 # \uc608\uce21\uc744 \ub3c4\ucd9c\ud558\ub294 \ucd5c\uc885 \ub808\uc774\uc5b4\uc758 \uac00\uc911\uce58\ub9cc \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4. # \uc0ac\uc804 \ud559\uc2b5\ub41c CNN\uc744 \uace0\uc815\ub41c \ud2b9\uc9d5 \ucd94\ucd9c\uae30(feature-extractor)\ub85c \uc0ac\uc6a9\ud558\uace0 # \ucd9c\ub825 \ub808\uc774\uc5b4\ub9cc \ubcc0\uacbd\ud558\uae30 \ub54c\ubb38\uc5d0 \uc774\ub97c \ud2b9\uc9d5 \ucd94\ucd9c\uc774\ub77c\uace0 \ud569\ub2c8\ub2e4. # \uc804\uc1a1(transfer)\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uae30\uc220 \uc815\ubcf4\ub294 # `\uc5ec\uae30 \u003chttps://cs231n.github.io/transfer-learning/\u003e`__ \uc640 # `\uc5ec\uae30 \u003chttps://ruder.io/transfer-learning/\u003e`__ \ub97c \uc7ac\uad6c\uc131\ud569\ub2c8\ub2e4. # # \uc77c\ubc18\uc801\uc73c\ub85c \ub450 \uc804\uc774 \ud559\uc2b5 \ubc29\ubc95 \ubaa8\ub450 \uba87 \uac00\uc9c0 \ub2e8\uacc4\ub97c \ub3d9\uc77c\ud558\uac8c \ub530\ub985\ub2c8\ub2e4. # # - \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378\uc744 \ucd08\uae30\ud654\ud569\ub2c8\ub2e4. # - \ucd5c\uc885 \ub808\uc774\uc5b4\ub97c \uc7ac\uad6c\uc131\ud558\uc5ec \uc0c8 \ub370\uc774\ud130 \uc9d1\ud569\uc758 \ud074\ub798\uc2a4 \uc218\uc640 \ub3d9\uc77c\ud55c \uc218\uc758 \ucd9c\ub825\uc744 \uac16\ub3c4\ub85d \ud569\ub2c8\ub2e4. # - \uc0c8 \ub370\uc774\ud130\uc14b\uc758 \ud074\ub798\uc2a4 \uc218\uc640 \ub3d9\uc77c\ud55c \ucd9c\ub825 \uc218\ub97c \uac16\ub3c4\ub85d \ucd5c\uc885 \ub808\uc774\uc5b4\ub97c \uc7ac\uad6c\uc131\ud569\ub2c8\ub2e4. # - \ud6c8\ub828 \uc911\uc5d0 \uc5c5\ub370\uc774\ud2b8\ud560 \ub9e4\uac1c\ubcc0\uc218\ub97c \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc5d0 \ub9de\uac8c \uc815\uc758\ud569\ub2c8\ub2e4. # - \ud559\uc2b5 \ub2e8\uacc4\ub97c \uc2e4\ud589\ud569\ub2c8\ub2e4. # from __future__ import print_function from __future__ import division import torch import torch.nn as nn import torch.optim as optim import numpy as np import torchvision from torchvision import datasets, models, transforms import matplotlib.pyplot as plt import time import os import copy print(\"PyTorch Version: \",torch.__version__) print(\"Torchvision Version: \",torchvision.__version__) PyTorch Version: 2.8.0+cu128 Torchvision Version: 0.23.0+cu128 \uc785\ub825# \uc2e4\ud589\uc744 \uc704\ud574 \ubcc0\uacbd\ud560 \ubaa8\ub4e0 \ub9e4\uac1c\ubcc0\uc218\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. hymenoptera_data \ub370\uc774\ud130\uc14b\uc744 \uc5ec\uae30 \uc5d0\uc11c \ub2e4\uc6b4\ubc1b\uc544 \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\uc14b\uc5d0\ub294 \ubc8c \uacfc \uac1c\ubbf8 \ub77c\ub294 \ub450 \uac1c\uc758 \ud074\ub798\uc2a4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70 \uc0ac\uc6a9\uc790 \uc815\uc758 \ub370\uc774\ud130\uc14b\uc744 \uc9c1\uc811 \uc791\uc131\ud558\uc9c0 \uc54a\uace0 ImageFolder \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \uad6c\uc870\ud654\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ub370\uc774\ud130\ub97c \ub2e4\uc6b4\ub85c\ub4dc\ud558\uace0 data_dir \uc785\ub825\uc744 \ub370\uc774\ud130\uc14b\uc758 \ub8e8\ud2b8(root) \ub514\ub809\ud1a0\ub9ac\ub85c \uc124\uc815\ud569\ub2c8\ub2e4. model_name \uc785\ub825\uc740 \uc0ac\uc6a9\ud558\ub824\ub294 \ubaa8\ub378\uc758 \uc774\ub984\uc774\uba70 \uc544\ub798\uc758 \ubaa9\ub85d\uc5d0\uc11c \uc120\ud0dd\ud574\uc57c \ud569\ub2c8\ub2e4. [resnet, alexnet, vgg, squeezenet, densenet, inception] \ub2e4\ub978 \uc785\ub825\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. num_classes \uc740 \ub370\uc774\ud130\uc14b\uc758 \ud074\ub798\uc2a4 \uc218, batch_size \ub294 \ud6c8\ub828\uc5d0 \uc0ac\uc6a9\ub418\ub294 \ubc30\uce58 \ud06c\uae30\ub85c \ubaa8\ub378\uc758 \uc131\ub2a5\uc5d0 \ub530\ub77c \uc870\uc815\ud560 \uc218 \uc788\uc73c\uba70, num_epochs \ub294 \uc2e4\ud589\ud558\ub824\ub294 \ud6c8\ub828 \uc5d0\ud3ed \uc218, feature_extract \ub294 \ubbf8\uc138 \uc870\uc815 \ub610\ub294 \ud2b9\uc9d5 \ucd94\ucd9c \uc5ec\ubd80\ub97c \uc815\uc758\ud558\ub294 \ubd80\uc6b8(boolean)\uc785\ub2c8\ub2e4. ``feature_extract = False``\uc774\uba74 \ubaa8\ub378\uc774 \ubbf8\uc138 \uc870\uc815\ub418\uace0 \ubaa8\ub4e0 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218\uac00 \uc5c5\ub370\uc774\ud2b8\ub429\ub2c8\ub2e4. ``feature_extract = True``\uc778 \uacbd\uc6b0 \ub9c8\uc9c0\ub9c9 \ub808\uc774\uc5b4\uc758 \ub9e4\uac1c\ubcc0\uc218\ub9cc \uc5c5\ub370\uc774\ud2b8\ub418\uace0 \ub2e4\ub978 \ub9e4\uac1c\ubcc0\uc218\ub294 \uace0\uc815\ub41c \uc0c1\ud0dc\ub85c \uc720\uc9c0\ub429\ub2c8\ub2e4. # \ucd5c\uc0c1\uc704 \ub370\uc774\ud130 \ub514\ub809\ud1a0\ub9ac\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 \ub514\ub809\ud1a0\ub9ac \ud615\uc2dd\uc774 # ImageFolder \uad6c\uc870\ub97c \ub530\ub978\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4. data_dir = \"./data/hymenoptera_data\" # [resnet, alexnet, vgg, squeezenet, densenet, inception] \uc774 \uc911 \ubaa8\ub378\uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4. model_name = \"squeezenet\" # \ub370\uc774\ud130 \uc9d1\ud569\uc758 \ud074\ub798\uc2a4 \uc218 num_classes = 2 # \ud6c8\ub828\uc744 \uc704\ud55c \ubc30\uce58 \ud06c\uae30 (\uba54\ubaa8\ub9ac \uc6a9\ub7c9\uc5d0 \ub530\ub77c \ubcc0\uacbd\ub429\ub2c8\ub2e4.) batch_size = 8 # \ud6c8\ub828\ud560 \uc5d0\ud3ed \uc218 num_epochs = 15 # \ud2b9\uc9d5 \ucd94\ucd9c\uc744 \uc704\ud55c \ud50c\ub798\uadf8(flag)\uc785\ub2c8\ub2e4. False\uc77c \uacbd\uc6b0, \uc804\uccb4 \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud558\uace0 # True\uc77c \uacbd\uc6b0 \uc7ac\ud615\uc131\ub41c \ub808\uc774\uc5b4\uc5b4\uc758 \ub9e4\uac1c\ubcc0\uc218\ub9cc \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4. feature_extract = True \ub3c4\uc6b0\ubbf8 \ud568\uc218(Helper Functions)# \ubaa8\ub378\uc744 \uc870\uc815\ud558\ub294 \ucf54\ub4dc\ub97c \uc791\uc131\ud558\uae30 \uc804\uc5d0 \uba87 \uac00\uc9c0 \ub3c4\uc6b0\ubbf8 \ud568\uc218(Helper Functions)\ub97c \uc815\uc758\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \ubaa8\ub378 \ud6c8\ub828 \ubc0f \uac80\uc99d \ucf54\ub4dc# train_model \ud568\uc218\ub294 \uc8fc\uc5b4\uc9c4 \ubaa8\ub378\uc758 \ud559\uc2b5\uacfc \uac80\uc99d\uc744 \ucc98\ub9ac\ud569\ub2c8\ub2e4. \uc774 \ud568\uc218\ub294 PyTorch \ubaa8\ub378, \ub370\uc774\ud130\ub85c\ub354(dataloader) \ub515\uc154\ub108\ub9ac, \uc190\uc2e4 \ud568\uc218, \uc635\ud2f0\ub9c8\uc774\uc800, \ud6c8\ub828 \ubc0f \uac80\uc99d\uc744 \uc704\ud574 \uc815\ud574\uc9c4 \uc5d0\ud3ed \uc218, \uadf8\ub9ac\uace0 Inception \ubaa8\ub378\uc77c \ub54c\ub97c \ub098\ud0c0\ub0b4\ub294 \ubd80\uc6b8 \ud50c\ub798\uadf8(boolean flag)\ub97c \uc785\ub825\uc73c\ub85c \ubc1b\uc2b5\ub2c8\ub2e4. \uc774 \uc544\ud0a4\ud14d\ucc98\ub294 \ubcf4\uc870(auxiliary) \ucd9c\ub825\uc744 \uc0ac\uc6a9\ud558\uace0, \uc804\uccb4 \ubaa8\ub378 \uc190\uc2e4\uc740 \uc5ec\uae30 \uc5d0 \uc124\uba85\ub41c \ub300\ub85c \ubcf4\uc870(auxiliary) \ucd9c\ub825\uacfc \ucd5c\uc885 \ucd9c\ub825\uc744 \ubaa8\ub450 \uc874\uc911\ud558\ubbc0\ub85c is_inception \ud50c\ub798\uadf8(flag)\ub294 Inception v3 \ubaa8\ub378\uc744 \uc218\uc6a9\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc774 \ud568\uc218\ub294 \uc9c0\uc815\ub41c \uc5d0\ud3ed \uc218 \ub3d9\uc548 \ud559\uc2b5\ud558\uace0 \uac01 \uc5d0\ud3ed\uc774 \ub05d\ub09c \ud6c4 \uc804\uccb4 \uac80\uc99d \ub2e8\uacc4\ub97c \uc2e4\ud589\ud569\ub2c8\ub2e4. \ub610\ud55c, \uac80\uc99d \uc815\ud655\ub3c4 \uce21\uba74\uc5d0\uc11c \uac00\uc7a5 \uc131\ub2a5\uc774 \uc88b\uc740 \ubaa8\ub378\uc744 \ucd94\uc801\ud558\uace0 \ud559\uc2b5\uc774 \ub05d\ub098\uba74 \ud574\ub2f9 \ubaa8\ub378\uc744 \ubc18\ud658\ud569\ub2c8\ub2e4. \uac01 \uc5d0\ud3ed\uc774 \ub05d\ub098\uba74 \ud6c8\ub828 \ubc0f \uac80\uc99d \uc815\ud655\ub3c4\ub97c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False): since = time.time() val_acc_history = [] best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): print(\u0027Epoch {}/{}\u0027.format(epoch, num_epochs - 1)) print(\u0027-\u0027 * 10) # \uac01 \uc5d0\ud3ed\uc740 \ud559\uc2b5 \ub2e8\uacc4\uc640 \uac80\uc99d \ub2e8\uacc4\ub97c \uac16\uc2b5\ub2c8\ub2e4. for phase in [\u0027train\u0027, \u0027val\u0027]: if phase == \u0027train\u0027: model.train() # \ud559\uc2b5 \ubaa8\ub4dc\ub85c \ubaa8\ub378 \uc124\uc815 else: model.eval() # \ud3c9\uac00 \ubaa8\ub4dc\ub85c \ubaa8\ub378 \uc124\uc815 running_loss = 0.0 running_corrects = 0 # \ub370\uc774\ud130\ub97c \ubc18\ubcf5 for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # \ub9e4\uac1c\ubcc0\uc218 \uacbd\uc0ac\ub3c4\ub97c 0\uc73c\ub85c \uc124\uc815 optimizer.zero_grad() # \uc21c\ubc29\ud5a5 # \ud6c8\ub828 \ud558\ub294 \ub3d9\uc548\ub9cc \uae30\ub85d\uc744 \ucd94\uc801\ud569\ub2c8\ub2e4. with torch.set_grad_enabled(phase == \u0027train\u0027): # \ubaa8\ub378\uc758 \ucd9c\ub825\uc744 \uac00\uc838\uc624\uace0 \uc190\uc2e4\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4. # \ud559\uc2b5 \uc2dc \ubcf4\uc870(auxiliary) \ucd9c\ub825\uc774 \uc788\ub294 inception\uc758 \ud2b9\ubcc4\ud55c \uacbd\uc6b0\uc785\ub2c8\ub2e4. # \ud559\uc2b5 \ubaa8\ub4dc\uc5d0\uc11c\ub294 \ucd5c\uc885 \ucd9c\ub825\uacfc \ubcf4\uc870(auxiliary) \ucd9c\ub825\uc744 \ud569\uc0b0\ud574 \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uc9c0\ub9cc # \ud14c\uc2a4\ud2b8\uc5d0\uc11c\ub294 \ucd5c\uc885 \ucd9c\ub825\ub9cc \uace0\ub824\ud569\ub2c8\ub2e4. if is_inception and phase == \u0027train\u0027: # https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958 \uc5d0\uc11c outputs, aux_outputs = model(inputs) loss1 = criterion(outputs, labels) loss2 = criterion(aux_outputs, labels) loss = loss1 + 0.4*loss2 else: outputs = model(inputs) loss = criterion(outputs, labels) _, preds = torch.max(outputs, 1) # \ud559\uc2b5 \ub2e8\uacc4\uc778 \uacbd\uc6b0 \uc5ed\uc804\ud30c + \ucd5c\uc801\ud654 if phase == \u0027train\u0027: loss.backward() optimizer.step() # \ud1b5\uacc4 running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / len(dataloaders[phase].dataset) epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset) print(\u0027{} Loss: {:.4f} Acc: {:.4f}\u0027.format(phase, epoch_loss, epoch_acc)) # \ubaa8\ub378\uc744 \uae4a\uc740 \ubcf5\uc0ac(deep copy)\ud568 if phase == \u0027val\u0027 and epoch_acc \u003e best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) if phase == \u0027val\u0027: val_acc_history.append(epoch_acc) print() time_elapsed = time.time() - since print(\u0027Training complete in {:.0f}m {:.0f}s\u0027.format(time_elapsed // 60, time_elapsed % 60)) print(\u0027Best val Acc: {:4f}\u0027.format(best_acc)) # \ucd5c\uace0\uc758 \ubaa8\ub378 \uac00\uc911\uce58 \ubd88\ub7ec\uc624\uae30 model.load_state_dict(best_model_wts) return model, val_acc_history \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218\uc758 .requires_grad \uc18d\uc131 \uc124\uc815# \uc774 \ub3c4\uc6b0\ubbf8 \ud568\uc218(Helper Functions)\ub294 \ud2b9\uc9d5 \ucd94\ucd9c \uc2dc \ubaa8\ub378\uc5d0 \uc788\ub294 \ub9e4\uac1c\ubcc0\uc218\uc758 .requires_grad \uc18d\uc131\uc744 False\ub85c \uc124\uc815\ud569\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc73c\ub85c, \uc0ac\uc804 \ud559\uc2b5\ub41c \ubaa8\ub378\uc744 \uc77d\uc5b4 \ub4e4\uc77c \ub54c \ubaa8\ub4e0 \ub9e4\uac1c\ubcc0\uc218\uac00 ``.requires_grad=True``\ub85c \uc124\uc815\ub418\uc5b4 \uc788\uc73c\ubbc0\ub85c \ucc98\uc74c\ubd80\ud130 \ud559\uc2b5\ud558\uac70\ub098 \ubbf8\uc138 \uc870\uc815\ud558\ub294 \uacbd\uc6b0\ub77c\uba74 \uad1c\ucc2e\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \ud2b9\uc9d5 \ucd94\ucd9c \uc911\uc774\uace0 \uc0c8\ub85c \ucd08\uae30\ud654\ub41c \ub808\uc774\uc5b4\uc5d0 \ub300\ud55c \uacbd\uc0ac\ub3c4\ub9cc \uacc4\uc0b0\ud558\ub824\ub294 \uacbd\uc6b0 \ub2e4\ub978 \ubaa8\ub4e0 \ub9e4\uac1c\ubcc0\uc218\uc5d0\ub294 \uacbd\uc0ac\ub3c4\uac00 \ud544\uc694\ud558\uc9c0 \uc54a\uc544\uc57c \ud569\ub2c8\ub2e4. \uc774\uac83\uc740 \ub098\uc911\uc5d0 \ub354 \uc774\ud574\ub97c \ud560 \uc218 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4. def set_parameter_requires_grad(model, feature_extracting): if feature_extracting: for param in model.parameters(): param.requires_grad = False \ub124\ud2b8\uc6cc\ud06c \ucd08\uae30\ud654 \ubc0f \uc7ac\uad6c\uc131\ud558\uae30# \uc774\uc81c \uac00\uc7a5 \ud765\ubbf8\ub85c\uc6b4 \ubd80\ubd84\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 \uac01 \ub124\ud2b8\uc6cc\ud06c\uc758 \uc7ac\uad6c\uc131\uc744 \ucc98\ub9ac\ud569\ub2c8\ub2e4. \uc774 \uc808\ucc28\ub294 \uc790\ub3d9 \uc808\ucc28\uac00 \uc544\ub2c8\uba70 \uac01 \ubaa8\ub378\ub9c8\ub2e4 \uace0\uc720\ud569\ub2c8\ub2e4. CNN \ubaa8\ub378\uc758 \ucd5c\uc885 \ub808\uc774\uc5b4(FC layer\ub77c\uace0\ub3c4 \ubd88\ub9bc)\ub294 \ub370\uc774\ud130\uc14b\uc758 \ucd9c\ub825 \ud074\ub798\uc2a4 \uc218\uc640 \ub3d9\uc77c\ud55c \uc218\uc758 \ub178\ub4dc\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ubaa8\ub4e0 \ubaa8\ub378\uc740 \uc774\ubbf8 ImageNet\uc5d0\uc11c \uc0ac\uc804 \ud559\uc2b5 \ub418\uc5c8\uae30 \ub54c\ubb38\uc5d0 \uac01 \ud074\ub798\uc2a4\ub2f9 \ud558\ub098\uc758 \ub178\ub4dc\uc529 1000 \ud06c\uae30\uc758 \ucd9c\ub825 \ub808\uc774\uc5b4\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c\uc758 \ubaa9\ud45c\ub294 \uc774\uc804\uacfc \ub3d9\uc77c\ud55c \uc218\uc758 \uc785\ub825\uc744 \uac16\uace0, \ub370\uc774\ud130\uc14b\uc758 \ud074\ub798\uc2a4 \uc218\uc640 \ub3d9\uc77c\ud55c \uc218\uc758 \ucd9c\ub825\uc744 \uac16\ub3c4\ub85d \ub9c8\uc9c0\ub9c9 \ub808\uc774\uc5b4\ub97c \uc7ac\uad6c\uc131\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \ub2e4\uc74c \uc139\uc158\uc5d0\uc11c\ub294 \uac01 \ubaa8\ub378\uc758 \uc544\ud0a4\ud14d\ucc98\ub97c \uac1c\ubcc4\uc801\uc73c\ub85c \ubcc0\uacbd\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574 \uc124\uba85\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uba3c\uc800, \ubbf8\uc138 \uc870\uc815\uacfc \ud2b9\uc9d5 \ucd94\ucd9c\uc758 \ucc28\uc774\uc810\uc5d0 \ub300\ud55c \ud55c \uac00\uc9c0 \uc911\uc694\ud55c \uc138\ubd80 \uc0ac\ud56d\uc774 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\uc9d5 \ucd94\ucd9c \uc2dc \ub9c8\uc9c0\ub9c9 \ub808\uc774\uc5b4\uc758 \ub9e4\uac1c\ubcc0\uc218\ub9cc \uc5c5\ub370\uc774\ud2b8 \ud558\uace0 \uc2f6\uc744 \ub54c \ub2e4\uc2dc \ub9d0\ud574, \uc7ac\uad6c\uc131\ud558\ub294 \ub808\uc774\uc5b4\uc758 \ub9e4\uac1c\ubcc0\uc218\ub9cc \uc5c5\ub370\uc774\ud2b8\ub97c \ud558\uace0 \uc2f6\uc740 \uacbd\uc6b0\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7f0 \uacbd\uc6b0\uc5d0\ub294 \ubcc0\uacbd\ud558\uc9c0 \uc54a\ub294 \ub9e4\uac1c\ubcc0\uc218\uc758 \uacbd\uc0ac\ub3c4\ub97c \uacc4\uc0b0\ud560 \ud544\uc694\uac00 \uc5c6\uc73c\ubbc0\ub85c \ud6a8\uc728\uc131\uc744 \uc704\ud574 .requires_grad \uc18d\uc131\uc744 False\ub85c \uc124\uc815\ud569\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc73c\ub85c \uc774 \uc18d\uc131\uc740 True\ub85c \uc124\uc815\ub418\uc5b4 \uc788\uae30 \ub54c\ubb38\uc5d0 \uc774 \uc124\uc815\uc740 \uc911\uc694\ud569\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c \uc0c8 \ub808\uc774\uc5b4\ub97c \ucd08\uae30\ud654\ud560 \ub54c \uae30\ubcf8\uc801\uc73c\ub85c \uc0c8 \ub9e4\uac1c\ubcc0\uc218\uc5d0\ub294 ``.requires_grad=True``\uac00 \uc788\uc73c\ubbc0\ub85c \uc0c8 \ub808\uc774\uc5b4\uc758 \ub9e4\uac1c\ubcc0\uc218\ub9cc \uc5c5\ub370\uc774\ud2b8\ub429\ub2c8\ub2e4. \ubbf8\uc138 \uc870\uc815\ud560 \ub54c\ub294 \ubaa8\ub4e0 .requires_grad\ub97c \uae30\ubcf8\uac12\uc778 True\ub85c \uc124\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c, inception_v3\ub294 \uc785\ub825 \ud06c\uae30\ub97c (299,299)\ub85c \uc694\uad6c\ud558\uc9c0\ub9cc, \ub2e4\ub978 \ubaa8\ub4e0 \ubaa8\ub378\uc740 (224,224)\ub97c \uae30\ub300\ud55c\ub2e4\ub294 \uc810\uc744 \uae30\uc5b5\ud558\uc138\uc694. Resnet# Resnet\uc740 \u2018Deep Residual Learning for Image Recognition \u003chttps://arxiv.org/abs/1512.03385\u003e`__ \ub17c\ubb38\uc5d0\uc11c \uc18c\uac1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4. Resnet18, Resnet34, Resnet50, Resnet101, and Resnet152 \ub4f1 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \uc5ec\ub7ec \uac00\uc9c0 \ubcc0\ud615\uc774 \uc788\uc73c\uba70 \ubaa8\ub450 torchvision \ubaa8\ub378\uc5d0\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 \ub370\uc774\ud130\uc14b\uc774 \uc791\uace0 \ud074\ub798\uc2a4\uac00 \ub450 \uac1c \ubfd0\uc778 Resnet18\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ubaa8\ub378\uc744 \ucd9c\ub825\ud558\uba74 \uc544\ub798 \uadf8\ub9bc\uacfc \uac19\uc774 \ub9c8\uc9c0\ub9c9 \ub808\uc774\uc5b4\uac00 \uc644\uc804\ud788 \uc5f0\uacb0\ub41c \ub808\uc774\uc5b4\uc784\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. (fc): Linear(in_features=512, out_features=1000, bias=True) \uc785\ub825 \ud2b9\uc9d5\uc774 512\uac1c, \ucd9c\ub825 \ud2b9\uc9d5\uc774 2\uac1c\uc778 \uc120\ud615 \ub808\uc774\uc5b4\uac00 \ub418\ub3c4\ub85d ``model.fc``\ub97c \ub2e4\uc2dc \ucd08\uae30\ud654\ud574\uc57c \ud569\ub2c8\ub2e4. model.fc = nn.Linear(512, num_classes) Alexnet# Alexnet\uc740 ImageNet Classification with Deep Convolutional Neural Networks \ub17c\ubb38\uc5d0 \uc18c\uac1c\ub41c \ubc14 \uc788\uc73c\uba70 ImageNet \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ucd5c\ucd08\ub85c \ub9e4\uc6b0 \uc131\uacf5\uc801\uc778 CNN\uc744 \uad6c\ud604\ud55c \ubc14 \uc788\uc2b5\ub2c8\ub2e4. \ubaa8\ub378\uc758 \uc544\ud0a4\ud14d\ucc98\ub97c \ucd9c\ub825\ud558\uba74 \ubaa8\ub378 \ucd9c\ub825\uc774 \ubd84\ub958\uae30(classifier)\uc758 6\ubc88\uc9f8 \ub808\uc774\uc5b4\uc5d0\uc11c \ub098\uc624\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. (classifier): Sequential( ... (6): Linear(in_features=4096, out_features=1000, bias=True) ) \ub370\uc774\ud130\uc14b\uacfc \ud568\uaed8 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\ub824\uba74 \uc774 \ub808\uc774\uc5b4\ub97c \ub2e4\uc74c\uacfc \uac19\uc774 \ub2e4\uc2dc \ucd08\uae30\ud654\ud569\ub2c8\ub2e4. model.classifier[6] = nn.Linear(4096,num_classes) VGG# VGG\ub294 Very Deep Convolutional Networks for Large-Scale Image Recognition \ub17c\ubb38\uc5d0\uc11c \uc18c\uac1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4. Torchvision \ub2e4\uc591\ud55c \uae38\uc774\uc640 \ubc30\uce58 \uc815\uaddc\ud654 \ub808\uc774\uc5b4\uac00 \uc788\ub294 8\uac00\uc9c0 \ubc84\uc804\uc758 VGG\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 \ubc30\uce58 \uc815\uaddc\ud654 \uae30\ub2a5\uc774 \uc788\ub294 VGG-11\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ucd9c\ub825 \ub808\uc774\uc5b4\ub294 Alexnet\uacfc \uc720\uc0ac\ud569\ub2c8\ub2e4. (classifier): Sequential( ... (6): Linear(in_features=4096, out_features=1000, bias=True) ) \ub530\ub77c\uc11c \ub3d9\uc77c\ud55c \uae30\uc220\uc744 \uc0ac\uc6a9\ud574 \ucd9c\ub825 \ub808\uc774\uc5b4\ub97c \uc218\uc815\ud569\ub2c8\ub2e4. model.classifier[6] = nn.Linear(4096,num_classes) Squeezenet# Squeeznet \uc544\ud0a4\ud14d\ucc98\ub294 SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \u003c0.5MB model size \ub17c\ubb38\uc5d0 \uc124\uba85\ub418\uc5b4 \uc788\uace0, AlexNet \uc218\uc900\uc758 \uc815\ud655\ub3c4\ub97c \uc81c\uacf5\ud558\uba74\uc11c \uc5ec\uae30\uc5d0 \ud45c\uc2dc\ub41c \ubaa8\ub378\ub4e4\uacfc\ub294 \ub2e4\ub978 \ucd9c\ub825 \uad6c\uc870\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. Torchvision\uc5d0\ub294 \ub450 \uac00\uc9c0 \ubc84\uc804\uc758 Squeezenet\uc774 \uc788\uace0 \uc5ec\uae30\uc11c\ub294 1.0 \ubc84\uc804\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ucd9c\ub825\uc740 \ubd84\ub958\uae30(classifier)\uc758 \uccab \ubc88\uc9f8 \ub808\uc774\uc5b4\uc778 1x1 \ud569\uc131\uacf1 \ub808\uc774\uc5b4\uc5d0\uc11c \ub098\uc635\ub2c8\ub2e4. (classifier): Sequential( (0): Dropout(p=0.5) (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1)) (2): ReLU(inplace) (3): AvgPool2d(kernel_size=13, stride=1, padding=0) ) \ub124\ud2b8\uc6cc\ud06c\ub97c \uc218\uc815\ud558\uae30 \uc704\ud574 Conv2d \ub808\uc774\uc5b4\ub97c \ub2e4\uc2dc \ucd08\uae30\ud654\ud558\uc5ec \uae4a\uc774 2\uc758 \ud2b9\uc9d5 \ub9f5\uc744 \ub2e4\uc74c\uacfc \uac19\uc774 \ucd9c\ub825\ud569\ub2c8\ub2e4. model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1)) Densenet# Densenet Densely Connected Convolutional Networks \ub17c\ubb38\uc5d0\uc11c \uc18c\uac1c\ub418\uc5c8\uc2b5\ub2c8\ub2e4. Torchvision\uc5d0\ub294 4\uac00\uc9c0\uc758 \ubcc0\ud615 Densenet\uc774 \uc788\uc9c0\ub9cc \uc5ec\uae30\uc11c\ub294 Densenet-121\ub9cc \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ucd9c\ub825 \ub808\uc774\ub294 1024\uac1c\uc758 \uc785\ub825 \ud2b9\uc9d5\uc744 \uac00\uc9c4 \uc120\ud615 \ub808\uc774\uc5b4 \uc785\ub2c8\ub2e4. (classifier): Linear(in_features=1024, out_features=1000, bias=True) \ub124\ud2b8\uc6cc\ud06c\ub97c \uc7ac\uad6c\uc131\ud558\uae30 \uc704\ud574 \ubd84\ub958\uae30(classifier)\uc758 \uc120\ud615 \ub808\uc774\uc5b4\ub97c \ub2e4\uc74c\uacfc \uac19\uc774 \ub2e4\uc2dc \ucd08\uae30\ud654\ud569\ub2c8\ub2e4. model.classifier = nn.Linear(1024, num_classes) Inception v3# \ub9c8\uc9c0\ub9c9\uc73c\ub85c Inception v3\uc740 Rethinking the Inception Architecture for Computer Vision \uc5d0\uc11c \ucc98\uc74c \uc124\uba85\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ub124\ud2b8\uc6cc\ud06c\ub294 \ud559\uc2b5 \uc2dc \ub450 \uac1c\uc758 \ucd9c\ub825 \ub808\uc774\uc5b4\uac00 \uc788\ub2e4\ub294 \uc810\uc774 \ub3c5\ud2b9\ud569\ub2c8\ub2e4. \ub450 \ubc88\uc9f8 \ucd9c\ub825\uc740 \ubcf4\uc870(axuiliary) \ucd9c\ub825\uc73c\ub85c \uc54c\ub824\uc838 \uc788\uc73c\uba70 \ub124\ud2b8\uc6cc\ud06c\uc758 AuxLogits \ubd80\ubd84\uc5d0 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uae30\ubcf8 \ucd9c\ub825\uc740 \ub124\ud2b8\uc6cc\ud06c \ub05d\uc5d0 \uc788\ub294 \uc120\ud615 \ub808\uc774\uc5b4\uc774\uba70 \ud14c\uc2a4\ud2b8\ud560 \ub54c\ub294 \uae30\ubcf8 \ucd9c\ub825\ub9cc \uace0\ub824\ud569\ub2c8\ub2e4. \uc77d\uc5b4 \ub4e4\uc778 \ubaa8\ub378\uc758 \ubcf4\uc870(auxiliary) \ucd9c\ub825\uacfc \uae30\ubcf8 \ucd9c\ub825\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \ucd9c\ub825\ub429\ub2c8\ub2e4. (AuxLogits): InceptionAux( ... (fc): Linear(in_features=768, out_features=1000, bias=True) ) ... (fc): Linear(in_features=2048, out_features=1000, bias=True) \uc774 \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815\ud558\ub824\uba74 \ub450 \ub808\uc774\uc5b4\ub97c \ub2e4\uc74c\uacfc \uac19\uc774 \ubaa8\ub450 \uc7ac\uad6c\uc131\ud574\uc57c \ud569\ub2c8\ub2e4. model.AuxLogits.fc = nn.Linear(768, num_classes) model.fc = nn.Linear(2048, num_classes) \ub9ce\uc740 \ubaa8\ub378\uc774 \ube44\uc2b7\ud55c \ucd9c\ub825 \uad6c\uc870\ub97c \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \uac01\uac01\uc740 \uc57d\uac04 \ub2e4\ub974\uac8c \ucc98\ub9ac\ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4. \ub610\ud55c, \uc7ac\uad6c\uc131\ub41c \ub124\ud2b8\uc6cc\ud06c\uc758 \ucd9c\ub825 \ubaa8\ub378 \uad6c\uc870\ub97c \ud655\uc778\ud558\uace0 \ucd9c\ub825 \uae30\ub2a5\uc758 \uc218\uac00 \ub370\uc774\ud130\uc14b\uc758 \ud074\ub798\uc2a4 \uc218\uc640 \ub3d9\uc77c\ud55c\uc9c0 \ud655\uc778\ud574\uc57c \ud569\ub2c8\ub2e4. def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True): # \uc774 if \ubb38\uc5d0\uc11c \uc124\uc815\ud560 \ubcc0\uc218\ub97c \ucd08\uae30\ud654 \ud569\ub2c8\ub2e4. # \uac01 \ubcc0\uc218\ub294 \ubaa8\ub378\uc5d0 \ub530\ub77c \ub2e4\ub985\ub2c8\ub2e4. model_ft = None input_size = 0 if model_name == \"resnet\": \"\"\" Resnet18 \"\"\" model_ft = models.resnet18(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs, num_classes) input_size = 224 elif model_name == \"alexnet\": \"\"\" Alexnet \"\"\" model_ft = models.alexnet(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier[6].in_features model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes) input_size = 224 elif model_name == \"vgg\": \"\"\" VGG11_bn \"\"\" model_ft = models.vgg11_bn(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier[6].in_features model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes) input_size = 224 elif model_name == \"squeezenet\": \"\"\" Squeezenet \"\"\" model_ft = models.squeezenet1_0(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1)) model_ft.num_classes = num_classes input_size = 224 elif model_name == \"densenet\": \"\"\" Densenet \"\"\" model_ft = models.densenet121(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) num_ftrs = model_ft.classifier.in_features model_ft.classifier = nn.Linear(num_ftrs, num_classes) input_size = 224 elif model_name == \"inception\": \"\"\" Inception v3 Be careful, expects (299,299) sized images and has auxiliary output \"\"\" model_ft = models.inception_v3(pretrained=use_pretrained) set_parameter_requires_grad(model_ft, feature_extract) # \ubcf4\uc870 \ub124\ud2b8\uc6cc\ud06c(auxilary net) \ucc98\ub9ac num_ftrs = model_ft.AuxLogits.fc.in_features model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes) # \uc8fc \ub124\ud2b8\uc6cc\ud06c(primary net) \ucc98\ub9ac num_ftrs = model_ft.fc.in_features model_ft.fc = nn.Linear(num_ftrs,num_classes) input_size = 299 else: print(\"Invalid model name, exiting...\") exit() return model_ft, input_size # \uc2e4\ud589\uc744 \uc704\ud55c \ubaa8\ub378 \ucd08\uae30\ud654 model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True) # \ubc29\uae08 \uc778\uc2a4\ud134\uc2a4\ud654\ud55c \ubaa8\ub378 \ucd9c\ub825 print(model_ft) /opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter \u0027pretrained\u0027 is deprecated since 0.13 and may be removed in the future, please use \u0027weights\u0027 instead. /opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for \u0027weights\u0027 are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_0_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_0_Weights.DEFAULT` to get the most up-to-date weights. Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth 0%| | 0.00/4.78M [00:00\u003c?, ?B/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.78M/4.78M [00:00\u003c00:00, 79.8MB/s] SqueezeNet( (features): Sequential( (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2)) (1): ReLU(inplace=True) (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True) (3): Fire( (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (4): Fire( (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (5): Fire( (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True) (7): Fire( (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (8): Fire( (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (9): Fire( (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (10): Fire( (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True) (12): Fire( (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1)) (squeeze_activation): ReLU(inplace=True) (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1)) (expand1x1_activation): ReLU(inplace=True) (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (expand3x3_activation): ReLU(inplace=True) ) ) (classifier): Sequential( (0): Dropout(p=0.5, inplace=False) (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1)) (2): ReLU(inplace=True) (3): AdaptiveAvgPool2d(output_size=(1, 1)) ) ) \ub370\uc774\ud130 \uc77d\uc5b4 \ub4e4\uc774\uae30# \uc785\ub825 \ud06c\uae30\ub97c \uc54c\uc558\uc73c\ub2c8 \uc774\uc81c \ub370\uc774\ud130 \uc804\uc774(transform), \uc774\ubbf8\uc9c0 \ub370\uc774\ud130\uc14b, \uadf8\ub9ac\uace0 \ub370\uc774\ud130\ub85c\ub354(dataloader)\ub97c \ucd08\uae30\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30 \uc5d0\uc11c \uc124\uba85\ub41c \ub300\ub85c \ubaa8\ub378\uc740 \ud558\ub4dc\ucf54\ub529(hard-coded)\ub41c \uc815\uaddc\ud654 \uac12\uc73c\ub85c \uc0ac\uc804 \ud559\uc2b5\ub418\uc5c8\uc2b5\ub2c8\ub2e4. # \ud559\uc2b5\uc744 \uc704\ud55c \ub370\uc774\ud130 \uc99d\uac15 \ubc0f \uc815\uaddc\ud654 # \uac80\uc99d\uc744 \uc704\ud55c \uc815\uaddc\ud654\ub9cc \uc218\ud589 data_transforms = { \u0027train\u0027: transforms.Compose([ transforms.RandomResizedCrop(input_size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), \u0027val\u0027: transforms.Compose([ transforms.Resize(input_size), transforms.CenterCrop(input_size), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]), } print(\"Initializing Datasets and Dataloaders...\") # \ud559\uc2b5 \ubc0f \uac80\uc99d \ub370\uc774\ud130\uc14b \uc0dd\uc131 image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [\u0027train\u0027, \u0027val\u0027]} # \ud559\uc2b5 \ubc0f \uac80\uc99d \ub370\uc774\ud130\ub85c\ub354(dataloader) \uc0dd\uc131 dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in [\u0027train\u0027, \u0027val\u0027]} # \uc0ac\uc6a9 \uac00\ub2a5\ud55c GPU \ud0d0\uc9c0 device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") Initializing Datasets and Dataloaders... \uc635\ud2f0\ub9c8\uc774\uc800 \uc0dd\uc131# \uc774\uc81c \ubaa8\ub378 \uad6c\uc870\uac00 \uc815\ud655\ud574\uc84c\uc73c\ub2c8, \ubbf8\uc138 \uc870\uc815 \ubc0f \ud2b9\uc9d5 \ucd94\ucd9c\uc744 \uc704\ud55c \ub9c8\uc9c0\ub9c9 \ub2e8\uacc4\ub294 \uc6d0\ud558\ub294 \ub9e4\uac1c\ubcc0\uc218\ub9cc \uc5c5\ub370\uc774\ud2b8\ud558\ub294 \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \uc0dd\uc131\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc0ac\uc804 \ud559\uc2b5\ub41c \ubaa8\ub378\uc744 \uc77d\uc5b4 \ub4e4\uc778 \ud6c4 \uad6c\uc870\ub97c \uc7ac\uc870\uc815\ud558\uae30 \uc804\uc5d0 feature_extract=True``\uc778 \uacbd\uc6b0 \ub9e4\uac1c\ubcc0\uc218\uc758 \ubaa8\ub4e0 ``.requires_grad \uc18d\uc131\uc744 \uc77c\uc77c\uc774 False\ub85c \uc124\uc815\ud55c \uac83\uc744 \uae30\uc5b5\ud558\uc138\uc694. \uadf8\ub7ec\uba74 \uc7ac\ucd08\uae30\ud654\ub41c \ub808\uc774\uc5b4\uc758 \ud30c\ub77c\ubbf8\ud130\ub294 \uae30\ubcf8\uc801\uc73c\ub85c ``.requires_grad=True``\ub97c \uac16\uc2b5\ub2c8\ub2e4. \uc774\uc81c .requires_grad=True\uc778 \ubaa8\ub4e0 \ub9e4\uac1c\ubcc0\uc218\uac00 \ucd5c\uc801\ud654\ub418\uc5b4\uc57c \ud55c\ub2e4\ub294 \uac83\uc744 \uc54c\uc558\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c\uc73c\ub85c \uc774\ub7ec\ud55c \ub9e4\uac1c\ubcc0\uc218 \ubaa9\ub85d\uc744 \ub9cc\ub4e4\uace0 \uc774 \ubaa9\ub85d\uc744 SGD \uc54c\uace0\ub9ac\uc998 \uc0dd\uc131\uc790(constructor)\uc5d0 \uc785\ub825\ud569\ub2c8\ub2e4. \uc774\ub97c \ud655\uc778\ud558\ub824\uba74 \ucd9c\ub825\ub41c \ub9e4\uac1c\ubcc0\uc218\ub97c \ud655\uc778\ud558\uc5ec \ud559\uc2b5\ud558\uc138\uc694. \ubbf8\uc138 \uc870\uc815\ud560 \ub54c \uc774 \ubaa9\ub85d\uc740 \uae38\uc5b4\uc57c \ud558\uba70 \ubaa8\ub4e0 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \ud3ec\ud568\ud574\uc57c \ud569\ub2c8\ub2e4. \ud558\uc9c0\ub9cc, \ud2b9\uc9d5\uc744 \ucd94\ucd9c\ud560 \ub54c\ub294 \uc774 \ubaa9\ub85d\uc774 \uc9e7\uc544\uc57c \ud558\uba70 \uc7ac\uad6c\uc131\ub41c \ub808\uc774\uc5b4\uc758 \uac00\uc911\uce58\uc640 \ud3b8\ud5a5(bias)\ub9cc \ud3ec\ud568\ud574\uc57c \ud569\ub2c8\ub2e4. # GPU\ub85c \ubaa8\ub378 \uc804\uc1a1 model_ft = model_ft.to(device) # \uc774 \uc2e4\ud589\uc5d0\uc11c \ucd5c\uc801\ud654/\uc5c5\ub370\uc774\ud2b8\ud560 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc218\uc9d1\ud569\ub2c8\ub2e4. # \ubbf8\uc138 \uc870\uc815\uc744 \ud558\ub294 \uacbd\uc6b0 \ubaa8\ub4e0 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4. # \ud558\uc9c0\ub9cc, \ud2b9\uc9d5 \ucd94\ucd9c \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0\uc5d0\ub294 # \ubc29\uae08 \ucd08\uae30\ud654\ud55c \ub9e4\uac1c\ubcc0\uc218, \uc989 requires_grad\uac00 Ture\uc778 \ub9e4\uac1c\ubcc0\uc218\ub9cc \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4. params_to_update = model_ft.parameters() print(\"Params to learn:\") if feature_extract: params_to_update = [] for name,param in model_ft.named_parameters(): if param.requires_grad == True: params_to_update.append(param) print(\"\\t\",name) else: for name,param in model_ft.named_parameters(): if param.requires_grad == True: print(\"\\t\",name) # \ubaa8\ub4e0 \ub9e4\uac1c\ubcc0\uc218\uac00 \ucd5c\uc801\ud654\ub418\uace0 \uc788\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4. optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9) Params to learn: classifier.1.weight classifier.1.bias \ud559\uc2b5 \ubc0f \uac80\uc99d \ub2e8\uacc4 \uc2e4\ud589# \ub9c8\uc9c0\ub9c9 \ub2e8\uacc4\ub294 \ubaa8\ub378\uc5d0 \ub300\ud55c \uc190\uc2e4\uc744 \uc124\uc815\ud55c \ub2e4\uc74c \uc124\uc815\ub41c \uc5d0\ud3ed \uc218\uc5d0 \ub300\ud574 \ud559\uc2b5 \ubc0f \uac80\uc99d \ud568\uc218(validation function)\ub97c \uc2e4\ud589\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774 \ub2e8\uacc4\ub294 \uc5d0\ud3ed \uc218\uc5d0 \ub530\ub77c CPU\uc5d0\uc11c\ub294 \uc2dc\uac04\uc774 \uac78\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uae30\ubcf8 \ud559\uc2b5\ub960\uc740 \ubaa8\ub4e0 \ubaa8\ub378\uc5d0 \ucd5c\uc801\uc774 \uc544\ub2c8\ubbc0\ub85c \ucd5c\ub300 \uc815\ud655\ub3c4\ub97c \uc5bb\uc73c\ub824\uba74 \uac01 \ubaa8\ub378\uc5d0 \ub300\ud574 \uac1c\ubcc4\uc801\uc73c\ub85c \uc870\uc815\ud574\uc57c \ud569\ub2c8\ub2e4. # \uc190\uc2e4 \ud568\uc218 \uc124\uc815 criterion = nn.CrossEntropyLoss() # \ud559\uc2b5 \ubc0f \ud3c9\uac00 model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\")) Epoch 0/14 ---------- train Loss: 0.5197 Acc: 0.7377 val Loss: 0.3218 Acc: 0.8889 Epoch 1/14 ---------- train Loss: 0.3047 Acc: 0.8852 val Loss: 0.2838 Acc: 0.8954 Epoch 2/14 ---------- train Loss: 0.2320 Acc: 0.8893 val Loss: 0.2880 Acc: 0.9150 Epoch 3/14 ---------- train Loss: 0.2131 Acc: 0.9262 val Loss: 0.2946 Acc: 0.9216 Epoch 4/14 ---------- train Loss: 0.1815 Acc: 0.9262 val Loss: 0.3670 Acc: 0.9150 Epoch 5/14 ---------- train Loss: 0.1677 Acc: 0.9221 val Loss: 0.3186 Acc: 0.9216 Epoch 6/14 ---------- train Loss: 0.1863 Acc: 0.9262 val Loss: 0.3685 Acc: 0.8954 Epoch 7/14 ---------- train Loss: 0.1349 Acc: 0.9385 val Loss: 0.3709 Acc: 0.9020 Epoch 8/14 ---------- train Loss: 0.1751 Acc: 0.9098 val Loss: 0.5201 Acc: 0.8497 Epoch 9/14 ---------- train Loss: 0.2799 Acc: 0.8730 val Loss: 0.3116 Acc: 0.9150 Epoch 10/14 ---------- train Loss: 0.1438 Acc: 0.9549 val Loss: 0.3341 Acc: 0.9281 Epoch 11/14 ---------- train Loss: 0.1545 Acc: 0.9303 val Loss: 0.3258 Acc: 0.9085 Epoch 12/14 ---------- train Loss: 0.1401 Acc: 0.9344 val Loss: 0.3512 Acc: 0.9020 Epoch 13/14 ---------- train Loss: 0.1455 Acc: 0.9303 val Loss: 0.3301 Acc: 0.9216 Epoch 14/14 ---------- train Loss: 0.1322 Acc: 0.9426 val Loss: 0.3269 Acc: 0.9346 Training complete in 0m 11s Best val Acc: 0.934641 \ucc98\uc74c\ubd80\ud130 \ud559\uc2b5\ub41c \ubaa8\ub378\uacfc\uc758 \ube44\uad50# \uc7ac\ubbf8\ub85c, \uc804\uc774 \ud559\uc2b5\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc744 \uacbd\uc6b0 \ubaa8\ub378\uc774 \uc5b4\ub5bb\uac8c \ud559\uc2b5\ud558\ub294\uc9c0 \uc0b4\ud3b4\ubd05\uc2dc\ub2e4. \ubbf8\uc138 \uc870\uc815\uacfc \ud2b9\uc9d5 \ucd94\ucd9c\uc758 \uc131\ub2a5\uc740 \ub370\uc774\ud130\uc14b\uc5d0 \ub530\ub77c \ud06c\uac8c \ub2e4\ub974\uc9c0\ub9cc \uc77c\ubc18\uc801\uc73c\ub85c \ub450 \uc804\uc774 \ud559\uc2b5 \ubc29\ubc95\uc740 \ucc98\uc74c\ubd80\ud130 \ud559\uc2b5\ud55c \ubaa8\ub378\uc5d0 \ube44\ud574 \ud559\uc2b5 \uc2dc\uac04 \ubc0f \uc804\ubc18\uc801\uc778 \uc815\ud655\ub3c4 \uce21\uba74\uc5d0\uc11c \uc720\ub9ac\ud55c \uacb0\uacfc\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. # \uc774 \uc2e4\ud589\uc5d0 \uc0ac\uc6a9\ub41c \ubaa8\ub378\uc758 \uc0ac\uc804 \ud559\uc2b5\ub418\uc9c0 \uc54a\uc740 \ubc84\uc804\uc744 \ucd08\uae30\ud654\ud569\ub2c8\ub2e4. scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False) scratch_model = scratch_model.to(device) scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9) scratch_criterion = nn.CrossEntropyLoss() _,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\")) # \uc804\uc774 \ud559\uc2b5 \ubc29\ubc95\uacfc \ucc98\uc74c\ubd80\ud130 \ud559\uc2b5\ub41c \ubaa8\ub378\uc5d0 \ub300\ud55c # \uac80\uc99d \uc815\ud655\ub3c4 vs. \ud559\uc2b5 \uc5d0\ud3ed \uc218\uc5d0 \ub300\ud55c \ud559\uc2b5 \uace1\uc120\uc744 \ud45c\uc2dc\ud569\ub2c8\ub2e4. ohist = [] shist = [] ohist = [h.cpu().numpy() for h in hist] shist = [h.cpu().numpy() for h in scratch_hist] plt.title(\"Validation Accuracy vs. Number of Training Epochs\") plt.xlabel(\"Training Epochs\") plt.ylabel(\"Validation Accuracy\") plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\") plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\") plt.ylim((0,1.)) plt.xticks(np.arange(1, num_epochs+1, 1.0)) plt.legend() plt.show() /opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for \u0027weights\u0027 are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`. Epoch 0/14 ---------- train Loss: 0.7011 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 1/14 ---------- train Loss: 0.6936 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 2/14 ---------- train Loss: 0.6951 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 3/14 ---------- train Loss: 0.6930 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 4/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 5/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 6/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 7/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 8/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 9/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 10/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 11/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 12/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 13/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Epoch 14/14 ---------- train Loss: 0.6931 Acc: 0.5041 val Loss: 0.6931 Acc: 0.4575 Training complete in 0m 13s Best val Acc: 0.457516 \ucd5c\uc885 \uc0dd\uac01\uacfc \uc55e\uc73c\ub85c\uc758 \ubc29\ud5a5# \ub2e4\ub978 \ubaa8\ub378 \uba87 \uac00\uc9c0\ub97c \uc2e4\ud589\ud574\ubcf4\uace0 \uc815\ud655\ub3c4\uac00 \uc5bc\ub9c8\ub098 \uc88b\uc544\uc9c0\ub294\uc9c0 \ud655\uc778\ud574 \ubcf4\uc138\uc694. \ub610\ud55c, \uc5ed\ubc29\ud5a5 \ud328\uc2a4\uc5d0\uc11c\ub294 \ub300\ubd80\ubd84\uc758 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud560 \ud544\uc694\uac00 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \ud2b9\uc9d5 \ucd94\ucd9c\uc5d0 \uc2dc\uac04\uc774 \ub35c \uac78\ub9b0\ub2e4\ub294 \uc810\uc5d0 \uc8fc\ubaa9\ud558\uc138\uc694. \uc5ec\uae30\uc5d0\uc11c \ud560 \uc218 \uc788\ub294 \uac83\uc740 \ub9ce\uc73c\uba70 \ub2e4\uc74c\uacfc \uac19\uc774 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub354 \uc5b4\ub824\uc6b4 \ub370\uc774\ud130 \uc9d1\ud569\uc73c\ub85c \uc774 \ucf54\ub4dc\ub97c \uc2e4\ud589\ud558\uace0 \uc804\uc774 \ud559\uc2b5\uc758 \uba87 \uac00\uc9c0 \uc774\uc810\uc744 \ub354 \ud655\uc778\ud574 \ubcf4\uc138\uc694. \uc5ec\uae30\uc5d0 \uc124\uba85\ub41c \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\uac70\ub098 \uc804\uc774 \ud559\uc2b5\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc0c8\ub85c\uc6b4 domain(\uc608: NLP, \uc624\ub514\uc624 \ub4f1)\uc5d0\uc11c \ub2e4\ub978 \ubaa8\ub378\uc744 \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4. \ubaa8\ub378\uc774 \ub9cc\uc871\ud558\uba74 ONNX \ubaa8\ub378\ub85c \ub0b4\ubcf4\ub0b4\uac70\ub098 \ud558\uc774\ube0c\ub9ac\ub4dc \ud504\ub860\ud2b8\uc5d4\ub4dc\ub97c \uc0ac\uc6a9\ud574 \ub354 \ube60\ub978 \uc18d\ub3c4\uc640 \ucd5c\uc801\ud654 \uae30\ud68c\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Total running time of the script: (0 minutes 28.330 seconds) Download Jupyter notebook: finetuning_torchvision_models_tutorial.ipynb Download Python source code: finetuning_torchvision_models_tutorial.py Download zipped: finetuning_torchvision_models_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/beginner/finetuning_torchvision_models_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>