
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="Ray Tune을 사용한 하이퍼파라미터 튜닝" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/beginner/hyperparameter_tuning_tutorial.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="번역: 심형준 하이퍼파라미터 튜닝은 보통의 모델과 매우 정확한 모델간의 차이를 만들어 낼 수 있습니다. 종종 다른 학습률(Learnig rate)을 선택하거나 layer size를 변경하는 것과 같은 간단한 작업만으로도 모델 성능에 큰 영향을 미치기도 합니다. 다행히, 최적의 매개변수 조합을 찾는데 도움이 되는 도구가 있습니다. Ray Tune 은 분산 하이퍼파라미터 튜닝을 위한 업계 표준 도구입니다. Ray Tune은 최신 하이퍼파라미터 검색 알고리즘을 포함하고 다양한 분석 라이브러리와 통합되며 기본적으로 Ray 의 분산 기..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="번역: 심형준 하이퍼파라미터 튜닝은 보통의 모델과 매우 정확한 모델간의 차이를 만들어 낼 수 있습니다. 종종 다른 학습률(Learnig rate)을 선택하거나 layer size를 변경하는 것과 같은 간단한 작업만으로도 모델 성능에 큰 영향을 미치기도 합니다. 다행히, 최적의 매개변수 조합을 찾는데 도움이 되는 도구가 있습니다. Ray Tune 은 분산 하이퍼파라미터 튜닝을 위한 업계 표준 도구입니다. Ray Tune은 최신 하이퍼파라미터 검색 알고리즘을 포함하고 다양한 분석 라이브러리와 통합되며 기본적으로 Ray 의 분산 기..." />
<meta property="og:ignore_canonical" content="true" />

    <title>Ray Tune을 사용한 하이퍼파라미터 튜닝 &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'beginner/hyperparameter_tuning_tutorial';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/beginner/hyperparameter_tuning_tutorial.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="Multi-Objective NAS with Ax" href="../intermediate/ax_multiobjective_nas_tutorial.html" />
    <link rel="prev" title="Ecosystem" href="../ecosystem.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Ray Tune을 사용한 하이퍼파라미터 튜닝</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/ax_multiobjective_nas_tutorial.html">Multi-Objective NAS with Ax</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/tensorboard_profiler_tutorial.html">텐서보드를 이용한 파이토치 프로파일러</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/realtime_rpi.html">Raspberry Pi 4 에서 실시간 추론(Inference) (30fps!)</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../ecosystem.html" class="nav-link">Ecosystem</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Ray Tune을...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../ecosystem.html">
        <meta itemprop="name" content="Ecosystem">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Ray Tune을 사용한 하이퍼파라미터 튜닝">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">beginner/hyperparameter_tuning_tutorial</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-beginner-hyperparameter-tuning-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="ray-tune">
<span id="sphx-glr-beginner-hyperparameter-tuning-tutorial-py"></span><h1>Ray Tune을 사용한 하이퍼파라미터 튜닝<a class="headerlink" href="#ray-tune" title="Link to this heading">#</a></h1>
<p><strong>번역</strong>: <a class="reference external" href="http://github.com/95hj">심형준</a></p>
<p>하이퍼파라미터 튜닝은 보통의 모델과 매우 정확한 모델간의 차이를 만들어 낼 수 있습니다.
종종 다른 학습률(Learnig rate)을 선택하거나 layer size를 변경하는 것과 같은 간단한 작업만으로도 모델 성능에 큰 영향을 미치기도 합니다.</p>
<p>다행히, 최적의 매개변수 조합을 찾는데 도움이 되는 도구가 있습니다.
<a class="reference external" href="https://docs.ray.io/en/latest/tune.html">Ray Tune</a> 은 분산 하이퍼파라미터 튜닝을 위한 업계 표준 도구입니다.
Ray Tune은 최신 하이퍼파라미터 검색 알고리즘을 포함하고 다양한 분석 라이브러리와 통합되며 기본적으로
<a class="reference external" href="https://ray.io/">Ray 의 분산 기계 학습 엔진</a> 을 통해 학습을 지원합니다.</p>
<p>이 튜토리얼은 Ray Tune을 파이토치 학습 workflow에 통합하는 방법을 알려줍니다.
CIFAR10 이미지 분류기를 훈련하기 위해 <a class="reference external" href="https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html">파이토치 문서에서 이 튜토리얼을</a> 확장할 것입니다.</p>
<p>아래와 같이 약간의 수정만 추가하면 됩니다.</p>
<ol class="arabic simple">
<li><p>함수에서 데이터 로딩 및 학습 부분을 감싸두고,</p></li>
<li><p>일부 네트워크 파라미터를 구성 가능하게 하고,</p></li>
<li><p>체크포인트를 추가하고 (선택 사항),</p></li>
<li><p>모델 튜닝을 위한 검색 공간을 정의합니다.</p></li>
</ol>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>이 튜토리얼을 실행하기 위해 아래의 패키지가 설치되어 있는지 확인하세요:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ray[tune]</span></code>: 배포된 하이퍼파라미터 튜닝 라이브러리</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torchvision</span></code>: 데이터 변형을 위해 필요</p></li>
</ul>
<section id="id4">
<h2>설정 / 불러오기<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<p>필요한 라이브러리들을 불러오는 것(import)으로 시작해보겠습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">random_split</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">transforms</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray</span><span class="w"> </span><span class="kn">import</span> <span class="n">tune</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray</span><span class="w"> </span><span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.train</span><span class="w"> </span><span class="kn">import</span> <span class="n">Checkpoint</span><span class="p">,</span> <span class="n">get_checkpoint</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune.schedulers</span><span class="w"> </span><span class="kn">import</span> <span class="n">ASHAScheduler</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">ray.cloudpickle</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pickle</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning:

pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
</pre></div>
</div>
<p>대부분의 import들은 파이토치 모델을 빌드하는데 필요합니다.
가장 마지막의 import만이 Ray Tune을 사용하기 위한 것입니다.</p>
</section>
<section id="data-loaders">
<h2>Data loaders<a class="headerlink" href="#data-loaders" title="Link to this heading">#</a></h2>
<p>data loader를 자체 함수로 감싸두고 전역 데이터 디렉토리로 전달합니다.
이런 식으로 서로 다른 실험들 간에 데이터 디렉토리를 공유할 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">load_data</span><span class="p">(</span><span class="n">data_dir</span><span class="o">=</span><span class="s2">&quot;./data&quot;</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
        <span class="p">[</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))]</span>
    <span class="p">)</span>

    <span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span>
        <span class="n">root</span><span class="o">=</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
    <span class="p">)</span>

    <span class="n">testset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span>
        <span class="n">root</span><span class="o">=</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">trainset</span><span class="p">,</span> <span class="n">testset</span>
</pre></div>
</div>
</section>
<section id="id5">
<h2>구성 가능한 신경망<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>구성 가능한 파라미터만 튜닝이 가능합니다.
이 예시를 통해 fully connected layer 크기를 지정할 수 있습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l1</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mi">84</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 배치(batch) 차원을 제외한 모든 차원을 평탄화(flatten)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
<section id="id6">
<h2>학습 함수<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<p>흥미를 더해보고자 <a class="reference external" href="https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html">파이토치 문서의 예제</a>
일부를 변경하여 소개합니다.</p>
<p>학습 스크립트를 <code class="docutils literal notranslate"><span class="pre">train_cifar(config,</span> <span class="pre">data_dir=None)</span></code> 함수로 감싸둡니다.
<code class="docutils literal notranslate"><span class="pre">config</span></code> 매개변수는 학습할 하이퍼파라미터(hyperparameter)를 받습니다.
<code class="docutils literal notranslate"><span class="pre">data_dir</span></code> 은 여러 번의 실행(run) 시 동일한 데이터 소스를 공유할 수 있도록
데이터를 읽고 저장하는 디렉토리를 지정합니다.
또한, checkpoint가 지정되는 경우에는 실행 시작 시점의 모델과 옵티마이저 상태(optimizer state)를
불러올 수 있습니다. 이 튜토리얼의 아래쪽에서 체크포인트(checkpoint)를 지정하는 방법과
체크포인트의 용도에 대한 정보를 확인할 수 있습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;l1&quot;</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;l2&quot;</span><span class="p">])</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">get_checkpoint</span><span class="p">()</span>
<span class="k">if</span> <span class="n">checkpoint</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">as_directory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
        <span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;data.pkl&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">checkpoint_state</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>
        <span class="n">start_epoch</span> <span class="o">=</span> <span class="n">checkpoint_state</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">]</span>
        <span class="n">net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint_state</span><span class="p">[</span><span class="s2">&quot;net_state_dict&quot;</span><span class="p">])</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint_state</span><span class="p">[</span><span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">])</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
<p>또한, 옵티마이저의 학습률(learning rate)을 구성할 수 있습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<p>또한 학습 데이터를 학습 및 검증 세트로 나눕니다. 따라서 데이터의 80%는 모델 학습에 사용하고,
나머지 20%에 대해 유효성 검사 및 손실을 계산합니다. 학습 및 테스트 세트를 반복하는 배치 크기도 구성할 수 있습니다.</p>
<section id="dataparallel-gpu">
<h3>DataParallel을 이용한 GPU(다중)지원 추가<a class="headerlink" href="#dataparallel-gpu" title="Link to this heading">#</a></h3>
<p>이미지 분류는 GPU를 사용할 때 이점이 많습니다. 운좋게도 Ray Tune에서 파이토치의 추상화를 계속 사용할 수 있습니다.
따라서 여러 GPU에서 데이터 병렬 훈련을 지원하기 위해 모델을 <code class="docutils literal notranslate"><span class="pre">nn.DataParallel</span></code> 으로 감쌀 수 있습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda:0&quot;</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">device</span></code> 변수를 사용하여 사용 가능한 GPU가 없을 때도 학습이 가능한지 확인합니다.
파이토치는 다음과 같이 데이터를 GPU메모리에 명시적으로 보내도록 요구합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>이 코드는 이제 CPU들, 단일 GPU 및 다중 GPU에 대한 학습을 지원합니다.
특히 Ray는 <a class="reference external" href="https://docs.ray.io/en/latest/ray-core/scheduling/accelerators.html#fractional-accelerators">fractional-GPUs</a> 도 지원하므로
모델이 GPU 메모리에 적합한 상황에서는 테스트 간에 GPU를 공유할 수 있습니다. 이는 나중에 다룰 것입니다.</p>
</section>
<section id="id8">
<h3>Ray Tune으로 통신하기<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p>가장 흥미로운 부분은 Ray Tune과의 통신입니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint_data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;epoch&quot;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
    <span class="s2">&quot;net_state_dict&quot;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
<span class="p">}</span>
<span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
    <span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;data.pkl&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">checkpoint_data</span><span class="p">,</span> <span class="n">fp</span><span class="p">)</span>

    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
    <span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">(</span>
        <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">val_loss</span> <span class="o">/</span> <span class="n">val_steps</span><span class="p">,</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">},</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>여기서 먼저 체크포인트를 저장한 다음 일부 메트릭을 Ray Tune에 다시 보냅니다. 특히, validation loss와 accuracy를
Ray Tune으로 다시 보냅니다. 그 후 Ray Tune은 이러한 메트릭을 사용하여 최상의 결과를 유도하는 하이퍼파라미터 구성을
결정할 수 있습니다. 이러한 메트릭들은 또한 리소스 낭비를 방지하기 위해 성능이 좋지 않은 실험을 조기에 중지하는 데 사용할 수 있습니다.</p>
<p>체크포인트 저장은 선택사항이지만,
<a class="reference external" href="https://docs.ray.io/en/latest/tune/examples/pbt_guide.html">Population Based Training</a> 과 같은 고급 스케줄러를
사용하기 위해서는 필요합니다.
또한, 체크포인트를 저장해두면 나중에 학습된 모델을 로드하고 평가 세트(test set)에서 검증할 수 있습니다.</p>
</section>
<section id="id9">
<h3>전체 학습 함수<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p>전체 예제 코드는 다음과 같습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_cifar</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;l1&quot;</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;l2&quot;</span><span class="p">])</span>

    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda:0&quot;</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">get_checkpoint</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">checkpoint</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">as_directory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
            <span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;data.pkl&quot;</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
                <span class="n">checkpoint_state</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>
            <span class="n">start_epoch</span> <span class="o">=</span> <span class="n">checkpoint_state</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">]</span>
            <span class="n">net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint_state</span><span class="p">[</span><span class="s2">&quot;net_state_dict&quot;</span><span class="p">])</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint_state</span><span class="p">[</span><span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">start_epoch</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">trainset</span><span class="p">,</span> <span class="n">testset</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>

    <span class="n">test_abs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trainset</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
    <span class="n">train_subset</span><span class="p">,</span> <span class="n">val_subset</span> <span class="o">=</span> <span class="n">random_split</span><span class="p">(</span>
        <span class="n">trainset</span><span class="p">,</span> <span class="p">[</span><span class="n">test_abs</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainset</span><span class="p">)</span> <span class="o">-</span> <span class="n">test_abs</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">train_subset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]),</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span>
    <span class="p">)</span>
    <span class="n">valloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">val_subset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]),</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>  <span class="c1"># loop over the dataset multiple times</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">epoch_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="c1"># get the inputs; data is a list of [inputs, labels]</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># zero the parameter gradients</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="c1"># forward + backward + optimize</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># print statistics</span>
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">epoch_steps</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2000</span> <span class="o">==</span> <span class="mi">1999</span><span class="p">:</span>  <span class="c1"># print every 2000 mini-batches</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;[</span><span class="si">%d</span><span class="s2">, </span><span class="si">%5d</span><span class="s2">] loss: </span><span class="si">%.3f</span><span class="s2">&quot;</span>
                    <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">running_loss</span> <span class="o">/</span> <span class="n">epoch_steps</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="c1"># Validation loss</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">val_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">valloader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
                <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="n">val_steps</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">checkpoint_data</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;epoch&quot;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
            <span class="s2">&quot;net_state_dict&quot;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="p">}</span>
        <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
            <span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;data.pkl&quot;</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
                <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">checkpoint_data</span><span class="p">,</span> <span class="n">fp</span><span class="p">)</span>

            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">Checkpoint</span><span class="o">.</span><span class="n">from_directory</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
            <span class="n">train</span><span class="o">.</span><span class="n">report</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">val_loss</span> <span class="o">/</span> <span class="n">val_steps</span><span class="p">,</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">},</span>
                <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Finished Training&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>보다시피, 대부분의 코드는 원본 예제에서 직접 적용되었습니다.</p>
</section>
</section>
<section id="test-set-accuracy">
<h2>테스트셋 정확도(Test set accuracy)<a class="headerlink" href="#test-set-accuracy" title="Link to this heading">#</a></h2>
<p>일반적으로 머신러닝 모델의 성능은 모델 학습 시 사용하지 않은 데이터를
테스트셋으로 따로 떼어낸 뒤, 이를 사용하여 테스트합니다.
이러한 테스트셋 또한 함수로 감싸둘 수 있습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">test_accuracy</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
    <span class="n">trainset</span><span class="p">,</span> <span class="n">testset</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">()</span>

    <span class="n">testloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">testset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>

    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>
            <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
</pre></div>
</div>
<p>이 함수는 또한 <code class="docutils literal notranslate"><span class="pre">device</span></code> 파라미터를 요구하므로, test set 평가를 GPU에서 수행할 수 있습니다.</p>
</section>
<section id="id10">
<h2>검색 공간 구성<a class="headerlink" href="#id10" title="Link to this heading">#</a></h2>
<p>마지막으로 Ray Tune의 검색 공간을 정의해야 합니다. 예시는 다음과 같습니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">2</span> <span class="o">**</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)]),</span>
    <span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">2</span> <span class="o">**</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)]),</span>
    <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">),</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
<span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tune.choice()</span></code> 함수는 균일하게 샘플링된 값들의 목록을 입력으로 받습니다.
위 예시에서 <code class="docutils literal notranslate"><span class="pre">l1</span></code> 및 <code class="docutils literal notranslate"><span class="pre">l2</span></code> 파라미터는 4와 256 사이의 2의 거듭제곱 값인 4, 8, 16, 32, 64, 128, 256 입니다.
<code class="docutils literal notranslate"><span class="pre">lr</span></code> (학습률)은 0.0001과 0.1 사이에서 균일하게 샘플링 되어야 합니다. 마지막으로, 배치 크기는 2, 4, 8, 16중에서 선택할 수 있습니다.</p>
<p>각 실험에서, Ray Tune은 이제 이러한 검색 공간에서 매개변수 조합을 무작위로 샘플링합니다.
그런 다음 여러 모델을 병렬로 훈련하고 이 중에서 가장 성능이 좋은 모델을 찾습니다. 또한 성능이 좋지 않은 실험을 조기에 종료하는 <code class="docutils literal notranslate"><span class="pre">ASHAScheduler</span></code> 를 사용합니다.</p>
<p>상수 <code class="docutils literal notranslate"><span class="pre">data_dir</span></code> 파라미터를 설정하기 위해 <code class="docutils literal notranslate"><span class="pre">functools.partial</span></code> 로 <code class="docutils literal notranslate"><span class="pre">train_cifar</span></code> 함수를 감싸둡니다. 또한 각 실험에 사용할 수 있는 자원들(resources)을 Ray Tune에 알릴 수 있습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gpus_per_trial</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># ...</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
    <span class="n">partial</span><span class="p">(</span><span class="n">train_cifar</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="n">data_dir</span><span class="p">),</span>
    <span class="n">resources_per_trial</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;cpu&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="s2">&quot;gpu&quot;</span><span class="p">:</span> <span class="n">gpus_per_trial</span><span class="p">},</span>
    <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
    <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
    <span class="n">checkpoint_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>파이토치 <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 인스턴스의 <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> 을 늘리기 위해 CPU 수를 지정하고 사용할 수 있습니다.
각 실험에서 선택한 수의 GPU들은 파이토치에 표시됩니다. 실험들은 요청되지 않은 GPU에 액세스할 수 없으므로 같은 자원들을 사용하는 중복된 실험에 대해 신경쓰지 않아도 됩니다.</p>
<p>부분 GPUs를 지정할 수도 있으므로, <code class="docutils literal notranslate"><span class="pre">gpus_per_trial=0.5</span></code> 와 같은 것 또한 가능합니다. 이후 각 실험은 GPU를 공유합니다. 사용자는 모델이 여전히 GPU메모리에 적합한지만 확인하면 됩니다.</p>
<p>모델을 훈련시킨 후, 가장 성능이 좋은 모델을 찾고 체크포인트 파일에서 학습된 모델을 로드합니다. 이후 test set 정확도(accuracy)를 얻고 모든 것들을 출력하여 확인할 수 있습니다.</p>
<p>전체 주요 기능은 다음과 같습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gpus_per_trial</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">data_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;./data&quot;</span><span class="p">)</span>
    <span class="n">load_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>
    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)]),</span>
        <span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">)]),</span>
        <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">),</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">]),</span>
    <span class="p">}</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">ASHAScheduler</span><span class="p">(</span>
        <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
        <span class="n">max_t</span><span class="o">=</span><span class="n">max_num_epochs</span><span class="p">,</span>
        <span class="n">grace_period</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">reduction_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span><span class="n">train_cifar</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="n">data_dir</span><span class="p">),</span>
        <span class="n">resources_per_trial</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;cpu&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;gpu&quot;</span><span class="p">:</span> <span class="n">gpus_per_trial</span><span class="p">},</span>
        <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span>
        <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">best_trial</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">get_best_trial</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;last&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best trial config: </span><span class="si">{</span><span class="n">best_trial</span><span class="o">.</span><span class="n">config</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best trial final validation loss: </span><span class="si">{</span><span class="n">best_trial</span><span class="o">.</span><span class="n">last_result</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best trial final validation accuracy: </span><span class="si">{</span><span class="n">best_trial</span><span class="o">.</span><span class="n">last_result</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">best_trained_model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">best_trial</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;l1&quot;</span><span class="p">],</span> <span class="n">best_trial</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;l2&quot;</span><span class="p">])</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda:0&quot;</span>
        <span class="k">if</span> <span class="n">gpus_per_trial</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">best_trained_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">best_trained_model</span><span class="p">)</span>
    <span class="n">best_trained_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">best_checkpoint</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">get_best_checkpoint</span><span class="p">(</span><span class="n">trial</span><span class="o">=</span><span class="n">best_trial</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">best_checkpoint</span><span class="o">.</span><span class="n">as_directory</span><span class="p">()</span> <span class="k">as</span> <span class="n">checkpoint_dir</span><span class="p">:</span>
        <span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;data.pkl&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">best_checkpoint_data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>

        <span class="n">best_trained_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">best_checkpoint_data</span><span class="p">[</span><span class="s2">&quot;net_state_dict&quot;</span><span class="p">])</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">test_accuracy</span><span class="p">(</span><span class="n">best_trained_model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best trial test set accuracy: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># 매 실험당 사용할 GPU 수를 여기에서 변경할 수 있습니다:</span>
    <span class="n">main</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gpus_per_trial</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>  0%|          | 0.00/170M [00:00&lt;?, ?B/s]
  0%|          | 32.8k/170M [00:00&lt;14:35, 195kB/s]
  0%|          | 65.5k/170M [00:00&lt;14:38, 194kB/s]
  0%|          | 98.3k/170M [00:00&lt;14:21, 198kB/s]
  0%|          | 229k/170M [00:00&lt;06:36, 430kB/s]
  0%|          | 459k/170M [00:00&lt;03:40, 772kB/s]
  1%|          | 918k/170M [00:01&lt;01:57, 1.44MB/s]
  1%|          | 1.84M/170M [00:01&lt;01:01, 2.76MB/s]
  2%|▏         | 3.70M/170M [00:01&lt;00:30, 5.43MB/s]
  4%|▍         | 7.37M/170M [00:01&lt;00:15, 10.6MB/s]
  6%|▌         | 10.4M/170M [00:01&lt;00:13, 12.2MB/s]
  8%|▊         | 13.3M/170M [00:01&lt;00:11, 13.8MB/s]
 10%|▉         | 16.3M/170M [00:02&lt;00:10, 14.8MB/s]
 11%|█         | 19.1M/170M [00:02&lt;00:09, 15.4MB/s]
 13%|█▎        | 22.1M/170M [00:02&lt;00:09, 15.9MB/s]
 15%|█▍        | 25.1M/170M [00:02&lt;00:08, 16.4MB/s]
 16%|█▋        | 28.1M/170M [00:02&lt;00:08, 16.7MB/s]
 18%|█▊        | 30.9M/170M [00:02&lt;00:08, 16.8MB/s]
 20%|█▉        | 33.8M/170M [00:03&lt;00:08, 16.8MB/s]
 22%|██▏       | 36.8M/170M [00:03&lt;00:07, 16.9MB/s]
 23%|██▎       | 39.6M/170M [00:03&lt;00:07, 16.9MB/s]
 25%|██▌       | 42.7M/170M [00:03&lt;00:07, 17.1MB/s]
 27%|██▋       | 45.6M/170M [00:03&lt;00:07, 17.1MB/s]
 28%|██▊       | 48.5M/170M [00:03&lt;00:07, 17.1MB/s]
 30%|███       | 51.5M/170M [00:04&lt;00:06, 17.1MB/s]
 32%|███▏      | 54.3M/170M [00:04&lt;00:06, 17.0MB/s]
 34%|███▎      | 57.3M/170M [00:04&lt;00:06, 17.2MB/s]
 35%|███▌      | 60.3M/170M [00:04&lt;00:06, 17.2MB/s]
 37%|███▋      | 63.2M/170M [00:04&lt;00:06, 17.1MB/s]
 39%|███▉      | 66.2M/170M [00:04&lt;00:06, 17.2MB/s]
 41%|████      | 69.1M/170M [00:05&lt;00:05, 17.1MB/s]
 42%|████▏     | 72.1M/170M [00:05&lt;00:05, 17.1MB/s]
 44%|████▍     | 75.0M/170M [00:05&lt;00:05, 17.1MB/s]
 46%|████▌     | 78.0M/170M [00:05&lt;00:05, 17.2MB/s]
 47%|████▋     | 80.8M/170M [00:05&lt;00:05, 17.1MB/s]
 49%|████▉     | 83.8M/170M [00:05&lt;00:05, 17.1MB/s]
 51%|█████     | 86.8M/170M [00:06&lt;00:04, 17.1MB/s]
 53%|█████▎    | 89.6M/170M [00:06&lt;00:04, 17.1MB/s]
 54%|█████▍    | 92.6M/170M [00:06&lt;00:04, 17.2MB/s]
 56%|█████▌    | 95.5M/170M [00:06&lt;00:04, 17.1MB/s]
 58%|█████▊    | 98.5M/170M [00:06&lt;00:04, 17.2MB/s]
 60%|█████▉    | 101M/170M [00:07&lt;00:04, 17.2MB/s]
 61%|██████    | 104M/170M [00:07&lt;00:03, 17.1MB/s]
 63%|██████▎   | 107M/170M [00:07&lt;00:03, 17.1MB/s]
 65%|██████▍   | 110M/170M [00:07&lt;00:03, 17.2MB/s]
 66%|██████▋   | 113M/170M [00:07&lt;00:03, 17.2MB/s]
 68%|██████▊   | 116M/170M [00:07&lt;00:03, 17.2MB/s]
 70%|██████▉   | 119M/170M [00:08&lt;00:02, 17.2MB/s]
 72%|███████▏  | 122M/170M [00:08&lt;00:02, 17.3MB/s]
 73%|███████▎  | 125M/170M [00:08&lt;00:02, 17.3MB/s]
 75%|███████▌  | 128M/170M [00:08&lt;00:02, 17.2MB/s]
 77%|███████▋  | 131M/170M [00:08&lt;00:02, 17.2MB/s]
 79%|███████▊  | 134M/170M [00:08&lt;00:02, 17.2MB/s]
 80%|████████  | 137M/170M [00:09&lt;00:01, 17.2MB/s]
 82%|████████▏ | 140M/170M [00:09&lt;00:01, 17.1MB/s]
 84%|████████▎ | 143M/170M [00:09&lt;00:01, 17.0MB/s]
 85%|████████▌ | 146M/170M [00:09&lt;00:01, 17.1MB/s]
 87%|████████▋ | 149M/170M [00:09&lt;00:01, 17.0MB/s]
 89%|████████▉ | 152M/170M [00:09&lt;00:01, 16.9MB/s]
 91%|█████████ | 155M/170M [00:10&lt;00:00, 17.0MB/s]
 92%|█████████▏| 157M/170M [00:10&lt;00:00, 17.1MB/s]
 94%|█████████▍| 160M/170M [00:10&lt;00:00, 17.0MB/s]
 96%|█████████▌| 163M/170M [00:10&lt;00:00, 17.1MB/s]
 97%|█████████▋| 166M/170M [00:10&lt;00:00, 17.1MB/s]
 99%|█████████▉| 169M/170M [00:10&lt;00:00, 17.2MB/s]
100%|██████████| 170M/170M [00:10&lt;00:00, 15.5MB/s]
2025-10-03 22:26:01,958 INFO worker.py:1642 -- Started a local Ray instance.
2025-10-03 22:26:04,049 INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.
2025-10-03 22:26:04,051 INFO tune.py:654 -- [output] This will use the new output engine with verbosity 2. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
╭────────────────────────────────────────────────────────────────────╮
│ Configuration for experiment     train_cifar_2025-10-03_22-26-04   │
├────────────────────────────────────────────────────────────────────┤
│ Search algorithm                 BasicVariantGenerator             │
│ Scheduler                        AsyncHyperBandScheduler           │
│ Number of trials                 10                                │
╰────────────────────────────────────────────────────────────────────╯

View detailed results here: /root/ray_results/train_cifar_2025-10-03_22-26-04
To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/train_cifar_2025-10-03_22-26-04`

Trial status: 10 PENDING
Current time: 2025-10-03 22:26:04. Total running time: 0s
Logical resource usage: 20.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200)
╭───────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status       l1     l2            lr     batch_size │
├───────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f2b95_00000   PENDING       2    256   0.000305994             16 │
│ train_cifar_f2b95_00001   PENDING     128     32   0.010341                16 │
│ train_cifar_f2b95_00002   PENDING       4    256   0.000582548              4 │
│ train_cifar_f2b95_00003   PENDING       8      2   0.03878                  4 │
│ train_cifar_f2b95_00004   PENDING     128     64   0.0275418                2 │
│ train_cifar_f2b95_00005   PENDING       4      8   0.000769138              4 │
│ train_cifar_f2b95_00006   PENDING      64      8   0.00236933              16 │
│ train_cifar_f2b95_00007   PENDING       8    128   0.00365739               2 │
│ train_cifar_f2b95_00008   PENDING      16    128   0.000192995             16 │
│ train_cifar_f2b95_00009   PENDING       8    256   0.00117126               8 │
╰───────────────────────────────────────────────────────────────────────────────╯
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources
(raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.
(raylet)   import pkg_resources

Trial train_cifar_f2b95_00009 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00009 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     8 │
│ l1                                             8 │
│ l2                                           256 │
│ lr                                       0.00117 │
╰──────────────────────────────────────────────────╯

Trial train_cifar_f2b95_00001 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00001 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                    16 │
│ l1                                           128 │
│ l2                                            32 │
│ lr                                       0.01034 │
╰──────────────────────────────────────────────────╯

Trial train_cifar_f2b95_00007 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00007 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     2 │
│ l1                                             8 │
│ l2                                           128 │
│ lr                                       0.00366 │
╰──────────────────────────────────────────────────╯

Trial train_cifar_f2b95_00004 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00004 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     2 │
│ l1                                           128 │
│ l2                                            64 │
│ lr                                       0.02754 │
╰──────────────────────────────────────────────────╯

Trial train_cifar_f2b95_00008 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00008 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                    16 │
│ l1                                            16 │
│ l2                                           128 │
│ lr                                       0.00019 │
╰──────────────────────────────────────────────────╯

Trial train_cifar_f2b95_00003 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00003 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     4 │
│ l1                                             8 │
│ l2                                             2 │
│ lr                                       0.03878 │
╰──────────────────────────────────────────────────╯

Trial train_cifar_f2b95_00000 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00000 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                    16 │
│ l1                                             2 │
│ l2                                           256 │
│ lr                                       0.00031 │
╰──────────────────────────────────────────────────╯

Trial train_cifar_f2b95_00002 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00002 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     4 │
│ l1                                             4 │
│ l2                                           256 │
│ lr                                       0.00058 │
╰──────────────────────────────────────────────────╯

Trial train_cifar_f2b95_00006 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00006 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                    16 │
│ l1                                            64 │
│ l2                                             8 │
│ lr                                       0.00237 │
╰──────────────────────────────────────────────────╯

Trial train_cifar_f2b95_00005 started with configuration:
╭──────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00005 config             │
├──────────────────────────────────────────────────┤
│ batch_size                                     4 │
│ l1                                             4 │
│ l2                                             8 │
│ lr                                       0.00077 │
╰──────────────────────────────────────────────────╯
(func pid=12600) [1,  2000] loss: 2.083

Trial train_cifar_f2b95_00001 finished iteration 1 at 2025-10-03 22:26:20. Total running time: 16s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00001 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                  13.00158 │
│ time_total_s                                      13.00158 │
│ training_iteration                                       1 │
│ accuracy                                            0.4763 │
│ loss                                               1.45586 │
╰────────────────────────────────────────────────────────────╯
(func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000000)
Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000000

Trial train_cifar_f2b95_00008 finished iteration 1 at 2025-10-03 22:26:21. Total running time: 17s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00008 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                  13.74059 │
│ time_total_s                                      13.74059 │
│ training_iteration                                       1 │
│ accuracy                                            0.1092 │
│ loss                                                2.3008 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00008 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00008_8_batch_size=16,l1=16,l2=128,lr=0.0002_2025-10-03_22-26-04/checkpoint_000000

Trial train_cifar_f2b95_00008 completed after 1 iterations at 2025-10-03 22:26:21. Total running time: 17s

Trial train_cifar_f2b95_00000 finished iteration 1 at 2025-10-03 22:26:21. Total running time: 17s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00000 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                  14.05161 │
│ time_total_s                                      14.05161 │
│ training_iteration                                       1 │
│ accuracy                                            0.2076 │
│ loss                                               2.05941 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00000 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00000_0_batch_size=16,l1=2,l2=256,lr=0.0003_2025-10-03_22-26-04/checkpoint_000000

Trial train_cifar_f2b95_00000 completed after 1 iterations at 2025-10-03 22:26:21. Total running time: 17s

Trial train_cifar_f2b95_00006 finished iteration 1 at 2025-10-03 22:26:22. Total running time: 17s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00006 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                    14.296 │
│ time_total_s                                        14.296 │
│ training_iteration                                       1 │
│ accuracy                                            0.3684 │
│ loss                                               1.73915 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00006 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00006_6_batch_size=16,l1=64,l2=8,lr=0.0024_2025-10-03_22-26-04/checkpoint_000000
(func pid=12594) [1,  4000] loss: 1.162 [repeated 11x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)

Trial train_cifar_f2b95_00009 finished iteration 1 at 2025-10-03 22:26:27. Total running time: 22s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00009 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                   19.5243 │
│ time_total_s                                       19.5243 │
│ training_iteration                                       1 │
│ accuracy                                            0.4436 │
│ loss                                               1.53569 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000000
(func pid=12593) [1,  6000] loss: 0.613 [repeated 7x across cluster]
(func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000000) [repeated 4x across cluster]

Trial train_cifar_f2b95_00001 finished iteration 2 at 2025-10-03 22:26:30. Total running time: 25s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00001 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000001 │
│ time_this_iter_s                                   9.42768 │
│ time_total_s                                      22.42926 │
│ training_iteration                                       2 │
│ accuracy                                            0.5172 │
│ loss                                               1.35872 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000001

Trial train_cifar_f2b95_00006 finished iteration 2 at 2025-10-03 22:26:31. Total running time: 27s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00006 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000001 │
│ time_this_iter_s                                    9.2087 │
│ time_total_s                                       23.5047 │
│ training_iteration                                       2 │
│ accuracy                                            0.4657 │
│ loss                                               1.48145 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00006 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00006_6_batch_size=16,l1=64,l2=8,lr=0.0024_2025-10-03_22-26-04/checkpoint_000001

Trial train_cifar_f2b95_00006 completed after 2 iterations at 2025-10-03 22:26:31. Total running time: 27s
(func pid=12593) [1,  8000] loss: 0.445 [repeated 7x across cluster]

Trial status: 3 TERMINATED | 7 RUNNING
Current time: 2025-10-03 22:26:34. Total running time: 30s
Logical resource usage: 14.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200)
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f2b95_00001   RUNNING       128     32   0.010341                16        2            22.4293   1.35872       0.5172 │
│ train_cifar_f2b95_00002   RUNNING         4    256   0.000582548              4                                                    │
│ train_cifar_f2b95_00003   RUNNING         8      2   0.03878                  4                                                    │
│ train_cifar_f2b95_00004   RUNNING       128     64   0.0275418                2                                                    │
│ train_cifar_f2b95_00005   RUNNING         4      8   0.000769138              4                                                    │
│ train_cifar_f2b95_00007   RUNNING         8    128   0.00365739               2                                                    │
│ train_cifar_f2b95_00009   RUNNING         8    256   0.00117126               8        1            19.5243   1.53569       0.4436 │
│ train_cifar_f2b95_00000   TERMINATED      2    256   0.000305994             16        1            14.0516   2.05941       0.2076 │
│ train_cifar_f2b95_00006   TERMINATED     64      8   0.00236933              16        2            23.5047   1.48145       0.4657 │
│ train_cifar_f2b95_00008   TERMINATED     16    128   0.000192995             16        1            13.7406   2.3008        0.1092 │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=12593) [1, 10000] loss: 0.339 [repeated 7x across cluster]

Trial train_cifar_f2b95_00001 finished iteration 3 at 2025-10-03 22:26:39. Total running time: 35s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00001 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000002 │
│ time_this_iter_s                                   9.45501 │
│ time_total_s                                      31.88427 │
│ training_iteration                                       3 │
│ accuracy                                            0.5172 │
│ loss                                               1.34038 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000002
(func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000002) [repeated 3x across cluster]

Trial train_cifar_f2b95_00003 finished iteration 1 at 2025-10-03 22:26:40. Total running time: 36s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00003 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                  32.93671 │
│ time_total_s                                      32.93671 │
│ training_iteration                                       1 │
│ accuracy                                            0.0976 │
│ loss                                               2.34242 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00003 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00003_3_batch_size=4,l1=8,l2=2,lr=0.0388_2025-10-03_22-26-04/checkpoint_000000

Trial train_cifar_f2b95_00003 completed after 1 iterations at 2025-10-03 22:26:40. Total running time: 36s

Trial train_cifar_f2b95_00002 finished iteration 1 at 2025-10-03 22:26:40. Total running time: 36s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00002 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                  33.11155 │
│ time_total_s                                      33.11155 │
│ training_iteration                                       1 │
│ accuracy                                            0.3566 │
│ loss                                               1.63892 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00002 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00002_2_batch_size=4,l1=4,l2=256,lr=0.0006_2025-10-03_22-26-04/checkpoint_000000

Trial train_cifar_f2b95_00005 finished iteration 1 at 2025-10-03 22:26:41. Total running time: 37s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00005 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                  33.66846 │
│ time_total_s                                      33.66846 │
│ training_iteration                                       1 │
│ accuracy                                            0.3463 │
│ loss                                               1.70408 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00005 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00005_5_batch_size=4,l1=4,l2=8,lr=0.0008_2025-10-03_22-26-04/checkpoint_000000

Trial train_cifar_f2b95_00009 finished iteration 2 at 2025-10-03 22:26:43. Total running time: 39s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00009 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000001 │
│ time_this_iter_s                                  16.19927 │
│ time_total_s                                      35.72356 │
│ training_iteration                                       2 │
│ accuracy                                            0.5049 │
│ loss                                               1.36047 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000001
(func pid=12598) [1, 14000] loss: 0.276 [repeated 5x across cluster]

Trial train_cifar_f2b95_00001 finished iteration 4 at 2025-10-03 22:26:48. Total running time: 44s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00001 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000003 │
│ time_this_iter_s                                   9.31968 │
│ time_total_s                                      41.20394 │
│ training_iteration                                       4 │
│ accuracy                                            0.5287 │
│ loss                                               1.35828 │
╰────────────────────────────────────────────────────────────╯
(func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000003) [repeated 5x across cluster]
Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000003
(func pid=12596) [1, 14000] loss: 0.334 [repeated 7x across cluster]
(func pid=12593) [2,  6000] loss: 0.534 [repeated 6x across cluster]

Trial train_cifar_f2b95_00001 finished iteration 5 at 2025-10-03 22:26:57. Total running time: 53s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00001 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000004 │
│ time_this_iter_s                                   9.13369 │
│ time_total_s                                      50.33764 │
│ training_iteration                                       5 │
│ accuracy                                            0.5405 │
│ loss                                               1.33497 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000004
(func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000004)

Trial train_cifar_f2b95_00009 finished iteration 3 at 2025-10-03 22:26:59. Total running time: 54s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00009 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000002 │
│ time_this_iter_s                                  15.66666 │
│ time_total_s                                      51.39022 │
│ training_iteration                                       3 │
│ accuracy                                            0.5157 │
│ loss                                               1.30244 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000002
(func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000002)
(func pid=12595) [2,  8000] loss: 0.401 [repeated 5x across cluster]

Trial status: 4 TERMINATED | 6 RUNNING
Current time: 2025-10-03 22:27:04. Total running time: 1min 0s
Logical resource usage: 12.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200)
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f2b95_00001   RUNNING       128     32   0.010341                16        5            50.3376   1.33497       0.5405 │
│ train_cifar_f2b95_00002   RUNNING         4    256   0.000582548              4        1            33.1116   1.63892       0.3566 │
│ train_cifar_f2b95_00004   RUNNING       128     64   0.0275418                2                                                    │
│ train_cifar_f2b95_00005   RUNNING         4      8   0.000769138              4        1            33.6685   1.70408       0.3463 │
│ train_cifar_f2b95_00007   RUNNING         8    128   0.00365739               2                                                    │
│ train_cifar_f2b95_00009   RUNNING         8    256   0.00117126               8        3            51.3902   1.30244       0.5157 │
│ train_cifar_f2b95_00000   TERMINATED      2    256   0.000305994             16        1            14.0516   2.05941       0.2076 │
│ train_cifar_f2b95_00003   TERMINATED      8      2   0.03878                  4        1            32.9367   2.34242       0.0976 │
│ train_cifar_f2b95_00006   TERMINATED     64      8   0.00236933              16        2            23.5047   1.48145       0.4657 │
│ train_cifar_f2b95_00008   TERMINATED     16    128   0.000192995             16        1            13.7406   2.3008        0.1092 │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Trial train_cifar_f2b95_00007 finished iteration 1 at 2025-10-03 22:27:05. Total running time: 1min 0s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00007 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                  57.44913 │
│ time_total_s                                      57.44913 │
│ training_iteration                                       1 │
│ accuracy                                            0.2941 │
│ loss                                               1.85399 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00007 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00007_7_batch_size=2,l1=8,l2=128,lr=0.0037_2025-10-03_22-26-04/checkpoint_000000

Trial train_cifar_f2b95_00007 completed after 1 iterations at 2025-10-03 22:27:05. Total running time: 1min 0s
(func pid=12598) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00007_7_batch_size=2,l1=8,l2=128,lr=0.0037_2025-10-03_22-26-04/checkpoint_000000)

Trial train_cifar_f2b95_00001 finished iteration 6 at 2025-10-03 22:27:07. Total running time: 1min 3s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00001 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000005 │
│ time_this_iter_s                                   9.20401 │
│ time_total_s                                      59.54165 │
│ training_iteration                                       6 │
│ accuracy                                            0.5458 │
│ loss                                               1.38728 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 6 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000005
(func pid=12596) [1, 20000] loss: 0.233 [repeated 6x across cluster]
(func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000005)

Trial train_cifar_f2b95_00002 finished iteration 2 at 2025-10-03 22:27:09. Total running time: 1min 5s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00002 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000001 │
│ time_this_iter_s                                  28.47496 │
│ time_total_s                                      61.58651 │
│ training_iteration                                       2 │
│ accuracy                                            0.4365 │
│ loss                                               1.50182 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00002 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00002_2_batch_size=4,l1=4,l2=256,lr=0.0006_2025-10-03_22-26-04/checkpoint_000001

Trial train_cifar_f2b95_00002 completed after 2 iterations at 2025-10-03 22:27:09. Total running time: 1min 5s

Trial train_cifar_f2b95_00005 finished iteration 2 at 2025-10-03 22:27:09. Total running time: 1min 5s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00005 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000001 │
│ time_this_iter_s                                  28.50694 │
│ time_total_s                                       62.1754 │
│ training_iteration                                       2 │
│ accuracy                                            0.4021 │
│ loss                                               1.57171 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00005 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00005_5_batch_size=4,l1=4,l2=8,lr=0.0008_2025-10-03_22-26-04/checkpoint_000001

Trial train_cifar_f2b95_00005 completed after 2 iterations at 2025-10-03 22:27:09. Total running time: 1min 5s

Trial train_cifar_f2b95_00004 finished iteration 1 at 2025-10-03 22:27:13. Total running time: 1min 9s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00004 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000000 │
│ time_this_iter_s                                   66.0309 │
│ time_total_s                                       66.0309 │
│ training_iteration                                       1 │
│ accuracy                                            0.0981 │
│ loss                                               2.32891 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00004 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00004_4_batch_size=2,l1=128,l2=64,lr=0.0275_2025-10-03_22-26-04/checkpoint_000000

Trial train_cifar_f2b95_00004 completed after 1 iterations at 2025-10-03 22:27:13. Total running time: 1min 9s
(func pid=12596) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00004_4_batch_size=2,l1=128,l2=64,lr=0.0275_2025-10-03_22-26-04/checkpoint_000000) [repeated 3x across cluster]
(func pid=12592) [7,  2000] loss: 1.145 [repeated 2x across cluster]

Trial train_cifar_f2b95_00009 finished iteration 4 at 2025-10-03 22:27:14. Total running time: 1min 10s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00009 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000003 │
│ time_this_iter_s                                  15.57003 │
│ time_total_s                                      66.96025 │
│ training_iteration                                       4 │
│ accuracy                                            0.5614 │
│ loss                                               1.22414 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000003

Trial train_cifar_f2b95_00001 finished iteration 7 at 2025-10-03 22:27:16. Total running time: 1min 12s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00001 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000006 │
│ time_this_iter_s                                   9.23141 │
│ time_total_s                                      68.77305 │
│ training_iteration                                       7 │
│ accuracy                                             0.551 │
│ loss                                               1.34653 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 7 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000006
(func pid=12600) [5,  2000] loss: 1.201
(func pid=12592) [8,  2000] loss: 1.135

Trial train_cifar_f2b95_00001 finished iteration 8 at 2025-10-03 22:27:25. Total running time: 1min 21s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00001 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000007 │
│ time_this_iter_s                                   9.14384 │
│ time_total_s                                      77.91689 │
│ training_iteration                                       8 │
│ accuracy                                            0.5385 │
│ loss                                               1.37364 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 8 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000007
(func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000007) [repeated 3x across cluster]

Trial train_cifar_f2b95_00009 finished iteration 5 at 2025-10-03 22:27:30. Total running time: 1min 26s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00009 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000004 │
│ time_this_iter_s                                  16.20591 │
│ time_total_s                                      83.16616 │
│ training_iteration                                       5 │
│ accuracy                                            0.5578 │
│ loss                                                1.2334 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000004
(func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000004)
(func pid=12592) [9,  2000] loss: 1.111 [repeated 2x across cluster]

Trial status: 8 TERMINATED | 2 RUNNING
Current time: 2025-10-03 22:27:34. Total running time: 1min 30s
Logical resource usage: 4.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200)
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f2b95_00001   RUNNING       128     32   0.010341                16        8            77.9169   1.37364       0.5385 │
│ train_cifar_f2b95_00009   RUNNING         8    256   0.00117126               8        5            83.1662   1.2334        0.5578 │
│ train_cifar_f2b95_00000   TERMINATED      2    256   0.000305994             16        1            14.0516   2.05941       0.2076 │
│ train_cifar_f2b95_00002   TERMINATED      4    256   0.000582548              4        2            61.5865   1.50182       0.4365 │
│ train_cifar_f2b95_00003   TERMINATED      8      2   0.03878                  4        1            32.9367   2.34242       0.0976 │
│ train_cifar_f2b95_00004   TERMINATED    128     64   0.0275418                2        1            66.0309   2.32891       0.0981 │
│ train_cifar_f2b95_00005   TERMINATED      4      8   0.000769138              4        2            62.1754   1.57171       0.4021 │
│ train_cifar_f2b95_00006   TERMINATED     64      8   0.00236933              16        2            23.5047   1.48145       0.4657 │
│ train_cifar_f2b95_00007   TERMINATED      8    128   0.00365739               2        1            57.4491   1.85399       0.2941 │
│ train_cifar_f2b95_00008   TERMINATED     16    128   0.000192995             16        1            13.7406   2.3008        0.1092 │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Trial train_cifar_f2b95_00001 finished iteration 9 at 2025-10-03 22:27:35. Total running time: 1min 31s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00001 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000008 │
│ time_this_iter_s                                   9.95506 │
│ time_total_s                                      87.87195 │
│ training_iteration                                       9 │
│ accuracy                                            0.5591 │
│ loss                                               1.31092 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 9 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000008
(func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000008)
(func pid=12600) [6,  4000] loss: 0.585 [repeated 2x across cluster]

Trial train_cifar_f2b95_00001 finished iteration 10 at 2025-10-03 22:27:45. Total running time: 1min 41s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00001 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000009 │
│ time_this_iter_s                                    9.5594 │
│ time_total_s                                      97.43135 │
│ training_iteration                                      10 │
│ accuracy                                            0.5712 │
│ loss                                               1.29955 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 10 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000009

Trial train_cifar_f2b95_00001 completed after 10 iterations at 2025-10-03 22:27:45. Total running time: 1min 41s
(func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000009)

Trial train_cifar_f2b95_00009 finished iteration 6 at 2025-10-03 22:27:46. Total running time: 1min 42s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00009 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000005 │
│ time_this_iter_s                                  16.12082 │
│ time_total_s                                      99.28699 │
│ training_iteration                                       6 │
│ accuracy                                            0.5753 │
│ loss                                               1.20882 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 6 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000005
(func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000005)
(func pid=12600) [7,  2000] loss: 1.131 [repeated 2x across cluster]
(func pid=12600) [7,  4000] loss: 0.576

Trial train_cifar_f2b95_00009 finished iteration 7 at 2025-10-03 22:28:03. Total running time: 1min 58s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00009 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000006 │
│ time_this_iter_s                                  16.09692 │
│ time_total_s                                     115.38391 │
│ training_iteration                                       7 │
│ accuracy                                            0.5877 │
│ loss                                               1.15432 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 7 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000006
(func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000006)

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2025-10-03 22:28:04. Total running time: 2min 0s
Logical resource usage: 2.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200)
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f2b95_00009   RUNNING         8    256   0.00117126               8        7           115.384    1.15432       0.5877 │
│ train_cifar_f2b95_00000   TERMINATED      2    256   0.000305994             16        1            14.0516   2.05941       0.2076 │
│ train_cifar_f2b95_00001   TERMINATED    128     32   0.010341                16       10            97.4314   1.29955       0.5712 │
│ train_cifar_f2b95_00002   TERMINATED      4    256   0.000582548              4        2            61.5865   1.50182       0.4365 │
│ train_cifar_f2b95_00003   TERMINATED      8      2   0.03878                  4        1            32.9367   2.34242       0.0976 │
│ train_cifar_f2b95_00004   TERMINATED    128     64   0.0275418                2        1            66.0309   2.32891       0.0981 │
│ train_cifar_f2b95_00005   TERMINATED      4      8   0.000769138              4        2            62.1754   1.57171       0.4021 │
│ train_cifar_f2b95_00006   TERMINATED     64      8   0.00236933              16        2            23.5047   1.48145       0.4657 │
│ train_cifar_f2b95_00007   TERMINATED      8    128   0.00365739               2        1            57.4491   1.85399       0.2941 │
│ train_cifar_f2b95_00008   TERMINATED     16    128   0.000192995             16        1            13.7406   2.3008        0.1092 │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
(func pid=12600) [8,  2000] loss: 1.116
(func pid=12600) [8,  4000] loss: 0.563

Trial train_cifar_f2b95_00009 finished iteration 8 at 2025-10-03 22:28:19. Total running time: 2min 14s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00009 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000007 │
│ time_this_iter_s                                  16.00349 │
│ time_total_s                                     131.38739 │
│ training_iteration                                       8 │
│ accuracy                                            0.5892 │
│ loss                                                 1.172 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 8 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000007
(func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000007)
(func pid=12600) [9,  2000] loss: 1.089
(func pid=12600) [9,  4000] loss: 0.556

Trial status: 9 TERMINATED | 1 RUNNING
Current time: 2025-10-03 22:28:34. Total running time: 2min 30s
Logical resource usage: 2.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200)
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f2b95_00009   RUNNING         8    256   0.00117126               8        8           131.387    1.172         0.5892 │
│ train_cifar_f2b95_00000   TERMINATED      2    256   0.000305994             16        1            14.0516   2.05941       0.2076 │
│ train_cifar_f2b95_00001   TERMINATED    128     32   0.010341                16       10            97.4314   1.29955       0.5712 │
│ train_cifar_f2b95_00002   TERMINATED      4    256   0.000582548              4        2            61.5865   1.50182       0.4365 │
│ train_cifar_f2b95_00003   TERMINATED      8      2   0.03878                  4        1            32.9367   2.34242       0.0976 │
│ train_cifar_f2b95_00004   TERMINATED    128     64   0.0275418                2        1            66.0309   2.32891       0.0981 │
│ train_cifar_f2b95_00005   TERMINATED      4      8   0.000769138              4        2            62.1754   1.57171       0.4021 │
│ train_cifar_f2b95_00006   TERMINATED     64      8   0.00236933              16        2            23.5047   1.48145       0.4657 │
│ train_cifar_f2b95_00007   TERMINATED      8    128   0.00365739               2        1            57.4491   1.85399       0.2941 │
│ train_cifar_f2b95_00008   TERMINATED     16    128   0.000192995             16        1            13.7406   2.3008        0.1092 │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

Trial train_cifar_f2b95_00009 finished iteration 9 at 2025-10-03 22:28:34. Total running time: 2min 30s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00009 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000008 │
│ time_this_iter_s                                  15.62216 │
│ time_total_s                                     147.00955 │
│ training_iteration                                       9 │
│ accuracy                                            0.5913 │
│ loss                                               1.16831 │
╰────────────────────────────────────────────────────────────╯
(func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000008)
Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 9 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000008
(func pid=12600) [10,  2000] loss: 1.090
(func pid=12600) [10,  4000] loss: 0.545

Trial train_cifar_f2b95_00009 finished iteration 10 at 2025-10-03 22:28:51. Total running time: 2min 47s
╭────────────────────────────────────────────────────────────╮
│ Trial train_cifar_f2b95_00009 result                       │
├────────────────────────────────────────────────────────────┤
│ checkpoint_dir_name                      checkpoint_000009 │
│ time_this_iter_s                                  16.46125 │
│ time_total_s                                     163.47079 │
│ training_iteration                                      10 │
│ accuracy                                            0.6025 │
│ loss                                                1.1439 │
╰────────────────────────────────────────────────────────────╯
Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 10 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000009

Trial train_cifar_f2b95_00009 completed after 10 iterations at 2025-10-03 22:28:51. Total running time: 2min 47s

Trial status: 10 TERMINATED
Current time: 2025-10-03 22:28:51. Total running time: 2min 47s
Logical resource usage: 2.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200)
╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy │
├────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ train_cifar_f2b95_00000   TERMINATED      2    256   0.000305994             16        1            14.0516   2.05941       0.2076 │
│ train_cifar_f2b95_00001   TERMINATED    128     32   0.010341                16       10            97.4314   1.29955       0.5712 │
│ train_cifar_f2b95_00002   TERMINATED      4    256   0.000582548              4        2            61.5865   1.50182       0.4365 │
│ train_cifar_f2b95_00003   TERMINATED      8      2   0.03878                  4        1            32.9367   2.34242       0.0976 │
│ train_cifar_f2b95_00004   TERMINATED    128     64   0.0275418                2        1            66.0309   2.32891       0.0981 │
│ train_cifar_f2b95_00005   TERMINATED      4      8   0.000769138              4        2            62.1754   1.57171       0.4021 │
│ train_cifar_f2b95_00006   TERMINATED     64      8   0.00236933              16        2            23.5047   1.48145       0.4657 │
│ train_cifar_f2b95_00007   TERMINATED      8    128   0.00365739               2        1            57.4491   1.85399       0.2941 │
│ train_cifar_f2b95_00008   TERMINATED     16    128   0.000192995             16        1            13.7406   2.3008        0.1092 │
│ train_cifar_f2b95_00009   TERMINATED      8    256   0.00117126               8       10           163.471    1.1439        0.6025 │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

(func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000009)
Best trial config: {&#39;l1&#39;: 8, &#39;l2&#39;: 256, &#39;lr&#39;: 0.001171259491329369, &#39;batch_size&#39;: 8}
Best trial final validation loss: 1.143903388774395
Best trial final validation accuracy: 0.6025
Best trial test set accuracy: 0.5994
</pre></div>
</div>
<p>코드를 실행하면 결과는 다음과 같이 나올 것입니다:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>Number<span class="w"> </span>of<span class="w"> </span>trials:<span class="w"> </span><span class="m">10</span>/10<span class="w"> </span><span class="o">(</span><span class="m">10</span><span class="w"> </span>TERMINATED<span class="o">)</span>
+-----+--------------+------+------+-------------+--------+---------+------------+
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">   </span>batch_size<span class="w"> </span><span class="p">|</span><span class="w">   </span>l1<span class="w"> </span><span class="p">|</span><span class="w">   </span>l2<span class="w"> </span><span class="p">|</span><span class="w">          </span>lr<span class="w"> </span><span class="p">|</span><span class="w">   </span>iter<span class="w"> </span><span class="p">|</span><span class="w">    </span>loss<span class="w"> </span><span class="p">|</span><span class="w">   </span>accuracy<span class="w"> </span><span class="p">|</span>
<span class="p">|</span>-----+--------------+------+------+-------------+--------+---------+------------<span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w">  </span><span class="m">256</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.000668163<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">2</span>.31479<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.0977<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">4</span><span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.0331514<span class="w">   </span><span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">2</span>.31605<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.0983<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">4</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.000150295<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">2</span>.30755<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.1023<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">           </span><span class="m">16</span><span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">32</span><span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">32</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.0128248<span class="w">   </span><span class="p">|</span><span class="w">     </span><span class="m">10</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">1</span>.66912<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.4391<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">4</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w">  </span><span class="m">128</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.00464561<span class="w">  </span><span class="p">|</span><span class="w">      </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">1</span>.7316<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.3463<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w">  </span><span class="m">256</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.00031556<span class="w">  </span><span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">2</span>.19409<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.1736<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">4</span><span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">16</span><span class="w"> </span><span class="p">|</span><span class="w">  </span><span class="m">256</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.00574329<span class="w">  </span><span class="p">|</span><span class="w">      </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">1</span>.85679<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.3368<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.00325652<span class="w">  </span><span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">2</span>.30272<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.0984<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.000342987<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">1</span>.76044<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.292<span class="w">  </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span>...<span class="w"> </span><span class="p">|</span><span class="w">            </span><span class="m">4</span><span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">32</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.003734<span class="w">    </span><span class="p">|</span><span class="w">      </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="m">1</span>.53101<span class="w"> </span><span class="p">|</span><span class="w">     </span><span class="m">0</span>.4761<span class="w"> </span><span class="p">|</span>
+-----+--------------+------+------+-------------+--------+---------+------------+

Best<span class="w"> </span>trial<span class="w"> </span>config:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;l1&#39;</span>:<span class="w"> </span><span class="m">64</span>,<span class="w"> </span><span class="s1">&#39;l2&#39;</span>:<span class="w"> </span><span class="m">32</span>,<span class="w"> </span><span class="s1">&#39;lr&#39;</span>:<span class="w"> </span><span class="m">0</span>.0037339984519545164,<span class="w"> </span><span class="s1">&#39;batch_size&#39;</span>:<span class="w"> </span><span class="m">4</span><span class="o">}</span>
Best<span class="w"> </span>trial<span class="w"> </span>final<span class="w"> </span>validation<span class="w"> </span>loss:<span class="w"> </span><span class="m">1</span>.5310075663924216
Best<span class="w"> </span>trial<span class="w"> </span>final<span class="w"> </span>validation<span class="w"> </span>accuracy:<span class="w"> </span><span class="m">0</span>.4761
Best<span class="w"> </span>trial<span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="nb">set</span><span class="w"> </span>accuracy:<span class="w"> </span><span class="m">0</span>.4737
</pre></div>
</div>
<p>대부분의 실험은 자원 낭비를 막기 위해 일찍 중단되었습니다.
가장 좋은 결과를 얻은 실험은 47%의 정확도를 달성했으며,
이는 테스트셋에서 확인할 수 있습니다.</p>
<p>이것이 전부입니다! 이제 파이토치 모델의 매개변수를 조정할 수 있습니다.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (3 minutes 16.218 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-hyperparameter-tuning-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/30bcc2970bf630097b13789b5cdcea48/hyperparameter_tuning_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">hyperparameter_tuning_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/b2e3bdbf14ea1e9b3a80770f0a498037/hyperparameter_tuning_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">hyperparameter_tuning_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/1e0488dfc19f08d47b44e8a248ce666e/hyperparameter_tuning_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">hyperparameter_tuning_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="../ecosystem.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Ecosystem</p>
      </div>
    </a>
    <a class="right-next"
       href="../intermediate/ax_multiobjective_nas_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Multi-Objective NAS with Ax</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../ecosystem.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Ecosystem</p>
      </div>
    </a>
    <a class="right-next"
       href="../intermediate/ax_multiobjective_nas_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Multi-Objective NAS with Ax</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">설정 / 불러오기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-loaders">Data loaders</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">구성 가능한 신경망</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">학습 함수</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataparallel-gpu">DataParallel을 이용한 GPU(다중)지원 추가</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Ray Tune으로 통신하기</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">전체 학습 함수</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-set-accuracy">테스트셋 정확도(Test set accuracy)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">검색 공간 구성</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Ray Tune\uc744 \uc0ac\uc6a9\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd",
       "headline": "Ray Tune\uc744 \uc0ac\uc6a9\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/beginner/hyperparameter_tuning_tutorial.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. Ray Tune\uc744 \uc0ac\uc6a9\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd# \ubc88\uc5ed: \uc2ec\ud615\uc900 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc740 \ubcf4\ud1b5\uc758 \ubaa8\ub378\uacfc \ub9e4\uc6b0 \uc815\ud655\ud55c \ubaa8\ub378\uac04\uc758 \ucc28\uc774\ub97c \ub9cc\ub4e4\uc5b4 \ub0bc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc885\uc885 \ub2e4\ub978 \ud559\uc2b5\ub960(Learnig rate)\uc744 \uc120\ud0dd\ud558\uac70\ub098 layer size\ub97c \ubcc0\uacbd\ud558\ub294 \uac83\uacfc \uac19\uc740 \uac04\ub2e8\ud55c \uc791\uc5c5\ub9cc\uc73c\ub85c\ub3c4 \ubaa8\ub378 \uc131\ub2a5\uc5d0 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce58\uae30\ub3c4 \ud569\ub2c8\ub2e4. \ub2e4\ud589\ud788, \ucd5c\uc801\uc758 \ub9e4\uac1c\ubcc0\uc218 \uc870\ud569\uc744 \ucc3e\ub294\ub370 \ub3c4\uc6c0\uc774 \ub418\ub294 \ub3c4\uad6c\uac00 \uc788\uc2b5\ub2c8\ub2e4. Ray Tune \uc740 \ubd84\uc0b0 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc744 \uc704\ud55c \uc5c5\uacc4 \ud45c\uc900 \ub3c4\uad6c\uc785\ub2c8\ub2e4. Ray Tune\uc740 \ucd5c\uc2e0 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uac80\uc0c9 \uc54c\uace0\ub9ac\uc998\uc744 \ud3ec\ud568\ud558\uace0 \ub2e4\uc591\ud55c \ubd84\uc11d \ub77c\uc774\ube0c\ub7ec\ub9ac\uc640 \ud1b5\ud569\ub418\uba70 \uae30\ubcf8\uc801\uc73c\ub85c Ray \uc758 \ubd84\uc0b0 \uae30\uacc4 \ud559\uc2b5 \uc5d4\uc9c4 \uc744 \ud1b5\ud574 \ud559\uc2b5\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 Ray Tune\uc744 \ud30c\uc774\ud1a0\uce58 \ud559\uc2b5 workflow\uc5d0 \ud1b5\ud569\ud558\ub294 \ubc29\ubc95\uc744 \uc54c\ub824\uc90d\ub2c8\ub2e4. CIFAR10 \uc774\ubbf8\uc9c0 \ubd84\ub958\uae30\ub97c \ud6c8\ub828\ud558\uae30 \uc704\ud574 \ud30c\uc774\ud1a0\uce58 \ubb38\uc11c\uc5d0\uc11c \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \ud655\uc7a5\ud560 \uac83\uc785\ub2c8\ub2e4. \uc544\ub798\uc640 \uac19\uc774 \uc57d\uac04\uc758 \uc218\uc815\ub9cc \ucd94\uac00\ud558\uba74 \ub429\ub2c8\ub2e4. \ud568\uc218\uc5d0\uc11c \ub370\uc774\ud130 \ub85c\ub529 \ubc0f \ud559\uc2b5 \ubd80\ubd84\uc744 \uac10\uc2f8\ub450\uace0, \uc77c\ubd80 \ub124\ud2b8\uc6cc\ud06c \ud30c\ub77c\ubbf8\ud130\ub97c \uad6c\uc131 \uac00\ub2a5\ud558\uac8c \ud558\uace0, \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \ucd94\uac00\ud558\uace0 (\uc120\ud0dd \uc0ac\ud56d), \ubaa8\ub378 \ud29c\ub2dd\uc744 \uc704\ud55c \uac80\uc0c9 \uacf5\uac04\uc744 \uc815\uc758\ud569\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc2e4\ud589\ud558\uae30 \uc704\ud574 \uc544\ub798\uc758 \ud328\ud0a4\uc9c0\uac00 \uc124\uce58\ub418\uc5b4 \uc788\ub294\uc9c0 \ud655\uc778\ud558\uc138\uc694: ray[tune]: \ubc30\ud3ec\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd \ub77c\uc774\ube0c\ub7ec\ub9ac torchvision: \ub370\uc774\ud130 \ubcc0\ud615\uc744 \uc704\ud574 \ud544\uc694 \uc124\uc815 / \ubd88\ub7ec\uc624\uae30# \ud544\uc694\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\ub4e4\uc744 \ubd88\ub7ec\uc624\ub294 \uac83(import)\uc73c\ub85c \uc2dc\uc791\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4: from functools import partial import os import tempfile from pathlib import Path import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import random_split import torchvision import torchvision.transforms as transforms from ray import tune from ray import train from ray.train import Checkpoint, get_checkpoint from ray.tune.schedulers import ASHAScheduler import ray.cloudpickle as pickle /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. \ub300\ubd80\ubd84\uc758 import\ub4e4\uc740 \ud30c\uc774\ud1a0\uce58 \ubaa8\ub378\uc744 \ube4c\ub4dc\ud558\ub294\ub370 \ud544\uc694\ud569\ub2c8\ub2e4. \uac00\uc7a5 \ub9c8\uc9c0\ub9c9\uc758 import\ub9cc\uc774 Ray Tune\uc744 \uc0ac\uc6a9\ud558\uae30 \uc704\ud55c \uac83\uc785\ub2c8\ub2e4. Data loaders# data loader\ub97c \uc790\uccb4 \ud568\uc218\ub85c \uac10\uc2f8\ub450\uace0 \uc804\uc5ed \ub370\uc774\ud130 \ub514\ub809\ud1a0\ub9ac\ub85c \uc804\ub2ec\ud569\ub2c8\ub2e4. \uc774\ub7f0 \uc2dd\uc73c\ub85c \uc11c\ub85c \ub2e4\ub978 \uc2e4\ud5d8\ub4e4 \uac04\uc5d0 \ub370\uc774\ud130 \ub514\ub809\ud1a0\ub9ac\ub97c \uacf5\uc720\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. def load_data(data_dir=\"./data\"): transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] ) trainset = torchvision.datasets.CIFAR10( root=data_dir, train=True, download=True, transform=transform ) testset = torchvision.datasets.CIFAR10( root=data_dir, train=False, download=True, transform=transform ) return trainset, testset \uad6c\uc131 \uac00\ub2a5\ud55c \uc2e0\uacbd\ub9dd# \uad6c\uc131 \uac00\ub2a5\ud55c \ud30c\ub77c\ubbf8\ud130\ub9cc \ud29c\ub2dd\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. \uc774 \uc608\uc2dc\ub97c \ud1b5\ud574 fully connected layer \ud06c\uae30\ub97c \uc9c0\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: class Net(nn.Module): def __init__(self, l1=120, l2=84): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, l1) self.fc2 = nn.Linear(l1, l2) self.fc3 = nn.Linear(l2, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = torch.flatten(x, 1) # \ubc30\uce58(batch) \ucc28\uc6d0\uc744 \uc81c\uc678\ud55c \ubaa8\ub4e0 \ucc28\uc6d0\uc744 \ud3c9\ud0c4\ud654(flatten) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x \ud559\uc2b5 \ud568\uc218# \ud765\ubbf8\ub97c \ub354\ud574\ubcf4\uace0\uc790 \ud30c\uc774\ud1a0\uce58 \ubb38\uc11c\uc758 \uc608\uc81c \uc77c\ubd80\ub97c \ubcc0\uacbd\ud558\uc5ec \uc18c\uac1c\ud569\ub2c8\ub2e4. \ud559\uc2b5 \uc2a4\ud06c\ub9bd\ud2b8\ub97c train_cifar(config, data_dir=None) \ud568\uc218\ub85c \uac10\uc2f8\ub461\ub2c8\ub2e4. config \ub9e4\uac1c\ubcc0\uc218\ub294 \ud559\uc2b5\ud560 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130(hyperparameter)\ub97c \ubc1b\uc2b5\ub2c8\ub2e4. data_dir \uc740 \uc5ec\ub7ec \ubc88\uc758 \uc2e4\ud589(run) \uc2dc \ub3d9\uc77c\ud55c \ub370\uc774\ud130 \uc18c\uc2a4\ub97c \uacf5\uc720\ud560 \uc218 \uc788\ub3c4\ub85d \ub370\uc774\ud130\ub97c \uc77d\uace0 \uc800\uc7a5\ud558\ub294 \ub514\ub809\ud1a0\ub9ac\ub97c \uc9c0\uc815\ud569\ub2c8\ub2e4. \ub610\ud55c, checkpoint\uac00 \uc9c0\uc815\ub418\ub294 \uacbd\uc6b0\uc5d0\ub294 \uc2e4\ud589 \uc2dc\uc791 \uc2dc\uc810\uc758 \ubaa8\ub378\uacfc \uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc(optimizer state)\ub97c \ubd88\ub7ec\uc62c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \uc544\ub798\ucabd\uc5d0\uc11c \uccb4\ud06c\ud3ec\uc778\ud2b8(checkpoint)\ub97c \uc9c0\uc815\ud558\ub294 \ubc29\ubc95\uacfc \uccb4\ud06c\ud3ec\uc778\ud2b8\uc758 \uc6a9\ub3c4\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. net = Net(config[\"l1\"], config[\"l2\"]) checkpoint = get_checkpoint() if checkpoint: with checkpoint.as_directory() as checkpoint_dir: data_path = Path(checkpoint_dir) / \"data.pkl\" with open(data_path, \"rb\") as fp: checkpoint_state = pickle.load(fp) start_epoch = checkpoint_state[\"epoch\"] net.load_state_dict(checkpoint_state[\"net_state_dict\"]) optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"]) else: start_epoch = 0 \ub610\ud55c, \uc635\ud2f0\ub9c8\uc774\uc800\uc758 \ud559\uc2b5\ub960(learning rate)\uc744 \uad6c\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9) \ub610\ud55c \ud559\uc2b5 \ub370\uc774\ud130\ub97c \ud559\uc2b5 \ubc0f \uac80\uc99d \uc138\ud2b8\ub85c \ub098\ub215\ub2c8\ub2e4. \ub530\ub77c\uc11c \ub370\uc774\ud130\uc758 80%\ub294 \ubaa8\ub378 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud558\uace0, \ub098\uba38\uc9c0 20%\uc5d0 \ub300\ud574 \uc720\ud6a8\uc131 \uac80\uc0ac \ubc0f \uc190\uc2e4\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4. \ud559\uc2b5 \ubc0f \ud14c\uc2a4\ud2b8 \uc138\ud2b8\ub97c \ubc18\ubcf5\ud558\ub294 \ubc30\uce58 \ud06c\uae30\ub3c4 \uad6c\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. DataParallel\uc744 \uc774\uc6a9\ud55c GPU(\ub2e4\uc911)\uc9c0\uc6d0 \ucd94\uac00# \uc774\ubbf8\uc9c0 \ubd84\ub958\ub294 GPU\ub97c \uc0ac\uc6a9\ud560 \ub54c \uc774\uc810\uc774 \ub9ce\uc2b5\ub2c8\ub2e4. \uc6b4\uc88b\uac8c\ub3c4 Ray Tune\uc5d0\uc11c \ud30c\uc774\ud1a0\uce58\uc758 \ucd94\uc0c1\ud654\ub97c \uacc4\uc18d \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc5ec\ub7ec GPU\uc5d0\uc11c \ub370\uc774\ud130 \ubcd1\ub82c \ud6c8\ub828\uc744 \uc9c0\uc6d0\ud558\uae30 \uc704\ud574 \ubaa8\ub378\uc744 nn.DataParallel \uc73c\ub85c \uac10\uc300 \uc218 \uc788\uc2b5\ub2c8\ub2e4. device = \"cpu\" if torch.cuda.is_available(): device = \"cuda:0\" if torch.cuda.device_count() \u003e 1: net = nn.DataParallel(net) net.to(device) device \ubcc0\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc0ac\uc6a9 \uac00\ub2a5\ud55c GPU\uac00 \uc5c6\uc744 \ub54c\ub3c4 \ud559\uc2b5\uc774 \uac00\ub2a5\ud55c\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4. \ud30c\uc774\ud1a0\uce58\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \ub370\uc774\ud130\ub97c GPU\uba54\ubaa8\ub9ac\uc5d0 \uba85\uc2dc\uc801\uc73c\ub85c \ubcf4\ub0b4\ub3c4\ub85d \uc694\uad6c\ud569\ub2c8\ub2e4. for i, data in enumerate(trainloader, 0): inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) \uc774 \ucf54\ub4dc\ub294 \uc774\uc81c CPU\ub4e4, \ub2e8\uc77c GPU \ubc0f \ub2e4\uc911 GPU\uc5d0 \ub300\ud55c \ud559\uc2b5\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \ud2b9\ud788 Ray\ub294 fractional-GPUs \ub3c4 \uc9c0\uc6d0\ud558\ubbc0\ub85c \ubaa8\ub378\uc774 GPU \uba54\ubaa8\ub9ac\uc5d0 \uc801\ud569\ud55c \uc0c1\ud669\uc5d0\uc11c\ub294 \ud14c\uc2a4\ud2b8 \uac04\uc5d0 GPU\ub97c \uacf5\uc720\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ub098\uc911\uc5d0 \ub2e4\ub8f0 \uac83\uc785\ub2c8\ub2e4. Ray Tune\uc73c\ub85c \ud1b5\uc2e0\ud558\uae30# \uac00\uc7a5 \ud765\ubbf8\ub85c\uc6b4 \ubd80\ubd84\uc740 Ray Tune\uacfc\uc758 \ud1b5\uc2e0\uc785\ub2c8\ub2e4: checkpoint_data = { \"epoch\": epoch, \"net_state_dict\": net.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), } with tempfile.TemporaryDirectory() as checkpoint_dir: data_path = Path(checkpoint_dir) / \"data.pkl\" with open(data_path, \"wb\") as fp: pickle.dump(checkpoint_data, fp) checkpoint = Checkpoint.from_directory(checkpoint_dir) train.report( {\"loss\": val_loss / val_steps, \"accuracy\": correct / total}, checkpoint=checkpoint, ) \uc5ec\uae30\uc11c \uba3c\uc800 \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uc800\uc7a5\ud55c \ub2e4\uc74c \uc77c\ubd80 \uba54\ud2b8\ub9ad\uc744 Ray Tune\uc5d0 \ub2e4\uc2dc \ubcf4\ub0c5\ub2c8\ub2e4. \ud2b9\ud788, validation loss\uc640 accuracy\ub97c Ray Tune\uc73c\ub85c \ub2e4\uc2dc \ubcf4\ub0c5\ub2c8\ub2e4. \uadf8 \ud6c4 Ray Tune\uc740 \uc774\ub7ec\ud55c \uba54\ud2b8\ub9ad\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucd5c\uc0c1\uc758 \uacb0\uacfc\ub97c \uc720\ub3c4\ud558\ub294 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uad6c\uc131\uc744 \uacb0\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uba54\ud2b8\ub9ad\ub4e4\uc740 \ub610\ud55c \ub9ac\uc18c\uc2a4 \ub0ad\ube44\ub97c \ubc29\uc9c0\ud558\uae30 \uc704\ud574 \uc131\ub2a5\uc774 \uc88b\uc9c0 \uc54a\uc740 \uc2e4\ud5d8\uc744 \uc870\uae30\uc5d0 \uc911\uc9c0\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uccb4\ud06c\ud3ec\uc778\ud2b8 \uc800\uc7a5\uc740 \uc120\ud0dd\uc0ac\ud56d\uc774\uc9c0\ub9cc, Population Based Training \uacfc \uac19\uc740 \uace0\uae09 \uc2a4\ucf00\uc904\ub7ec\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574\uc11c\ub294 \ud544\uc694\ud569\ub2c8\ub2e4. \ub610\ud55c, \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uc800\uc7a5\ud574\ub450\uba74 \ub098\uc911\uc5d0 \ud559\uc2b5\ub41c \ubaa8\ub378\uc744 \ub85c\ub4dc\ud558\uace0 \ud3c9\uac00 \uc138\ud2b8(test set)\uc5d0\uc11c \uac80\uc99d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc804\uccb4 \ud559\uc2b5 \ud568\uc218# \uc804\uccb4 \uc608\uc81c \ucf54\ub4dc\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. def train_cifar(config, data_dir=None): net = Net(config[\"l1\"], config[\"l2\"]) device = \"cpu\" if torch.cuda.is_available(): device = \"cuda:0\" if torch.cuda.device_count() \u003e 1: net = nn.DataParallel(net) net.to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9) checkpoint = get_checkpoint() if checkpoint: with checkpoint.as_directory() as checkpoint_dir: data_path = Path(checkpoint_dir) / \"data.pkl\" with open(data_path, \"rb\") as fp: checkpoint_state = pickle.load(fp) start_epoch = checkpoint_state[\"epoch\"] net.load_state_dict(checkpoint_state[\"net_state_dict\"]) optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"]) else: start_epoch = 0 trainset, testset = load_data(data_dir) test_abs = int(len(trainset) * 0.8) train_subset, val_subset = random_split( trainset, [test_abs, len(trainset) - test_abs] ) trainloader = torch.utils.data.DataLoader( train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8 ) valloader = torch.utils.data.DataLoader( val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8 ) for epoch in range(start_epoch, 10): # loop over the dataset multiple times running_loss = 0.0 epoch_steps = 0 for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() epoch_steps += 1 if i % 2000 == 1999: # print every 2000 mini-batches print( \"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / epoch_steps) ) running_loss = 0.0 # Validation loss val_loss = 0.0 val_steps = 0 total = 0 correct = 0 for i, data in enumerate(valloader, 0): with torch.no_grad(): inputs, labels = data inputs, labels = inputs.to(device), labels.to(device) outputs = net(inputs) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() loss = criterion(outputs, labels) val_loss += loss.cpu().numpy() val_steps += 1 checkpoint_data = { \"epoch\": epoch, \"net_state_dict\": net.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), } with tempfile.TemporaryDirectory() as checkpoint_dir: data_path = Path(checkpoint_dir) / \"data.pkl\" with open(data_path, \"wb\") as fp: pickle.dump(checkpoint_data, fp) checkpoint = Checkpoint.from_directory(checkpoint_dir) train.report( {\"loss\": val_loss / val_steps, \"accuracy\": correct / total}, checkpoint=checkpoint, ) print(\"Finished Training\") \ubcf4\ub2e4\uc2dc\ud53c, \ub300\ubd80\ubd84\uc758 \ucf54\ub4dc\ub294 \uc6d0\ubcf8 \uc608\uc81c\uc5d0\uc11c \uc9c1\uc811 \uc801\uc6a9\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ud14c\uc2a4\ud2b8\uc14b \uc815\ud655\ub3c4(Test set accuracy)# \uc77c\ubc18\uc801\uc73c\ub85c \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc758 \uc131\ub2a5\uc740 \ubaa8\ub378 \ud559\uc2b5 \uc2dc \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 \ub370\uc774\ud130\ub97c \ud14c\uc2a4\ud2b8\uc14b\uc73c\ub85c \ub530\ub85c \ub5bc\uc5b4\ub0b8 \ub4a4, \uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ud14c\uc2a4\ud2b8\uc14b \ub610\ud55c \ud568\uc218\ub85c \uac10\uc2f8\ub458 \uc218 \uc788\uc2b5\ub2c8\ub2e4: def test_accuracy(net, device=\"cpu\"): trainset, testset = load_data() testloader = torch.utils.data.DataLoader( testset, batch_size=4, shuffle=False, num_workers=2 ) correct = 0 total = 0 with torch.no_grad(): for data in testloader: images, labels = data images, labels = images.to(device), labels.to(device) outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() return correct / total \uc774 \ud568\uc218\ub294 \ub610\ud55c device \ud30c\ub77c\ubbf8\ud130\ub97c \uc694\uad6c\ud558\ubbc0\ub85c, test set \ud3c9\uac00\ub97c GPU\uc5d0\uc11c \uc218\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uac80\uc0c9 \uacf5\uac04 \uad6c\uc131# \ub9c8\uc9c0\ub9c9\uc73c\ub85c Ray Tune\uc758 \uac80\uc0c9 \uacf5\uac04\uc744 \uc815\uc758\ud574\uc57c \ud569\ub2c8\ub2e4. \uc608\uc2dc\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4: config = { \"l1\": tune.choice([2 ** i for i in range(9)]), \"l2\": tune.choice([2 ** i for i in range(9)]), \"lr\": tune.loguniform(1e-4, 1e-1), \"batch_size\": tune.choice([2, 4, 8, 16]) } tune.choice() \ud568\uc218\ub294 \uade0\uc77c\ud558\uac8c \uc0d8\ud50c\ub9c1\ub41c \uac12\ub4e4\uc758 \ubaa9\ub85d\uc744 \uc785\ub825\uc73c\ub85c \ubc1b\uc2b5\ub2c8\ub2e4. \uc704 \uc608\uc2dc\uc5d0\uc11c l1 \ubc0f l2 \ud30c\ub77c\ubbf8\ud130\ub294 4\uc640 256 \uc0ac\uc774\uc758 2\uc758 \uac70\ub4ed\uc81c\uacf1 \uac12\uc778 4, 8, 16, 32, 64, 128, 256 \uc785\ub2c8\ub2e4. lr (\ud559\uc2b5\ub960)\uc740 0.0001\uacfc 0.1 \uc0ac\uc774\uc5d0\uc11c \uade0\uc77c\ud558\uac8c \uc0d8\ud50c\ub9c1 \ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c, \ubc30\uce58 \ud06c\uae30\ub294 2, 4, 8, 16\uc911\uc5d0\uc11c \uc120\ud0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uc2e4\ud5d8\uc5d0\uc11c, Ray Tune\uc740 \uc774\uc81c \uc774\ub7ec\ud55c \uac80\uc0c9 \uacf5\uac04\uc5d0\uc11c \ub9e4\uac1c\ubcc0\uc218 \uc870\ud569\uc744 \ubb34\uc791\uc704\ub85c \uc0d8\ud50c\ub9c1\ud569\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c \uc5ec\ub7ec \ubaa8\ub378\uc744 \ubcd1\ub82c\ub85c \ud6c8\ub828\ud558\uace0 \uc774 \uc911\uc5d0\uc11c \uac00\uc7a5 \uc131\ub2a5\uc774 \uc88b\uc740 \ubaa8\ub378\uc744 \ucc3e\uc2b5\ub2c8\ub2e4. \ub610\ud55c \uc131\ub2a5\uc774 \uc88b\uc9c0 \uc54a\uc740 \uc2e4\ud5d8\uc744 \uc870\uae30\uc5d0 \uc885\ub8cc\ud558\ub294 ASHAScheduler \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc0c1\uc218 data_dir \ud30c\ub77c\ubbf8\ud130\ub97c \uc124\uc815\ud558\uae30 \uc704\ud574 functools.partial \ub85c train_cifar \ud568\uc218\ub97c \uac10\uc2f8\ub461\ub2c8\ub2e4. \ub610\ud55c \uac01 \uc2e4\ud5d8\uc5d0 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uc790\uc6d0\ub4e4(resources)\uc744 Ray Tune\uc5d0 \uc54c\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. gpus_per_trial = 2 # ... result = tune.run( partial(train_cifar, data_dir=data_dir), resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial}, config=config, num_samples=num_samples, scheduler=scheduler, checkpoint_at_end=True) \ud30c\uc774\ud1a0\uce58 DataLoader \uc778\uc2a4\ud134\uc2a4\uc758 num_workers \uc744 \ub298\ub9ac\uae30 \uc704\ud574 CPU \uc218\ub97c \uc9c0\uc815\ud558\uace0 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uc2e4\ud5d8\uc5d0\uc11c \uc120\ud0dd\ud55c \uc218\uc758 GPU\ub4e4\uc740 \ud30c\uc774\ud1a0\uce58\uc5d0 \ud45c\uc2dc\ub429\ub2c8\ub2e4. \uc2e4\ud5d8\ub4e4\uc740 \uc694\uccad\ub418\uc9c0 \uc54a\uc740 GPU\uc5d0 \uc561\uc138\uc2a4\ud560 \uc218 \uc5c6\uc73c\ubbc0\ub85c \uac19\uc740 \uc790\uc6d0\ub4e4\uc744 \uc0ac\uc6a9\ud558\ub294 \uc911\ubcf5\ub41c \uc2e4\ud5d8\uc5d0 \ub300\ud574 \uc2e0\uacbd\uc4f0\uc9c0 \uc54a\uc544\ub3c4 \ub429\ub2c8\ub2e4. \ubd80\ubd84 GPUs\ub97c \uc9c0\uc815\ud560 \uc218\ub3c4 \uc788\uc73c\ubbc0\ub85c, gpus_per_trial=0.5 \uc640 \uac19\uc740 \uac83 \ub610\ud55c \uac00\ub2a5\ud569\ub2c8\ub2e4. \uc774\ud6c4 \uac01 \uc2e4\ud5d8\uc740 GPU\ub97c \uacf5\uc720\ud569\ub2c8\ub2e4. \uc0ac\uc6a9\uc790\ub294 \ubaa8\ub378\uc774 \uc5ec\uc804\ud788 GPU\uba54\ubaa8\ub9ac\uc5d0 \uc801\ud569\ud55c\uc9c0\ub9cc \ud655\uc778\ud558\uba74 \ub429\ub2c8\ub2e4. \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0a8 \ud6c4, \uac00\uc7a5 \uc131\ub2a5\uc774 \uc88b\uc740 \ubaa8\ub378\uc744 \ucc3e\uace0 \uccb4\ud06c\ud3ec\uc778\ud2b8 \ud30c\uc77c\uc5d0\uc11c \ud559\uc2b5\ub41c \ubaa8\ub378\uc744 \ub85c\ub4dc\ud569\ub2c8\ub2e4. \uc774\ud6c4 test set \uc815\ud655\ub3c4(accuracy)\ub97c \uc5bb\uace0 \ubaa8\ub4e0 \uac83\ub4e4\uc744 \ucd9c\ub825\ud558\uc5ec \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc804\uccb4 \uc8fc\uc694 \uae30\ub2a5\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2): data_dir = os.path.abspath(\"./data\") load_data(data_dir) config = { \"l1\": tune.choice([2**i for i in range(9)]), \"l2\": tune.choice([2**i for i in range(9)]), \"lr\": tune.loguniform(1e-4, 1e-1), \"batch_size\": tune.choice([2, 4, 8, 16]), } scheduler = ASHAScheduler( metric=\"loss\", mode=\"min\", max_t=max_num_epochs, grace_period=1, reduction_factor=2, ) result = tune.run( partial(train_cifar, data_dir=data_dir), resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial}, config=config, num_samples=num_samples, scheduler=scheduler, ) best_trial = result.get_best_trial(\"loss\", \"min\", \"last\") print(f\"Best trial config: {best_trial.config}\") print(f\"Best trial final validation loss: {best_trial.last_result[\u0027loss\u0027]}\") print(f\"Best trial final validation accuracy: {best_trial.last_result[\u0027accuracy\u0027]}\") best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"]) device = \"cpu\" if torch.cuda.is_available(): device = \"cuda:0\" if gpus_per_trial \u003e 1: best_trained_model = nn.DataParallel(best_trained_model) best_trained_model.to(device) best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric=\"accuracy\", mode=\"max\") with best_checkpoint.as_directory() as checkpoint_dir: data_path = Path(checkpoint_dir) / \"data.pkl\" with open(data_path, \"rb\") as fp: best_checkpoint_data = pickle.load(fp) best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"]) test_acc = test_accuracy(best_trained_model, device) print(\"Best trial test set accuracy: {}\".format(test_acc)) if __name__ == \"__main__\": # \ub9e4 \uc2e4\ud5d8\ub2f9 \uc0ac\uc6a9\ud560 GPU \uc218\ub97c \uc5ec\uae30\uc5d0\uc11c \ubcc0\uacbd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: main(num_samples=10, max_num_epochs=10, gpus_per_trial=0) 0%| | 0.00/170M [00:00\u003c?, ?B/s] 0%| | 32.8k/170M [00:00\u003c14:35, 195kB/s] 0%| | 65.5k/170M [00:00\u003c14:38, 194kB/s] 0%| | 98.3k/170M [00:00\u003c14:21, 198kB/s] 0%| | 229k/170M [00:00\u003c06:36, 430kB/s] 0%| | 459k/170M [00:00\u003c03:40, 772kB/s] 1%| | 918k/170M [00:01\u003c01:57, 1.44MB/s] 1%| | 1.84M/170M [00:01\u003c01:01, 2.76MB/s] 2%|\u258f | 3.70M/170M [00:01\u003c00:30, 5.43MB/s] 4%|\u258d | 7.37M/170M [00:01\u003c00:15, 10.6MB/s] 6%|\u258c | 10.4M/170M [00:01\u003c00:13, 12.2MB/s] 8%|\u258a | 13.3M/170M [00:01\u003c00:11, 13.8MB/s] 10%|\u2589 | 16.3M/170M [00:02\u003c00:10, 14.8MB/s] 11%|\u2588 | 19.1M/170M [00:02\u003c00:09, 15.4MB/s] 13%|\u2588\u258e | 22.1M/170M [00:02\u003c00:09, 15.9MB/s] 15%|\u2588\u258d | 25.1M/170M [00:02\u003c00:08, 16.4MB/s] 16%|\u2588\u258b | 28.1M/170M [00:02\u003c00:08, 16.7MB/s] 18%|\u2588\u258a | 30.9M/170M [00:02\u003c00:08, 16.8MB/s] 20%|\u2588\u2589 | 33.8M/170M [00:03\u003c00:08, 16.8MB/s] 22%|\u2588\u2588\u258f | 36.8M/170M [00:03\u003c00:07, 16.9MB/s] 23%|\u2588\u2588\u258e | 39.6M/170M [00:03\u003c00:07, 16.9MB/s] 25%|\u2588\u2588\u258c | 42.7M/170M [00:03\u003c00:07, 17.1MB/s] 27%|\u2588\u2588\u258b | 45.6M/170M [00:03\u003c00:07, 17.1MB/s] 28%|\u2588\u2588\u258a | 48.5M/170M [00:03\u003c00:07, 17.1MB/s] 30%|\u2588\u2588\u2588 | 51.5M/170M [00:04\u003c00:06, 17.1MB/s] 32%|\u2588\u2588\u2588\u258f | 54.3M/170M [00:04\u003c00:06, 17.0MB/s] 34%|\u2588\u2588\u2588\u258e | 57.3M/170M [00:04\u003c00:06, 17.2MB/s] 35%|\u2588\u2588\u2588\u258c | 60.3M/170M [00:04\u003c00:06, 17.2MB/s] 37%|\u2588\u2588\u2588\u258b | 63.2M/170M [00:04\u003c00:06, 17.1MB/s] 39%|\u2588\u2588\u2588\u2589 | 66.2M/170M [00:04\u003c00:06, 17.2MB/s] 41%|\u2588\u2588\u2588\u2588 | 69.1M/170M [00:05\u003c00:05, 17.1MB/s] 42%|\u2588\u2588\u2588\u2588\u258f | 72.1M/170M [00:05\u003c00:05, 17.1MB/s] 44%|\u2588\u2588\u2588\u2588\u258d | 75.0M/170M [00:05\u003c00:05, 17.1MB/s] 46%|\u2588\u2588\u2588\u2588\u258c | 78.0M/170M [00:05\u003c00:05, 17.2MB/s] 47%|\u2588\u2588\u2588\u2588\u258b | 80.8M/170M [00:05\u003c00:05, 17.1MB/s] 49%|\u2588\u2588\u2588\u2588\u2589 | 83.8M/170M [00:05\u003c00:05, 17.1MB/s] 51%|\u2588\u2588\u2588\u2588\u2588 | 86.8M/170M [00:06\u003c00:04, 17.1MB/s] 53%|\u2588\u2588\u2588\u2588\u2588\u258e | 89.6M/170M [00:06\u003c00:04, 17.1MB/s] 54%|\u2588\u2588\u2588\u2588\u2588\u258d | 92.6M/170M [00:06\u003c00:04, 17.2MB/s] 56%|\u2588\u2588\u2588\u2588\u2588\u258c | 95.5M/170M [00:06\u003c00:04, 17.1MB/s] 58%|\u2588\u2588\u2588\u2588\u2588\u258a | 98.5M/170M [00:06\u003c00:04, 17.2MB/s] 60%|\u2588\u2588\u2588\u2588\u2588\u2589 | 101M/170M [00:07\u003c00:04, 17.2MB/s] 61%|\u2588\u2588\u2588\u2588\u2588\u2588 | 104M/170M [00:07\u003c00:03, 17.1MB/s] 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 107M/170M [00:07\u003c00:03, 17.1MB/s] 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 110M/170M [00:07\u003c00:03, 17.2MB/s] 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 113M/170M [00:07\u003c00:03, 17.2MB/s] 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 116M/170M [00:07\u003c00:03, 17.2MB/s] 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 119M/170M [00:08\u003c00:02, 17.2MB/s] 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 122M/170M [00:08\u003c00:02, 17.3MB/s] 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 125M/170M [00:08\u003c00:02, 17.3MB/s] 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 128M/170M [00:08\u003c00:02, 17.2MB/s] 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 131M/170M [00:08\u003c00:02, 17.2MB/s] 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 134M/170M [00:08\u003c00:02, 17.2MB/s] 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 137M/170M [00:09\u003c00:01, 17.2MB/s] 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 140M/170M [00:09\u003c00:01, 17.1MB/s] 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 143M/170M [00:09\u003c00:01, 17.0MB/s] 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 146M/170M [00:09\u003c00:01, 17.1MB/s] 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 149M/170M [00:09\u003c00:01, 17.0MB/s] 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 152M/170M [00:09\u003c00:01, 16.9MB/s] 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 155M/170M [00:10\u003c00:00, 17.0MB/s] 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 157M/170M [00:10\u003c00:00, 17.1MB/s] 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 160M/170M [00:10\u003c00:00, 17.0MB/s] 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 163M/170M [00:10\u003c00:00, 17.1MB/s] 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 166M/170M [00:10\u003c00:00, 17.1MB/s] 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 169M/170M [00:10\u003c00:00, 17.2MB/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 170M/170M [00:10\u003c00:00, 15.5MB/s] 2025-10-03 22:26:01,958 INFO worker.py:1642 -- Started a local Ray instance. 2025-10-03 22:26:04,049 INFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`. 2025-10-03 22:26:04,051 INFO tune.py:654 -- [output] This will use the new output engine with verbosity 2. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Configuration for experiment train_cifar_2025-10-03_22-26-04 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 Search algorithm BasicVariantGenerator \u2502 \u2502 Scheduler AsyncHyperBandScheduler \u2502 \u2502 Number of trials 10 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f View detailed results here: /root/ray_results/train_cifar_2025-10-03_22-26-04 To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/train_cifar_2025-10-03_22-26-04` Trial status: 10 PENDING Current time: 2025-10-03 22:26:04. Total running time: 0s Logical resource usage: 20.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f2b95_00000 PENDING 2 256 0.000305994 16 \u2502 \u2502 train_cifar_f2b95_00001 PENDING 128 32 0.010341 16 \u2502 \u2502 train_cifar_f2b95_00002 PENDING 4 256 0.000582548 4 \u2502 \u2502 train_cifar_f2b95_00003 PENDING 8 2 0.03878 4 \u2502 \u2502 train_cifar_f2b95_00004 PENDING 128 64 0.0275418 2 \u2502 \u2502 train_cifar_f2b95_00005 PENDING 4 8 0.000769138 4 \u2502 \u2502 train_cifar_f2b95_00006 PENDING 64 8 0.00236933 16 \u2502 \u2502 train_cifar_f2b95_00007 PENDING 8 128 0.00365739 2 \u2502 \u2502 train_cifar_f2b95_00008 PENDING 16 128 0.000192995 16 \u2502 \u2502 train_cifar_f2b95_00009 PENDING 8 256 0.00117126 8 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources (raylet) /opt/conda/lib/python3.11/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools\u003c81. (raylet) import pkg_resources Trial train_cifar_f2b95_00009 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00009 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 8 \u2502 \u2502 l1 8 \u2502 \u2502 l2 256 \u2502 \u2502 lr 0.00117 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00001 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00001 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 16 \u2502 \u2502 l1 128 \u2502 \u2502 l2 32 \u2502 \u2502 lr 0.01034 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00007 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00007 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 2 \u2502 \u2502 l1 8 \u2502 \u2502 l2 128 \u2502 \u2502 lr 0.00366 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00004 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00004 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 2 \u2502 \u2502 l1 128 \u2502 \u2502 l2 64 \u2502 \u2502 lr 0.02754 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00008 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00008 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 16 \u2502 \u2502 l1 16 \u2502 \u2502 l2 128 \u2502 \u2502 lr 0.00019 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00003 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00003 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 4 \u2502 \u2502 l1 8 \u2502 \u2502 l2 2 \u2502 \u2502 lr 0.03878 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00000 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00000 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 16 \u2502 \u2502 l1 2 \u2502 \u2502 l2 256 \u2502 \u2502 lr 0.00031 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00002 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00002 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 4 \u2502 \u2502 l1 4 \u2502 \u2502 l2 256 \u2502 \u2502 lr 0.00058 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00006 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00006 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 16 \u2502 \u2502 l1 64 \u2502 \u2502 l2 8 \u2502 \u2502 lr 0.00237 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00005 started with configuration: \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00005 config \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 batch_size 4 \u2502 \u2502 l1 4 \u2502 \u2502 l2 8 \u2502 \u2502 lr 0.00077 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=12600) [1, 2000] loss: 2.083 Trial train_cifar_f2b95_00001 finished iteration 1 at 2025-10-03 22:26:20. Total running time: 16s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00001 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 13.00158 \u2502 \u2502 time_total_s 13.00158 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.4763 \u2502 \u2502 loss 1.45586 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000000) Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000000 Trial train_cifar_f2b95_00008 finished iteration 1 at 2025-10-03 22:26:21. Total running time: 17s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00008 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 13.74059 \u2502 \u2502 time_total_s 13.74059 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.1092 \u2502 \u2502 loss 2.3008 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00008 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00008_8_batch_size=16,l1=16,l2=128,lr=0.0002_2025-10-03_22-26-04/checkpoint_000000 Trial train_cifar_f2b95_00008 completed after 1 iterations at 2025-10-03 22:26:21. Total running time: 17s Trial train_cifar_f2b95_00000 finished iteration 1 at 2025-10-03 22:26:21. Total running time: 17s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00000 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 14.05161 \u2502 \u2502 time_total_s 14.05161 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.2076 \u2502 \u2502 loss 2.05941 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00000 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00000_0_batch_size=16,l1=2,l2=256,lr=0.0003_2025-10-03_22-26-04/checkpoint_000000 Trial train_cifar_f2b95_00000 completed after 1 iterations at 2025-10-03 22:26:21. Total running time: 17s Trial train_cifar_f2b95_00006 finished iteration 1 at 2025-10-03 22:26:22. Total running time: 17s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00006 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 14.296 \u2502 \u2502 time_total_s 14.296 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.3684 \u2502 \u2502 loss 1.73915 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00006 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00006_6_batch_size=16,l1=64,l2=8,lr=0.0024_2025-10-03_22-26-04/checkpoint_000000 (func pid=12594) [1, 4000] loss: 1.162 [repeated 11x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.) Trial train_cifar_f2b95_00009 finished iteration 1 at 2025-10-03 22:26:27. Total running time: 22s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00009 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 19.5243 \u2502 \u2502 time_total_s 19.5243 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.4436 \u2502 \u2502 loss 1.53569 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000000 (func pid=12593) [1, 6000] loss: 0.613 [repeated 7x across cluster] (func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000000) [repeated 4x across cluster] Trial train_cifar_f2b95_00001 finished iteration 2 at 2025-10-03 22:26:30. Total running time: 25s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00001 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000001 \u2502 \u2502 time_this_iter_s 9.42768 \u2502 \u2502 time_total_s 22.42926 \u2502 \u2502 training_iteration 2 \u2502 \u2502 accuracy 0.5172 \u2502 \u2502 loss 1.35872 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000001 Trial train_cifar_f2b95_00006 finished iteration 2 at 2025-10-03 22:26:31. Total running time: 27s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00006 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000001 \u2502 \u2502 time_this_iter_s 9.2087 \u2502 \u2502 time_total_s 23.5047 \u2502 \u2502 training_iteration 2 \u2502 \u2502 accuracy 0.4657 \u2502 \u2502 loss 1.48145 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00006 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00006_6_batch_size=16,l1=64,l2=8,lr=0.0024_2025-10-03_22-26-04/checkpoint_000001 Trial train_cifar_f2b95_00006 completed after 2 iterations at 2025-10-03 22:26:31. Total running time: 27s (func pid=12593) [1, 8000] loss: 0.445 [repeated 7x across cluster] Trial status: 3 TERMINATED | 7 RUNNING Current time: 2025-10-03 22:26:34. Total running time: 30s Logical resource usage: 14.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f2b95_00001 RUNNING 128 32 0.010341 16 2 22.4293 1.35872 0.5172 \u2502 \u2502 train_cifar_f2b95_00002 RUNNING 4 256 0.000582548 4 \u2502 \u2502 train_cifar_f2b95_00003 RUNNING 8 2 0.03878 4 \u2502 \u2502 train_cifar_f2b95_00004 RUNNING 128 64 0.0275418 2 \u2502 \u2502 train_cifar_f2b95_00005 RUNNING 4 8 0.000769138 4 \u2502 \u2502 train_cifar_f2b95_00007 RUNNING 8 128 0.00365739 2 \u2502 \u2502 train_cifar_f2b95_00009 RUNNING 8 256 0.00117126 8 1 19.5243 1.53569 0.4436 \u2502 \u2502 train_cifar_f2b95_00000 TERMINATED 2 256 0.000305994 16 1 14.0516 2.05941 0.2076 \u2502 \u2502 train_cifar_f2b95_00006 TERMINATED 64 8 0.00236933 16 2 23.5047 1.48145 0.4657 \u2502 \u2502 train_cifar_f2b95_00008 TERMINATED 16 128 0.000192995 16 1 13.7406 2.3008 0.1092 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=12593) [1, 10000] loss: 0.339 [repeated 7x across cluster] Trial train_cifar_f2b95_00001 finished iteration 3 at 2025-10-03 22:26:39. Total running time: 35s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00001 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000002 \u2502 \u2502 time_this_iter_s 9.45501 \u2502 \u2502 time_total_s 31.88427 \u2502 \u2502 training_iteration 3 \u2502 \u2502 accuracy 0.5172 \u2502 \u2502 loss 1.34038 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000002 (func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000002) [repeated 3x across cluster] Trial train_cifar_f2b95_00003 finished iteration 1 at 2025-10-03 22:26:40. Total running time: 36s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00003 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 32.93671 \u2502 \u2502 time_total_s 32.93671 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.0976 \u2502 \u2502 loss 2.34242 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00003 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00003_3_batch_size=4,l1=8,l2=2,lr=0.0388_2025-10-03_22-26-04/checkpoint_000000 Trial train_cifar_f2b95_00003 completed after 1 iterations at 2025-10-03 22:26:40. Total running time: 36s Trial train_cifar_f2b95_00002 finished iteration 1 at 2025-10-03 22:26:40. Total running time: 36s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00002 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 33.11155 \u2502 \u2502 time_total_s 33.11155 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.3566 \u2502 \u2502 loss 1.63892 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00002 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00002_2_batch_size=4,l1=4,l2=256,lr=0.0006_2025-10-03_22-26-04/checkpoint_000000 Trial train_cifar_f2b95_00005 finished iteration 1 at 2025-10-03 22:26:41. Total running time: 37s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00005 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 33.66846 \u2502 \u2502 time_total_s 33.66846 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.3463 \u2502 \u2502 loss 1.70408 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00005 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00005_5_batch_size=4,l1=4,l2=8,lr=0.0008_2025-10-03_22-26-04/checkpoint_000000 Trial train_cifar_f2b95_00009 finished iteration 2 at 2025-10-03 22:26:43. Total running time: 39s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00009 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000001 \u2502 \u2502 time_this_iter_s 16.19927 \u2502 \u2502 time_total_s 35.72356 \u2502 \u2502 training_iteration 2 \u2502 \u2502 accuracy 0.5049 \u2502 \u2502 loss 1.36047 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000001 (func pid=12598) [1, 14000] loss: 0.276 [repeated 5x across cluster] Trial train_cifar_f2b95_00001 finished iteration 4 at 2025-10-03 22:26:48. Total running time: 44s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00001 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000003 \u2502 \u2502 time_this_iter_s 9.31968 \u2502 \u2502 time_total_s 41.20394 \u2502 \u2502 training_iteration 4 \u2502 \u2502 accuracy 0.5287 \u2502 \u2502 loss 1.35828 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000003) [repeated 5x across cluster] Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000003 (func pid=12596) [1, 14000] loss: 0.334 [repeated 7x across cluster] (func pid=12593) [2, 6000] loss: 0.534 [repeated 6x across cluster] Trial train_cifar_f2b95_00001 finished iteration 5 at 2025-10-03 22:26:57. Total running time: 53s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00001 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000004 \u2502 \u2502 time_this_iter_s 9.13369 \u2502 \u2502 time_total_s 50.33764 \u2502 \u2502 training_iteration 5 \u2502 \u2502 accuracy 0.5405 \u2502 \u2502 loss 1.33497 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000004 (func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000004) Trial train_cifar_f2b95_00009 finished iteration 3 at 2025-10-03 22:26:59. Total running time: 54s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00009 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000002 \u2502 \u2502 time_this_iter_s 15.66666 \u2502 \u2502 time_total_s 51.39022 \u2502 \u2502 training_iteration 3 \u2502 \u2502 accuracy 0.5157 \u2502 \u2502 loss 1.30244 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000002 (func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000002) (func pid=12595) [2, 8000] loss: 0.401 [repeated 5x across cluster] Trial status: 4 TERMINATED | 6 RUNNING Current time: 2025-10-03 22:27:04. Total running time: 1min 0s Logical resource usage: 12.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f2b95_00001 RUNNING 128 32 0.010341 16 5 50.3376 1.33497 0.5405 \u2502 \u2502 train_cifar_f2b95_00002 RUNNING 4 256 0.000582548 4 1 33.1116 1.63892 0.3566 \u2502 \u2502 train_cifar_f2b95_00004 RUNNING 128 64 0.0275418 2 \u2502 \u2502 train_cifar_f2b95_00005 RUNNING 4 8 0.000769138 4 1 33.6685 1.70408 0.3463 \u2502 \u2502 train_cifar_f2b95_00007 RUNNING 8 128 0.00365739 2 \u2502 \u2502 train_cifar_f2b95_00009 RUNNING 8 256 0.00117126 8 3 51.3902 1.30244 0.5157 \u2502 \u2502 train_cifar_f2b95_00000 TERMINATED 2 256 0.000305994 16 1 14.0516 2.05941 0.2076 \u2502 \u2502 train_cifar_f2b95_00003 TERMINATED 8 2 0.03878 4 1 32.9367 2.34242 0.0976 \u2502 \u2502 train_cifar_f2b95_00006 TERMINATED 64 8 0.00236933 16 2 23.5047 1.48145 0.4657 \u2502 \u2502 train_cifar_f2b95_00008 TERMINATED 16 128 0.000192995 16 1 13.7406 2.3008 0.1092 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00007 finished iteration 1 at 2025-10-03 22:27:05. Total running time: 1min 0s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00007 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 57.44913 \u2502 \u2502 time_total_s 57.44913 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.2941 \u2502 \u2502 loss 1.85399 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00007 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00007_7_batch_size=2,l1=8,l2=128,lr=0.0037_2025-10-03_22-26-04/checkpoint_000000 Trial train_cifar_f2b95_00007 completed after 1 iterations at 2025-10-03 22:27:05. Total running time: 1min 0s (func pid=12598) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00007_7_batch_size=2,l1=8,l2=128,lr=0.0037_2025-10-03_22-26-04/checkpoint_000000) Trial train_cifar_f2b95_00001 finished iteration 6 at 2025-10-03 22:27:07. Total running time: 1min 3s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00001 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000005 \u2502 \u2502 time_this_iter_s 9.20401 \u2502 \u2502 time_total_s 59.54165 \u2502 \u2502 training_iteration 6 \u2502 \u2502 accuracy 0.5458 \u2502 \u2502 loss 1.38728 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 6 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000005 (func pid=12596) [1, 20000] loss: 0.233 [repeated 6x across cluster] (func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000005) Trial train_cifar_f2b95_00002 finished iteration 2 at 2025-10-03 22:27:09. Total running time: 1min 5s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00002 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000001 \u2502 \u2502 time_this_iter_s 28.47496 \u2502 \u2502 time_total_s 61.58651 \u2502 \u2502 training_iteration 2 \u2502 \u2502 accuracy 0.4365 \u2502 \u2502 loss 1.50182 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00002 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00002_2_batch_size=4,l1=4,l2=256,lr=0.0006_2025-10-03_22-26-04/checkpoint_000001 Trial train_cifar_f2b95_00002 completed after 2 iterations at 2025-10-03 22:27:09. Total running time: 1min 5s Trial train_cifar_f2b95_00005 finished iteration 2 at 2025-10-03 22:27:09. Total running time: 1min 5s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00005 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000001 \u2502 \u2502 time_this_iter_s 28.50694 \u2502 \u2502 time_total_s 62.1754 \u2502 \u2502 training_iteration 2 \u2502 \u2502 accuracy 0.4021 \u2502 \u2502 loss 1.57171 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00005 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00005_5_batch_size=4,l1=4,l2=8,lr=0.0008_2025-10-03_22-26-04/checkpoint_000001 Trial train_cifar_f2b95_00005 completed after 2 iterations at 2025-10-03 22:27:09. Total running time: 1min 5s Trial train_cifar_f2b95_00004 finished iteration 1 at 2025-10-03 22:27:13. Total running time: 1min 9s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00004 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000000 \u2502 \u2502 time_this_iter_s 66.0309 \u2502 \u2502 time_total_s 66.0309 \u2502 \u2502 training_iteration 1 \u2502 \u2502 accuracy 0.0981 \u2502 \u2502 loss 2.32891 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00004 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00004_4_batch_size=2,l1=128,l2=64,lr=0.0275_2025-10-03_22-26-04/checkpoint_000000 Trial train_cifar_f2b95_00004 completed after 1 iterations at 2025-10-03 22:27:13. Total running time: 1min 9s (func pid=12596) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00004_4_batch_size=2,l1=128,l2=64,lr=0.0275_2025-10-03_22-26-04/checkpoint_000000) [repeated 3x across cluster] (func pid=12592) [7, 2000] loss: 1.145 [repeated 2x across cluster] Trial train_cifar_f2b95_00009 finished iteration 4 at 2025-10-03 22:27:14. Total running time: 1min 10s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00009 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000003 \u2502 \u2502 time_this_iter_s 15.57003 \u2502 \u2502 time_total_s 66.96025 \u2502 \u2502 training_iteration 4 \u2502 \u2502 accuracy 0.5614 \u2502 \u2502 loss 1.22414 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000003 Trial train_cifar_f2b95_00001 finished iteration 7 at 2025-10-03 22:27:16. Total running time: 1min 12s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00001 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000006 \u2502 \u2502 time_this_iter_s 9.23141 \u2502 \u2502 time_total_s 68.77305 \u2502 \u2502 training_iteration 7 \u2502 \u2502 accuracy 0.551 \u2502 \u2502 loss 1.34653 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 7 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000006 (func pid=12600) [5, 2000] loss: 1.201 (func pid=12592) [8, 2000] loss: 1.135 Trial train_cifar_f2b95_00001 finished iteration 8 at 2025-10-03 22:27:25. Total running time: 1min 21s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00001 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000007 \u2502 \u2502 time_this_iter_s 9.14384 \u2502 \u2502 time_total_s 77.91689 \u2502 \u2502 training_iteration 8 \u2502 \u2502 accuracy 0.5385 \u2502 \u2502 loss 1.37364 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 8 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000007 (func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000007) [repeated 3x across cluster] Trial train_cifar_f2b95_00009 finished iteration 5 at 2025-10-03 22:27:30. Total running time: 1min 26s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00009 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000004 \u2502 \u2502 time_this_iter_s 16.20591 \u2502 \u2502 time_total_s 83.16616 \u2502 \u2502 training_iteration 5 \u2502 \u2502 accuracy 0.5578 \u2502 \u2502 loss 1.2334 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000004 (func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000004) (func pid=12592) [9, 2000] loss: 1.111 [repeated 2x across cluster] Trial status: 8 TERMINATED | 2 RUNNING Current time: 2025-10-03 22:27:34. Total running time: 1min 30s Logical resource usage: 4.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f2b95_00001 RUNNING 128 32 0.010341 16 8 77.9169 1.37364 0.5385 \u2502 \u2502 train_cifar_f2b95_00009 RUNNING 8 256 0.00117126 8 5 83.1662 1.2334 0.5578 \u2502 \u2502 train_cifar_f2b95_00000 TERMINATED 2 256 0.000305994 16 1 14.0516 2.05941 0.2076 \u2502 \u2502 train_cifar_f2b95_00002 TERMINATED 4 256 0.000582548 4 2 61.5865 1.50182 0.4365 \u2502 \u2502 train_cifar_f2b95_00003 TERMINATED 8 2 0.03878 4 1 32.9367 2.34242 0.0976 \u2502 \u2502 train_cifar_f2b95_00004 TERMINATED 128 64 0.0275418 2 1 66.0309 2.32891 0.0981 \u2502 \u2502 train_cifar_f2b95_00005 TERMINATED 4 8 0.000769138 4 2 62.1754 1.57171 0.4021 \u2502 \u2502 train_cifar_f2b95_00006 TERMINATED 64 8 0.00236933 16 2 23.5047 1.48145 0.4657 \u2502 \u2502 train_cifar_f2b95_00007 TERMINATED 8 128 0.00365739 2 1 57.4491 1.85399 0.2941 \u2502 \u2502 train_cifar_f2b95_00008 TERMINATED 16 128 0.000192995 16 1 13.7406 2.3008 0.1092 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00001 finished iteration 9 at 2025-10-03 22:27:35. Total running time: 1min 31s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00001 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000008 \u2502 \u2502 time_this_iter_s 9.95506 \u2502 \u2502 time_total_s 87.87195 \u2502 \u2502 training_iteration 9 \u2502 \u2502 accuracy 0.5591 \u2502 \u2502 loss 1.31092 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 9 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000008 (func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000008) (func pid=12600) [6, 4000] loss: 0.585 [repeated 2x across cluster] Trial train_cifar_f2b95_00001 finished iteration 10 at 2025-10-03 22:27:45. Total running time: 1min 41s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00001 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000009 \u2502 \u2502 time_this_iter_s 9.5594 \u2502 \u2502 time_total_s 97.43135 \u2502 \u2502 training_iteration 10 \u2502 \u2502 accuracy 0.5712 \u2502 \u2502 loss 1.29955 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00001 saved a checkpoint for iteration 10 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000009 Trial train_cifar_f2b95_00001 completed after 10 iterations at 2025-10-03 22:27:45. Total running time: 1min 41s (func pid=12592) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00001_1_batch_size=16,l1=128,l2=32,lr=0.0103_2025-10-03_22-26-04/checkpoint_000009) Trial train_cifar_f2b95_00009 finished iteration 6 at 2025-10-03 22:27:46. Total running time: 1min 42s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00009 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000005 \u2502 \u2502 time_this_iter_s 16.12082 \u2502 \u2502 time_total_s 99.28699 \u2502 \u2502 training_iteration 6 \u2502 \u2502 accuracy 0.5753 \u2502 \u2502 loss 1.20882 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 6 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000005 (func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000005) (func pid=12600) [7, 2000] loss: 1.131 [repeated 2x across cluster] (func pid=12600) [7, 4000] loss: 0.576 Trial train_cifar_f2b95_00009 finished iteration 7 at 2025-10-03 22:28:03. Total running time: 1min 58s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00009 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000006 \u2502 \u2502 time_this_iter_s 16.09692 \u2502 \u2502 time_total_s 115.38391 \u2502 \u2502 training_iteration 7 \u2502 \u2502 accuracy 0.5877 \u2502 \u2502 loss 1.15432 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 7 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000006 (func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000006) Trial status: 9 TERMINATED | 1 RUNNING Current time: 2025-10-03 22:28:04. Total running time: 2min 0s Logical resource usage: 2.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f2b95_00009 RUNNING 8 256 0.00117126 8 7 115.384 1.15432 0.5877 \u2502 \u2502 train_cifar_f2b95_00000 TERMINATED 2 256 0.000305994 16 1 14.0516 2.05941 0.2076 \u2502 \u2502 train_cifar_f2b95_00001 TERMINATED 128 32 0.010341 16 10 97.4314 1.29955 0.5712 \u2502 \u2502 train_cifar_f2b95_00002 TERMINATED 4 256 0.000582548 4 2 61.5865 1.50182 0.4365 \u2502 \u2502 train_cifar_f2b95_00003 TERMINATED 8 2 0.03878 4 1 32.9367 2.34242 0.0976 \u2502 \u2502 train_cifar_f2b95_00004 TERMINATED 128 64 0.0275418 2 1 66.0309 2.32891 0.0981 \u2502 \u2502 train_cifar_f2b95_00005 TERMINATED 4 8 0.000769138 4 2 62.1754 1.57171 0.4021 \u2502 \u2502 train_cifar_f2b95_00006 TERMINATED 64 8 0.00236933 16 2 23.5047 1.48145 0.4657 \u2502 \u2502 train_cifar_f2b95_00007 TERMINATED 8 128 0.00365739 2 1 57.4491 1.85399 0.2941 \u2502 \u2502 train_cifar_f2b95_00008 TERMINATED 16 128 0.000192995 16 1 13.7406 2.3008 0.1092 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=12600) [8, 2000] loss: 1.116 (func pid=12600) [8, 4000] loss: 0.563 Trial train_cifar_f2b95_00009 finished iteration 8 at 2025-10-03 22:28:19. Total running time: 2min 14s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00009 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000007 \u2502 \u2502 time_this_iter_s 16.00349 \u2502 \u2502 time_total_s 131.38739 \u2502 \u2502 training_iteration 8 \u2502 \u2502 accuracy 0.5892 \u2502 \u2502 loss 1.172 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 8 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000007 (func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000007) (func pid=12600) [9, 2000] loss: 1.089 (func pid=12600) [9, 4000] loss: 0.556 Trial status: 9 TERMINATED | 1 RUNNING Current time: 2025-10-03 22:28:34. Total running time: 2min 30s Logical resource usage: 2.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f2b95_00009 RUNNING 8 256 0.00117126 8 8 131.387 1.172 0.5892 \u2502 \u2502 train_cifar_f2b95_00000 TERMINATED 2 256 0.000305994 16 1 14.0516 2.05941 0.2076 \u2502 \u2502 train_cifar_f2b95_00001 TERMINATED 128 32 0.010341 16 10 97.4314 1.29955 0.5712 \u2502 \u2502 train_cifar_f2b95_00002 TERMINATED 4 256 0.000582548 4 2 61.5865 1.50182 0.4365 \u2502 \u2502 train_cifar_f2b95_00003 TERMINATED 8 2 0.03878 4 1 32.9367 2.34242 0.0976 \u2502 \u2502 train_cifar_f2b95_00004 TERMINATED 128 64 0.0275418 2 1 66.0309 2.32891 0.0981 \u2502 \u2502 train_cifar_f2b95_00005 TERMINATED 4 8 0.000769138 4 2 62.1754 1.57171 0.4021 \u2502 \u2502 train_cifar_f2b95_00006 TERMINATED 64 8 0.00236933 16 2 23.5047 1.48145 0.4657 \u2502 \u2502 train_cifar_f2b95_00007 TERMINATED 8 128 0.00365739 2 1 57.4491 1.85399 0.2941 \u2502 \u2502 train_cifar_f2b95_00008 TERMINATED 16 128 0.000192995 16 1 13.7406 2.3008 0.1092 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00009 finished iteration 9 at 2025-10-03 22:28:34. Total running time: 2min 30s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00009 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000008 \u2502 \u2502 time_this_iter_s 15.62216 \u2502 \u2502 time_total_s 147.00955 \u2502 \u2502 training_iteration 9 \u2502 \u2502 accuracy 0.5913 \u2502 \u2502 loss 1.16831 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000008) Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 9 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000008 (func pid=12600) [10, 2000] loss: 1.090 (func pid=12600) [10, 4000] loss: 0.545 Trial train_cifar_f2b95_00009 finished iteration 10 at 2025-10-03 22:28:51. Total running time: 2min 47s \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial train_cifar_f2b95_00009 result \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 checkpoint_dir_name checkpoint_000009 \u2502 \u2502 time_this_iter_s 16.46125 \u2502 \u2502 time_total_s 163.47079 \u2502 \u2502 training_iteration 10 \u2502 \u2502 accuracy 0.6025 \u2502 \u2502 loss 1.1439 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Trial train_cifar_f2b95_00009 saved a checkpoint for iteration 10 at: (local)/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000009 Trial train_cifar_f2b95_00009 completed after 10 iterations at 2025-10-03 22:28:51. Total running time: 2min 47s Trial status: 10 TERMINATED Current time: 2025-10-03 22:28:51. Total running time: 2min 47s Logical resource usage: 2.0/256 CPUs, 0/8 GPUs (0.0/1.0 accelerator_type:H200) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Trial name status l1 l2 lr batch_size iter total time (s) loss accuracy \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 train_cifar_f2b95_00000 TERMINATED 2 256 0.000305994 16 1 14.0516 2.05941 0.2076 \u2502 \u2502 train_cifar_f2b95_00001 TERMINATED 128 32 0.010341 16 10 97.4314 1.29955 0.5712 \u2502 \u2502 train_cifar_f2b95_00002 TERMINATED 4 256 0.000582548 4 2 61.5865 1.50182 0.4365 \u2502 \u2502 train_cifar_f2b95_00003 TERMINATED 8 2 0.03878 4 1 32.9367 2.34242 0.0976 \u2502 \u2502 train_cifar_f2b95_00004 TERMINATED 128 64 0.0275418 2 1 66.0309 2.32891 0.0981 \u2502 \u2502 train_cifar_f2b95_00005 TERMINATED 4 8 0.000769138 4 2 62.1754 1.57171 0.4021 \u2502 \u2502 train_cifar_f2b95_00006 TERMINATED 64 8 0.00236933 16 2 23.5047 1.48145 0.4657 \u2502 \u2502 train_cifar_f2b95_00007 TERMINATED 8 128 0.00365739 2 1 57.4491 1.85399 0.2941 \u2502 \u2502 train_cifar_f2b95_00008 TERMINATED 16 128 0.000192995 16 1 13.7406 2.3008 0.1092 \u2502 \u2502 train_cifar_f2b95_00009 TERMINATED 8 256 0.00117126 8 10 163.471 1.1439 0.6025 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f (func pid=12600) Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-10-03_22-26-04/train_cifar_f2b95_00009_9_batch_size=8,l1=8,l2=256,lr=0.0012_2025-10-03_22-26-04/checkpoint_000009) Best trial config: {\u0027l1\u0027: 8, \u0027l2\u0027: 256, \u0027lr\u0027: 0.001171259491329369, \u0027batch_size\u0027: 8} Best trial final validation loss: 1.143903388774395 Best trial final validation accuracy: 0.6025 Best trial test set accuracy: 0.5994 \ucf54\ub4dc\ub97c \uc2e4\ud589\ud558\uba74 \uacb0\uacfc\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \ub098\uc62c \uac83\uc785\ub2c8\ub2e4: Number of trials: 10/10 (10 TERMINATED) +-----+--------------+------+------+-------------+--------+---------+------------+ | ... | batch_size | l1 | l2 | lr | iter | loss | accuracy | |-----+--------------+------+------+-------------+--------+---------+------------| | ... | 2 | 1 | 256 | 0.000668163 | 1 | 2.31479 | 0.0977 | | ... | 4 | 64 | 8 | 0.0331514 | 1 | 2.31605 | 0.0983 | | ... | 4 | 2 | 1 | 0.000150295 | 1 | 2.30755 | 0.1023 | | ... | 16 | 32 | 32 | 0.0128248 | 10 | 1.66912 | 0.4391 | | ... | 4 | 8 | 128 | 0.00464561 | 2 | 1.7316 | 0.3463 | | ... | 8 | 256 | 8 | 0.00031556 | 1 | 2.19409 | 0.1736 | | ... | 4 | 16 | 256 | 0.00574329 | 2 | 1.85679 | 0.3368 | | ... | 8 | 2 | 2 | 0.00325652 | 1 | 2.30272 | 0.0984 | | ... | 2 | 2 | 2 | 0.000342987 | 2 | 1.76044 | 0.292 | | ... | 4 | 64 | 32 | 0.003734 | 8 | 1.53101 | 0.4761 | +-----+--------------+------+------+-------------+--------+---------+------------+ Best trial config: {\u0027l1\u0027: 64, \u0027l2\u0027: 32, \u0027lr\u0027: 0.0037339984519545164, \u0027batch_size\u0027: 4} Best trial final validation loss: 1.5310075663924216 Best trial final validation accuracy: 0.4761 Best trial test set accuracy: 0.4737 \ub300\ubd80\ubd84\uc758 \uc2e4\ud5d8\uc740 \uc790\uc6d0 \ub0ad\ube44\ub97c \ub9c9\uae30 \uc704\ud574 \uc77c\ucc0d \uc911\ub2e8\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uac00\uc7a5 \uc88b\uc740 \uacb0\uacfc\ub97c \uc5bb\uc740 \uc2e4\ud5d8\uc740 47%\uc758 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud588\uc73c\uba70, \uc774\ub294 \ud14c\uc2a4\ud2b8\uc14b\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc774 \uc804\ubd80\uc785\ub2c8\ub2e4! \uc774\uc81c \ud30c\uc774\ud1a0\uce58 \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc870\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Total running time of the script: (3 minutes 16.218 seconds) Download Jupyter notebook: hyperparameter_tuning_tutorial.ipynb Download Python source code: hyperparameter_tuning_tutorial.py Download zipped: hyperparameter_tuning_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/beginner/hyperparameter_tuning_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>