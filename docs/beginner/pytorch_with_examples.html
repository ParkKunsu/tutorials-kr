
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="예제로 배우는 파이토치(PyTorch)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Author: Justin Johnson, 번역: 박정환,. 이 튜토리얼에서는 PyTorch 의 핵심적인 개념을 예제를 통해 소개합니다. 본질적으로, PyTorch에는 두가지 주요한 특징이 있습니다: NumPy와 유사하지만 GPU 상에서 실행 가능한 n-차원 텐서(Tensor), 신경망을 구성하고 학습하는 과정에서의 자동 미분(Automatic differentiation). 이 튜토리얼에서는 3차 다항식(third order polynomial)을 사용하여 y=\sin(x) 에 근사(fit)하는 문제를 다뤄보겠습니다. 신경망..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="Author: Justin Johnson, 번역: 박정환,. 이 튜토리얼에서는 PyTorch 의 핵심적인 개념을 예제를 통해 소개합니다. 본질적으로, PyTorch에는 두가지 주요한 특징이 있습니다: NumPy와 유사하지만 GPU 상에서 실행 가능한 n-차원 텐서(Tensor), 신경망을 구성하고 학습하는 과정에서의 자동 미분(Automatic differentiation). 이 튜토리얼에서는 3차 다항식(third order polynomial)을 사용하여 y=\sin(x) 에 근사(fit)하는 문제를 다뤄보겠습니다. 신경망..." />
<meta property="og:ignore_canonical" content="true" />

    <title>예제로 배우는 파이토치(PyTorch) &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'beginner/pytorch_with_examples';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="준비 운동: NumPy" href="examples_tensor/polynomial_numpy.html" />
    <link rel="prev" title="분류기(Classifier) 학습하기" href="blitz/cifar10_tutorial.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="basics/intro.html">파이토치(PyTorch) 기본 익히기</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="introyt/introyt_index.html">PyTorch 소개 - YouTube 시리즈</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="introyt/introyt1_tutorial.html">PyTorch 소개</a></li>
<li class="toctree-l2"><a class="reference internal" href="introyt/tensors_deeper_tutorial.html">Pytorch Tensor 소개</a></li>
<li class="toctree-l2"><a class="reference internal" href="introyt/autogradyt_tutorial.html">The Fundamentals of Autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="introyt/modelsyt_tutorial.html">Building Models with PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="introyt/tensorboardyt_tutorial.html">PyTorch TensorBoard 지원</a></li>
<li class="toctree-l2"><a class="reference internal" href="introyt/trainingyt.html">Training with PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="introyt/captumyt.html">Model Understanding with Captum</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning PyTorch</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="deep_learning_60min_blitz.html">PyTorch로 딥러닝하기: 60분만에 끝장내기</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="blitz/tensor_tutorial.html">텐서(Tensor)</a></li>
<li class="toctree-l2"><a class="reference internal" href="blitz/autograd_tutorial.html"><code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code> 에 대한 간단한 소개</a></li>
<li class="toctree-l2"><a class="reference internal" href="blitz/neural_networks_tutorial.html">신경망(Neural Networks)</a></li>
<li class="toctree-l2"><a class="reference internal" href="blitz/cifar10_tutorial.html">분류기(Classifier) 학습하기</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">예제로 배우는 파이토치(PyTorch)</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="examples_tensor/polynomial_numpy.html">준비 운동: NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_tensor/polynomial_tensor.html">파이토치(PyTorch): 텐서(Tensor)</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_autograd/polynomial_autograd.html">PyTorch: 텐서(Tensor)와 autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_autograd/polynomial_custom_function.html">PyTorch: 새 autograd Function 정의하기</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_nn/polynomial_nn.html">PyTorch: nn</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_nn/polynomial_optim.html">PyTorch: optim</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_nn/polynomial_module.html">PyTorch: 사용자 정의 nn.Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples_nn/dynamic_net.html">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="nn_tutorial.html"><cite>torch.nn</cite> 이 <em>실제로</em> 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="understanding_leaf_vs_nonleaf_tutorial.html">Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/nlp_from_scratch_index.html">NLP from Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/tensorboard_tutorial.html">TensorBoard로 모델, 데이터, 학습 시각화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/pinmem_nonblock.html">A guide on good usage of <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> and <code class="docutils literal notranslate"><span class="pre">pin_memory()</span></code> in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/visualizing_gradients_tutorial.html">Visualizing Gradients</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../intro.html" class="nav-link">Intro</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">예제로 배우는...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../intro.html">
        <meta itemprop="name" content="Intro">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="예제로 배우는 파이토치(PyTorch)">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">beginner/pytorch_with_examples</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="pytorch">
<h1>예제로 배우는 파이토치(PyTorch)<a class="headerlink" href="#pytorch" title="Link to this heading">#</a></h1>
<dl class="simple">
<dt><strong>Author</strong>: <a class="reference external" href="https://github.com/jcjohnson/pytorch-examples">Justin Johnson</a></dt><dd><p><strong>번역</strong>: <a class="reference external" href="https://github.com/9bow">박정환</a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>이 튜토리얼은 다소 오래된 PyTorch 튜토리얼입니다.
<a class="reference external" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 다지기</a> 에서
입문자를 위한 최신의 내용을 보실 수 있습니다.</p>
</div>
<p>이 튜토리얼에서는 <a class="reference external" href="https://github.com/pytorch/pytorch">PyTorch</a> 의 핵심적인 개념을
예제를 통해 소개합니다.</p>
<p>본질적으로, PyTorch에는 두가지 주요한 특징이 있습니다:</p>
<ul class="simple">
<li><p>NumPy와 유사하지만 GPU 상에서 실행 가능한 n-차원 텐서(Tensor)</p></li>
<li><p>신경망을 구성하고 학습하는 과정에서의 자동 미분(Automatic differentiation)</p></li>
</ul>
<p>이 튜토리얼에서는 3차 다항식(third order polynomial)을 사용하여 <span class="math">\(y=\sin(x)\)</span> 에 근사(fit)하는 문제를 다뤄보겠습니다.
신경망은 4개의 매개변수를 가지며, 정답과 신경망이 예측한 결과 사이의 유클리드 거리(Euclidean distance)를
최소화하여 임의의 값을 근사할 수 있도록 경사하강법(gradient descent)을 사용하여 학습하겠습니다.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>각각의 예제들은 <a class="reference internal" href="#examples-download"><span class="std std-ref">이 문서의 마지막</span></a>
부분에서 살펴볼 수 있습니다.</p>
</div>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#tensor" id="id7">텐서(Tensor)</a></p>
<ul>
<li><p><a class="reference internal" href="#numpy" id="id8">준비 운동: numpy</a></p></li>
<li><p><a class="reference internal" href="#pytorch-tensor" id="id9">파이토치(PyTorch): 텐서(Tensor)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#autograd" id="id10">Autograd</a></p>
<ul>
<li><p><a class="reference internal" href="#pytorch-tensor-autograd" id="id11">PyTorch: 텐서(Tensor)와 autograd</a></p></li>
<li><p><a class="reference internal" href="#pytorch-autograd-function" id="id12">PyTorch: 새 autograd Function 정의하기</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#nn" id="id13"><code class="docutils literal notranslate"><span class="pre">nn</span></code> 모듈</a></p>
<ul>
<li><p><a class="reference internal" href="#pytorch-nn" id="id14">PyTorch: <code class="docutils literal notranslate"><span class="pre">nn</span></code></a></p></li>
<li><p><a class="reference internal" href="#pytorch-optim" id="id15">PyTorch: optim</a></p></li>
<li><p><a class="reference internal" href="#id3" id="id16">PyTorch: 사용자 정의 <code class="docutils literal notranslate"><span class="pre">nn</span></code> 모듈</a></p></li>
<li><p><a class="reference internal" href="#pytorch-control-flow-weight-sharing" id="id17">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#examples-download" id="id18">예제 코드</a></p>
<ul>
<li><p><a class="reference internal" href="#tensors" id="id19">Tensors</a></p></li>
<li><p><a class="reference internal" href="#id5" id="id20">Autograd</a></p></li>
<li><p><a class="reference internal" href="#id6" id="id21"><code class="docutils literal notranslate"><span class="pre">nn</span></code> 모듈</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="tensor">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">텐서(Tensor)</a><a class="headerlink" href="#tensor" title="Link to this heading">#</a></h2>
<section id="numpy">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">준비 운동: numpy</a><a class="headerlink" href="#numpy" title="Link to this heading">#</a></h3>
<p>PyTorch를 소개하기 전에, 먼저 NumPy를 사용하여 신경망을 구성해보겠습니다.</p>
<p>NumPy는 n-차원 배열 객체와 이러한 배열들을 조작하기 위한 다양한 함수들을 제공합니다. NumPy는 과학 분야의
연산을 위한 포괄적인 프레임워크(generic framework)입니다;
NumPy는 연산 그래프(computation graph)나 딥러닝, 변화도(gradient)에 대해서는 알지 못합니다.
하지만 NumPy 연산을 사용하여 신경망의 순전파 단계와 역전파 단계를 직접 구현함으로써,
3차 다항식이 사인(sine) 함수에 근사하도록 만들 수 있습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="c1"># 무작위로 입력과 출력 데이터를 생성합니다</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 무작위로 가중치를 초기화합니다</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: 예측값 y를 계산합니다</span>
    <span class="c1"># y = a + b x + c x^2 + d x^3</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span>

    <span class="c1"># 손실(loss)을 계산하고 출력합니다</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

    <span class="c1"># 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.</span>
    <span class="n">grad_y_pred</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_y_pred</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">grad_b</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_y_pred</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">grad_c</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_y_pred</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">grad_d</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_y_pred</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># 가중치를 갱신합니다.</span>
    <span class="n">a</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_a</span>
    <span class="n">b</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_b</span>
    <span class="n">c</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_c</span>
    <span class="n">d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_d</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Result: y = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s1"> x + </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s1"> x^2 + </span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s1"> x^3&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="pytorch-tensor">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">파이토치(PyTorch): 텐서(Tensor)</a><a class="headerlink" href="#pytorch-tensor" title="Link to this heading">#</a></h3>
<p>NumPy는 훌륭한 프레임워크지만, GPU를 사용하여 수치 연산을 가속화할 수는 없습니다.
현대의 심층 신경망에서 GPU는 종종 <a class="reference external" href="https://github.com/jcjohnson/cnn-benchmarks">50배 또는 그 이상</a> 의
속도 향상을 제공하기 때문에, 안타깝게도 NumPy는 현대의 딥러닝에는 충분치 않습니다.</p>
<p>이번에는 PyTorch의 가장 핵심적인 개념인 <strong>텐서(Tensor)</strong> 에 대해서 알아보겠습니다.
PyTorch 텐서(Tensor)는 개념적으로 NumPy 배열과 동일합니다:
텐서(Tensor)는 n-차원 배열이며, PyTorch는 이러한 텐서들의 연산을 위한 다양한 기능들을 제공합니다.
NumPy 배열처럼 PyTorch Tensor는 딥러닝이나 연산 그래프, 변화도는 알지 못하며, 과학적 분야의 연산을 위한 포괄적인 도구입니다.
텐서는 연산 그래프와 변화도를 추적할 수도 있지만, 과학적 연산을 위한 일반적인 도구로도 유용합니다.</p>
<p>또한 NumPy와는 다르게, PyTorch 텐서는 GPU를 사용하여 수치 연산을 가속할 수 있습니다.
PyTorch 텐서를 GPU에서 실행하기 위해서는 단지 적절한 장치를 지정해주기만 하면 됩니다.</p>
<p>여기에서는 PyTorch 텐서를 사용하여 3차 다항식을 사인(sine) 함수에 근사해보겠습니다.
위의 NumPy 예제에서와 같이 신경망의 순전파 단계와 역전파 단계는 직접 구현하겠습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>


<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="c1"># device = torch.device(&quot;cuda:0&quot;) # GPU에서 실행하려면 이 주석을 제거하세요</span>

<span class="c1"># 무작위로 입력과 출력 데이터를 생성합니다</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 무작위로 가중치를 초기화합니다</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: 예측값 y를 계산합니다</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span>

    <span class="c1"># 손실(loss)을 계산하고 출력합니다</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

    <span class="c1"># 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.</span>
    <span class="n">grad_y_pred</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_y_pred</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">grad_b</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_y_pred</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">grad_c</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_y_pred</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">grad_d</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_y_pred</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># 가중치를 갱신합니다.</span>
    <span class="n">a</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_a</span>
    <span class="n">b</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_b</span>
    <span class="n">c</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_c</span>
    <span class="n">d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_d</span>


<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Result: y = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">b</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x + </span><span class="si">{</span><span class="n">c</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^2 + </span><span class="si">{</span><span class="n">d</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^3&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="autograd">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Autograd</a><a class="headerlink" href="#autograd" title="Link to this heading">#</a></h2>
<section id="pytorch-tensor-autograd">
<h3><a class="toc-backref" href="#id11" role="doc-backlink">PyTorch: 텐서(Tensor)와 autograd</a><a class="headerlink" href="#pytorch-tensor-autograd" title="Link to this heading">#</a></h3>
<p>위의 예제들에서는 신경망의 순전파 단계와 역전파 단계를 직접 구현해보았습니다.
작은 2계층(2-layer) 신경망에서는 역전파 단계를 직접 구현하는 것이 큰일이 아니지만,
복잡한 대규모 신경망에서는 매우 아슬아슬한 일일 것입니다.</p>
<p>다행히도, <a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">자동 미분</a> 을
사용하여 신경망의 역전파 단계 연산을 자동화할 수 있습니다. PyTorch의 <strong>autograd</strong> 패키지는 정확히
이런 기능을 제공합니다. Autograd를 사용하면, 신경망의 순전파 단계에서 <strong>연산 그래프(computational graph)</strong>
를 정의하게 됩니다; 이 그래프의 노드(node)는 텐서(tensor)이고, 엣지(edge)는 입력 텐서로부터 출력 텐서를
만들어내는 함수가 됩니다. 이 그래프를 통해 역전파를 하게 되면 변화도를 쉽게 계산할 수 있습니다.</p>
<p>이는 복잡하게 들리겠지만, 실제로 사용하는 것은 매우 간단합니다. 각 텐서는 연산그래프에서 노드로 표현됩니다.
만약 <code class="docutils literal notranslate"><span class="pre">x</span></code> 가 <code class="docutils literal notranslate"><span class="pre">x.requires_grad=True</span></code> 인 텐서라면 <code class="docutils literal notranslate"><span class="pre">x.grad</span></code> 어떤 스칼라 값에 대한 <code class="docutils literal notranslate"><span class="pre">x</span></code> 의 변화도를 갖는
또 다른 텐서입니다.</p>
<p>여기서는 PyTorch 텐서와 autograd를 사용하여 3차 다항식을 사인파(sine wave)에 근사하는 예제를
구현해보겠습니다; 이제 더 이상 신경망의 역전파 단계를 직접 구현할 필요가 없습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_default_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 입력값과 출력값을 갖는 텐서들을 생성합니다.</span>
<span class="c1"># requires_grad=False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를</span>
<span class="c1"># 계산할 필요가 없음을 나타냅니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 가중치를 갖는 임의의 텐서를 생성합니다. 3차 다항식이므로 4개의 가중치가 필요합니다:</span>
<span class="c1"># y = a + b x + c x^2 + d x^3</span>
<span class="c1"># requires_grad=True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가</span>
<span class="c1"># 있음을 나타냅니다.</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: 텐서들 간의 연산을 사용하여 예측값 y를 계산합니다.</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span>

    <span class="c1"># 텐서들간의 연산을 사용하여 손실(loss)을 계산하고 출력합니다.</span>
    <span class="c1"># 이 때 손실은 (1,) shape을 갖는 텐서입니다.</span>
    <span class="c1"># loss.item() 으로 손실이 갖고 있는 스칼라 값을 가져올 수 있습니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># autograd 를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를 갖는</span>
    <span class="c1"># 모든 텐서들에 대한 손실의 변화도를 계산합니다.</span>
    <span class="c1"># 이후 a.grad와 b.grad, c.grad, d.grad는 각각 a, b, c, d에 대한 손실의 변화도를</span>
    <span class="c1"># 갖는 텐서가 됩니다.</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 경사하강법(gradient descent)을 사용하여 가중치를 직접 갱신합니다.</span>
    <span class="c1"># torch.no_grad()로 감싸는 이유는, 가중치들이 requires_grad=True 지만</span>
    <span class="c1"># autograd에서는 이를 추적하지 않을 것이기 때문입니다.</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">a</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">b</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">c</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">c</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d</span><span class="o">.</span><span class="n">grad</span>

        <span class="c1"># 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.</span>
        <span class="n">a</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">c</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">d</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Result: y = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">b</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x + </span><span class="si">{</span><span class="n">c</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^2 + </span><span class="si">{</span><span class="n">d</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^3&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="pytorch-autograd-function">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">PyTorch: 새 autograd Function 정의하기</a><a class="headerlink" href="#pytorch-autograd-function" title="Link to this heading">#</a></h3>
<p>내부적으로, autograd의 기본(primitive) 연산자는 실제로 텐서를 조작하는 2개의 함수입니다.
<strong>forward</strong> 함수는 입력 텐서로부터 출력 텐서를 계산합니다.
<strong>backward</strong> 함수는 어떤 스칼라 값에 대한 출력 텐서의 변화도(gradient)를 전달받고,
동일한 스칼라 값에 대한 입력 텐서의 변화도를 계산합니다.</p>
<p>PyTorch에서 <code class="docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code> 의 하위클래스(subclass)를 정의하고
<code class="docutils literal notranslate"><span class="pre">forward</span></code> 와 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 함수를 구현함으로써 사용자 정의 autograd 연산자를 손쉽게
정의할 수 있습니다. 그 후, 인스턴스(instance)를 생성하고 이를 함수처럼 호출하고,
입력 데이터를 갖는 텐서를 전달하는 식으로 새로운 autograd 연산자를 사용할 수 있습니다.</p>
<p>이 예제에서는 <span class="math">\(y=a+bx+cx^2+dx^3\)</span> 대신 <span class="math">\(y=a+b P_3(c+dx)\)</span> 로 모델을
정의합니다. 여기서 <span class="math">\(P_3(x)=\frac{1}{2}\left(5x^3-3x\right)\)</span> 은 3차
<a class="reference external" href="https://en.wikipedia.org/wiki/Legendre_polynomials">르장드르 다항식(Legendre polynomial)</a> 입니다. <span class="math">\(P_3\)</span> 의 순전파와 역전파 연산을
위한 새로운 autograd Function를 작성하고, 이를 사용하여 모델을 구현합니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>


<span class="k">class</span><span class="w"> </span><span class="nc">LegendrePolynomial3</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    torch.autograd.Function을 상속받아 사용자 정의 autograd Function을 구현하고,</span>
<span class="sd">    텐서 연산을 하는 순전파 단계와 역전파 단계를 구현해보겠습니다.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        순전파 단계에서는 입력을 갖는 텐서를 받아 출력을 갖는 텐서를 반환합니다.</span>
<span class="sd">        ctx는 컨텍스트 객체(context object)로 역전파 연산을 위한 정보 저장에 사용합니다.</span>
<span class="sd">        ctx.save_for_backward 메소드를 사용하여 역전파 단계에서 사용할 어떤 객체도</span>
<span class="sd">        저장(cache)해 둘 수 있습니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="nb">input</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">-</span> <span class="mi">3</span> <span class="o">*</span> <span class="nb">input</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        역전파 단계에서는 출력에 대한 손실(loss)의 변화도(gradient)를 갖는 텐서를 받고,</span>
<span class="sd">        입력에 대한 손실의 변화도를 계산해야 합니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">input</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="k">return</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="nb">input</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="c1"># device = torch.device(&quot;cuda:0&quot;) # GPU에서 실행하려면 이 주석을 제거하세요</span>

<span class="c1"># 입력값과 출력값을 갖는 텐서들을 생성합니다.</span>
<span class="c1"># requires_grad=False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할</span>
<span class="c1"># 필요가 없음을 나타냅니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 가중치를 갖는 임의의 텐서를 생성합니다. 3차 다항식이므로 4개의 가중치가 필요합니다:</span>
<span class="c1"># y = a + b * P3(c + d * x)</span>
<span class="c1"># 이 가중치들이 수렴(convergence)하기 위해서는 정답으로부터 너무 멀리 떨어지지 않은 값으로</span>
<span class="c1"># 초기화가 되어야 합니다.</span>
<span class="c1"># requires_grad=True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가</span>
<span class="c1"># 있음을 나타냅니다.</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((),</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((),</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((),</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((),</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">5e-6</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="c1"># 사용자 정의 Function을 적용하기 위해 Function.apply 메소드를 사용합니다.</span>
    <span class="c1"># 여기에 &#39;P3&#39;라고 이름을 붙였습니다.</span>
    <span class="n">P3</span> <span class="o">=</span> <span class="n">LegendrePolynomial3</span><span class="o">.</span><span class="n">apply</span>

    <span class="c1"># 순전파 단계: 연산을 하여 예측값 y를 계산합니다;</span>
    <span class="c1"># 사용자 정의 autograd 연산을 사용하여 P3를 계산합니다.</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">P3</span><span class="p">(</span><span class="n">c</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

    <span class="c1"># 손실을 계산하고 출력합니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># autograd를 사용하여 역전파 단계를 계산합니다.</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 경사하강법(gradient descent)을 사용하여 가중치를 갱신합니다.</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">a</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">b</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">c</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">c</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">d</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">d</span><span class="o">.</span><span class="n">grad</span>

        <span class="c1"># 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.</span>
        <span class="n">a</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">c</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">d</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Result: y = </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">b</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> * P3(</span><span class="si">{</span><span class="n">c</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">d</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="nn">
<h2><a class="toc-backref" href="#id13" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">nn</span></code> 모듈</a><a class="headerlink" href="#nn" title="Link to this heading">#</a></h2>
<section id="pytorch-nn">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">PyTorch: <code class="docutils literal notranslate"><span class="pre">nn</span></code></a><a class="headerlink" href="#pytorch-nn" title="Link to this heading">#</a></h3>
<p>연산 그래프와 autograd는 복잡한 연산자를 정의하고 도함수(derivative)를 자동으로 계산하는
매우 강력한 패러다임(paradigm)입니다; 하지만 대규모 신경망에서는 autograd 그 자체만으로는 너무
저수준(low-level)일 수 있습니다.</p>
<p>신경망을 구성하는 것을 종종 연산을 <strong>계층(layer)</strong> 에 배열(arrange)하는 것으로 생각하는데,
이 중 일부는 학습 도중 최적화가 될 <strong>학습 가능한 매개변수</strong> 를 갖고 있습니다.</p>
<p>텐서플로우(Tensorflow)에서는, <a class="reference external" href="https://github.com/fchollet/keras">Keras</a> 와
<a class="reference external" href="https://github.com/google-research/tf-slim">TensorFlow-Slim</a>,
<a class="reference external" href="http://tflearn.org/">TFLearn</a> 같은 패키지들이 연산 그래프를 고수준(high-level)으로 추상화(abstraction)하여
제공하므로 신경망을 구축하는데 유용합니다.</p>
<p>파이토치(PyTorch)에서는 <code class="docutils literal notranslate"><span class="pre">nn</span></code> 패키지가 동일한 목적으로 제공됩니다. <code class="docutils literal notranslate"><span class="pre">nn</span></code> 패키지는
신경망 계층(layer)과 거의 비슷한 <strong>Module</strong> 의 집합을 정의합니다. Module은 입력 텐서를 받고
출력 텐서를 계산하는 한편, 학습 가능한 매개변수를 갖는 텐서들을 내부 상태(internal state)로
갖습니다. <code class="docutils literal notranslate"><span class="pre">nn</span></code> 패키지는 또한 신경망을 학습시킬 때 주로 사용하는 유용한 손실 함수(loss function)들도
정의하고 있습니다.</p>
<p>이 예제에서는 <code class="docutils literal notranslate"><span class="pre">nn</span></code> 패키지를 사용하여 다항식 모델을 구현해보겠습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="c1"># 입력값과 출력값을 갖는 텐서들을 생성합니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 이 예제에서, 출력 y는 (x, x^2, x^3)의 선형 함수이므로, 선형 계층 신경망으로 간주할 수 있습니다.</span>
<span class="c1"># (x, x^2, x^3)를 위한 텐서를 준비합니다.</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

<span class="c1"># 위 코드에서, x.unsqueeze(-1)은 (2000, 1)의 shape을, p는 (3,)의 shape을 가지므로,</span>
<span class="c1"># 이 경우 브로드캐스트(broadcast)가 적용되어 (2000, 3)의 shape을 갖는 텐서를 얻습니다.</span>

<span class="c1"># nn 패키지를 사용하여 모델을 순차적 계층(sequence of layers)으로 정의합니다.</span>
<span class="c1"># nn.Sequential은 다른 Module을 포함하는 Module로, 포함되는 Module들을 순차적으로 적용하여 </span>
<span class="c1"># 출력을 생성합니다. 각각의 Linear Module은 선형 함수(linear function)를 사용하여 입력으로부터</span>
<span class="c1"># 출력을 계산하고, 내부 Tensor에 가중치와 편향을 저장합니다.</span>
<span class="c1"># Flatten 계층은 선형 계층의 출력을 `y` 의 shape과 맞도록(match) 1D 텐서로 폅니다(flatten).</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># 또한 nn 패키지에는 주로 사용되는 손실 함수(loss function)들에 대한 정의도 포함되어 있습니다;</span>
<span class="c1"># 여기에서는 평균 제곱 오차(MSE; Mean Squared Error)를 손실 함수로 사용하겠습니다.</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>

    <span class="c1"># 순전파 단계: x를 모델에 전달하여 예측값 y를 계산합니다. Module 객체는 __call__ 연산자를 </span>
    <span class="c1"># 덮어써서(override) 함수처럼 호출할 수 있도록 합니다. 이렇게 함으로써 입력 데이터의 텐서를 Module에 전달하여</span>
    <span class="c1"># 출력 데이터의 텐서를 생성합니다.</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>

    <span class="c1"># 손실을 계산하고 출력합니다. 예측한 y와 정답인 y를 갖는 텐서들을 전달하고,</span>
    <span class="c1"># 손실 함수는 손실(loss)을 갖는 텐서를 반환합니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># 역전파 단계를 실행하기 전에 변화도(gradient)를 0으로 만듭니다.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를 계산합니다.</span>
    <span class="c1"># 내부적으로 각 Module의 매개변수는 requires_grad=True일 때 텐서에 저장되므로,</span>
    <span class="c1"># 아래 호출은 모델의 모든 학습 가능한 매개변수의 변화도를 계산하게 됩니다.</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># 경사하강법을 사용하여 가중치를 갱신합니다.</span>
    <span class="c1"># 각 매개변수는 텐서이므로, 이전에 했던 것처럼 변화도에 접근할 수 있습니다.</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">param</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span>

<span class="c1"># list의 첫번째 항목에 접근하는 것처럼 `model` 의 첫번째 계층(layer)에 접근할 수 있습니다.</span>
<span class="n">linear_layer</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># 선형 계층에서, 매개변수는 `weights` 와 `bias` 로 저장됩니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Result: y = </span><span class="si">{</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span><span class="w"> </span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x + </span><span class="si">{</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^2 + </span><span class="si">{</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span><span class="w"> </span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^3&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="pytorch-optim">
<h3><a class="toc-backref" href="#id15" role="doc-backlink">PyTorch: optim</a><a class="headerlink" href="#pytorch-optim" title="Link to this heading">#</a></h3>
<p>지금까지는 <code class="docutils literal notranslate"><span class="pre">torch.no_grad()</span></code> 로 학습 가능한 매개변수를 갖는 텐서들을 직접 조작하여 모델의 가중치(weight)를 갱신하였습니다.
이것은 확률적 경사하강법(SGD; stochastic gradient descent)와 같은 간단한 최적화 알고리즘에서는 크게 부담이 되지 않지만,
실제로 신경망을 학습할 때는 <code class="docutils literal notranslate"><span class="pre">AdaGrad</span></code>, <code class="docutils literal notranslate"><span class="pre">RMSProp</span></code>, <code class="docutils literal notranslate"><span class="pre">Adam</span></code> 등과 같은 더 정교한 옵티마이저(optimizer)를 사용하곤 합니다.</p>
<p>PyTorch의 <code class="docutils literal notranslate"><span class="pre">optim</span></code> 패키지는 최적화 알고리즘에 대한 아이디어를 추상화하고 일반적으로 사용하는 최적화 알고리즘의 구현체(implementation)를
제공합니다.</p>
<p>이 예제에서는 지금까지와 같이 <code class="docutils literal notranslate"><span class="pre">nn</span></code> 패키지를 사용하여 모델을 정의하지만, 모델을 최적화할 때는 <code class="docutils literal notranslate"><span class="pre">optim</span></code> 패키지가 제공하는
<code class="docutils literal notranslate"><span class="pre">RMSProp</span></code> 알고리즘을 사용하겠습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="c1"># 입력값과 출력값을 갖는 텐서들을 생성합니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 입력 텐서 (x, x^2, x^3)를 준비합니다.</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

<span class="c1"># nn 패키지를 사용하여 모델과 손실 함수를 정의합니다.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>

<span class="c1"># optim 패키지를 사용하여 모델의 가중치를 갱신할 optimizer를 정의합니다.</span>
<span class="c1"># 여기서는 RMSprop을 사용하겠습니다; optim 패키지는 다른 다양한 최적화 알고리즘을 포함하고 있습니다.</span>
<span class="c1"># RMSprop 생성자의 첫번째 인자는 어떤 텐서가 갱신되어야 하는지를 알려줍니다.</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산합니다.</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>

    <span class="c1"># 손실을 계산하고 출력합니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># 역전파 단계 전에, optimizer 객체를 사용하여 (모델의 학습 가능한 가중치인) 갱신할</span>
    <span class="c1"># 변수들에 대한 모든 변화도(gradient)를 0으로 만듭니다. 이렇게 하는 이유는 기본적으로 </span>
    <span class="c1"># .backward()를 호출할 때마다 변화도가 버퍼(buffer)에 (덮어쓰지 않고) 누적되기</span>
    <span class="c1"># 때문입니다. 더 자세한 내용은 torch.autograd.backward에 대한 문서를 참조하세요.</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># 역전파 단계: 모델의 매개변수들에 대한 손실의 변화도를 계산합니다.</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># optimizer의 step 함수를 호출하면 매개변수가 갱신됩니다.</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


<span class="n">linear_layer</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Result: y = </span><span class="si">{</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span><span class="w"> </span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x + </span><span class="si">{</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^2 + </span><span class="si">{</span><span class="n">linear_layer</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span><span class="w"> </span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^3&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id3">
<h3><a class="toc-backref" href="#id16" role="doc-backlink">PyTorch: 사용자 정의 <code class="docutils literal notranslate"><span class="pre">nn</span></code> 모듈</a><a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>때대로 기존 Module의 구성(sequence)보다 더 복잡한 모델을 구성해야 할 때가 있습니다;
이러한 경우에는 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 의 하위 클래스(subclass)로 새로운 Module을 정의하고,
입력 텐서를 받아 다른 모듈 및 autograd 연산을 사용하여 출력 텐서를 만드는 <code class="docutils literal notranslate"><span class="pre">forward</span></code> 를 정의합니다.</p>
<p>이 예제에서는 3차 다항식을 사용자 정의 Module 하위클래스(subclass)로 구현해보겠습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>


<span class="k">class</span><span class="w"> </span><span class="nc">Polynomial3</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        생성자에서 4개의 매개변수를 생성(instantiate)하고, 멤버 변수로 지정합니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(()))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        순전파 함수에서는 입력 데이터의 텐서를 받고 출력 데이터의 텐서를 반환해야 합니다.</span>
<span class="sd">        텐서들 간의 임의의 연산뿐만 아니라, 생성자에서 정의한 Module을 사용할 수 있습니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">string</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Python의 다른 클래스(class)처럼, PyTorch 모듈을 사용해서 사용자 정의 메소드를 정의할 수 있습니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;y = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x + </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^2 + </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^3&#39;</span>


<span class="c1"># 입력값과 출력값을 갖는 텐서들을 생성합니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 위에서 정의한 클래스로 모델을 생성합니다.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Polynomial3</span><span class="p">()</span>

<span class="c1"># 손실 함수와 optimizer를 생성합니다. SGD 생성자에 model.paramaters()를 호출해주면</span>
<span class="c1"># 모델의 멤버 학습 가능한 (torch.nn.Parameter로 정의된) 매개변수들이 포함됩니다.</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산합니다.</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># 손실을 계산하고 출력합니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Result: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">string</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="pytorch-control-flow-weight-sharing">
<h3><a class="toc-backref" href="#id17" role="doc-backlink">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</a><a class="headerlink" href="#pytorch-control-flow-weight-sharing" title="Link to this heading">#</a></h3>
<p>동적 그래프와 가중치 공유의 예를 보이기 위해, 매우 이상한 모델을 구현해보겠습니다:
각 순전파 단계에서 3 ~ 5 사이의 임의의 숫자(random number)를 선택하여 다차항들에서 사용하고,
동일한 가중치를 여러번 재사용하여 4차항과 5차항을 계산합니다.</p>
<p>이 모델에서는 일반적인 Python 제어 흐름을 사용하여 반복(loop)을 구현할 수 있으며, 순전파 단계를 정의할 때
동일한 매개변수를 여러번 재사용하여 가중치 공유를 구현할 수 있습니다.</p>
<p>이러한 모델을 Module을 상속받는 하위클래스로 간단히 구현해보겠습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DynamicNet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        생성자에서 5개의 매개변수를 생성(instantiate)하고 멤버 변수로 지정합니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(()))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        모델의 순전파 단계에서는 무작위로 4, 5 중 하나를 선택한 뒤 매개변수 e를 재사용하여</span>
<span class="sd">        이 차수들의의 기여도(contribution)를 계산합니다.</span>

<span class="sd">        각 순전파 단계는 동적 연산 그래프를 구성하기 때문에, 모델의 순전파 단계를 정의할 때</span>
<span class="sd">        반복문이나 조건문과 같은 일반적인 Python 제어-흐름 연산자를 사용할 수 있습니다.</span>

<span class="sd">        여기에서 연산 그래프를 정의할 때 동일한 매개변수를 여러번 사용하는 것이 완벽히 안전하다는</span>
<span class="sd">        것을 알 수 있습니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span>
        <span class="k">for</span> <span class="n">exp</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">e</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="n">exp</span>
        <span class="k">return</span> <span class="n">y</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">string</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Python의 다른 클래스(class)처럼, PyTorch 모듈을 사용해서 사용자 정의 메소드를 정의할 수 있습니다.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;y = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x + </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^2 + </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^3 + </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">e</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^4 ? + </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">e</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s1"> x^5 ?&#39;</span>


<span class="c1"># 입력값과 출력값을 갖는 텐서들을 생성합니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 위에서 정의한 클래스로 모델을 생성합니다.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DynamicNet</span><span class="p">()</span>

<span class="c1"># 손실 함수와 optimizer를 생성합니다. 이 이상한 모델을 순수한 확률적 경사하강법(SGD; Stochastic Gradient Descent)으로</span>
<span class="c1"># 학습하는 것은 어려우므로, 모멘텀(momentum)을 사용합니다.</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30000</span><span class="p">):</span>
    <span class="c1"># 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산합니다.</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># 손실을 계산하고 출력합니다.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">2000</span> <span class="o">==</span> <span class="mi">1999</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Result: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">string</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="examples-download">
<span id="id4"></span><h2><a class="toc-backref" href="#id18" role="doc-backlink">예제 코드</a><a class="headerlink" href="#examples-download" title="Link to this heading">#</a></h2>
<p>위의 예제들을 여기서 찾아볼 수 있습니다.</p>
<section id="tensors">
<h3><a class="toc-backref" href="#id19" role="doc-backlink">Tensors</a><a class="headerlink" href="#tensors" title="Link to this heading">#</a></h3>
<div class="toctree-wrapper compound">
</div>
<p></p>
<p></p>
<div style='clear:both'></div></section>
<section id="id5">
<h3><a class="toc-backref" href="#id20" role="doc-backlink">Autograd</a><a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<div class="toctree-wrapper compound">
</div>
<p></p>
<p></p>
<div style='clear:both'></div></section>
<section id="id6">
<h3><a class="toc-backref" href="#id21" role="doc-backlink"><code class="docutils literal notranslate"><span class="pre">nn</span></code> 모듈</a><a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<div class="toctree-wrapper compound">
</div>
<p></p>
<p></p>
<p></p>
<p></p>
<div style='clear:both'></div></section>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="blitz/cifar10_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">분류기(Classifier) 학습하기</p>
      </div>
    </a>
    <a class="right-next"
       href="examples_tensor/polynomial_numpy.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">준비 운동: NumPy</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="blitz/cifar10_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">분류기(Classifier) 학습하기</p>
      </div>
    </a>
    <a class="right-next"
       href="examples_tensor/polynomial_numpy.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">준비 운동: NumPy</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor">텐서(Tensor)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numpy">준비 운동: numpy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-tensor">파이토치(PyTorch): 텐서(Tensor)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autograd">Autograd</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-tensor-autograd">PyTorch: 텐서(Tensor)와 autograd</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-autograd-function">PyTorch: 새 autograd Function 정의하기</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nn"><code class="docutils literal notranslate"><span class="pre">nn</span></code> 모듈</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-nn">PyTorch: <code class="docutils literal notranslate"><span class="pre">nn</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-optim">PyTorch: optim</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">PyTorch: 사용자 정의 <code class="docutils literal notranslate"><span class="pre">nn</span></code> 모듈</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-control-flow-weight-sharing">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-download">예제 코드</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors">Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Autograd</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6"><code class="docutils literal notranslate"><span class="pre">nn</span></code> 모듈</a></li>
</ul>
</li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "\uc608\uc81c\ub85c \ubc30\uc6b0\ub294 \ud30c\uc774\ud1a0\uce58(PyTorch)",
       "headline": "\uc608\uc81c\ub85c \ubc30\uc6b0\ub294 \ud30c\uc774\ud1a0\uce58(PyTorch)",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/beginner/pytorch_with_examples.html",
       "articleBody": "\uc608\uc81c\ub85c \ubc30\uc6b0\ub294 \ud30c\uc774\ud1a0\uce58(PyTorch)# Author: Justin Johnson\ubc88\uc5ed: \ubc15\uc815\ud658 \ucc38\uace0 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 \ub2e4\uc18c \uc624\ub798\ub41c PyTorch \ud29c\ud1a0\ub9ac\uc5bc\uc785\ub2c8\ub2e4. \uae30\ubcf8 \ub2e4\uc9c0\uae30 \uc5d0\uc11c \uc785\ubb38\uc790\ub97c \uc704\ud55c \ucd5c\uc2e0\uc758 \ub0b4\uc6a9\uc744 \ubcf4\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 PyTorch \uc758 \ud575\uc2ec\uc801\uc778 \uac1c\ub150\uc744 \uc608\uc81c\ub97c \ud1b5\ud574 \uc18c\uac1c\ud569\ub2c8\ub2e4. \ubcf8\uc9c8\uc801\uc73c\ub85c, PyTorch\uc5d0\ub294 \ub450\uac00\uc9c0 \uc8fc\uc694\ud55c \ud2b9\uc9d5\uc774 \uc788\uc2b5\ub2c8\ub2e4: NumPy\uc640 \uc720\uc0ac\ud558\uc9c0\ub9cc GPU \uc0c1\uc5d0\uc11c \uc2e4\ud589 \uac00\ub2a5\ud55c n-\ucc28\uc6d0 \ud150\uc11c(Tensor) \uc2e0\uacbd\ub9dd\uc744 \uad6c\uc131\ud558\uace0 \ud559\uc2b5\ud558\ub294 \uacfc\uc815\uc5d0\uc11c\uc758 \uc790\ub3d9 \ubbf8\ubd84(Automatic differentiation) \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 3\ucc28 \ub2e4\ud56d\uc2dd(third order polynomial)\uc744 \uc0ac\uc6a9\ud558\uc5ec \\(y=\\sin(x)\\) \uc5d0 \uadfc\uc0ac(fit)\ud558\ub294 \ubb38\uc81c\ub97c \ub2e4\ub904\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc2e0\uacbd\ub9dd\uc740 4\uac1c\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \uac00\uc9c0\uba70, \uc815\ub2f5\uacfc \uc2e0\uacbd\ub9dd\uc774 \uc608\uce21\ud55c \uacb0\uacfc \uc0ac\uc774\uc758 \uc720\ud074\ub9ac\ub4dc \uac70\ub9ac(Euclidean distance)\ub97c \ucd5c\uc18c\ud654\ud558\uc5ec \uc784\uc758\uc758 \uac12\uc744 \uadfc\uc0ac\ud560 \uc218 \uc788\ub3c4\ub85d \uacbd\uc0ac\ud558\uac15\ubc95(gradient descent)\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ucc38\uace0 \uac01\uac01\uc758 \uc608\uc81c\ub4e4\uc740 \uc774 \ubb38\uc11c\uc758 \ub9c8\uc9c0\ub9c9 \ubd80\ubd84\uc5d0\uc11c \uc0b4\ud3b4\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. Table of Contents \ud150\uc11c(Tensor) \uc900\ube44 \uc6b4\ub3d9: numpy \ud30c\uc774\ud1a0\uce58(PyTorch): \ud150\uc11c(Tensor) Autograd PyTorch: \ud150\uc11c(Tensor)\uc640 autograd PyTorch: \uc0c8 autograd Function \uc815\uc758\ud558\uae30 nn \ubaa8\ub4c8 PyTorch: nn PyTorch: optim PyTorch: \uc0ac\uc6a9\uc790 \uc815\uc758 nn \ubaa8\ub4c8 PyTorch: \uc81c\uc5b4 \ud750\ub984(Control Flow) + \uac00\uc911\uce58 \uacf5\uc720(Weight Sharing) \uc608\uc81c \ucf54\ub4dc Tensors Autograd nn \ubaa8\ub4c8 \ud150\uc11c(Tensor)# \uc900\ube44 \uc6b4\ub3d9: numpy# PyTorch\ub97c \uc18c\uac1c\ud558\uae30 \uc804\uc5d0, \uba3c\uc800 NumPy\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e0\uacbd\ub9dd\uc744 \uad6c\uc131\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. NumPy\ub294 n-\ucc28\uc6d0 \ubc30\uc5f4 \uac1d\uccb4\uc640 \uc774\ub7ec\ud55c \ubc30\uc5f4\ub4e4\uc744 \uc870\uc791\ud558\uae30 \uc704\ud55c \ub2e4\uc591\ud55c \ud568\uc218\ub4e4\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. NumPy\ub294 \uacfc\ud559 \ubd84\uc57c\uc758 \uc5f0\uc0b0\uc744 \uc704\ud55c \ud3ec\uad04\uc801\uc778 \ud504\ub808\uc784\uc6cc\ud06c(generic framework)\uc785\ub2c8\ub2e4; NumPy\ub294 \uc5f0\uc0b0 \uadf8\ub798\ud504(computation graph)\ub098 \ub525\ub7ec\ub2dd, \ubcc0\ud654\ub3c4(gradient)\uc5d0 \ub300\ud574\uc11c\ub294 \uc54c\uc9c0 \ubabb\ud569\ub2c8\ub2e4. \ud558\uc9c0\ub9cc NumPy \uc5f0\uc0b0\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2e0\uacbd\ub9dd\uc758 \uc21c\uc804\ud30c \ub2e8\uacc4\uc640 \uc5ed\uc804\ud30c \ub2e8\uacc4\ub97c \uc9c1\uc811 \uad6c\ud604\ud568\uc73c\ub85c\uc368, 3\ucc28 \ub2e4\ud56d\uc2dd\uc774 \uc0ac\uc778(sine) \ud568\uc218\uc5d0 \uadfc\uc0ac\ud558\ub3c4\ub85d \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4: # -*- coding: utf-8 -*- import numpy as np import math # \ubb34\uc791\uc704\ub85c \uc785\ub825\uacfc \ucd9c\ub825 \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4 x = np.linspace(-math.pi, math.pi, 2000) y = np.sin(x) # \ubb34\uc791\uc704\ub85c \uac00\uc911\uce58\ub97c \ucd08\uae30\ud654\ud569\ub2c8\ub2e4 a = np.random.randn() b = np.random.randn() c = np.random.randn() d = np.random.randn() learning_rate = 1e-6 for t in range(2000): # \uc21c\uc804\ud30c \ub2e8\uacc4: \uc608\uce21\uac12 y\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4 # y = a + b x + c x^2 + d x^3 y_pred = a + b * x + c * x ** 2 + d * x ** 3 # \uc190\uc2e4(loss)\uc744 \uacc4\uc0b0\ud558\uace0 \ucd9c\ub825\ud569\ub2c8\ub2e4 loss = np.square(y_pred - y).sum() if t % 100 == 99: print(t, loss) # \uc190\uc2e4\uc5d0 \ub530\ub978 a, b, c, d\uc758 \ubcc0\ud654\ub3c4(gradient)\ub97c \uacc4\uc0b0\ud558\uace0 \uc5ed\uc804\ud30c\ud569\ub2c8\ub2e4. grad_y_pred = 2.0 * (y_pred - y) grad_a = grad_y_pred.sum() grad_b = (grad_y_pred * x).sum() grad_c = (grad_y_pred * x ** 2).sum() grad_d = (grad_y_pred * x ** 3).sum() # \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud569\ub2c8\ub2e4. a -= learning_rate * grad_a b -= learning_rate * grad_b c -= learning_rate * grad_c d -= learning_rate * grad_d print(f\u0027Result: y = {a} + {b} x + {c} x^2 + {d} x^3\u0027) \ud30c\uc774\ud1a0\uce58(PyTorch): \ud150\uc11c(Tensor)# NumPy\ub294 \ud6cc\ub96d\ud55c \ud504\ub808\uc784\uc6cc\ud06c\uc9c0\ub9cc, GPU\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc218\uce58 \uc5f0\uc0b0\uc744 \uac00\uc18d\ud654\ud560 \uc218\ub294 \uc5c6\uc2b5\ub2c8\ub2e4. \ud604\ub300\uc758 \uc2ec\uce35 \uc2e0\uacbd\ub9dd\uc5d0\uc11c GPU\ub294 \uc885\uc885 50\ubc30 \ub610\ub294 \uadf8 \uc774\uc0c1 \uc758 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \uc81c\uacf5\ud558\uae30 \ub54c\ubb38\uc5d0, \uc548\ud0c0\uae5d\uac8c\ub3c4 NumPy\ub294 \ud604\ub300\uc758 \ub525\ub7ec\ub2dd\uc5d0\ub294 \ucda9\ubd84\uce58 \uc54a\uc2b5\ub2c8\ub2e4. \uc774\ubc88\uc5d0\ub294 PyTorch\uc758 \uac00\uc7a5 \ud575\uc2ec\uc801\uc778 \uac1c\ub150\uc778 \ud150\uc11c(Tensor) \uc5d0 \ub300\ud574\uc11c \uc54c\uc544\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. PyTorch \ud150\uc11c(Tensor)\ub294 \uac1c\ub150\uc801\uc73c\ub85c NumPy \ubc30\uc5f4\uacfc \ub3d9\uc77c\ud569\ub2c8\ub2e4: \ud150\uc11c(Tensor)\ub294 n-\ucc28\uc6d0 \ubc30\uc5f4\uc774\uba70, PyTorch\ub294 \uc774\ub7ec\ud55c \ud150\uc11c\ub4e4\uc758 \uc5f0\uc0b0\uc744 \uc704\ud55c \ub2e4\uc591\ud55c \uae30\ub2a5\ub4e4\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. NumPy \ubc30\uc5f4\ucc98\ub7fc PyTorch Tensor\ub294 \ub525\ub7ec\ub2dd\uc774\ub098 \uc5f0\uc0b0 \uadf8\ub798\ud504, \ubcc0\ud654\ub3c4\ub294 \uc54c\uc9c0 \ubabb\ud558\uba70, \uacfc\ud559\uc801 \ubd84\uc57c\uc758 \uc5f0\uc0b0\uc744 \uc704\ud55c \ud3ec\uad04\uc801\uc778 \ub3c4\uad6c\uc785\ub2c8\ub2e4. \ud150\uc11c\ub294 \uc5f0\uc0b0 \uadf8\ub798\ud504\uc640 \ubcc0\ud654\ub3c4\ub97c \ucd94\uc801\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uacfc\ud559\uc801 \uc5f0\uc0b0\uc744 \uc704\ud55c \uc77c\ubc18\uc801\uc778 \ub3c4\uad6c\ub85c\ub3c4 \uc720\uc6a9\ud569\ub2c8\ub2e4. \ub610\ud55c NumPy\uc640\ub294 \ub2e4\ub974\uac8c, PyTorch \ud150\uc11c\ub294 GPU\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc218\uce58 \uc5f0\uc0b0\uc744 \uac00\uc18d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. PyTorch \ud150\uc11c\ub97c GPU\uc5d0\uc11c \uc2e4\ud589\ud558\uae30 \uc704\ud574\uc11c\ub294 \ub2e8\uc9c0 \uc801\uc808\ud55c \uc7a5\uce58\ub97c \uc9c0\uc815\ud574\uc8fc\uae30\ub9cc \ud558\uba74 \ub429\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 PyTorch \ud150\uc11c\ub97c \uc0ac\uc6a9\ud558\uc5ec 3\ucc28 \ub2e4\ud56d\uc2dd\uc744 \uc0ac\uc778(sine) \ud568\uc218\uc5d0 \uadfc\uc0ac\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc704\uc758 NumPy \uc608\uc81c\uc5d0\uc11c\uc640 \uac19\uc774 \uc2e0\uacbd\ub9dd\uc758 \uc21c\uc804\ud30c \ub2e8\uacc4\uc640 \uc5ed\uc804\ud30c \ub2e8\uacc4\ub294 \uc9c1\uc811 \uad6c\ud604\ud558\uaca0\uc2b5\ub2c8\ub2e4: # -*- coding: utf-8 -*- import torch import math dtype = torch.float device = torch.device(\"cpu\") # device = torch.device(\"cuda:0\") # GPU\uc5d0\uc11c \uc2e4\ud589\ud558\ub824\uba74 \uc774 \uc8fc\uc11d\uc744 \uc81c\uac70\ud558\uc138\uc694 # \ubb34\uc791\uc704\ub85c \uc785\ub825\uacfc \ucd9c\ub825 \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4 x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype) y = torch.sin(x) # \ubb34\uc791\uc704\ub85c \uac00\uc911\uce58\ub97c \ucd08\uae30\ud654\ud569\ub2c8\ub2e4 a = torch.randn((), device=device, dtype=dtype) b = torch.randn((), device=device, dtype=dtype) c = torch.randn((), device=device, dtype=dtype) d = torch.randn((), device=device, dtype=dtype) learning_rate = 1e-6 for t in range(2000): # \uc21c\uc804\ud30c \ub2e8\uacc4: \uc608\uce21\uac12 y\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4 y_pred = a + b * x + c * x ** 2 + d * x ** 3 # \uc190\uc2e4(loss)\uc744 \uacc4\uc0b0\ud558\uace0 \ucd9c\ub825\ud569\ub2c8\ub2e4 loss = (y_pred - y).pow(2).sum().item() if t % 100 == 99: print(t, loss) # \uc190\uc2e4\uc5d0 \ub530\ub978 a, b, c, d\uc758 \ubcc0\ud654\ub3c4(gradient)\ub97c \uacc4\uc0b0\ud558\uace0 \uc5ed\uc804\ud30c\ud569\ub2c8\ub2e4. grad_y_pred = 2.0 * (y_pred - y) grad_a = grad_y_pred.sum() grad_b = (grad_y_pred * x).sum() grad_c = (grad_y_pred * x ** 2).sum() grad_d = (grad_y_pred * x ** 3).sum() # \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud569\ub2c8\ub2e4. a -= learning_rate * grad_a b -= learning_rate * grad_b c -= learning_rate * grad_c d -= learning_rate * grad_d print(f\u0027Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3\u0027) Autograd# PyTorch: \ud150\uc11c(Tensor)\uc640 autograd# \uc704\uc758 \uc608\uc81c\ub4e4\uc5d0\uc11c\ub294 \uc2e0\uacbd\ub9dd\uc758 \uc21c\uc804\ud30c \ub2e8\uacc4\uc640 \uc5ed\uc804\ud30c \ub2e8\uacc4\ub97c \uc9c1\uc811 \uad6c\ud604\ud574\ubcf4\uc558\uc2b5\ub2c8\ub2e4. \uc791\uc740 2\uacc4\uce35(2-layer) \uc2e0\uacbd\ub9dd\uc5d0\uc11c\ub294 \uc5ed\uc804\ud30c \ub2e8\uacc4\ub97c \uc9c1\uc811 \uad6c\ud604\ud558\ub294 \uac83\uc774 \ud070\uc77c\uc774 \uc544\ub2c8\uc9c0\ub9cc, \ubcf5\uc7a1\ud55c \ub300\uaddc\ubaa8 \uc2e0\uacbd\ub9dd\uc5d0\uc11c\ub294 \ub9e4\uc6b0 \uc544\uc2ac\uc544\uc2ac\ud55c \uc77c\uc77c \uac83\uc785\ub2c8\ub2e4. \ub2e4\ud589\ud788\ub3c4, \uc790\ub3d9 \ubbf8\ubd84 \uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2e0\uacbd\ub9dd\uc758 \uc5ed\uc804\ud30c \ub2e8\uacc4 \uc5f0\uc0b0\uc744 \uc790\ub3d9\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. PyTorch\uc758 autograd \ud328\ud0a4\uc9c0\ub294 \uc815\ud655\ud788 \uc774\ub7f0 \uae30\ub2a5\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. Autograd\ub97c \uc0ac\uc6a9\ud558\uba74, \uc2e0\uacbd\ub9dd\uc758 \uc21c\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c \uc5f0\uc0b0 \uadf8\ub798\ud504(computational graph) \ub97c \uc815\uc758\ud558\uac8c \ub429\ub2c8\ub2e4; \uc774 \uadf8\ub798\ud504\uc758 \ub178\ub4dc(node)\ub294 \ud150\uc11c(tensor)\uc774\uace0, \uc5e3\uc9c0(edge)\ub294 \uc785\ub825 \ud150\uc11c\ub85c\ubd80\ud130 \ucd9c\ub825 \ud150\uc11c\ub97c \ub9cc\ub4e4\uc5b4\ub0b4\ub294 \ud568\uc218\uac00 \ub429\ub2c8\ub2e4. \uc774 \uadf8\ub798\ud504\ub97c \ud1b5\ud574 \uc5ed\uc804\ud30c\ub97c \ud558\uac8c \ub418\uba74 \ubcc0\ud654\ub3c4\ub97c \uc27d\uac8c \uacc4\uc0b0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ubcf5\uc7a1\ud558\uac8c \ub4e4\ub9ac\uaca0\uc9c0\ub9cc, \uc2e4\uc81c\ub85c \uc0ac\uc6a9\ud558\ub294 \uac83\uc740 \ub9e4\uc6b0 \uac04\ub2e8\ud569\ub2c8\ub2e4. \uac01 \ud150\uc11c\ub294 \uc5f0\uc0b0\uadf8\ub798\ud504\uc5d0\uc11c \ub178\ub4dc\ub85c \ud45c\ud604\ub429\ub2c8\ub2e4. \ub9cc\uc57d x \uac00 x.requires_grad=True \uc778 \ud150\uc11c\ub77c\uba74 x.grad \uc5b4\ub5a4 \uc2a4\uce7c\ub77c \uac12\uc5d0 \ub300\ud55c x \uc758 \ubcc0\ud654\ub3c4\ub97c \uac16\ub294 \ub610 \ub2e4\ub978 \ud150\uc11c\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 PyTorch \ud150\uc11c\uc640 autograd\ub97c \uc0ac\uc6a9\ud558\uc5ec 3\ucc28 \ub2e4\ud56d\uc2dd\uc744 \uc0ac\uc778\ud30c(sine wave)\uc5d0 \uadfc\uc0ac\ud558\ub294 \uc608\uc81c\ub97c \uad6c\ud604\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4; \uc774\uc81c \ub354 \uc774\uc0c1 \uc2e0\uacbd\ub9dd\uc758 \uc5ed\uc804\ud30c \ub2e8\uacc4\ub97c \uc9c1\uc811 \uad6c\ud604\ud560 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4: # -*- coding: utf-8 -*- import torch import math dtype = torch.float device = \"cuda\" if torch.cuda.is_available() else \"cpu\" torch.set_default_device(device) # \uc785\ub825\uac12\uacfc \ucd9c\ub825\uac12\uc744 \uac16\ub294 \ud150\uc11c\ub4e4\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. # requires_grad=False\uac00 \uae30\ubcf8\uac12\uc73c\ub85c \uc124\uc815\ub418\uc5b4 \uc5ed\uc804\ud30c \ub2e8\uacc4 \uc911\uc5d0 \uc774 \ud150\uc11c\ub4e4\uc5d0 \ub300\ud55c \ubcc0\ud654\ub3c4\ub97c # \uacc4\uc0b0\ud560 \ud544\uc694\uac00 \uc5c6\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. x = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype) y = torch.sin(x) # \uac00\uc911\uce58\ub97c \uac16\ub294 \uc784\uc758\uc758 \ud150\uc11c\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. 3\ucc28 \ub2e4\ud56d\uc2dd\uc774\ubbc0\ub85c 4\uac1c\uc758 \uac00\uc911\uce58\uac00 \ud544\uc694\ud569\ub2c8\ub2e4: # y = a + b x + c x^2 + d x^3 # requires_grad=True\ub85c \uc124\uc815\ud558\uc5ec \uc5ed\uc804\ud30c \ub2e8\uacc4 \uc911\uc5d0 \uc774 \ud150\uc11c\ub4e4\uc5d0 \ub300\ud55c \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud560 \ud544\uc694\uac00 # \uc788\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. a = torch.randn((), dtype=dtype, requires_grad=True) b = torch.randn((), dtype=dtype, requires_grad=True) c = torch.randn((), dtype=dtype, requires_grad=True) d = torch.randn((), dtype=dtype, requires_grad=True) learning_rate = 1e-6 for t in range(2000): # \uc21c\uc804\ud30c \ub2e8\uacc4: \ud150\uc11c\ub4e4 \uac04\uc758 \uc5f0\uc0b0\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc608\uce21\uac12 y\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. y_pred = a + b * x + c * x ** 2 + d * x ** 3 # \ud150\uc11c\ub4e4\uac04\uc758 \uc5f0\uc0b0\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc190\uc2e4(loss)\uc744 \uacc4\uc0b0\ud558\uace0 \ucd9c\ub825\ud569\ub2c8\ub2e4. # \uc774 \ub54c \uc190\uc2e4\uc740 (1,) shape\uc744 \uac16\ub294 \ud150\uc11c\uc785\ub2c8\ub2e4. # loss.item() \uc73c\ub85c \uc190\uc2e4\uc774 \uac16\uace0 \uc788\ub294 \uc2a4\uce7c\ub77c \uac12\uc744 \uac00\uc838\uc62c \uc218 \uc788\uc2b5\ub2c8\ub2e4. loss = (y_pred - y).pow(2).sum() if t % 100 == 99: print(t, loss.item()) # autograd \ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5ed\uc804\ud30c \ub2e8\uacc4\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. \uc774\ub294 requires_grad=True\ub97c \uac16\ub294 # \ubaa8\ub4e0 \ud150\uc11c\ub4e4\uc5d0 \ub300\ud55c \uc190\uc2e4\uc758 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. # \uc774\ud6c4 a.grad\uc640 b.grad, c.grad, d.grad\ub294 \uac01\uac01 a, b, c, d\uc5d0 \ub300\ud55c \uc190\uc2e4\uc758 \ubcc0\ud654\ub3c4\ub97c # \uac16\ub294 \ud150\uc11c\uac00 \ub429\ub2c8\ub2e4. loss.backward() # \uacbd\uc0ac\ud558\uac15\ubc95(gradient descent)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac00\uc911\uce58\ub97c \uc9c1\uc811 \uac31\uc2e0\ud569\ub2c8\ub2e4. # torch.no_grad()\ub85c \uac10\uc2f8\ub294 \uc774\uc720\ub294, \uac00\uc911\uce58\ub4e4\uc774 requires_grad=True \uc9c0\ub9cc # autograd\uc5d0\uc11c\ub294 \uc774\ub97c \ucd94\uc801\ud558\uc9c0 \uc54a\uc744 \uac83\uc774\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. with torch.no_grad(): a -= learning_rate * a.grad b -= learning_rate * b.grad c -= learning_rate * c.grad d -= learning_rate * d.grad # \uac00\uc911\uce58 \uac31\uc2e0 \ud6c4\uc5d0\ub294 \ubcc0\ud654\ub3c4\ub97c \uc9c1\uc811 0\uc73c\ub85c \ub9cc\ub4ed\ub2c8\ub2e4. a.grad = None b.grad = None c.grad = None d.grad = None print(f\u0027Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3\u0027) PyTorch: \uc0c8 autograd Function \uc815\uc758\ud558\uae30# \ub0b4\ubd80\uc801\uc73c\ub85c, autograd\uc758 \uae30\ubcf8(primitive) \uc5f0\uc0b0\uc790\ub294 \uc2e4\uc81c\ub85c \ud150\uc11c\ub97c \uc870\uc791\ud558\ub294 2\uac1c\uc758 \ud568\uc218\uc785\ub2c8\ub2e4. forward \ud568\uc218\ub294 \uc785\ub825 \ud150\uc11c\ub85c\ubd80\ud130 \ucd9c\ub825 \ud150\uc11c\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. backward \ud568\uc218\ub294 \uc5b4\ub5a4 \uc2a4\uce7c\ub77c \uac12\uc5d0 \ub300\ud55c \ucd9c\ub825 \ud150\uc11c\uc758 \ubcc0\ud654\ub3c4(gradient)\ub97c \uc804\ub2ec\ubc1b\uace0, \ub3d9\uc77c\ud55c \uc2a4\uce7c\ub77c \uac12\uc5d0 \ub300\ud55c \uc785\ub825 \ud150\uc11c\uc758 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. PyTorch\uc5d0\uc11c torch.autograd.Function \uc758 \ud558\uc704\ud074\ub798\uc2a4(subclass)\ub97c \uc815\uc758\ud558\uace0 forward \uc640 backward \ud568\uc218\ub97c \uad6c\ud604\ud568\uc73c\ub85c\uc368 \uc0ac\uc6a9\uc790 \uc815\uc758 autograd \uc5f0\uc0b0\uc790\ub97c \uc190\uc27d\uac8c \uc815\uc758\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8 \ud6c4, \uc778\uc2a4\ud134\uc2a4(instance)\ub97c \uc0dd\uc131\ud558\uace0 \uc774\ub97c \ud568\uc218\ucc98\ub7fc \ud638\ucd9c\ud558\uace0, \uc785\ub825 \ub370\uc774\ud130\ub97c \uac16\ub294 \ud150\uc11c\ub97c \uc804\ub2ec\ud558\ub294 \uc2dd\uc73c\ub85c \uc0c8\ub85c\uc6b4 autograd \uc5f0\uc0b0\uc790\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uc608\uc81c\uc5d0\uc11c\ub294 \\(y=a+bx+cx^2+dx^3\\) \ub300\uc2e0 \\(y=a+b P_3(c+dx)\\) \ub85c \ubaa8\ub378\uc744 \uc815\uc758\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc11c \\(P_3(x)=\\frac{1}{2}\\left(5x^3-3x\\right)\\) \uc740 3\ucc28 \ub974\uc7a5\ub4dc\ub974 \ub2e4\ud56d\uc2dd(Legendre polynomial) \uc785\ub2c8\ub2e4. \\(P_3\\) \uc758 \uc21c\uc804\ud30c\uc640 \uc5ed\uc804\ud30c \uc5f0\uc0b0\uc744 \uc704\ud55c \uc0c8\ub85c\uc6b4 autograd Function\ub97c \uc791\uc131\ud558\uace0, \uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \uad6c\ud604\ud569\ub2c8\ub2e4: # -*- coding: utf-8 -*- import torch import math class LegendrePolynomial3(torch.autograd.Function): \"\"\" torch.autograd.Function\uc744 \uc0c1\uc18d\ubc1b\uc544 \uc0ac\uc6a9\uc790 \uc815\uc758 autograd Function\uc744 \uad6c\ud604\ud558\uace0, \ud150\uc11c \uc5f0\uc0b0\uc744 \ud558\ub294 \uc21c\uc804\ud30c \ub2e8\uacc4\uc640 \uc5ed\uc804\ud30c \ub2e8\uacc4\ub97c \uad6c\ud604\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \"\"\" @staticmethod def forward(ctx, input): \"\"\" \uc21c\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c\ub294 \uc785\ub825\uc744 \uac16\ub294 \ud150\uc11c\ub97c \ubc1b\uc544 \ucd9c\ub825\uc744 \uac16\ub294 \ud150\uc11c\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4. ctx\ub294 \ucee8\ud14d\uc2a4\ud2b8 \uac1d\uccb4(context object)\ub85c \uc5ed\uc804\ud30c \uc5f0\uc0b0\uc744 \uc704\ud55c \uc815\ubcf4 \uc800\uc7a5\uc5d0 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. ctx.save_for_backward \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5ed\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c \uc0ac\uc6a9\ud560 \uc5b4\ub5a4 \uac1d\uccb4\ub3c4 \uc800\uc7a5(cache)\ud574 \ub458 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \"\"\" ctx.save_for_backward(input) return 0.5 * (5 * input ** 3 - 3 * input) @staticmethod def backward(ctx, grad_output): \"\"\" \uc5ed\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c\ub294 \ucd9c\ub825\uc5d0 \ub300\ud55c \uc190\uc2e4(loss)\uc758 \ubcc0\ud654\ub3c4(gradient)\ub97c \uac16\ub294 \ud150\uc11c\ub97c \ubc1b\uace0, \uc785\ub825\uc5d0 \ub300\ud55c \uc190\uc2e4\uc758 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud574\uc57c \ud569\ub2c8\ub2e4. \"\"\" input, = ctx.saved_tensors return grad_output * 1.5 * (5 * input ** 2 - 1) dtype = torch.float device = torch.device(\"cpu\") # device = torch.device(\"cuda:0\") # GPU\uc5d0\uc11c \uc2e4\ud589\ud558\ub824\uba74 \uc774 \uc8fc\uc11d\uc744 \uc81c\uac70\ud558\uc138\uc694 # \uc785\ub825\uac12\uacfc \ucd9c\ub825\uac12\uc744 \uac16\ub294 \ud150\uc11c\ub4e4\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. # requires_grad=False\uac00 \uae30\ubcf8\uac12\uc73c\ub85c \uc124\uc815\ub418\uc5b4 \uc5ed\uc804\ud30c \ub2e8\uacc4 \uc911\uc5d0 \uc774 \ud150\uc11c\ub4e4\uc5d0 \ub300\ud55c \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud560 # \ud544\uc694\uac00 \uc5c6\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype) y = torch.sin(x) # \uac00\uc911\uce58\ub97c \uac16\ub294 \uc784\uc758\uc758 \ud150\uc11c\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. 3\ucc28 \ub2e4\ud56d\uc2dd\uc774\ubbc0\ub85c 4\uac1c\uc758 \uac00\uc911\uce58\uac00 \ud544\uc694\ud569\ub2c8\ub2e4: # y = a + b * P3(c + d * x) # \uc774 \uac00\uc911\uce58\ub4e4\uc774 \uc218\ub834(convergence)\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc815\ub2f5\uc73c\ub85c\ubd80\ud130 \ub108\ubb34 \uba40\ub9ac \ub5a8\uc5b4\uc9c0\uc9c0 \uc54a\uc740 \uac12\uc73c\ub85c # \ucd08\uae30\ud654\uac00 \ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4. # requires_grad=True\ub85c \uc124\uc815\ud558\uc5ec \uc5ed\uc804\ud30c \ub2e8\uacc4 \uc911\uc5d0 \uc774 \ud150\uc11c\ub4e4\uc5d0 \ub300\ud55c \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud560 \ud544\uc694\uac00 # \uc788\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True) b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True) c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True) d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True) learning_rate = 5e-6 for t in range(2000): # \uc0ac\uc6a9\uc790 \uc815\uc758 Function\uc744 \uc801\uc6a9\ud558\uae30 \uc704\ud574 Function.apply \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. # \uc5ec\uae30\uc5d0 \u0027P3\u0027\ub77c\uace0 \uc774\ub984\uc744 \ubd99\uc600\uc2b5\ub2c8\ub2e4. P3 = LegendrePolynomial3.apply # \uc21c\uc804\ud30c \ub2e8\uacc4: \uc5f0\uc0b0\uc744 \ud558\uc5ec \uc608\uce21\uac12 y\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4; # \uc0ac\uc6a9\uc790 \uc815\uc758 autograd \uc5f0\uc0b0\uc744 \uc0ac\uc6a9\ud558\uc5ec P3\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. y_pred = a + b * P3(c + d * x) # \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uace0 \ucd9c\ub825\ud569\ub2c8\ub2e4. loss = (y_pred - y).pow(2).sum() if t % 100 == 99: print(t, loss.item()) # autograd\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5ed\uc804\ud30c \ub2e8\uacc4\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. loss.backward() # \uacbd\uc0ac\ud558\uac15\ubc95(gradient descent)\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud569\ub2c8\ub2e4. with torch.no_grad(): a -= learning_rate * a.grad b -= learning_rate * b.grad c -= learning_rate * c.grad d -= learning_rate * d.grad # \uac00\uc911\uce58 \uac31\uc2e0 \ud6c4\uc5d0\ub294 \ubcc0\ud654\ub3c4\ub97c \uc9c1\uc811 0\uc73c\ub85c \ub9cc\ub4ed\ub2c8\ub2e4. a.grad = None b.grad = None c.grad = None d.grad = None print(f\u0027Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)\u0027) nn \ubaa8\ub4c8# PyTorch: nn# \uc5f0\uc0b0 \uadf8\ub798\ud504\uc640 autograd\ub294 \ubcf5\uc7a1\ud55c \uc5f0\uc0b0\uc790\ub97c \uc815\uc758\ud558\uace0 \ub3c4\ud568\uc218(derivative)\ub97c \uc790\ub3d9\uc73c\ub85c \uacc4\uc0b0\ud558\ub294 \ub9e4\uc6b0 \uac15\ub825\ud55c \ud328\ub7ec\ub2e4\uc784(paradigm)\uc785\ub2c8\ub2e4; \ud558\uc9c0\ub9cc \ub300\uaddc\ubaa8 \uc2e0\uacbd\ub9dd\uc5d0\uc11c\ub294 autograd \uadf8 \uc790\uccb4\ub9cc\uc73c\ub85c\ub294 \ub108\ubb34 \uc800\uc218\uc900(low-level)\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc2e0\uacbd\ub9dd\uc744 \uad6c\uc131\ud558\ub294 \uac83\uc744 \uc885\uc885 \uc5f0\uc0b0\uc744 \uacc4\uce35(layer) \uc5d0 \ubc30\uc5f4(arrange)\ud558\ub294 \uac83\uc73c\ub85c \uc0dd\uac01\ud558\ub294\ub370, \uc774 \uc911 \uc77c\ubd80\ub294 \ud559\uc2b5 \ub3c4\uc911 \ucd5c\uc801\ud654\uac00 \ub420 \ud559\uc2b5 \uac00\ub2a5\ud55c \ub9e4\uac1c\ubcc0\uc218 \ub97c \uac16\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ud150\uc11c\ud50c\ub85c\uc6b0(Tensorflow)\uc5d0\uc11c\ub294, Keras \uc640 TensorFlow-Slim, TFLearn \uac19\uc740 \ud328\ud0a4\uc9c0\ub4e4\uc774 \uc5f0\uc0b0 \uadf8\ub798\ud504\ub97c \uace0\uc218\uc900(high-level)\uc73c\ub85c \ucd94\uc0c1\ud654(abstraction)\ud558\uc5ec \uc81c\uacf5\ud558\ubbc0\ub85c \uc2e0\uacbd\ub9dd\uc744 \uad6c\ucd95\ud558\ub294\ub370 \uc720\uc6a9\ud569\ub2c8\ub2e4. \ud30c\uc774\ud1a0\uce58(PyTorch)\uc5d0\uc11c\ub294 nn \ud328\ud0a4\uc9c0\uac00 \ub3d9\uc77c\ud55c \ubaa9\uc801\uc73c\ub85c \uc81c\uacf5\ub429\ub2c8\ub2e4. nn \ud328\ud0a4\uc9c0\ub294 \uc2e0\uacbd\ub9dd \uacc4\uce35(layer)\uacfc \uac70\uc758 \ube44\uc2b7\ud55c Module \uc758 \uc9d1\ud569\uc744 \uc815\uc758\ud569\ub2c8\ub2e4. Module\uc740 \uc785\ub825 \ud150\uc11c\ub97c \ubc1b\uace0 \ucd9c\ub825 \ud150\uc11c\ub97c \uacc4\uc0b0\ud558\ub294 \ud55c\ud3b8, \ud559\uc2b5 \uac00\ub2a5\ud55c \ub9e4\uac1c\ubcc0\uc218\ub97c \uac16\ub294 \ud150\uc11c\ub4e4\uc744 \ub0b4\ubd80 \uc0c1\ud0dc(internal state)\ub85c \uac16\uc2b5\ub2c8\ub2e4. nn \ud328\ud0a4\uc9c0\ub294 \ub610\ud55c \uc2e0\uacbd\ub9dd\uc744 \ud559\uc2b5\uc2dc\ud0ac \ub54c \uc8fc\ub85c \uc0ac\uc6a9\ud558\ub294 \uc720\uc6a9\ud55c \uc190\uc2e4 \ud568\uc218(loss function)\ub4e4\ub3c4 \uc815\uc758\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uc608\uc81c\uc5d0\uc11c\ub294 nn \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\ud56d\uc2dd \ubaa8\ub378\uc744 \uad6c\ud604\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4: # -*- coding: utf-8 -*- import torch import math # \uc785\ub825\uac12\uacfc \ucd9c\ub825\uac12\uc744 \uac16\ub294 \ud150\uc11c\ub4e4\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. x = torch.linspace(-math.pi, math.pi, 2000) y = torch.sin(x) # \uc774 \uc608\uc81c\uc5d0\uc11c, \ucd9c\ub825 y\ub294 (x, x^2, x^3)\uc758 \uc120\ud615 \ud568\uc218\uc774\ubbc0\ub85c, \uc120\ud615 \uacc4\uce35 \uc2e0\uacbd\ub9dd\uc73c\ub85c \uac04\uc8fc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. # (x, x^2, x^3)\ub97c \uc704\ud55c \ud150\uc11c\ub97c \uc900\ube44\ud569\ub2c8\ub2e4. p = torch.tensor([1, 2, 3]) xx = x.unsqueeze(-1).pow(p) # \uc704 \ucf54\ub4dc\uc5d0\uc11c, x.unsqueeze(-1)\uc740 (2000, 1)\uc758 shape\uc744, p\ub294 (3,)\uc758 shape\uc744 \uac00\uc9c0\ubbc0\ub85c, # \uc774 \uacbd\uc6b0 \ube0c\ub85c\ub4dc\uce90\uc2a4\ud2b8(broadcast)\uac00 \uc801\uc6a9\ub418\uc5b4 (2000, 3)\uc758 shape\uc744 \uac16\ub294 \ud150\uc11c\ub97c \uc5bb\uc2b5\ub2c8\ub2e4. # nn \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \uc21c\ucc28\uc801 \uacc4\uce35(sequence of layers)\uc73c\ub85c \uc815\uc758\ud569\ub2c8\ub2e4. # nn.Sequential\uc740 \ub2e4\ub978 Module\uc744 \ud3ec\ud568\ud558\ub294 Module\ub85c, \ud3ec\ud568\ub418\ub294 Module\ub4e4\uc744 \uc21c\ucc28\uc801\uc73c\ub85c \uc801\uc6a9\ud558\uc5ec # \ucd9c\ub825\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uac01\uac01\uc758 Linear Module\uc740 \uc120\ud615 \ud568\uc218(linear function)\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc785\ub825\uc73c\ub85c\ubd80\ud130 # \ucd9c\ub825\uc744 \uacc4\uc0b0\ud558\uace0, \ub0b4\ubd80 Tensor\uc5d0 \uac00\uc911\uce58\uc640 \ud3b8\ud5a5\uc744 \uc800\uc7a5\ud569\ub2c8\ub2e4. # Flatten \uacc4\uce35\uc740 \uc120\ud615 \uacc4\uce35\uc758 \ucd9c\ub825\uc744 `y` \uc758 shape\uacfc \ub9de\ub3c4\ub85d(match) 1D \ud150\uc11c\ub85c \ud3c5\ub2c8\ub2e4(flatten). model = torch.nn.Sequential( torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1) ) # \ub610\ud55c nn \ud328\ud0a4\uc9c0\uc5d0\ub294 \uc8fc\ub85c \uc0ac\uc6a9\ub418\ub294 \uc190\uc2e4 \ud568\uc218(loss function)\ub4e4\uc5d0 \ub300\ud55c \uc815\uc758\ub3c4 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4; # \uc5ec\uae30\uc5d0\uc11c\ub294 \ud3c9\uade0 \uc81c\uacf1 \uc624\ucc28(MSE; Mean Squared Error)\ub97c \uc190\uc2e4 \ud568\uc218\ub85c \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4. loss_fn = torch.nn.MSELoss(reduction=\u0027sum\u0027) learning_rate = 1e-6 for t in range(2000): # \uc21c\uc804\ud30c \ub2e8\uacc4: x\ub97c \ubaa8\ub378\uc5d0 \uc804\ub2ec\ud558\uc5ec \uc608\uce21\uac12 y\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. Module \uac1d\uccb4\ub294 __call__ \uc5f0\uc0b0\uc790\ub97c # \ub36e\uc5b4\uc368\uc11c(override) \ud568\uc218\ucc98\ub7fc \ud638\ucd9c\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. \uc774\ub807\uac8c \ud568\uc73c\ub85c\uc368 \uc785\ub825 \ub370\uc774\ud130\uc758 \ud150\uc11c\ub97c Module\uc5d0 \uc804\ub2ec\ud558\uc5ec # \ucd9c\ub825 \ub370\uc774\ud130\uc758 \ud150\uc11c\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. y_pred = model(xx) # \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uace0 \ucd9c\ub825\ud569\ub2c8\ub2e4. \uc608\uce21\ud55c y\uc640 \uc815\ub2f5\uc778 y\ub97c \uac16\ub294 \ud150\uc11c\ub4e4\uc744 \uc804\ub2ec\ud558\uace0, # \uc190\uc2e4 \ud568\uc218\ub294 \uc190\uc2e4(loss)\uc744 \uac16\ub294 \ud150\uc11c\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4. loss = loss_fn(y_pred, y) if t % 100 == 99: print(t, loss.item()) # \uc5ed\uc804\ud30c \ub2e8\uacc4\ub97c \uc2e4\ud589\ud558\uae30 \uc804\uc5d0 \ubcc0\ud654\ub3c4(gradient)\ub97c 0\uc73c\ub85c \ub9cc\ub4ed\ub2c8\ub2e4. model.zero_grad() # \uc5ed\uc804\ud30c \ub2e8\uacc4: \ubaa8\ub378\uc758 \ud559\uc2b5 \uac00\ub2a5\ud55c \ubaa8\ub4e0 \ub9e4\uac1c\ubcc0\uc218\uc5d0 \ub300\ud574 \uc190\uc2e4\uc758 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. # \ub0b4\ubd80\uc801\uc73c\ub85c \uac01 Module\uc758 \ub9e4\uac1c\ubcc0\uc218\ub294 requires_grad=True\uc77c \ub54c \ud150\uc11c\uc5d0 \uc800\uc7a5\ub418\ubbc0\ub85c, # \uc544\ub798 \ud638\ucd9c\uc740 \ubaa8\ub378\uc758 \ubaa8\ub4e0 \ud559\uc2b5 \uac00\ub2a5\ud55c \ub9e4\uac1c\ubcc0\uc218\uc758 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud558\uac8c \ub429\ub2c8\ub2e4. loss.backward() # \uacbd\uc0ac\ud558\uac15\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud569\ub2c8\ub2e4. # \uac01 \ub9e4\uac1c\ubcc0\uc218\ub294 \ud150\uc11c\uc774\ubbc0\ub85c, \uc774\uc804\uc5d0 \ud588\ub358 \uac83\ucc98\ub7fc \ubcc0\ud654\ub3c4\uc5d0 \uc811\uadfc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. with torch.no_grad(): for param in model.parameters(): param -= learning_rate * param.grad # list\uc758 \uccab\ubc88\uc9f8 \ud56d\ubaa9\uc5d0 \uc811\uadfc\ud558\ub294 \uac83\ucc98\ub7fc `model` \uc758 \uccab\ubc88\uc9f8 \uacc4\uce35(layer)\uc5d0 \uc811\uadfc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. linear_layer = model[0] # \uc120\ud615 \uacc4\uce35\uc5d0\uc11c, \ub9e4\uac1c\ubcc0\uc218\ub294 `weights` \uc640 `bias` \ub85c \uc800\uc7a5\ub429\ub2c8\ub2e4. print(f\u0027Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3\u0027) PyTorch: optim# \uc9c0\uae08\uae4c\uc9c0\ub294 torch.no_grad() \ub85c \ud559\uc2b5 \uac00\ub2a5\ud55c \ub9e4\uac1c\ubcc0\uc218\ub97c \uac16\ub294 \ud150\uc11c\ub4e4\uc744 \uc9c1\uc811 \uc870\uc791\ud558\uc5ec \ubaa8\ub378\uc758 \uac00\uc911\uce58(weight)\ub97c \uac31\uc2e0\ud558\uc600\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc740 \ud655\ub960\uc801 \uacbd\uc0ac\ud558\uac15\ubc95(SGD; stochastic gradient descent)\uc640 \uac19\uc740 \uac04\ub2e8\ud55c \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc5d0\uc11c\ub294 \ud06c\uac8c \ubd80\ub2f4\uc774 \ub418\uc9c0 \uc54a\uc9c0\ub9cc, \uc2e4\uc81c\ub85c \uc2e0\uacbd\ub9dd\uc744 \ud559\uc2b5\ud560 \ub54c\ub294 AdaGrad, RMSProp, Adam \ub4f1\uacfc \uac19\uc740 \ub354 \uc815\uad50\ud55c \uc635\ud2f0\ub9c8\uc774\uc800(optimizer)\ub97c \uc0ac\uc6a9\ud558\uace4 \ud569\ub2c8\ub2e4. PyTorch\uc758 optim \ud328\ud0a4\uc9c0\ub294 \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc5d0 \ub300\ud55c \uc544\uc774\ub514\uc5b4\ub97c \ucd94\uc0c1\ud654\ud558\uace0 \uc77c\ubc18\uc801\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc758 \uad6c\ud604\uccb4(implementation)\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774 \uc608\uc81c\uc5d0\uc11c\ub294 \uc9c0\uae08\uae4c\uc9c0\uc640 \uac19\uc774 nn \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \uc815\uc758\ud558\uc9c0\ub9cc, \ubaa8\ub378\uc744 \ucd5c\uc801\ud654\ud560 \ub54c\ub294 optim \ud328\ud0a4\uc9c0\uac00 \uc81c\uacf5\ud558\ub294 RMSProp \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4: # -*- coding: utf-8 -*- import torch import math # \uc785\ub825\uac12\uacfc \ucd9c\ub825\uac12\uc744 \uac16\ub294 \ud150\uc11c\ub4e4\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. x = torch.linspace(-math.pi, math.pi, 2000) y = torch.sin(x) # \uc785\ub825 \ud150\uc11c (x, x^2, x^3)\ub97c \uc900\ube44\ud569\ub2c8\ub2e4. p = torch.tensor([1, 2, 3]) xx = x.unsqueeze(-1).pow(p) # nn \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uacfc \uc190\uc2e4 \ud568\uc218\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. model = torch.nn.Sequential( torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1) ) loss_fn = torch.nn.MSELoss(reduction=\u0027sum\u0027) # optim \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud560 optimizer\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. # \uc5ec\uae30\uc11c\ub294 RMSprop\uc744 \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4; optim \ud328\ud0a4\uc9c0\ub294 \ub2e4\ub978 \ub2e4\uc591\ud55c \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. # RMSprop \uc0dd\uc131\uc790\uc758 \uccab\ubc88\uc9f8 \uc778\uc790\ub294 \uc5b4\ub5a4 \ud150\uc11c\uac00 \uac31\uc2e0\ub418\uc5b4\uc57c \ud558\ub294\uc9c0\ub97c \uc54c\ub824\uc90d\ub2c8\ub2e4. learning_rate = 1e-3 optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate) for t in range(2000): # \uc21c\uc804\ud30c \ub2e8\uacc4: \ubaa8\ub378\uc5d0 x\ub97c \uc804\ub2ec\ud558\uc5ec \uc608\uce21\uac12 y\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. y_pred = model(xx) # \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uace0 \ucd9c\ub825\ud569\ub2c8\ub2e4. loss = loss_fn(y_pred, y) if t % 100 == 99: print(t, loss.item()) # \uc5ed\uc804\ud30c \ub2e8\uacc4 \uc804\uc5d0, optimizer \uac1d\uccb4\ub97c \uc0ac\uc6a9\ud558\uc5ec (\ubaa8\ub378\uc758 \ud559\uc2b5 \uac00\ub2a5\ud55c \uac00\uc911\uce58\uc778) \uac31\uc2e0\ud560 # \ubcc0\uc218\ub4e4\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \ubcc0\ud654\ub3c4(gradient)\ub97c 0\uc73c\ub85c \ub9cc\ub4ed\ub2c8\ub2e4. \uc774\ub807\uac8c \ud558\ub294 \uc774\uc720\ub294 \uae30\ubcf8\uc801\uc73c\ub85c # .backward()\ub97c \ud638\ucd9c\ud560 \ub54c\ub9c8\ub2e4 \ubcc0\ud654\ub3c4\uac00 \ubc84\ud37c(buffer)\uc5d0 (\ub36e\uc5b4\uc4f0\uc9c0 \uc54a\uace0) \ub204\uc801\ub418\uae30 # \ub54c\ubb38\uc785\ub2c8\ub2e4. \ub354 \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 torch.autograd.backward\uc5d0 \ub300\ud55c \ubb38\uc11c\ub97c \ucc38\uc870\ud558\uc138\uc694. optimizer.zero_grad() # \uc5ed\uc804\ud30c \ub2e8\uacc4: \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218\ub4e4\uc5d0 \ub300\ud55c \uc190\uc2e4\uc758 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. loss.backward() # optimizer\uc758 step \ud568\uc218\ub97c \ud638\ucd9c\ud558\uba74 \ub9e4\uac1c\ubcc0\uc218\uac00 \uac31\uc2e0\ub429\ub2c8\ub2e4. optimizer.step() linear_layer = model[0] print(f\u0027Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3\u0027) PyTorch: \uc0ac\uc6a9\uc790 \uc815\uc758 nn \ubaa8\ub4c8# \ub54c\ub300\ub85c \uae30\uc874 Module\uc758 \uad6c\uc131(sequence)\ubcf4\ub2e4 \ub354 \ubcf5\uc7a1\ud55c \ubaa8\ub378\uc744 \uad6c\uc131\ud574\uc57c \ud560 \ub54c\uac00 \uc788\uc2b5\ub2c8\ub2e4; \uc774\ub7ec\ud55c \uacbd\uc6b0\uc5d0\ub294 nn.Module \uc758 \ud558\uc704 \ud074\ub798\uc2a4(subclass)\ub85c \uc0c8\ub85c\uc6b4 Module\uc744 \uc815\uc758\ud558\uace0, \uc785\ub825 \ud150\uc11c\ub97c \ubc1b\uc544 \ub2e4\ub978 \ubaa8\ub4c8 \ubc0f autograd \uc5f0\uc0b0\uc744 \uc0ac\uc6a9\ud558\uc5ec \ucd9c\ub825 \ud150\uc11c\ub97c \ub9cc\ub4dc\ub294 forward \ub97c \uc815\uc758\ud569\ub2c8\ub2e4. \uc774 \uc608\uc81c\uc5d0\uc11c\ub294 3\ucc28 \ub2e4\ud56d\uc2dd\uc744 \uc0ac\uc6a9\uc790 \uc815\uc758 Module \ud558\uc704\ud074\ub798\uc2a4(subclass)\ub85c \uad6c\ud604\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4: # -*- coding: utf-8 -*- import torch import math class Polynomial3(torch.nn.Module): def __init__(self): \"\"\" \uc0dd\uc131\uc790\uc5d0\uc11c 4\uac1c\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc0dd\uc131(instantiate)\ud558\uace0, \uba64\ubc84 \ubcc0\uc218\ub85c \uc9c0\uc815\ud569\ub2c8\ub2e4. \"\"\" super().__init__() self.a = torch.nn.Parameter(torch.randn(())) self.b = torch.nn.Parameter(torch.randn(())) self.c = torch.nn.Parameter(torch.randn(())) self.d = torch.nn.Parameter(torch.randn(())) def forward(self, x): \"\"\" \uc21c\uc804\ud30c \ud568\uc218\uc5d0\uc11c\ub294 \uc785\ub825 \ub370\uc774\ud130\uc758 \ud150\uc11c\ub97c \ubc1b\uace0 \ucd9c\ub825 \ub370\uc774\ud130\uc758 \ud150\uc11c\ub97c \ubc18\ud658\ud574\uc57c \ud569\ub2c8\ub2e4. \ud150\uc11c\ub4e4 \uac04\uc758 \uc784\uc758\uc758 \uc5f0\uc0b0\ubfd0\ub9cc \uc544\ub2c8\ub77c, \uc0dd\uc131\uc790\uc5d0\uc11c \uc815\uc758\ud55c Module\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \"\"\" return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3 def string(self): \"\"\" Python\uc758 \ub2e4\ub978 \ud074\ub798\uc2a4(class)\ucc98\ub7fc, PyTorch \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud574\uc11c \uc0ac\uc6a9\uc790 \uc815\uc758 \uba54\uc18c\ub4dc\ub97c \uc815\uc758\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \"\"\" return f\u0027y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3\u0027 # \uc785\ub825\uac12\uacfc \ucd9c\ub825\uac12\uc744 \uac16\ub294 \ud150\uc11c\ub4e4\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. x = torch.linspace(-math.pi, math.pi, 2000) y = torch.sin(x) # \uc704\uc5d0\uc11c \uc815\uc758\ud55c \ud074\ub798\uc2a4\ub85c \ubaa8\ub378\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. model = Polynomial3() # \uc190\uc2e4 \ud568\uc218\uc640 optimizer\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. SGD \uc0dd\uc131\uc790\uc5d0 model.paramaters()\ub97c \ud638\ucd9c\ud574\uc8fc\uba74 # \ubaa8\ub378\uc758 \uba64\ubc84 \ud559\uc2b5 \uac00\ub2a5\ud55c (torch.nn.Parameter\ub85c \uc815\uc758\ub41c) \ub9e4\uac1c\ubcc0\uc218\ub4e4\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4. criterion = torch.nn.MSELoss(reduction=\u0027sum\u0027) optimizer = torch.optim.SGD(model.parameters(), lr=1e-6) for t in range(2000): # \uc21c\uc804\ud30c \ub2e8\uacc4: \ubaa8\ub378\uc5d0 x\ub97c \uc804\ub2ec\ud558\uc5ec \uc608\uce21\uac12 y\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. y_pred = model(x) # \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uace0 \ucd9c\ub825\ud569\ub2c8\ub2e4. loss = criterion(y_pred, y) if t % 100 == 99: print(t, loss.item()) # \ubcc0\ud654\ub3c4\ub97c 0\uc73c\ub85c \ub9cc\ub4e4\uace0, \uc5ed\uc804\ud30c \ub2e8\uacc4\ub97c \uc218\ud589\ud558\uace0, \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud569\ub2c8\ub2e4. optimizer.zero_grad() loss.backward() optimizer.step() print(f\u0027Result: {model.string()}\u0027) PyTorch: \uc81c\uc5b4 \ud750\ub984(Control Flow) + \uac00\uc911\uce58 \uacf5\uc720(Weight Sharing)# \ub3d9\uc801 \uadf8\ub798\ud504\uc640 \uac00\uc911\uce58 \uacf5\uc720\uc758 \uc608\ub97c \ubcf4\uc774\uae30 \uc704\ud574, \ub9e4\uc6b0 \uc774\uc0c1\ud55c \ubaa8\ub378\uc744 \uad6c\ud604\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4: \uac01 \uc21c\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c 3 ~ 5 \uc0ac\uc774\uc758 \uc784\uc758\uc758 \uc22b\uc790(random number)\ub97c \uc120\ud0dd\ud558\uc5ec \ub2e4\ucc28\ud56d\ub4e4\uc5d0\uc11c \uc0ac\uc6a9\ud558\uace0, \ub3d9\uc77c\ud55c \uac00\uc911\uce58\ub97c \uc5ec\ub7ec\ubc88 \uc7ac\uc0ac\uc6a9\ud558\uc5ec 4\ucc28\ud56d\uacfc 5\ucc28\ud56d\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4. \uc774 \ubaa8\ub378\uc5d0\uc11c\ub294 \uc77c\ubc18\uc801\uc778 Python \uc81c\uc5b4 \ud750\ub984\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubc18\ubcf5(loop)\uc744 \uad6c\ud604\ud560 \uc218 \uc788\uc73c\uba70, \uc21c\uc804\ud30c \ub2e8\uacc4\ub97c \uc815\uc758\ud560 \ub54c \ub3d9\uc77c\ud55c \ub9e4\uac1c\ubcc0\uc218\ub97c \uc5ec\ub7ec\ubc88 \uc7ac\uc0ac\uc6a9\ud558\uc5ec \uac00\uc911\uce58 \uacf5\uc720\ub97c \uad6c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubaa8\ub378\uc744 Module\uc744 \uc0c1\uc18d\ubc1b\ub294 \ud558\uc704\ud074\ub798\uc2a4\ub85c \uac04\ub2e8\ud788 \uad6c\ud604\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4: # -*- coding: utf-8 -*- import random import torch import math class DynamicNet(torch.nn.Module): def __init__(self): \"\"\" \uc0dd\uc131\uc790\uc5d0\uc11c 5\uac1c\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc0dd\uc131(instantiate)\ud558\uace0 \uba64\ubc84 \ubcc0\uc218\ub85c \uc9c0\uc815\ud569\ub2c8\ub2e4. \"\"\" super().__init__() self.a = torch.nn.Parameter(torch.randn(())) self.b = torch.nn.Parameter(torch.randn(())) self.c = torch.nn.Parameter(torch.randn(())) self.d = torch.nn.Parameter(torch.randn(())) self.e = torch.nn.Parameter(torch.randn(())) def forward(self, x): \"\"\" \ubaa8\ub378\uc758 \uc21c\uc804\ud30c \ub2e8\uacc4\uc5d0\uc11c\ub294 \ubb34\uc791\uc704\ub85c 4, 5 \uc911 \ud558\ub098\ub97c \uc120\ud0dd\ud55c \ub4a4 \ub9e4\uac1c\ubcc0\uc218 e\ub97c \uc7ac\uc0ac\uc6a9\ud558\uc5ec \uc774 \ucc28\uc218\ub4e4\uc758\uc758 \uae30\uc5ec\ub3c4(contribution)\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. \uac01 \uc21c\uc804\ud30c \ub2e8\uacc4\ub294 \ub3d9\uc801 \uc5f0\uc0b0 \uadf8\ub798\ud504\ub97c \uad6c\uc131\ud558\uae30 \ub54c\ubb38\uc5d0, \ubaa8\ub378\uc758 \uc21c\uc804\ud30c \ub2e8\uacc4\ub97c \uc815\uc758\ud560 \ub54c \ubc18\ubcf5\ubb38\uc774\ub098 \uc870\uac74\ubb38\uacfc \uac19\uc740 \uc77c\ubc18\uc801\uc778 Python \uc81c\uc5b4-\ud750\ub984 \uc5f0\uc0b0\uc790\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\uc11c \uc5f0\uc0b0 \uadf8\ub798\ud504\ub97c \uc815\uc758\ud560 \ub54c \ub3d9\uc77c\ud55c \ub9e4\uac1c\ubcc0\uc218\ub97c \uc5ec\ub7ec\ubc88 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc644\ubcbd\ud788 \uc548\uc804\ud558\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \"\"\" y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3 for exp in range(4, random.randint(4, 6)): y = y + self.e * x ** exp return y def string(self): \"\"\" Python\uc758 \ub2e4\ub978 \ud074\ub798\uc2a4(class)\ucc98\ub7fc, PyTorch \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud574\uc11c \uc0ac\uc6a9\uc790 \uc815\uc758 \uba54\uc18c\ub4dc\ub97c \uc815\uc758\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \"\"\" return f\u0027y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?\u0027 # \uc785\ub825\uac12\uacfc \ucd9c\ub825\uac12\uc744 \uac16\ub294 \ud150\uc11c\ub4e4\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. x = torch.linspace(-math.pi, math.pi, 2000) y = torch.sin(x) # \uc704\uc5d0\uc11c \uc815\uc758\ud55c \ud074\ub798\uc2a4\ub85c \ubaa8\ub378\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. model = DynamicNet() # \uc190\uc2e4 \ud568\uc218\uc640 optimizer\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774 \uc774\uc0c1\ud55c \ubaa8\ub378\uc744 \uc21c\uc218\ud55c \ud655\ub960\uc801 \uacbd\uc0ac\ud558\uac15\ubc95(SGD; Stochastic Gradient Descent)\uc73c\ub85c # \ud559\uc2b5\ud558\ub294 \uac83\uc740 \uc5b4\ub824\uc6b0\ubbc0\ub85c, \ubaa8\uba58\ud140(momentum)\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. criterion = torch.nn.MSELoss(reduction=\u0027sum\u0027) optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9) for t in range(30000): # \uc21c\uc804\ud30c \ub2e8\uacc4: \ubaa8\ub378\uc5d0 x\ub97c \uc804\ub2ec\ud558\uc5ec \uc608\uce21\uac12 y\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. y_pred = model(x) # \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uace0 \ucd9c\ub825\ud569\ub2c8\ub2e4. loss = criterion(y_pred, y) if t % 2000 == 1999: print(t, loss.item()) # \ubcc0\ud654\ub3c4\ub97c 0\uc73c\ub85c \ub9cc\ub4e4\uace0, \uc5ed\uc804\ud30c \ub2e8\uacc4\ub97c \uc218\ud589\ud558\uace0, \uac00\uc911\uce58\ub97c \uac31\uc2e0\ud569\ub2c8\ub2e4. optimizer.zero_grad() loss.backward() optimizer.step() print(f\u0027Result: {model.string()}\u0027) \uc608\uc81c \ucf54\ub4dc# \uc704\uc758 \uc608\uc81c\ub4e4\uc744 \uc5ec\uae30\uc11c \ucc3e\uc544\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. Tensors# Autograd# nn \ubaa8\ub4c8#",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/beginner/pytorch_with_examples.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>