
<!DOCTYPE html>


<html lang="ko" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="Pytorch Tensor 소개" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/beginner/introyt/tensors_deeper_tutorial.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Introduction|| Tensors|| Autograd|| Building Models|| TensorBoard Support|| Training Models|| Model Understanding 번역: 이상윤 아래 영상이나 youtube 를 참고하세요. Tensor는 PyTorch에서 중요한 추상 데이터 자료형입니다. 이 interactive notebook은 torch.Tensor 클래스에 대한 심층적인 소개를 제공합니다. 먼저 가장 중요한 것은 PyTorch 모듈을 import 하는 것입니다. 또한 몇 가지 예제에..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="Introduction|| Tensors|| Autograd|| Building Models|| TensorBoard Support|| Training Models|| Model Understanding 번역: 이상윤 아래 영상이나 youtube 를 참고하세요. Tensor는 PyTorch에서 중요한 추상 데이터 자료형입니다. 이 interactive notebook은 torch.Tensor 클래스에 대한 심층적인 소개를 제공합니다. 먼저 가장 중요한 것은 PyTorch 모듈을 import 하는 것입니다. 또한 몇 가지 예제에..." />
<meta property="og:ignore_canonical" content="true" />

    <title>Pytorch Tensor 소개 &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../../_static/doctools.js?v=92e14aea"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/translations.js?v=b5f768d8"></script>
    <script src="../../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'beginner/introyt/tensors_deeper_tutorial';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/beginner/introyt/tensors_deeper_tutorial.html" />
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../../genindex.html" />
    <link rel="search" title="검색" href="../../search.html" />
    <link rel="next" title="The Fundamentals of Autograd" href="autogradyt_tutorial.html" />
    <link rel="prev" title="PyTorch 소개" href="introyt1_tutorial.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../_static/js/theme.js"></script>
<script type="text/javascript" src="../../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with PyTorch</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../basics/intro.html">파이토치(PyTorch) 기본 익히기</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="introyt_index.html">PyTorch 소개 - YouTube 시리즈</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="introyt1_tutorial.html">PyTorch 소개</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Pytorch Tensor 소개</a></li>
<li class="toctree-l2"><a class="reference internal" href="autogradyt_tutorial.html">The Fundamentals of Autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="modelsyt_tutorial.html">Building Models with PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorboardyt_tutorial.html">PyTorch TensorBoard 지원</a></li>
<li class="toctree-l2"><a class="reference internal" href="trainingyt.html">Training with PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="captumyt.html">Model Understanding with Captum</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../deep_learning_60min_blitz.html">PyTorch로 딥러닝하기: 60분만에 끝장내기</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../blitz/tensor_tutorial.html">텐서(Tensor)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blitz/autograd_tutorial.html"><code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code> 에 대한 간단한 소개</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blitz/neural_networks_tutorial.html">신경망(Neural Networks)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blitz/cifar10_tutorial.html">분류기(Classifier) 학습하기</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch_with_examples.html">예제로 배우는 파이토치(PyTorch)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples_tensor/polynomial_numpy.html">준비 운동: NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples_tensor/polynomial_tensor.html">파이토치(PyTorch): 텐서(Tensor)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples_autograd/polynomial_autograd.html">PyTorch: 텐서(Tensor)와 autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples_autograd/polynomial_custom_function.html">PyTorch: 새 autograd Function 정의하기</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples_nn/polynomial_nn.html">PyTorch: nn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples_nn/polynomial_optim.html">PyTorch: optim</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples_nn/polynomial_module.html">PyTorch: 사용자 정의 nn.Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples_nn/dynamic_net.html">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../nn_tutorial.html"><cite>torch.nn</cite> 이 <em>실제로</em> 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../understanding_leaf_vs_nonleaf_tutorial.html">Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/nlp_from_scratch_index.html">NLP from Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tensorboard_tutorial.html">TensorBoard로 모델, 데이터, 학습 시각화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/pinmem_nonblock.html">A guide on good usage of <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> and <code class="docutils literal notranslate"><span class="pre">pin_memory()</span></code> in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/visualizing_gradients_tutorial.html">Visualizing Gradients</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../intro.html" class="nav-link">Intro</a></li>
    
    
    <li class="breadcrumb-item"><a href="introyt_index.html" class="nav-link">PyTorch 소개 - YouTube 시리즈</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Pytorch Tensor 소개</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../../intro.html">
        <meta itemprop="name" content="Intro">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="introyt_index.html">
        <meta itemprop="name" content="PyTorch 소개 - YouTube 시리즈">
        <meta itemprop="position" content="2">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Pytorch Tensor 소개">
        <meta itemprop="position" content="3">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">beginner/introyt/tensors_deeper_tutorial</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-beginner-introyt-tensors-deeper-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<p class="sphx-glr-example-title" id="sphx-glr-beginner-introyt-tensors-deeper-tutorial-py"><a class="reference external" href="introyt1_tutorial.html">Introduction</a> ||
<strong>Tensors</strong> ||
<a class="reference external" href="autogradyt_tutorial.html">Autograd</a> ||
<a class="reference external" href="modelsyt_tutorial.html">Building Models</a> ||
<a class="reference external" href="tensorboardyt_tutorial.html">TensorBoard Support</a> ||
<a class="reference external" href="trainingyt.html">Training Models</a> ||
<a class="reference external" href="captumyt.html">Model Understanding</a></p>
<section id="pytorch-tensor">
<h1>Pytorch Tensor 소개<a class="headerlink" href="#pytorch-tensor" title="Link to this heading">#</a></h1>
<p>번역: <a class="reference external" href="https://github.com/falconlee236">이상윤</a></p>
<p>아래 영상이나 <a class="reference external" href="https://www.youtube.com/watch?v=r7QDUPb2dCM">youtube</a> 를 참고하세요.</p>
<div style="margin-top:10px; margin-bottom:10px;">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/r7QDUPb2dCM" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div><p>Tensor는 PyTorch에서 중요한 추상 데이터 자료형입니다. 이 interactive
notebook은 <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> 클래스에 대한 심층적인 소개를 제공합니다.</p>
<p>먼저 가장 중요한 것은 PyTorch 모듈을 import 하는 것입니다. 또한 몇 가지
예제에 사용할 math 모듈도 import 합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
</pre></div>
</div>
<section id="tensor">
<h2>Tensor 생성하기<a class="headerlink" href="#tensor" title="Link to this heading">#</a></h2>
<p>tensor를 생성하는 가장 간단한 방법은 <code class="docutils literal notranslate"><span class="pre">torch.empty()</span></code> 를 호출하는 것입니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;torch.Tensor&#39;&gt;
tensor([[-1.0616e-12,  4.5577e-41, -6.4935e-15,  3.4088e-41],
        [-6.4448e-15,  3.4088e-41, -1.3217e+00,  3.4088e-41],
        [-1.0616e-12,  4.5577e-41, -1.0617e-12,  4.5577e-41]])
</pre></div>
</div>
<p>방금 무엇을 한 것인지 들여다봅시다:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch</span></code> 모듈에 있는 수많은 메소드 중 하나를 사용해서 tensor를 만들었습니다.</p></li>
<li><p>이 tensor는 3개의 행과 4개의 열을 가진 2차원 tensor입니다.</p></li>
<li><p>객체가 반환한 type은 <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> 이며 이는 <code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> 의 별칭입니다.
기본적으로 PyTorch tensor는 32-bit 부동 소수점 표현 실수로 채워 집니다.
(아래에서 더 많은 데이터 자료형을 소개합니다)</p></li>
<li><p>생성한 tensor를 출력하면 아마 무작위 값을 볼 수 있을 것 입니다.
<code class="docutils literal notranslate"><span class="pre">torch.empty()</span></code> 는 tensor를 위한 메모리를 할당해 주지만 임의의 값으로 초기화하지는 않습니다
- 그렇기 때문에 할당 당시에 메모리가 가지고 있는 값을 보는 것입니다.</p></li>
</ul>
<p>간략하게 tensor와 tensor의 차원 수, 그리고 각 tensor의 용어에 대해 알아봅시다:</p>
<ul class="simple">
<li><p>때로는 1차원 tensor를 보게 될 것인데 이는 <em>vector</em> 라고 합니다.</p></li>
<li><p>이와 마찬가지로 2차원 tensor는 주로 <em>matrix</em> 라고 합니다.</p></li>
<li><p>2차원보다 큰 차원을 가진 것들은 일반적으로 그냥 tensor라고 합니다.</p></li>
</ul>
<p>코드를 작성하며 주로 tensor를 몇 가지 값으로 초기화하고 싶을 수가 있습니다.
일반적인 경우로는 모두 0으로 초기화하거나, 모두 1로 초기화하거나,
혹은 모두 무작위 값으로 초기화 할 때가 있는데,
<code class="docutils literal notranslate"><span class="pre">torch</span></code> 모듈은 이러한 모든 경우에 대한 메소드를 제공합니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">zeros</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">zeros</span><span class="p">)</span>

<span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ones</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1729</span><span class="p">)</span>
<span class="n">random</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[0.3126, 0.3791, 0.3087],
        [0.0736, 0.4216, 0.0691]])
</pre></div>
</div>
<p>이 모든 팩토리 메소드들은 우리가 기대하던 것들을 모두 수행합니다 - 0으로 모두 채워 진 tensor,
1로 모두 채워 진 tensor 그리고 0과 1사이의 무작위 값으로 채워 진 tensor를 얻었습니다.</p>
<section id="tensor-seed">
<h3>무작위 Tensor와 Seed 사용하기<a class="headerlink" href="#tensor-seed" title="Link to this heading">#</a></h3>
<p>무작위 tensor에 대해 말하자면, 바로 앞에서 호출하는 <code class="docutils literal notranslate"><span class="pre">torch.manual_seed()</span></code> 를 눈치채셨나요?
특히 연구 환경에서 연구 결과의 재현 가능성에 대한 확신을 얻고 싶을 때,
모델의 학습 가중치와 같은 무작위 값을 가진 tensor로 초기화 하는 것은 흔하거나 종종 일어나는 일입니다.
직접 무작위 난수 생성기의 seed를 설정하는 것이 다음 방법입니다. 다음 코드를 보며 더 자세히 알아봅시다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1729</span><span class="p">)</span>
<span class="n">random1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random1</span><span class="p">)</span>

<span class="n">random2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random2</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1729</span><span class="p">)</span>
<span class="n">random3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random3</span><span class="p">)</span>

<span class="n">random4</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random4</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0.3126, 0.3791, 0.3087],
        [0.0736, 0.4216, 0.0691]])
tensor([[0.2332, 0.4047, 0.2162],
        [0.9927, 0.4128, 0.5938]])
tensor([[0.3126, 0.3791, 0.3087],
        [0.0736, 0.4216, 0.0691]])
tensor([[0.2332, 0.4047, 0.2162],
        [0.9927, 0.4128, 0.5938]])
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">random1</span></code> 과 <code class="docutils literal notranslate"><span class="pre">random3</span></code> 그리고 <code class="docutils literal notranslate"><span class="pre">random2</span></code> 과 <code class="docutils literal notranslate"><span class="pre">random4</span></code> ,
이 각각 서로 동일한 결과가 나온다는 것을 볼 수 있습니다.
무작위 난수 생성기의 seed를 수동으로 설정하면 난수가 재 설정되어 대부분의 환경에서
무작위 숫자에 의존하는 동일한 계산이 이루어지고 동일한 결과를 제공합니다.</p>
<p>보다 자세한 정보는 다음 문서를 참고하세요 <a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html">PyTorch documentation on
reproducibility</a>.</p>
</section>
<section id="tensor-shape">
<h3>Tensor의 shape<a class="headerlink" href="#tensor-shape" title="Link to this heading">#</a></h3>
<p>두 개 혹은 그 이상의 tensor에 대한 연산을 수행할 때, tensor들은 같은 shape를 필요로 합니다
- 다시 말해서 차원의 개수가 같아야 하고, 각 차원마다 원소의 수가 같아야 합니다.
그러기 위해서는 <code class="docutils literal notranslate"><span class="pre">torch.*_like()</span></code> 함수를 사용합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">empty_like_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">empty_like_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">empty_like_x</span><span class="p">)</span>

<span class="n">zeros_like_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">zeros_like_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">zeros_like_x</span><span class="p">)</span>

<span class="n">ones_like_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ones_like_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ones_like_x</span><span class="p">)</span>

<span class="n">rand_like_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rand_like_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rand_like_x</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 2, 3])
tensor([[[-9.8661e-01,  3.4088e-41,  4.5167e+24],
         [ 4.5580e-41,  8.9683e-44,  0.0000e+00]],

        [[ 1.1210e-43,  0.0000e+00,  6.1904e+09],
         [ 3.4084e-41,  0.0000e+00,  1.3881e+00]]])
torch.Size([2, 2, 3])
tensor([[[-1.3879e+00,  3.4088e-41, -1.8240e+00],
         [ 3.4088e-41, -2.0000e+00,  1.6543e+00]],

        [[ 0.0000e+00,  1.3972e+00,  2.0000e+00],
         [ 1.7108e+00,  0.0000e+00,  1.3881e+00]]])
torch.Size([2, 2, 3])
tensor([[[0., 0., 0.],
         [0., 0., 0.]],

        [[0., 0., 0.],
         [0., 0., 0.]]])
torch.Size([2, 2, 3])
tensor([[[1., 1., 1.],
         [1., 1., 1.]],

        [[1., 1., 1.],
         [1., 1., 1.]]])
torch.Size([2, 2, 3])
tensor([[[0.6128, 0.1519, 0.0453],
         [0.5035, 0.9978, 0.3884]],

        [[0.6929, 0.1703, 0.1384],
         [0.4759, 0.7481, 0.0361]]])
</pre></div>
</div>
<p>위쪽의 코드 셀에 있는 것들 중에 첫 번째는 tensor에 있는 <code class="docutils literal notranslate"><span class="pre">.shape</span></code> 속성을 사용했습니다.
이 속성은 tensor의 각 차원 크기에 대한 리스트를 포함합니다
- 이 경우에, <code class="docutils literal notranslate"><span class="pre">x</span></code> 는 shape가 2 x 2 x 3 인 3차원 tensor입니다.</p>
<p>그 아래에는 <code class="docutils literal notranslate"><span class="pre">.empty_like()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zeros_like()</span></code>,
<code class="docutils literal notranslate"><span class="pre">.ones_like()</span></code>, and <code class="docutils literal notranslate"><span class="pre">.rand_like()</span></code> 메소드를 호출 합니다.
<code class="docutils literal notranslate"><span class="pre">.shape</span></code> 속성을 통해서, 위의 메소드들이 동일한 차원값을 반환한다는 것을 검증할 수 있습니다.</p>
<p>여기서 다루는 tensor를 생성하는 마지막 방법은 PyTorch collection
형식의 데이터를 직접적으로 명시하는 것 입니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">some_constants</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">3.1415926</span><span class="p">,</span> <span class="mf">2.71828</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.61803</span><span class="p">,</span> <span class="mf">0.0072897</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">some_constants</span><span class="p">)</span>

<span class="n">some_integers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">19</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">some_integers</span><span class="p">)</span>

<span class="n">more_integers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">more_integers</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[3.1416, 2.7183],
        [1.6180, 0.0073]])
tensor([ 2,  3,  5,  7, 11, 13, 17, 19])
tensor([[2, 4, 6],
        [3, 6, 9]])
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch.tensor()</span></code> 는 이미 Python tuple이나 list 형태로 이루어진 데이터를
가지고 있는 경우 tensor를 만들기 가장 쉬운 방법입니다.
위에서 본 것 처럼 중첩된 형태의 collection 자료형은 다차원 tensor가 결과로 나옵니다.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.tensor()</span></code> 는 데이터의 복사본을 생성합니다.</p>
</div>
</section>
<section id="id2">
<h3>Tensor 자료형<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>tensor의 자료형을 설정하는 것은 다양한 방식이 가능합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">*</span> <span class="mf">20.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[1, 1, 1],
        [1, 1, 1]], dtype=torch.int16)
tensor([[ 0.9956,  1.4148,  5.8364],
        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)
tensor([[ 0,  1,  5],
        [11, 11, 11]], dtype=torch.int32)
</pre></div>
</div>
<p>tensor의 자료형을 설정하는 가장 단순한 방식은 생성할 때 선택적 인자를 사용하는 것 입니다.
위에 있는 cell의 첫 번째 줄을 보면, tensor <code class="docutils literal notranslate"><span class="pre">a</span></code> 를
<code class="docutils literal notranslate"><span class="pre">dtype=torch.int16</span></code> 자료형으로 설정했습니다. <code class="docutils literal notranslate"><span class="pre">a</span></code> 를 출력할 때,
<code class="docutils literal notranslate"><span class="pre">1.</span></code> 대신에 <code class="docutils literal notranslate"><span class="pre">1</span></code> 로 가득 찬 모습을 볼 수 있습니다
- 파이썬에서 아래 점이 없으면 실수 자료형이 아닌 정수 자료형을 의미합니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">a</span></code> 를 출력할 때 또 한가지 주목할 점은,
<code class="docutils literal notranslate"><span class="pre">dtype</span></code> 을 기본값 (32-bit 부동 소수점)
으로 남길 때와 다르게 tensor를 출력하는 경우
각 tensor의 <code class="docutils literal notranslate"><span class="pre">dtype</span></code> 을 명시한다는 것입니다.</p>
<p>tensor의 shape를 정수형 인자의 나열, 즉 이 인자를 tuple 자료형 형태로
묶는다는 것을 발견할 수 있습니다. 이것은 반드시 필요한 것은 아닙니다
- PyTorch에서는 첫 번째 인자로 tensor shape라는 값을 의미하는 라벨이 없는 정수 인자를 여러개를 받습니다 -
하지만 선택 인자를 추가했을 때, 이 방식은 코드를 더 읽기 쉽게 만들 수 있습니다.</p>
<p>자료형을 설정하는 다른 방법은 <code class="docutils literal notranslate"><span class="pre">.to()</span></code> 메소드랑 함께 사용하는 것 입니다.
위쪽 셀에서 평범한 방식으로 무작위 실수 자료형 tensor <code class="docutils literal notranslate"><span class="pre">b</span></code> 를 생성합니다.
이어서 <code class="docutils literal notranslate"><span class="pre">.to()</span></code> 메소드를 사용해서 <code class="docutils literal notranslate"><span class="pre">b</span></code> 를 32-bit 정수 자료형으로 변환한 <code class="docutils literal notranslate"><span class="pre">c</span></code> 를 생성합니다.
<code class="docutils literal notranslate"><span class="pre">c</span></code> 는 모든 <code class="docutils literal notranslate"><span class="pre">b</span></code> 의 값과 같은 값을 가지고 있지만 소수점 아래 자리를 버린다는 점이 다릅니다.</p>
<p>가능한 데이터 자료형은 다음을 포함합니다:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.bool</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.int8</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.uint8</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.int16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.int32</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.int64</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.half</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.float</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.double</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.bfloat</span></code></p></li>
</ul>
</section>
</section>
<section id="id3">
<h2>PyTorch Tensor에서 산술 &amp; 논리 연산<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>지금까지 tensor를 생성하는 몇 가지 방식을 알아봤습니다…
이것을 가지고 무엇을 할 수 있을까요?</p>
<p>먼저 기본적인 산술 연산을 알아보고,
그 다음 tensor가 단순 스칼라 값과 어떻게 상호작용 하는지 알아봅시다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">twos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">threes</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">fours</span> <span class="o">=</span> <span class="n">twos</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">sqrt2s</span> <span class="o">=</span> <span class="n">twos</span> <span class="o">**</span> <span class="mf">0.5</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ones</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">twos</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">threes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fours</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sqrt2s</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[1., 1.],
        [1., 1.]])
tensor([[2., 2.],
        [2., 2.]])
tensor([[3., 3.],
        [3., 3.]])
tensor([[4., 4.],
        [4., 4.]])
tensor([[1.4142, 1.4142],
        [1.4142, 1.4142]])
</pre></div>
</div>
<p>위에서 볼 수 있듯이 tensor들과 스칼라 값 사이 산술연산,
예를 들면 덧셈, 뺄셈, 곱셈, 나눗셈 그리고 거듭제곱은
tensor의 각 원소에 나눠서 계산을 합니다.
이러한 연산의 결과는 tensor가 될 것이기 때문에,
<code class="docutils literal notranslate"><span class="pre">threes</span></code> 변수를 생성하는 줄에서 처럼
일반적인 연산자 우선순위 규칙과 함께 연산자를 연결할 수 있습니다.</p>
<p>두 tensor 사이 유사한 연산도 직관적으로 예상할 수 있는 방식으로 동작합니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">powers2</span> <span class="o">=</span> <span class="n">twos</span> <span class="o">**</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">powers2</span><span class="p">)</span>

<span class="n">fives</span> <span class="o">=</span> <span class="n">ones</span> <span class="o">+</span> <span class="n">fours</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fives</span><span class="p">)</span>

<span class="n">dozens</span> <span class="o">=</span> <span class="n">threes</span> <span class="o">*</span> <span class="n">fours</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dozens</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[ 2.,  4.],
        [ 8., 16.]])
tensor([[5., 5.],
        [5., 5.]])
tensor([[12., 12.],
        [12., 12.]])
</pre></div>
</div>
<p>여기서 주목해야 할 점은 이전 코드 cell에 있는 모든 tensor는 동일한 shape를 가져야 한다는 것 입니다.
만약 서로 다른 shape를 가진 tensor끼리 이진 연산을 수행한다면 무슨 일이 일어날까요?</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>다음 cell은 run-time error가 발생합니다. 이것은 의도한 것입니다.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nv">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.rand<span class="o">(</span><span class="m">2</span>,<span class="w"> </span><span class="m">3</span><span class="o">)</span>
<span class="nv">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.rand<span class="o">(</span><span class="m">3</span>,<span class="w"> </span><span class="m">2</span><span class="o">)</span>

print<span class="o">(</span>a<span class="w"> </span>*<span class="w"> </span>b<span class="o">)</span>
</pre></div>
</div>
</div>
<p>일반적인 경우에, 다른 shape의 tensor를 이러한 방식으로 연산할 수 없습니다.
심지어 위에 있는 cell에 있는 경우처럼 tensor가 서로 같은 개수의 원소를 가지고 있는 경우에도 연산할 수 없습니다.</p>
<section id="tensor-broadcasting">
<h3>개요: Tensor Broadcasting<a class="headerlink" href="#tensor-broadcasting" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>만약 NumPy의 ndarrays에서 사용하는 broadcasting 문법에 익숙하다면,
여기서도 같은 규칙이 적용된다는 것을 확인할 수 있습니다.</p>
</div>
<p>tensor는 같은 shape끼리만 연산이 가능하다는 규칙의 예외가 바로 <em>tensor broadcasting</em> 입니다.
다음은 그 예시입니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">doubled</span> <span class="o">=</span> <span class="n">rand</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">rand</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">doubled</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0.6146, 0.5999, 0.5013, 0.9397],
        [0.8656, 0.5207, 0.6865, 0.3614]])
tensor([[1.2291, 1.1998, 1.0026, 1.8793],
        [1.7312, 1.0413, 1.3730, 0.7228]])
</pre></div>
</div>
<p>여기서 무슨 트릭이 사용되고 있는 것일까요?
어떻게 2x4 tensor에 1x4 tensor를 곱한 값을 얻을 수 있을까요?</p>
<p>Broadcasting은 서로 비슷한 shape를 가진 tensor사이 연산을 수행하는 방법입니다.
위의 예시를 보면, 행의 값이 1이고, 열의 값이 4인 tensor가
행의 값이 2이고, 열의 값이 4인 tensor의 <em>모든 행</em> 에 곱하게 됩니다.</p>
<p>이것은 딥러닝에서 중요한 연산입니다.
일반적인 예시는 학습 가중치 tensor에 입력 tensor의 <em>배치</em> 를 곱하고,
배치의 각 인스턴스에 곱하기 연산을 개별적으로 적용한 이후
위의 (2, 4) * (1, 4) tensor연산의 결과가 (2, 4) shape tensor인 것처럼 -
동일한 shape의 학습 가중치 tensor를 반환하는 것입니다.</p>
<p>Broadcasting의 규칙은 다음과 같습니다:</p>
<ul class="simple">
<li><p>각 tensor는 최소한 1차원 이상을 반드시 가지고 있어야 합니다 - 빈 tensor는 사용할 수 없습니다.</p></li>
<li><p>두 tensor의 각 차원 크기 원소가 다음 조건을 만족하는지 확인하며 비교합니다. <em>이때 비교 순서는 맨 뒤에서부터 맨 앞으로 입니다;</em></p>
<ul>
<li><p>각 차원이 서로 동일합니다, <em>또는</em></p></li>
<li><p>각 차원중의 하나의 크기가 반드시 1입니다, <em>또는</em></p></li>
<li><p>tensor들 중 하나의 차원이 존재하지 않습니다.</p></li>
</ul>
</li>
</ul>
<p>이전에 봤던 것처럼,
물론 동일한 shape를 가진 Tensor들은 자명하게 “broadcastable” 합니다.</p>
<p>다음 예제는 위의 규칙을 준수하고
broadcasting을 허용하는 몇 가지 상황입니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span>     <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>   <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># 세번째와 두번째 차원이 a랑 동일하고, 첫번째 차원은 존재하지 않습니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>   <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># 세번째 차원 = 1이고, 두번째 차원은 a랑 동일합니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>   <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># 세번째 차원이 a랑 동일하고, 두번째 차원 = 1입니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[[0.6493, 0.2633],
         [0.4762, 0.0548],
         [0.2024, 0.5731]],

        [[0.6493, 0.2633],
         [0.4762, 0.0548],
         [0.2024, 0.5731]],

        [[0.6493, 0.2633],
         [0.4762, 0.0548],
         [0.2024, 0.5731]],

        [[0.6493, 0.2633],
         [0.4762, 0.0548],
         [0.2024, 0.5731]]])
tensor([[[0.7191, 0.7191],
         [0.4067, 0.4067],
         [0.7301, 0.7301]],

        [[0.7191, 0.7191],
         [0.4067, 0.4067],
         [0.7301, 0.7301]],

        [[0.7191, 0.7191],
         [0.4067, 0.4067],
         [0.7301, 0.7301]],

        [[0.7191, 0.7191],
         [0.4067, 0.4067],
         [0.7301, 0.7301]]])
tensor([[[0.6276, 0.7357],
         [0.6276, 0.7357],
         [0.6276, 0.7357]],

        [[0.6276, 0.7357],
         [0.6276, 0.7357],
         [0.6276, 0.7357]],

        [[0.6276, 0.7357],
         [0.6276, 0.7357],
         [0.6276, 0.7357]],

        [[0.6276, 0.7357],
         [0.6276, 0.7357],
         [0.6276, 0.7357]]])
</pre></div>
</div>
<p>위의 예시에 있는 각 tensor의 값을 자세히 살펴봅시다:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">b</span></code> 를 만드는 곱셈 연산은
<code class="docutils literal notranslate"><span class="pre">a</span></code> 의 모든 “계층” 에 broadcast 되었습니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">c</span></code> 에 대해서, 연산은 <code class="docutils literal notranslate"><span class="pre">a</span></code> 의 모든 계층과 행에 대해서 broadcast 되었습니다
- 모든 열은 3개의 원소값 모두 동일합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">d</span></code> 에 대해서, 연산이 이전과 반대로 모든 계층과 열에 대해서 수행합니다
- 이재 모든 <em>행</em> 이 동일합니다.</p></li>
</ul>
<p>broadcasting에 대한 더 많은 정보는, <a class="reference external" href="https://pytorch.org/docs/stable/notes/broadcasting.html">PyTorch
documentation</a>
에 있는 주제를 참고하세요.</p>
<p>다음 예시는 broadcasting 시도가 실패한 사례입니다:</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>다음 cell에서는 run-time error가 발생합니다. 이것은 의도한 것입니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span>     <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>    <span class="c1"># 차원은 반드시 맨 뒤 원소부터 맨 앞 원소로 차례대로 맞춰야 합니다.</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>   <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># 세번째와 두번째 차원 모두 서로 다릅니다.</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="p">))</span>   <span class="c1"># 비어있는 tensor는 broadcast 할 수 없습니다.</span>
</pre></div>
</div>
</div>
</section>
<section id="id4">
<h3>Tensor를 사용하는 다양한 연산들<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>PyTorch tensor는 tensor들 끼리 수행할 수 있는 300개 이상의
연산을 가지고 있습니다.</p>
<p>다음 작은 예시는 주로 사용하는 연산 종류 몇 개를 보여줍니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 공용 함수</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Common functions:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>

<span class="c1"># 삼각 함수와 그 역함수들</span>
<span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">sines</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>
<span class="n">inverses</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">asin</span><span class="p">(</span><span class="n">sines</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Sine and arcsine:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sines</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inverses</span><span class="p">)</span>

<span class="c1"># 비트 연산</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Bitwise XOR:&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bitwise_xor</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>

<span class="c1"># 비교 연산:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Broadcasted, element-wise equality comparison:&#39;</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]])</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># 많은 비교 연산자들은 broadcasting을 지원합니다!</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">))</span> <span class="c1"># bool 자료형을 가진 tensor를 반환합니다.</span>

<span class="c1"># 차원 감소 연산:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Reduction ops:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>        <span class="c1"># 단일 원소 tensor를 반환합니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span> <span class="c1"># 반환한 tensor로부터 값을 추출합니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>       <span class="c1"># 평균</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>        <span class="c1"># 표준 편차</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>       <span class="c1"># 모든 숫자의 곱</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])))</span> <span class="c1"># 중복되지 않은 값들을 걸러냅니다.</span>

<span class="c1"># 벡터와 선형 대수 연산</span>
<span class="n">v1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>         <span class="c1"># x축 단위 벡터</span>
<span class="n">v2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>         <span class="c1"># y축 단위 벡터</span>
<span class="n">m1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>                   <span class="c1"># 무작위 행렬</span>
<span class="n">m2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]])</span> <span class="c1"># 단위 행렬에 3을 곱한 결과</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Vectors &amp; Matrices:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cross</span><span class="p">(</span><span class="n">v2</span><span class="p">,</span> <span class="n">v1</span><span class="p">))</span> <span class="c1"># z축 단위 벡터의 음수값 (v1 x v2 == -v2 x v1)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m1</span><span class="p">)</span>
<span class="n">m3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m3</span><span class="p">)</span>                  <span class="c1"># m1 행렬을 3번 곱한 결과</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">m3</span><span class="p">))</span>       <span class="c1"># 특이값 분해</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Common functions:
tensor([[0.9238, 0.5724, 0.0791, 0.2629],
        [0.1986, 0.4439, 0.6434, 0.4776]])
tensor([[-0., -0., 1., -0.],
        [-0., 1., 1., -0.]])
tensor([[-1., -1.,  0., -1.],
        [-1.,  0.,  0., -1.]])
tensor([[-0.5000, -0.5000,  0.0791, -0.2629],
        [-0.1986,  0.4439,  0.5000, -0.4776]])

Sine and arcsine:
tensor([0.0000, 0.7854, 1.5708, 2.3562])
tensor([0.0000, 0.7071, 1.0000, 0.7071])
tensor([0.0000, 0.7854, 1.5708, 0.7854])

Bitwise XOR:
tensor([3, 2, 1])

Broadcasted, element-wise equality comparison:
tensor([[ True, False],
        [False, False]])

Reduction ops:
tensor(4.)
4.0
tensor(2.5000)
tensor(1.2910)
tensor(24.)
tensor([1, 2])

Vectors &amp; Matrices:
/workspace/tutorials-kr/beginner_source/introyt/tensors_deeper_tutorial.py:443: UserWarning:

Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /pytorch/aten/src/ATen/native/Cross.cpp:63.)

tensor([ 0.,  0., -1.])
tensor([[0.7375, 0.8328],
        [0.8444, 0.2941]])
tensor([[2.2125, 2.4985],
        [2.5332, 0.8822]])
torch.return_types.svd(
U=tensor([[-0.7889, -0.6145],
        [-0.6145,  0.7889]]),
S=tensor([4.1498, 1.0548]),
V=tensor([[-0.7957,  0.6056],
        [-0.6056, -0.7957]]))
</pre></div>
</div>
<p>이것은 수많은 연산의 일부분입니다.
더 자세한 내용이나 수학 함수의 전체적인 목록은, 다음
<a class="reference external" href="https://pytorch.org/docs/stable/torch.html#math-operations">documentation</a>
를 읽어주세요.</p>
</section>
<section id="id5">
<h3>Tensor의 값을 변경하기<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>대부분 tensor들의 이진 연산은 제3자의 새로운 tensor를 생성합니다.
<code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">*</span> <span class="pre">b</span></code> ( <code class="docutils literal notranslate"><span class="pre">a</span></code> 와 <code class="docutils literal notranslate"><span class="pre">b</span></code> 는 tensor)연산을 수행할 때,
새로운 tensor <code class="docutils literal notranslate"><span class="pre">c</span></code> 는 다른 tensor와 구별되는 메모리 영역을 차지하게 됩니다.</p>
<p>그럼에도 불구하고 tensor의 값을 변경하고 싶은 순간이 있을 수 있습니다 -
예를 들어, 중간 연산 결과 값을 버릴 수 있는 각 원소 단위 연산을 수행하는 경우가 있습니다.
이런 연산을 위해, 대부분의 수학 함수들은 tensor 내부의 값을
변경할 수 있는 함수 이름 맨 뒤에 밑줄 (<code class="docutils literal notranslate"><span class="pre">_</span></code>)이 추가된 버전을 가지고 있습니다.</p>
<p>예시:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;a:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>   <span class="c1"># 이 연산은 메모리에 새로운 tensor를 생성합니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>              <span class="c1"># a는 변하지 않습니다.</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">b:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sin_</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>  <span class="c1"># 밑줄에 주목하세요.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>              <span class="c1"># b가 변합니다.</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>a:
tensor([0.0000, 0.7854, 1.5708, 2.3562])
tensor([0.0000, 0.7071, 1.0000, 0.7071])
tensor([0.0000, 0.7854, 1.5708, 2.3562])

b:
tensor([0.0000, 0.7854, 1.5708, 2.3562])
tensor([0.0000, 0.7071, 1.0000, 0.7071])
tensor([0.0000, 0.7071, 1.0000, 0.7071])
</pre></div>
</div>
<p>산술 연산에서, 비슷한 행동을 하는 함수가 있습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Before:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">After adding:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">After multiplying&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Before:
tensor([[1., 1.],
        [1., 1.]])
tensor([[0.3788, 0.4567],
        [0.0649, 0.6677]])

After adding:
tensor([[1.3788, 1.4567],
        [1.0649, 1.6677]])
tensor([[1.3788, 1.4567],
        [1.0649, 1.6677]])
tensor([[0.3788, 0.4567],
        [0.0649, 0.6677]])

After multiplying
tensor([[0.1435, 0.2086],
        [0.0042, 0.4459]])
tensor([[0.1435, 0.2086],
        [0.0042, 0.4459]])
</pre></div>
</div>
<p>이러한 내부의 값을 변경하는 산술 함수는 다른 많은 함수들
(e.g., <code class="docutils literal notranslate"><span class="pre">torch.sin()</span></code>)처럼 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 모듈의 메소드가 아니라
<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> 객체의 메소드인 점에 주목해야 합니다.
<code class="docutils literal notranslate"><span class="pre">a.add_(b)</span></code> 와 같은 경우처럼, <em>메소드를 호출하는 tensor는 값이 변경됩니다.</em></p>
<p>이미 존재하고 있는 메모리에 할당된 tensor에 계산 결과값을 저장하는 또 다른 옵션이 있습니다.
tensor를 생성하는 메소드 뿐만 아니라 지금까지 이 문서에서 봤던 수많은 함수나 메소드는
결과 값을 받는 특정 tensor를 명시하는 <code class="docutils literal notranslate"><span class="pre">out</span></code> 이라는 인자를 가지고 있습니다.
만약 <code class="docutils literal notranslate"><span class="pre">out</span></code> tensor가 알맞은 shape와 <code class="docutils literal notranslate"><span class="pre">dtype</span></code> 을 가지고 있다면,
새로운 메모리 할당 없이 결과값이 저장됩니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">old_id</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>                <span class="c1"># c의 값이 변경되었습니다.</span>

<span class="k">assert</span> <span class="n">c</span> <span class="ow">is</span> <span class="n">d</span>           <span class="c1"># c와 d가 서로 단순히 같은 값을 가지는지가 아니라 같은 객체인지 테스트합니다.</span>
<span class="k">assert</span> <span class="nb">id</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">==</span> <span class="n">old_id</span>  <span class="c1"># 새로운 c는 이전 객체와 확실히 같은 객체입니다.</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">c</span><span class="p">)</span> <span class="c1"># 다시 한번 생성해봅시다!</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>                <span class="c1"># c의 값이 다시 바뀌었습니다.</span>
<span class="k">assert</span> <span class="nb">id</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">==</span> <span class="n">old_id</span>  <span class="c1"># 하지만 여전히 같은 객체네요!</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0., 0.],
        [0., 0.]])
tensor([[0.3653, 0.8699],
        [0.2364, 0.3604]])
tensor([[0.0776, 0.4004],
        [0.9877, 0.0352]])
</pre></div>
</div>
</section>
</section>
<section id="id6">
<h2>Tensor를 복사하기<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<p>파이썬의 다른 객체와 마찬가지로 변수에 tensor를 할당하는 것은
변수가 tensor의 <em>label</em> 이 되고 값을 복사하지 않습니다. 다음 예시를 보시죠:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span>

<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">561</span>  <span class="c1"># a의 값을 바꾸면...</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>       <span class="c1"># ...b의 값이 바뀝니다.</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[  1., 561.],
        [  1.,   1.]])
</pre></div>
</div>
<p>하지만 만약 우리가 작업할 별도의 데이터 복사본을 원하면 어떻게 해야할까요?
<code class="docutils literal notranslate"><span class="pre">clone()</span></code> 메소드가 당신이 찾던 해답이 될 것입니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

<span class="k">assert</span> <span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">a</span>      <span class="c1"># 메모리 상의 다른 객체입니다...</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>  <span class="c1"># ...하지만 여전히 같은 값을 가지고 있네요!</span>

<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">561</span>          <span class="c1"># a가 변경되었습니다...</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>               <span class="c1"># ...하지만 여전히 b는 이전 값을 가지고 있네요.</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[True, True],
        [True, True]])
tensor([[1., 1.],
        [1., 1.]])
</pre></div>
</div>
<p><strong>``clone()`` 메소드를 사용할 때 알아야 할 중요한 점이 있습니다.</strong>
만약 source tensor가 autograd를 가진다면 clone이 가능합니다.
<strong>이 부분은 autograd와 관련된 동영상에서 더 깊이 다룰 것 입니다.</strong>
하지만 만약 자세한 내용을 간단히 알고 싶다면 계속 설명하겠습니다.</p>
<p><em>대부분의 경우에서 이것이 바로 여러분이 원하는 것입니다.</em>
예를 들어, 만약 여러분의 모델이 그 모델의 <code class="docutils literal notranslate"><span class="pre">forward()</span></code> 메소드에 여러 갈래의 계산 경로가 있고
원본 tensor와 그것의 복제본 <em>모두</em> 가 모델의 출력에 기여를 한다면,
두 tensor에 대한 autograd를 설정하는 모델 학습을 활성화 합니다.
만약 여러분의 source tensor가 autograd를 사용할 수 있다면
(일반적으로 학습 가중치의 집합이거나, 가중치를 포함하는 계산에서 파생된 경우),
여러분이 원하는 결과를 얻을 수 있습니다.</p>
<p>반면에 원본 tensor나 그것의 복제본 <em>모두</em> 가 변화도를 추적할 필요가 없다면,
source tensor의 autograd가 꺼져있다면
clone을 사용할 수 있습니다.</p>
<p>그러나 <em>세번째 경우</em> 가 있습니다:
기본적으로 변화도가 모든 것을 위해 켜져있지만 일부 지표를 생성하기 위해서
스트림 중간에서 일부 값을 생성하고 싶어 하는
여러분 모델의 <code class="docutils literal notranslate"><span class="pre">forward()</span></code> 함수에서 계산을 수행한다고 상상해 보세요.
이 경우에는 변화도를 추적하기 위해서 source tensor의 복제본을 원하지 <em>않을</em> 수 있습니다
- 성능이 autograd의 히스토리 추적 기능을 끄면서 향상됩니다.
이 경우를 위해서는 source tensor에 <code class="docutils literal notranslate"><span class="pre">.detach()</span></code> 메소드를 사용할 수 있습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># autograd를 켭니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0.0905, 0.4485],
        [0.8740, 0.2526]], requires_grad=True)
tensor([[0.0905, 0.4485],
        [0.8740, 0.2526]], grad_fn=&lt;CloneBackward0&gt;)
tensor([[0.0905, 0.4485],
        [0.8740, 0.2526]])
tensor([[0.0905, 0.4485],
        [0.8740, 0.2526]], requires_grad=True)
</pre></div>
</div>
<p>여기서 무슨 일이 일어나는걸까요?</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">a</span></code> 를  <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 옵션을 킨 상태로 생성합니다.
<strong>아직 이 선택적 인자를 다루지 않았지만, autograd 단원 동안만 다룰 것입니다.</strong></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">a</span></code> 를 출력할 때, <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 속성을 가지고 있다고 알려줍니다 -
이 뜻은 autograd와 계산 히스토리 추적 기능을 켠다는 것입니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">a</span></code> 를 복제하고 그것을 <code class="docutils literal notranslate"><span class="pre">b</span></code> 라고 라벨을 붙입니다. <code class="docutils literal notranslate"><span class="pre">b</span></code> 를 출력할 때,
그것의 계산 히스토리가 추적되고 있다는 것을 확인할 수 있습니다 -
<code class="docutils literal notranslate"><span class="pre">a</span></code> 의 autograd 설정에 내장되어 있는 기능이며, 이 설정은 계산 히스토리에 추가합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">a</span></code> 를 <code class="docutils literal notranslate"><span class="pre">c</span></code> 에 복제를 하지만 <code class="docutils literal notranslate"><span class="pre">detach()</span></code> 를 먼저 호출을 합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">c</span></code> 를 출력합니다. 계산 히스토리가 없다는 것을 확인할 수 있고,
<code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 옵션이 없다는 것 또한 확인할 수 있습니다.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">detach()</span></code> 메소드는 <em>tensor의 계산 히스토리로 부터 tensor를 떼어냅니다.</em>
이 메소드의 의미는 “메소드 뒤에 어떤 것이든 와도 autograd를 끈 것처럼 작동하라.“ 라는 뜻입니다.
<code class="docutils literal notranslate"><span class="pre">a</span></code> 를 변경하지 <em>않고</em> 이 메소드를 수행합니다 -
마지막에 <code class="docutils literal notranslate"><span class="pre">a</span></code> 를 다시 출력할 때, 여전히 <code class="docutils literal notranslate"><span class="pre">a</span></code> 가 가진 <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>
속성이 남아 있다는 것을 확인할 수 있습니다.</p>
</section>
<section id="gpu">
<h2>GPU 환경으로 이동하기<a class="headerlink" href="#gpu" title="Link to this heading">#</a></h2>
<p>PyTorch의 주된 장점중 하나는 CUDA가 호환되는 Nvidia GPU에서의 강력한 성능 가속화입니다.
(“CUDA” 는 <em>Compute Unified Device Architecture</em> 의 약자이며,
병렬 컴퓨팅을 위한 Nvidia의 플랫폼입니다.)
지금까지 모든 작업을 CPU에서 처리했습니다. 어떻게 더 빠른 하드웨어로 이동할 수 있을까요?</p>
<p>먼저 <code class="docutils literal notranslate"><span class="pre">is_available()</span></code> 메소드를 사용해서 GPU가 사용 가능한지 아닌지 확인해야 합니다.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>만약 CUDA가 호환되는 GPU가 없고 CUDA 드라이버가 설치되어있지 않다면
이 섹션에서의 실행 가능한 cell은 어떤 GPU와 관련된 코드도 실행할 수 없습니다.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;We have a GPU!&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sorry, CPU only.&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>We have a GPU!
</pre></div>
</div>
<p>일단 1개 혹은 그 이상의 GPU가 사용 가능하다는 것을 확인했다면,
데이터를 GPU가 확인할 수 있는 어떤 공간에 저장할 필요가 있습니다.
CPU는 컴퓨터의 RAM에서 데이터를 이용해서 계산을 수행합니다.
GPU는 전용 메모리가 연결되어 있습니다. 해당 장치에서 계산을 수행하고 싶을 때마다
계산에 필요한 <em>모든</em> 데이터를 GPU장치가 접근 가능한 메모리로 이동해야 합니다.
(평소에는 “GPU가 접근 가능한 메모리로 데이터를 이동한다“
를 “데이터를 GPU로 옮긴다“ 라고 줄여서 말합니다.)</p>
<p>목적 장치에서 데이터를 가져오는 다양한 방법이 있습니다.
객체를 생성할 때 데이터를 가져올 수 있습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">gpu_rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">gpu_rand</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sorry, CPU only.&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0.3344, 0.2640],
        [0.2119, 0.0582]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
<p>기본적으로 새로운 tensor는 CPU에 생성됩니다. 따라서 tensor를 GPU에 생성하고 싶을 때
<code class="docutils literal notranslate"><span class="pre">device</span></code> 선택 인자를 반드시 명시해줘야 합니다.
새로운 tensor를 출력할 때, (만약 CPU에 존재하지 않는다면)
PyTorch는 어느 장치에 객체가 있는지 알려준다는 것을 확인할 수 있습니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.cuda.device_count()</span></code> 를 사용해서 GPU의 개수를 조회할 수 있습니다.
만약 1개보다 많은 GPU를 가지고 있다면, 각 GPU를 인덱스로 지정할 수 있습니다:
<code class="docutils literal notranslate"><span class="pre">device='cuda:0'</span></code>, <code class="docutils literal notranslate"><span class="pre">device='cuda:1'</span></code>, 와 같이 말이죠.</p>
<p>코딩을 할 때, 어디에서나 장치 이름을 문자열 상수로 지정하는 것은 상당히 유지 보수에 취약합니다.
CPU 하드웨어나 GPU 하드웨어 어떤 것을 사용하는지에 관계없이 여러분의 코드는 잘 작동해야 합니다.
문자열 대신에 tensor를 저장할 장치 핸들러를 생성하는 것으로 유지 보수가 쉬운 코드를 작성할 수 있습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">my_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">my_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Device: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">my_device</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">my_device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Device: cuda
tensor([[0.0024, 0.6778],
        [0.2441, 0.6812]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
<p>만약 한 장치에 tensor가 있을 때, <code class="docutils literal notranslate"><span class="pre">to()</span></code> 메소드를 사용해서 다른 장치로 이동할 수 있습니다.
다음 코드는 CPU에 tensor를 생성하고, 이전 cell에서 얻은 장치 핸들러로 tensor를 이동합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">my_device</span><span class="p">)</span>
</pre></div>
</div>
<p>2개 혹은 그 이상의 tensor를 포함한 계산을 하기 위해서는
<em>모든 tensor가 같은 장치에 있어야 한다</em> 는 것을 아는 것이 중요합니다.
다음 코드는 GPU 장치의 사용 가능 여부와 관계없이 runtime error를 발생할 것입니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;gpu&#39;</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>  <span class="c1"># 오류가 발생할 것입니다.</span>
</pre></div>
</div>
</section>
<section id="id7">
<h2>Tensor의 shape 다루기<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<p>때로는 tensor의 shape를 변환할 필요가 있습니다.
아래에 있는 몇 가지 흔한 경우와 함께 tensor의 shape를 다루는 방법에 대해 알아볼 것 입니다.</p>
<section id="id8">
<h3>차원의 개수 변경하기<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p>차원의 개수를 변경할 필요가 있는 한가지 경우는 모델의 입력에 단일 인스턴스를 전달할 때 입니다.
PyTorch 모델은 일반적으로 입력에 <em>배치</em> 가 들어오기를 기대합니다.</p>
<p>예를 들어, 3개의 색깔 채널을 가진 226픽셀 정사각형 이미지인 3 x 226 x 226 개 데이터와
함께 작동하는 모델을 가지고 있다고 상상해보세요.
이미지를 불러오고 tensor로 변환하면 <code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">226,</span> <span class="pre">226)</span></code> shape를 가진 tensor가 됩니다.
그럼에도 불구하고 이 모델은 <code class="docutils literal notranslate"><span class="pre">(N,</span> <span class="pre">3,</span> <span class="pre">226,</span> <span class="pre">226)</span></code> shape를 가진 tensor를 입력으로 기대합니다.
이때 <code class="docutils literal notranslate"><span class="pre">N</span></code> 은 배치에 포함된 이미지의 개수입니다. 그렇다면 어떻게 한 배치를 만들 수 있을까요?</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">226</span><span class="p">,</span> <span class="mi">226</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 226, 226])
torch.Size([1, 3, 226, 226])
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> 메소드는 크기가 1인 차원을 추가합니다.
<code class="docutils literal notranslate"><span class="pre">unsqueeze(0)</span></code> 는 새로운 0번째 차원을 추가합니다
- 이제 한 배치를 가지게 되었습니다!</p>
<p>이게 <em>un</em>squeezing이면, squeezing은 무슨 뜻 일까요?
여기서는 차원을 하나 확장해도 tensor에 있는 원소의 개수는 변하지
<em>않는다</em> 는 사실을 이용하고 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[[[[0.2347]]]]])
</pre></div>
</div>
<p>위의 예제에 이어서 각 입력 값에 대한 모델의 출력 값이 20개의 원소를 가진 vector라고 생각해봅시다.
그렇다면 <code class="docutils literal notranslate"><span class="pre">N</span></code> 이 입력 배치에 있는 인스턴스의 개수라고 할 때,
출력 값의 shape는 <code class="docutils literal notranslate"><span class="pre">(N,</span> <span class="pre">20)</span></code> 라고 기대할 수 있습니다.
이 뜻은 입력으로 단일 배치가 들어왔을 때,
<code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">20)</span></code> 의 shape를 가진 출력 값을 얻는다는 것 입니다.</p>
<p>만약 그저 20개의 원소를 가진 벡터와 같이
- <em>배치 shape가 아닌</em> 연산 결과를 얻고 싶으면 어떻게 해야할까요?</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 20])
tensor([[0.1899, 0.4067, 0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367,
         0.8194, 0.4509, 0.2690, 0.8381, 0.8207, 0.6818, 0.5057, 0.9335, 0.9769,
         0.2792, 0.3277]])
torch.Size([20])
tensor([0.1899, 0.4067, 0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367,
        0.8194, 0.4509, 0.2690, 0.8381, 0.8207, 0.6818, 0.5057, 0.9335, 0.9769,
        0.2792, 0.3277])
torch.Size([2, 2])
torch.Size([2, 2])
</pre></div>
</div>
<p>결과로 나온 shape로 부터 2차원 tensor가 이제 1차원으로 바뀐 것을 볼 수 있고,
위에 있는 cell의 결과를 자세히 보면
추가적인 차원을 가졌기 때문에
<code class="docutils literal notranslate"><span class="pre">a</span></code> 를 출력하는 것에서 “추가” 대괄호 집합 <code class="docutils literal notranslate"><span class="pre">[]</span></code> 을 볼 수 있습니다.</p>
<p>오직 차원의 값이 1인 경우에만 <code class="docutils literal notranslate"><span class="pre">squeeze()</span></code> 를 사용할 수 있습니다.
<code class="docutils literal notranslate"><span class="pre">c</span></code> 에서 크기가 2인 차원을 squeeze 하려고 하는 위 예시를 보면,
처음 그 shape로 다시 돌아온다는 사실을 알 수 있습니다.
<code class="docutils literal notranslate"><span class="pre">squeeze()</span></code> 와 <code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> 를 호출하는 것은 오직 차원의 크기가 1일 때만 작동합니다.
왜냐하면 이 경우가 아니면 tensor의 원소 개수가 바뀌기 때문입니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> 는 broadcasting을 쉽게 하는 경우에도 사용합니다.
다음 코드를 보고 이전 예시를 떠올려 보세요:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span>     <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>   <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 3번째 차원은 1이고, 2번째 차원은 a와 동일합니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<p>broadcast의 순수한 효과는 차원 0과 차원 2에 대한 연산을 broadcast해서
무작위 3 x 1 shape의 tensor를 <code class="docutils literal notranslate"><span class="pre">a</span></code> 의 원소 개수가 3인 모든 열에 곱하는 것이었습니다.</p>
<p>만약 무작위 벡터가 오직 3개의 원소만을 가지면 어떻게 될까요?
broadcast를 할 능력을 잃어버리게 됩니다, 왜냐하면 마지막 차원이
broadcasting 규칙에 맞지 않기 때문입니다.
하지만 <code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> 가 도와줍니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>   <span class="mi">3</span><span class="p">)</span>     <span class="c1"># a * b를 시도하는 것은 runtime error가 발생합니다.</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>       <span class="c1"># 끝에 새로운 차원을 추가해서 2차원 tensor로 바꿉니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">c</span><span class="p">)</span>             <span class="c1"># broadcasting이 다시 작동합니다!</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 1])
tensor([[[0.1891, 0.1891],
         [0.3952, 0.3952],
         [0.9176, 0.9176]],

        [[0.1891, 0.1891],
         [0.3952, 0.3952],
         [0.9176, 0.9176]],

        [[0.1891, 0.1891],
         [0.3952, 0.3952],
         [0.9176, 0.9176]],

        [[0.1891, 0.1891],
         [0.3952, 0.3952],
         [0.9176, 0.9176]]])
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">squeeze()</span></code> 와 <code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> 메소드는 tensor 자체의 값을 변경하는
<code class="docutils literal notranslate"><span class="pre">squeeze_()</span></code> 와 <code class="docutils literal notranslate"><span class="pre">unsqueeze_()</span></code> 또한 가지고 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_me</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">226</span><span class="p">,</span> <span class="mi">226</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch_me</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">batch_me</span><span class="o">.</span><span class="n">unsqueeze_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch_me</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 226, 226])
torch.Size([1, 3, 226, 226])
</pre></div>
</div>
<p>때로는 원소의 개수와 원소의 값을 여전히 유지하면서
tensor의 shape를 한번에 바꾸고 싶을 때가 있습니다.
모델의 합성곱 계층과 선형 계층 사이 인터페이스에서 이러한 상황이 발생합니다
- 이 상황은 이미지 분류 모델에서 흔히 일어나는 일입니다.
합성곱 커널은 <em>특성의 수 x 너비 x 높이</em> shpae의 tensor를 출력 값으로 생성하지만
이후에 있는 선형 계층은 입력 값으로 1차원을 기대합니다.
여러분이 요청한 차원에 입력 tensor가 가진 원소와 같은 개수를 생성하는
<code class="docutils literal notranslate"><span class="pre">reshape()</span></code> 를 여러분을 위해서 제공합니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">output3d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output3d</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">input1d</span> <span class="o">=</span> <span class="n">output3d</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input1d</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># torch 모듈에 있는 메소드에 대해서도 호출할 수 있습니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output3d</span><span class="p">,</span> <span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,))</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([6, 20, 20])
torch.Size([2400])
torch.Size([2400])
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>위에 있는 cell의 마지막 줄에 있는 인자 <code class="docutils literal notranslate"><span class="pre">(6</span> <span class="pre">*</span> <span class="pre">20</span> <span class="pre">*</span> <span class="pre">20,)</span></code> 는
PyTorch는 tensor shape를 나타낼 때 <strong>tuple</strong> 을 기대하기 때문입니다.
하지만 shape가 메소드의 첫번째 인자라면 - 연속된 정수라고 속여서 사용할 수 있습니다.
여기에서는 메소드에게 이 인자가 진짜 1개 원소를 가진 튜플이라고 알려주기 위해서
편의상 소괄호와 콤마를 추가해야 합니다.</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">reshape()</span></code> 는 tensor를 바라보는 <em>관점</em> 을 변경합니다.
- 즉, 메모리의 같은 지역을 바라보는 서로 다른 관점을 가진 tensor 객체라는 뜻입니다.
<em>이 내용은 정말 중요합니다:</em> source tensor에 어떠한 변화가 있으면
<code class="docutils literal notranslate"><span class="pre">clone()</span></code> 을 사용하지 않는 한, 해당 tensor를 바라보고 있는 다른 객체 또한
값이 변한다는 뜻 입니다.</p>
<p>해당 소개의 범위를 벗어난 조건 <em>들</em> 이 있습니다.
그것은 <code class="docutils literal notranslate"><span class="pre">reshape()</span></code> 가 data의 복사본을 가진 tensor를 반환 해야 한다는 것 입니다.
더 많은 정보는 다음 문서를 참고하세요
<a class="reference external" href="https://pytorch.org/docs/stable/torch.html#torch.reshape">docs</a>.</p>
</section>
</section>
<section id="numpy">
<h2>NumPy로 변환<a class="headerlink" href="#numpy" title="Link to this heading">#</a></h2>
<p>위에 있는 broadcasting 부분에서, PyTorch의 broadcast
문법은 Numpy와 호환 가능하다고 말했었습니다
- 하지만 PyTorch와 NumPy 사이 유사성은 우리가 생각한 것 보다 더욱 깊습니다.</p>
<p>만약 NumPy의 ndarrays에 저장되어 있는 데이터를 사용하는
머신 러닝 혹은 과학 분야와 관련된 코드를 가지고 있다면,
같은 데이터를 PyTorch의 GPU 가속을 사용할 수 있고
머신 러닝 모델을 만드는데 필요한 효과적인 추상화를 제공하는
PyTorch tensor로 표현하고 싶을 수 있습니다.
ndarray와 PyTorch tensor끼리 바꾸는 것은 쉽습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">numpy_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">)</span>

<span class="n">pytorch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pytorch_tensor</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[1. 1. 1.]
 [1. 1. 1.]]
tensor([[1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
</pre></div>
</div>
<p>PyTorch는 NumPy array와 같은 shape의 tensor를 생성하고, 같은 데이터를 포함합니다.
심지어 NumPy의 기본적인 64비트 실수 데이터 자료형을 유지합니다.</p>
<p>PyTorch에서 NumPy로 변환은 다른 방식을 사용해서 쉽게 할 수 있습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">pytorch_rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pytorch_rand</span><span class="p">)</span>

<span class="n">numpy_rand</span> <span class="o">=</span> <span class="n">pytorch_rand</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy_rand</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0.8716, 0.2459, 0.3499],
        [0.2853, 0.9091, 0.5695]])
[[0.87163675 0.2458961  0.34993553]
 [0.2853077  0.90905803 0.5695162 ]]
</pre></div>
</div>
<p>이러한 변환된 객체들은 해당 객체의 source 객체가 위치한
<em>메모리의 같은 공간</em> 을 사용한다는 점을 아는 것이 중요합니다.
이것은 한 객체가 변하면 다른 것에 영향을 준다는 의미입니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">numpy_array</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">23</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pytorch_tensor</span><span class="p">)</span>

<span class="n">pytorch_rand</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">17</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy_rand</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.,  1.,  1.],
        [ 1., 23.,  1.]], dtype=torch.float64)
[[ 0.87163675  0.2458961   0.34993553]
 [ 0.2853077  17.          0.5695162 ]]
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 4.742 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-introyt-tensors-deeper-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/63a0f0fc7b3ffb15d3a5ac8db3d521ee/tensors_deeper_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">tensors_deeper_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/be017e7b39198fdf668c138fd8d57abe/tensors_deeper_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">tensors_deeper_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/3661507963f5283da25e44c8ac84d1a4/tensors_deeper_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">tensors_deeper_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="introyt1_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">PyTorch 소개</p>
      </div>
    </a>
    <a class="right-next"
       href="autogradyt_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">The Fundamentals of Autograd</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="introyt1_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">PyTorch 소개</p>
      </div>
    </a>
    <a class="right-next"
       href="autogradyt_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">The Fundamentals of Autograd</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor">Tensor 생성하기</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-seed">무작위 Tensor와 Seed 사용하기</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-shape">Tensor의 shape</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Tensor 자료형</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">PyTorch Tensor에서 산술 &amp; 논리 연산</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-broadcasting">개요: Tensor Broadcasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Tensor를 사용하는 다양한 연산들</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Tensor의 값을 변경하기</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Tensor를 복사하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu">GPU 환경으로 이동하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Tensor의 shape 다루기</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">차원의 개수 변경하기</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numpy">NumPy로 변환</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Pytorch Tensor \uc18c\uac1c",
       "headline": "Pytorch Tensor \uc18c\uac1c",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/beginner/introyt/tensors_deeper_tutorial.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. Introduction || Tensors || Autograd || Building Models || TensorBoard Support || Training Models || Model Understanding Pytorch Tensor \uc18c\uac1c# \ubc88\uc5ed: \uc774\uc0c1\uc724 \uc544\ub798 \uc601\uc0c1\uc774\ub098 youtube \ub97c \ucc38\uace0\ud558\uc138\uc694. Tensor\ub294 PyTorch\uc5d0\uc11c \uc911\uc694\ud55c \ucd94\uc0c1 \ub370\uc774\ud130 \uc790\ub8cc\ud615\uc785\ub2c8\ub2e4. \uc774 interactive notebook\uc740 torch.Tensor \ud074\ub798\uc2a4\uc5d0 \ub300\ud55c \uc2ec\uce35\uc801\uc778 \uc18c\uac1c\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uba3c\uc800 \uac00\uc7a5 \uc911\uc694\ud55c \uac83\uc740 PyTorch \ubaa8\ub4c8\uc744 import \ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \ub610\ud55c \uba87 \uac00\uc9c0 \uc608\uc81c\uc5d0 \uc0ac\uc6a9\ud560 math \ubaa8\ub4c8\ub3c4 import \ud569\ub2c8\ub2e4. import torch import math Tensor \uc0dd\uc131\ud558\uae30# tensor\ub97c \uc0dd\uc131\ud558\ub294 \uac00\uc7a5 \uac04\ub2e8\ud55c \ubc29\ubc95\uc740 torch.empty() \ub97c \ud638\ucd9c\ud558\ub294 \uac83\uc785\ub2c8\ub2e4: x = torch.empty(3, 4) print(type(x)) print(x) \u003cclass \u0027torch.Tensor\u0027\u003e tensor([[-1.0616e-12, 4.5577e-41, -6.4935e-15, 3.4088e-41], [-6.4448e-15, 3.4088e-41, -1.3217e+00, 3.4088e-41], [-1.0616e-12, 4.5577e-41, -1.0617e-12, 4.5577e-41]]) \ubc29\uae08 \ubb34\uc5c7\uc744 \ud55c \uac83\uc778\uc9c0 \ub4e4\uc5ec\ub2e4\ubd05\uc2dc\ub2e4: torch \ubaa8\ub4c8\uc5d0 \uc788\ub294 \uc218\ub9ce\uc740 \uba54\uc18c\ub4dc \uc911 \ud558\ub098\ub97c \uc0ac\uc6a9\ud574\uc11c tensor\ub97c \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 tensor\ub294 3\uac1c\uc758 \ud589\uacfc 4\uac1c\uc758 \uc5f4\uc744 \uac00\uc9c4 2\ucc28\uc6d0 tensor\uc785\ub2c8\ub2e4. \uac1d\uccb4\uac00 \ubc18\ud658\ud55c type\uc740 torch.Tensor \uc774\uba70 \uc774\ub294 torch.FloatTensor \uc758 \ubcc4\uce6d\uc785\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc73c\ub85c PyTorch tensor\ub294 32-bit \ubd80\ub3d9 \uc18c\uc218\uc810 \ud45c\ud604 \uc2e4\uc218\ub85c \ucc44\uc6cc \uc9d1\ub2c8\ub2e4. (\uc544\ub798\uc5d0\uc11c \ub354 \ub9ce\uc740 \ub370\uc774\ud130 \uc790\ub8cc\ud615\uc744 \uc18c\uac1c\ud569\ub2c8\ub2e4) \uc0dd\uc131\ud55c tensor\ub97c \ucd9c\ub825\ud558\uba74 \uc544\ub9c8 \ubb34\uc791\uc704 \uac12\uc744 \ubcfc \uc218 \uc788\uc744 \uac83 \uc785\ub2c8\ub2e4. torch.empty() \ub294 tensor\ub97c \uc704\ud55c \uba54\ubaa8\ub9ac\ub97c \ud560\ub2f9\ud574 \uc8fc\uc9c0\ub9cc \uc784\uc758\uc758 \uac12\uc73c\ub85c \ucd08\uae30\ud654\ud558\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4 - \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 \ud560\ub2f9 \ub2f9\uc2dc\uc5d0 \uba54\ubaa8\ub9ac\uac00 \uac00\uc9c0\uace0 \uc788\ub294 \uac12\uc744 \ubcf4\ub294 \uac83\uc785\ub2c8\ub2e4. \uac04\ub7b5\ud558\uac8c tensor\uc640 tensor\uc758 \ucc28\uc6d0 \uc218, \uadf8\ub9ac\uace0 \uac01 tensor\uc758 \uc6a9\uc5b4\uc5d0 \ub300\ud574 \uc54c\uc544\ubd05\uc2dc\ub2e4: \ub54c\ub85c\ub294 1\ucc28\uc6d0 tensor\ub97c \ubcf4\uac8c \ub420 \uac83\uc778\ub370 \uc774\ub294 vector \ub77c\uace0 \ud569\ub2c8\ub2e4. \uc774\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c 2\ucc28\uc6d0 tensor\ub294 \uc8fc\ub85c matrix \ub77c\uace0 \ud569\ub2c8\ub2e4. 2\ucc28\uc6d0\ubcf4\ub2e4 \ud070 \ucc28\uc6d0\uc744 \uac00\uc9c4 \uac83\ub4e4\uc740 \uc77c\ubc18\uc801\uc73c\ub85c \uadf8\ub0e5 tensor\ub77c\uace0 \ud569\ub2c8\ub2e4. \ucf54\ub4dc\ub97c \uc791\uc131\ud558\uba70 \uc8fc\ub85c tensor\ub97c \uba87 \uac00\uc9c0 \uac12\uc73c\ub85c \ucd08\uae30\ud654\ud558\uace0 \uc2f6\uc744 \uc218\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc778 \uacbd\uc6b0\ub85c\ub294 \ubaa8\ub450 0\uc73c\ub85c \ucd08\uae30\ud654\ud558\uac70\ub098, \ubaa8\ub450 1\ub85c \ucd08\uae30\ud654\ud558\uac70\ub098, \ud639\uc740 \ubaa8\ub450 \ubb34\uc791\uc704 \uac12\uc73c\ub85c \ucd08\uae30\ud654 \ud560 \ub54c\uac00 \uc788\ub294\ub370, torch \ubaa8\ub4c8\uc740 \uc774\ub7ec\ud55c \ubaa8\ub4e0 \uacbd\uc6b0\uc5d0 \ub300\ud55c \uba54\uc18c\ub4dc\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4: zeros = torch.zeros(2, 3) print(zeros) ones = torch.ones(2, 3) print(ones) torch.manual_seed(1729) random = torch.rand(2, 3) print(random) tensor([[0., 0., 0.], [0., 0., 0.]]) tensor([[1., 1., 1.], [1., 1., 1.]]) tensor([[0.3126, 0.3791, 0.3087], [0.0736, 0.4216, 0.0691]]) \uc774 \ubaa8\ub4e0 \ud329\ud1a0\ub9ac \uba54\uc18c\ub4dc\ub4e4\uc740 \uc6b0\ub9ac\uac00 \uae30\ub300\ud558\ub358 \uac83\ub4e4\uc744 \ubaa8\ub450 \uc218\ud589\ud569\ub2c8\ub2e4 - 0\uc73c\ub85c \ubaa8\ub450 \ucc44\uc6cc \uc9c4 tensor, 1\ub85c \ubaa8\ub450 \ucc44\uc6cc \uc9c4 tensor \uadf8\ub9ac\uace0 0\uacfc 1\uc0ac\uc774\uc758 \ubb34\uc791\uc704 \uac12\uc73c\ub85c \ucc44\uc6cc \uc9c4 tensor\ub97c \uc5bb\uc5c8\uc2b5\ub2c8\ub2e4. \ubb34\uc791\uc704 Tensor\uc640 Seed \uc0ac\uc6a9\ud558\uae30# \ubb34\uc791\uc704 tensor\uc5d0 \ub300\ud574 \ub9d0\ud558\uc790\uba74, \ubc14\ub85c \uc55e\uc5d0\uc11c \ud638\ucd9c\ud558\ub294 torch.manual_seed() \ub97c \ub208\uce58\ucc44\uc168\ub098\uc694? \ud2b9\ud788 \uc5f0\uad6c \ud658\uacbd\uc5d0\uc11c \uc5f0\uad6c \uacb0\uacfc\uc758 \uc7ac\ud604 \uac00\ub2a5\uc131\uc5d0 \ub300\ud55c \ud655\uc2e0\uc744 \uc5bb\uace0 \uc2f6\uc744 \ub54c, \ubaa8\ub378\uc758 \ud559\uc2b5 \uac00\uc911\uce58\uc640 \uac19\uc740 \ubb34\uc791\uc704 \uac12\uc744 \uac00\uc9c4 tensor\ub85c \ucd08\uae30\ud654 \ud558\ub294 \uac83\uc740 \ud754\ud558\uac70\ub098 \uc885\uc885 \uc77c\uc5b4\ub098\ub294 \uc77c\uc785\ub2c8\ub2e4. \uc9c1\uc811 \ubb34\uc791\uc704 \ub09c\uc218 \uc0dd\uc131\uae30\uc758 seed\ub97c \uc124\uc815\ud558\ub294 \uac83\uc774 \ub2e4\uc74c \ubc29\ubc95\uc785\ub2c8\ub2e4. \ub2e4\uc74c \ucf54\ub4dc\ub97c \ubcf4\uba70 \ub354 \uc790\uc138\ud788 \uc54c\uc544\ubd05\uc2dc\ub2e4: torch.manual_seed(1729) random1 = torch.rand(2, 3) print(random1) random2 = torch.rand(2, 3) print(random2) torch.manual_seed(1729) random3 = torch.rand(2, 3) print(random3) random4 = torch.rand(2, 3) print(random4) tensor([[0.3126, 0.3791, 0.3087], [0.0736, 0.4216, 0.0691]]) tensor([[0.2332, 0.4047, 0.2162], [0.9927, 0.4128, 0.5938]]) tensor([[0.3126, 0.3791, 0.3087], [0.0736, 0.4216, 0.0691]]) tensor([[0.2332, 0.4047, 0.2162], [0.9927, 0.4128, 0.5938]]) random1 \uacfc random3 \uadf8\ub9ac\uace0 random2 \uacfc random4 , \uc774 \uac01\uac01 \uc11c\ub85c \ub3d9\uc77c\ud55c \uacb0\uacfc\uac00 \ub098\uc628\ub2e4\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubb34\uc791\uc704 \ub09c\uc218 \uc0dd\uc131\uae30\uc758 seed\ub97c \uc218\ub3d9\uc73c\ub85c \uc124\uc815\ud558\uba74 \ub09c\uc218\uac00 \uc7ac \uc124\uc815\ub418\uc5b4 \ub300\ubd80\ubd84\uc758 \ud658\uacbd\uc5d0\uc11c \ubb34\uc791\uc704 \uc22b\uc790\uc5d0 \uc758\uc874\ud558\ub294 \ub3d9\uc77c\ud55c \uacc4\uc0b0\uc774 \uc774\ub8e8\uc5b4\uc9c0\uace0 \ub3d9\uc77c\ud55c \uacb0\uacfc\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \ubcf4\ub2e4 \uc790\uc138\ud55c \uc815\ubcf4\ub294 \ub2e4\uc74c \ubb38\uc11c\ub97c \ucc38\uace0\ud558\uc138\uc694 PyTorch documentation on reproducibility. Tensor\uc758 shape# \ub450 \uac1c \ud639\uc740 \uadf8 \uc774\uc0c1\uc758 tensor\uc5d0 \ub300\ud55c \uc5f0\uc0b0\uc744 \uc218\ud589\ud560 \ub54c, tensor\ub4e4\uc740 \uac19\uc740 shape\ub97c \ud544\uc694\ub85c \ud569\ub2c8\ub2e4 - \ub2e4\uc2dc \ub9d0\ud574\uc11c \ucc28\uc6d0\uc758 \uac1c\uc218\uac00 \uac19\uc544\uc57c \ud558\uace0, \uac01 \ucc28\uc6d0\ub9c8\ub2e4 \uc6d0\uc18c\uc758 \uc218\uac00 \uac19\uc544\uc57c \ud569\ub2c8\ub2e4. \uadf8\ub7ec\uae30 \uc704\ud574\uc11c\ub294 torch.*_like() \ud568\uc218\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. x = torch.empty(2, 2, 3) print(x.shape) print(x) empty_like_x = torch.empty_like(x) print(empty_like_x.shape) print(empty_like_x) zeros_like_x = torch.zeros_like(x) print(zeros_like_x.shape) print(zeros_like_x) ones_like_x = torch.ones_like(x) print(ones_like_x.shape) print(ones_like_x) rand_like_x = torch.rand_like(x) print(rand_like_x.shape) print(rand_like_x) torch.Size([2, 2, 3]) tensor([[[-9.8661e-01, 3.4088e-41, 4.5167e+24], [ 4.5580e-41, 8.9683e-44, 0.0000e+00]], [[ 1.1210e-43, 0.0000e+00, 6.1904e+09], [ 3.4084e-41, 0.0000e+00, 1.3881e+00]]]) torch.Size([2, 2, 3]) tensor([[[-1.3879e+00, 3.4088e-41, -1.8240e+00], [ 3.4088e-41, -2.0000e+00, 1.6543e+00]], [[ 0.0000e+00, 1.3972e+00, 2.0000e+00], [ 1.7108e+00, 0.0000e+00, 1.3881e+00]]]) torch.Size([2, 2, 3]) tensor([[[0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.]]]) torch.Size([2, 2, 3]) tensor([[[1., 1., 1.], [1., 1., 1.]], [[1., 1., 1.], [1., 1., 1.]]]) torch.Size([2, 2, 3]) tensor([[[0.6128, 0.1519, 0.0453], [0.5035, 0.9978, 0.3884]], [[0.6929, 0.1703, 0.1384], [0.4759, 0.7481, 0.0361]]]) \uc704\ucabd\uc758 \ucf54\ub4dc \uc140\uc5d0 \uc788\ub294 \uac83\ub4e4 \uc911\uc5d0 \uccab \ubc88\uc9f8\ub294 tensor\uc5d0 \uc788\ub294 .shape \uc18d\uc131\uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uc774 \uc18d\uc131\uc740 tensor\uc758 \uac01 \ucc28\uc6d0 \ud06c\uae30\uc5d0 \ub300\ud55c \ub9ac\uc2a4\ud2b8\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4 - \uc774 \uacbd\uc6b0\uc5d0, x \ub294 shape\uac00 2 x 2 x 3 \uc778 3\ucc28\uc6d0 tensor\uc785\ub2c8\ub2e4. \uadf8 \uc544\ub798\uc5d0\ub294 .empty_like(), .zeros_like(), .ones_like(), and .rand_like() \uba54\uc18c\ub4dc\ub97c \ud638\ucd9c \ud569\ub2c8\ub2e4. .shape \uc18d\uc131\uc744 \ud1b5\ud574\uc11c, \uc704\uc758 \uba54\uc18c\ub4dc\ub4e4\uc774 \ub3d9\uc77c\ud55c \ucc28\uc6d0\uac12\uc744 \ubc18\ud658\ud55c\ub2e4\ub294 \uac83\uc744 \uac80\uc99d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c \ub2e4\ub8e8\ub294 tensor\ub97c \uc0dd\uc131\ud558\ub294 \ub9c8\uc9c0\ub9c9 \ubc29\ubc95\uc740 PyTorch collection \ud615\uc2dd\uc758 \ub370\uc774\ud130\ub97c \uc9c1\uc811\uc801\uc73c\ub85c \uba85\uc2dc\ud558\ub294 \uac83 \uc785\ub2c8\ub2e4: some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]]) print(some_constants) some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19)) print(some_integers) more_integers = torch.tensor(((2, 4, 6), [3, 6, 9])) print(more_integers) tensor([[3.1416, 2.7183], [1.6180, 0.0073]]) tensor([ 2, 3, 5, 7, 11, 13, 17, 19]) tensor([[2, 4, 6], [3, 6, 9]]) torch.tensor() \ub294 \uc774\ubbf8 Python tuple\uc774\ub098 list \ud615\ud0dc\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 \uc788\ub294 \uacbd\uc6b0 tensor\ub97c \ub9cc\ub4e4\uae30 \uac00\uc7a5 \uc26c\uc6b4 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc704\uc5d0\uc11c \ubcf8 \uac83 \ucc98\ub7fc \uc911\ucca9\ub41c \ud615\ud0dc\uc758 collection \uc790\ub8cc\ud615\uc740 \ub2e4\ucc28\uc6d0 tensor\uac00 \uacb0\uacfc\ub85c \ub098\uc635\ub2c8\ub2e4. \ucc38\uace0 torch.tensor() \ub294 \ub370\uc774\ud130\uc758 \ubcf5\uc0ac\ubcf8\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. Tensor \uc790\ub8cc\ud615# tensor\uc758 \uc790\ub8cc\ud615\uc744 \uc124\uc815\ud558\ub294 \uac83\uc740 \ub2e4\uc591\ud55c \ubc29\uc2dd\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. a = torch.ones((2, 3), dtype=torch.int16) print(a) b = torch.rand((2, 3), dtype=torch.float64) * 20. print(b) c = b.to(torch.int32) print(c) tensor([[1, 1, 1], [1, 1, 1]], dtype=torch.int16) tensor([[ 0.9956, 1.4148, 5.8364], [11.2406, 11.2083, 11.6692]], dtype=torch.float64) tensor([[ 0, 1, 5], [11, 11, 11]], dtype=torch.int32) tensor\uc758 \uc790\ub8cc\ud615\uc744 \uc124\uc815\ud558\ub294 \uac00\uc7a5 \ub2e8\uc21c\ud55c \ubc29\uc2dd\uc740 \uc0dd\uc131\ud560 \ub54c \uc120\ud0dd\uc801 \uc778\uc790\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83 \uc785\ub2c8\ub2e4. \uc704\uc5d0 \uc788\ub294 cell\uc758 \uccab \ubc88\uc9f8 \uc904\uc744 \ubcf4\uba74, tensor a \ub97c dtype=torch.int16 \uc790\ub8cc\ud615\uc73c\ub85c \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4. a \ub97c \ucd9c\ub825\ud560 \ub54c, 1. \ub300\uc2e0\uc5d0 1 \ub85c \uac00\ub4dd \ucc2c \ubaa8\uc2b5\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4 - \ud30c\uc774\uc36c\uc5d0\uc11c \uc544\ub798 \uc810\uc774 \uc5c6\uc73c\uba74 \uc2e4\uc218 \uc790\ub8cc\ud615\uc774 \uc544\ub2cc \uc815\uc218 \uc790\ub8cc\ud615\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. a \ub97c \ucd9c\ub825\ud560 \ub54c \ub610 \ud55c\uac00\uc9c0 \uc8fc\ubaa9\ud560 \uc810\uc740, dtype \uc744 \uae30\ubcf8\uac12 (32-bit \ubd80\ub3d9 \uc18c\uc218\uc810) \uc73c\ub85c \ub0a8\uae38 \ub54c\uc640 \ub2e4\ub974\uac8c tensor\ub97c \ucd9c\ub825\ud558\ub294 \uacbd\uc6b0 \uac01 tensor\uc758 dtype \uc744 \uba85\uc2dc\ud55c\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. tensor\uc758 shape\ub97c \uc815\uc218\ud615 \uc778\uc790\uc758 \ub098\uc5f4, \uc989 \uc774 \uc778\uc790\ub97c tuple \uc790\ub8cc\ud615 \ud615\ud0dc\ub85c \ubb36\ub294\ub2e4\ub294 \uac83\uc744 \ubc1c\uacac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc740 \ubc18\ub4dc\uc2dc \ud544\uc694\ud55c \uac83\uc740 \uc544\ub2d9\ub2c8\ub2e4 - PyTorch\uc5d0\uc11c\ub294 \uccab \ubc88\uc9f8 \uc778\uc790\ub85c tensor shape\ub77c\ub294 \uac12\uc744 \uc758\ubbf8\ud558\ub294 \ub77c\ubca8\uc774 \uc5c6\ub294 \uc815\uc218 \uc778\uc790\ub97c \uc5ec\ub7ec\uac1c\ub97c \ubc1b\uc2b5\ub2c8\ub2e4 - \ud558\uc9c0\ub9cc \uc120\ud0dd \uc778\uc790\ub97c \ucd94\uac00\ud588\uc744 \ub54c, \uc774 \ubc29\uc2dd\uc740 \ucf54\ub4dc\ub97c \ub354 \uc77d\uae30 \uc27d\uac8c \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc790\ub8cc\ud615\uc744 \uc124\uc815\ud558\ub294 \ub2e4\ub978 \ubc29\ubc95\uc740 .to() \uba54\uc18c\ub4dc\ub791 \ud568\uaed8 \uc0ac\uc6a9\ud558\ub294 \uac83 \uc785\ub2c8\ub2e4. \uc704\ucabd \uc140\uc5d0\uc11c \ud3c9\ubc94\ud55c \ubc29\uc2dd\uc73c\ub85c \ubb34\uc791\uc704 \uc2e4\uc218 \uc790\ub8cc\ud615 tensor b \ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774\uc5b4\uc11c .to() \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud574\uc11c b \ub97c 32-bit \uc815\uc218 \uc790\ub8cc\ud615\uc73c\ub85c \ubcc0\ud658\ud55c c \ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. c \ub294 \ubaa8\ub4e0 b \uc758 \uac12\uacfc \uac19\uc740 \uac12\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc \uc18c\uc218\uc810 \uc544\ub798 \uc790\ub9ac\ub97c \ubc84\ub9b0\ub2e4\ub294 \uc810\uc774 \ub2e4\ub985\ub2c8\ub2e4. \uac00\ub2a5\ud55c \ub370\uc774\ud130 \uc790\ub8cc\ud615\uc740 \ub2e4\uc74c\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4: torch.bool torch.int8 torch.uint8 torch.int16 torch.int32 torch.int64 torch.half torch.float torch.double torch.bfloat PyTorch Tensor\uc5d0\uc11c \uc0b0\uc220 \u0026 \ub17c\ub9ac \uc5f0\uc0b0# \uc9c0\uae08\uae4c\uc9c0 tensor\ub97c \uc0dd\uc131\ud558\ub294 \uba87 \uac00\uc9c0 \ubc29\uc2dd\uc744 \uc54c\uc544\ubd24\uc2b5\ub2c8\ub2e4\u2026 \uc774\uac83\uc744 \uac00\uc9c0\uace0 \ubb34\uc5c7\uc744 \ud560 \uc218 \uc788\uc744\uae4c\uc694? \uba3c\uc800 \uae30\ubcf8\uc801\uc778 \uc0b0\uc220 \uc5f0\uc0b0\uc744 \uc54c\uc544\ubcf4\uace0, \uadf8 \ub2e4\uc74c tensor\uac00 \ub2e8\uc21c \uc2a4\uce7c\ub77c \uac12\uacfc \uc5b4\ub5bb\uac8c \uc0c1\ud638\uc791\uc6a9 \ud558\ub294\uc9c0 \uc54c\uc544\ubd05\uc2dc\ub2e4: ones = torch.zeros(2, 2) + 1 twos = torch.ones(2, 2) * 2 threes = (torch.ones(2, 2) * 7 - 1) / 2 fours = twos ** 2 sqrt2s = twos ** 0.5 print(ones) print(twos) print(threes) print(fours) print(sqrt2s) tensor([[1., 1.], [1., 1.]]) tensor([[2., 2.], [2., 2.]]) tensor([[3., 3.], [3., 3.]]) tensor([[4., 4.], [4., 4.]]) tensor([[1.4142, 1.4142], [1.4142, 1.4142]]) \uc704\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774 tensor\ub4e4\uacfc \uc2a4\uce7c\ub77c \uac12 \uc0ac\uc774 \uc0b0\uc220\uc5f0\uc0b0, \uc608\ub97c \ub4e4\uba74 \ub367\uc148, \ube84\uc148, \uacf1\uc148, \ub098\ub217\uc148 \uadf8\ub9ac\uace0 \uac70\ub4ed\uc81c\uacf1\uc740 tensor\uc758 \uac01 \uc6d0\uc18c\uc5d0 \ub098\ub220\uc11c \uacc4\uc0b0\uc744 \ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc5f0\uc0b0\uc758 \uacb0\uacfc\ub294 tensor\uac00 \ub420 \uac83\uc774\uae30 \ub54c\ubb38\uc5d0, threes \ubcc0\uc218\ub97c \uc0dd\uc131\ud558\ub294 \uc904\uc5d0\uc11c \ucc98\ub7fc \uc77c\ubc18\uc801\uc778 \uc5f0\uc0b0\uc790 \uc6b0\uc120\uc21c\uc704 \uaddc\uce59\uacfc \ud568\uaed8 \uc5f0\uc0b0\uc790\ub97c \uc5f0\uacb0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub450 tensor \uc0ac\uc774 \uc720\uc0ac\ud55c \uc5f0\uc0b0\ub3c4 \uc9c1\uad00\uc801\uc73c\ub85c \uc608\uc0c1\ud560 \uc218 \uc788\ub294 \ubc29\uc2dd\uc73c\ub85c \ub3d9\uc791\ud569\ub2c8\ub2e4: powers2 = twos ** torch.tensor([[1, 2], [3, 4]]) print(powers2) fives = ones + fours print(fives) dozens = threes * fours print(dozens) tensor([[ 2., 4.], [ 8., 16.]]) tensor([[5., 5.], [5., 5.]]) tensor([[12., 12.], [12., 12.]]) \uc5ec\uae30\uc11c \uc8fc\ubaa9\ud574\uc57c \ud560 \uc810\uc740 \uc774\uc804 \ucf54\ub4dc cell\uc5d0 \uc788\ub294 \ubaa8\ub4e0 tensor\ub294 \ub3d9\uc77c\ud55c shape\ub97c \uac00\uc838\uc57c \ud55c\ub2e4\ub294 \uac83 \uc785\ub2c8\ub2e4. \ub9cc\uc57d \uc11c\ub85c \ub2e4\ub978 shape\ub97c \uac00\uc9c4 tensor\ub07c\ub9ac \uc774\uc9c4 \uc5f0\uc0b0\uc744 \uc218\ud589\ud55c\ub2e4\uba74 \ubb34\uc2a8 \uc77c\uc774 \uc77c\uc5b4\ub0a0\uae4c\uc694? \ucc38\uace0 \ub2e4\uc74c cell\uc740 run-time error\uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4. \uc774\uac83\uc740 \uc758\ub3c4\ud55c \uac83\uc785\ub2c8\ub2e4. a = torch.rand(2, 3) b = torch.rand(3, 2) print(a * b) \uc77c\ubc18\uc801\uc778 \uacbd\uc6b0\uc5d0, \ub2e4\ub978 shape\uc758 tensor\ub97c \uc774\ub7ec\ud55c \ubc29\uc2dd\uc73c\ub85c \uc5f0\uc0b0\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. \uc2ec\uc9c0\uc5b4 \uc704\uc5d0 \uc788\ub294 cell\uc5d0 \uc788\ub294 \uacbd\uc6b0\ucc98\ub7fc tensor\uac00 \uc11c\ub85c \uac19\uc740 \uac1c\uc218\uc758 \uc6d0\uc18c\ub97c \uac00\uc9c0\uace0 \uc788\ub294 \uacbd\uc6b0\uc5d0\ub3c4 \uc5f0\uc0b0\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. \uac1c\uc694: Tensor Broadcasting# \ucc38\uace0 \ub9cc\uc57d NumPy\uc758 ndarrays\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 broadcasting \ubb38\ubc95\uc5d0 \uc775\uc219\ud558\ub2e4\uba74, \uc5ec\uae30\uc11c\ub3c4 \uac19\uc740 \uaddc\uce59\uc774 \uc801\uc6a9\ub41c\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. tensor\ub294 \uac19\uc740 shape\ub07c\ub9ac\ub9cc \uc5f0\uc0b0\uc774 \uac00\ub2a5\ud558\ub2e4\ub294 \uaddc\uce59\uc758 \uc608\uc678\uac00 \ubc14\ub85c tensor broadcasting \uc785\ub2c8\ub2e4. \ub2e4\uc74c\uc740 \uadf8 \uc608\uc2dc\uc785\ub2c8\ub2e4: rand = torch.rand(2, 4) doubled = rand * (torch.ones(1, 4) * 2) print(rand) print(doubled) tensor([[0.6146, 0.5999, 0.5013, 0.9397], [0.8656, 0.5207, 0.6865, 0.3614]]) tensor([[1.2291, 1.1998, 1.0026, 1.8793], [1.7312, 1.0413, 1.3730, 0.7228]]) \uc5ec\uae30\uc11c \ubb34\uc2a8 \ud2b8\ub9ad\uc774 \uc0ac\uc6a9\ub418\uace0 \uc788\ub294 \uac83\uc77c\uae4c\uc694? \uc5b4\ub5bb\uac8c 2x4 tensor\uc5d0 1x4 tensor\ub97c \uacf1\ud55c \uac12\uc744 \uc5bb\uc744 \uc218 \uc788\uc744\uae4c\uc694? Broadcasting\uc740 \uc11c\ub85c \ube44\uc2b7\ud55c shape\ub97c \uac00\uc9c4 tensor\uc0ac\uc774 \uc5f0\uc0b0\uc744 \uc218\ud589\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc704\uc758 \uc608\uc2dc\ub97c \ubcf4\uba74, \ud589\uc758 \uac12\uc774 1\uc774\uace0, \uc5f4\uc758 \uac12\uc774 4\uc778 tensor\uac00 \ud589\uc758 \uac12\uc774 2\uc774\uace0, \uc5f4\uc758 \uac12\uc774 4\uc778 tensor\uc758 \ubaa8\ub4e0 \ud589 \uc5d0 \uacf1\ud558\uac8c \ub429\ub2c8\ub2e4. \uc774\uac83\uc740 \ub525\ub7ec\ub2dd\uc5d0\uc11c \uc911\uc694\ud55c \uc5f0\uc0b0\uc785\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc778 \uc608\uc2dc\ub294 \ud559\uc2b5 \uac00\uc911\uce58 tensor\uc5d0 \uc785\ub825 tensor\uc758 \ubc30\uce58 \ub97c \uacf1\ud558\uace0, \ubc30\uce58\uc758 \uac01 \uc778\uc2a4\ud134\uc2a4\uc5d0 \uacf1\ud558\uae30 \uc5f0\uc0b0\uc744 \uac1c\ubcc4\uc801\uc73c\ub85c \uc801\uc6a9\ud55c \uc774\ud6c4 \uc704\uc758 (2, 4) * (1, 4) tensor\uc5f0\uc0b0\uc758 \uacb0\uacfc\uac00 (2, 4) shape tensor\uc778 \uac83\ucc98\ub7fc - \ub3d9\uc77c\ud55c shape\uc758 \ud559\uc2b5 \uac00\uc911\uce58 tensor\ub97c \ubc18\ud658\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. Broadcasting\uc758 \uaddc\uce59\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4: \uac01 tensor\ub294 \ucd5c\uc18c\ud55c 1\ucc28\uc6d0 \uc774\uc0c1\uc744 \ubc18\ub4dc\uc2dc \uac00\uc9c0\uace0 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4 - \ube48 tensor\ub294 \uc0ac\uc6a9\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. \ub450 tensor\uc758 \uac01 \ucc28\uc6d0 \ud06c\uae30 \uc6d0\uc18c\uac00 \ub2e4\uc74c \uc870\uac74\uc744 \ub9cc\uc871\ud558\ub294\uc9c0 \ud655\uc778\ud558\uba70 \ube44\uad50\ud569\ub2c8\ub2e4. \uc774\ub54c \ube44\uad50 \uc21c\uc11c\ub294 \ub9e8 \ub4a4\uc5d0\uc11c\ubd80\ud130 \ub9e8 \uc55e\uc73c\ub85c \uc785\ub2c8\ub2e4; \uac01 \ucc28\uc6d0\uc774 \uc11c\ub85c \ub3d9\uc77c\ud569\ub2c8\ub2e4, \ub610\ub294 \uac01 \ucc28\uc6d0\uc911\uc758 \ud558\ub098\uc758 \ud06c\uae30\uac00 \ubc18\ub4dc\uc2dc 1\uc785\ub2c8\ub2e4, \ub610\ub294 tensor\ub4e4 \uc911 \ud558\ub098\uc758 \ucc28\uc6d0\uc774 \uc874\uc7ac\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc774\uc804\uc5d0 \ubd24\ub358 \uac83\ucc98\ub7fc, \ubb3c\ub860 \ub3d9\uc77c\ud55c shape\ub97c \uac00\uc9c4 Tensor\ub4e4\uc740 \uc790\uba85\ud558\uac8c \u201cbroadcastable\u201d \ud569\ub2c8\ub2e4. \ub2e4\uc74c \uc608\uc81c\ub294 \uc704\uc758 \uaddc\uce59\uc744 \uc900\uc218\ud558\uace0 broadcasting\uc744 \ud5c8\uc6a9\ud558\ub294 \uba87 \uac00\uc9c0 \uc0c1\ud669\uc785\ub2c8\ub2e4. a = torch.ones(4, 3, 2) b = a * torch.rand( 3, 2) # \uc138\ubc88\uc9f8\uc640 \ub450\ubc88\uc9f8 \ucc28\uc6d0\uc774 a\ub791 \ub3d9\uc77c\ud558\uace0, \uccab\ubc88\uc9f8 \ucc28\uc6d0\uc740 \uc874\uc7ac\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. print(b) c = a * torch.rand( 3, 1) # \uc138\ubc88\uc9f8 \ucc28\uc6d0 = 1\uc774\uace0, \ub450\ubc88\uc9f8 \ucc28\uc6d0\uc740 a\ub791 \ub3d9\uc77c\ud569\ub2c8\ub2e4. print(c) d = a * torch.rand( 1, 2) # \uc138\ubc88\uc9f8 \ucc28\uc6d0\uc774 a\ub791 \ub3d9\uc77c\ud558\uace0, \ub450\ubc88\uc9f8 \ucc28\uc6d0 = 1\uc785\ub2c8\ub2e4. print(d) tensor([[[0.6493, 0.2633], [0.4762, 0.0548], [0.2024, 0.5731]], [[0.6493, 0.2633], [0.4762, 0.0548], [0.2024, 0.5731]], [[0.6493, 0.2633], [0.4762, 0.0548], [0.2024, 0.5731]], [[0.6493, 0.2633], [0.4762, 0.0548], [0.2024, 0.5731]]]) tensor([[[0.7191, 0.7191], [0.4067, 0.4067], [0.7301, 0.7301]], [[0.7191, 0.7191], [0.4067, 0.4067], [0.7301, 0.7301]], [[0.7191, 0.7191], [0.4067, 0.4067], [0.7301, 0.7301]], [[0.7191, 0.7191], [0.4067, 0.4067], [0.7301, 0.7301]]]) tensor([[[0.6276, 0.7357], [0.6276, 0.7357], [0.6276, 0.7357]], [[0.6276, 0.7357], [0.6276, 0.7357], [0.6276, 0.7357]], [[0.6276, 0.7357], [0.6276, 0.7357], [0.6276, 0.7357]], [[0.6276, 0.7357], [0.6276, 0.7357], [0.6276, 0.7357]]]) \uc704\uc758 \uc608\uc2dc\uc5d0 \uc788\ub294 \uac01 tensor\uc758 \uac12\uc744 \uc790\uc138\ud788 \uc0b4\ud3b4\ubd05\uc2dc\ub2e4: b \ub97c \ub9cc\ub4dc\ub294 \uacf1\uc148 \uc5f0\uc0b0\uc740 a \uc758 \ubaa8\ub4e0 \u201c\uacc4\uce35\u201d \uc5d0 broadcast \ub418\uc5c8\uc2b5\ub2c8\ub2e4. c \uc5d0 \ub300\ud574\uc11c, \uc5f0\uc0b0\uc740 a \uc758 \ubaa8\ub4e0 \uacc4\uce35\uacfc \ud589\uc5d0 \ub300\ud574\uc11c broadcast \ub418\uc5c8\uc2b5\ub2c8\ub2e4 - \ubaa8\ub4e0 \uc5f4\uc740 3\uac1c\uc758 \uc6d0\uc18c\uac12 \ubaa8\ub450 \ub3d9\uc77c\ud569\ub2c8\ub2e4. d \uc5d0 \ub300\ud574\uc11c, \uc5f0\uc0b0\uc774 \uc774\uc804\uacfc \ubc18\ub300\ub85c \ubaa8\ub4e0 \uacc4\uce35\uacfc \uc5f4\uc5d0 \ub300\ud574\uc11c \uc218\ud589\ud569\ub2c8\ub2e4 - \uc774\uc7ac \ubaa8\ub4e0 \ud589 \uc774 \ub3d9\uc77c\ud569\ub2c8\ub2e4. broadcasting\uc5d0 \ub300\ud55c \ub354 \ub9ce\uc740 \uc815\ubcf4\ub294, PyTorch documentation \uc5d0 \uc788\ub294 \uc8fc\uc81c\ub97c \ucc38\uace0\ud558\uc138\uc694. \ub2e4\uc74c \uc608\uc2dc\ub294 broadcasting \uc2dc\ub3c4\uac00 \uc2e4\ud328\ud55c \uc0ac\ub840\uc785\ub2c8\ub2e4: \ucc38\uace0 \ub2e4\uc74c cell\uc5d0\uc11c\ub294 run-time error\uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4. \uc774\uac83\uc740 \uc758\ub3c4\ud55c \uac83\uc785\ub2c8\ub2e4. a = torch.ones(4, 3, 2) b = a * torch.rand(4, 3) # \ucc28\uc6d0\uc740 \ubc18\ub4dc\uc2dc \ub9e8 \ub4a4 \uc6d0\uc18c\ubd80\ud130 \ub9e8 \uc55e \uc6d0\uc18c\ub85c \ucc28\ub840\ub300\ub85c \ub9de\ucdb0\uc57c \ud569\ub2c8\ub2e4. c = a * torch.rand( 2, 3) # \uc138\ubc88\uc9f8\uc640 \ub450\ubc88\uc9f8 \ucc28\uc6d0 \ubaa8\ub450 \uc11c\ub85c \ub2e4\ub985\ub2c8\ub2e4. d = a * torch.rand((0, )) # \ube44\uc5b4\uc788\ub294 tensor\ub294 broadcast \ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. Tensor\ub97c \uc0ac\uc6a9\ud558\ub294 \ub2e4\uc591\ud55c \uc5f0\uc0b0\ub4e4# PyTorch tensor\ub294 tensor\ub4e4 \ub07c\ub9ac \uc218\ud589\ud560 \uc218 \uc788\ub294 300\uac1c \uc774\uc0c1\uc758 \uc5f0\uc0b0\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \uc791\uc740 \uc608\uc2dc\ub294 \uc8fc\ub85c \uc0ac\uc6a9\ud558\ub294 \uc5f0\uc0b0 \uc885\ub958 \uba87 \uac1c\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4: # \uacf5\uc6a9 \ud568\uc218 a = torch.rand(2, 4) * 2 - 1 print(\u0027Common functions:\u0027) print(torch.abs(a)) print(torch.ceil(a)) print(torch.floor(a)) print(torch.clamp(a, -0.5, 0.5)) # \uc0bc\uac01 \ud568\uc218\uc640 \uadf8 \uc5ed\ud568\uc218\ub4e4 angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4]) sines = torch.sin(angles) inverses = torch.asin(sines) print(\u0027\\nSine and arcsine:\u0027) print(angles) print(sines) print(inverses) # \ube44\ud2b8 \uc5f0\uc0b0 print(\u0027\\nBitwise XOR:\u0027) b = torch.tensor([1, 5, 11]) c = torch.tensor([2, 7, 10]) print(torch.bitwise_xor(b, c)) # \ube44\uad50 \uc5f0\uc0b0: print(\u0027\\nBroadcasted, element-wise equality comparison:\u0027) d = torch.tensor([[1., 2.], [3., 4.]]) e = torch.ones(1, 2) # \ub9ce\uc740 \ube44\uad50 \uc5f0\uc0b0\uc790\ub4e4\uc740 broadcasting\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4! print(torch.eq(d, e)) # bool \uc790\ub8cc\ud615\uc744 \uac00\uc9c4 tensor\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4. # \ucc28\uc6d0 \uac10\uc18c \uc5f0\uc0b0: print(\u0027\\nReduction ops:\u0027) print(torch.max(d)) # \ub2e8\uc77c \uc6d0\uc18c tensor\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4. print(torch.max(d).item()) # \ubc18\ud658\ud55c tensor\ub85c\ubd80\ud130 \uac12\uc744 \ucd94\ucd9c\ud569\ub2c8\ub2e4. print(torch.mean(d)) # \ud3c9\uade0 print(torch.std(d)) # \ud45c\uc900 \ud3b8\ucc28 print(torch.prod(d)) # \ubaa8\ub4e0 \uc22b\uc790\uc758 \uacf1 print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # \uc911\ubcf5\ub418\uc9c0 \uc54a\uc740 \uac12\ub4e4\uc744 \uac78\ub7ec\ub0c5\ub2c8\ub2e4. # \ubca1\ud130\uc640 \uc120\ud615 \ub300\uc218 \uc5f0\uc0b0 v1 = torch.tensor([1., 0., 0.]) # x\ucd95 \ub2e8\uc704 \ubca1\ud130 v2 = torch.tensor([0., 1., 0.]) # y\ucd95 \ub2e8\uc704 \ubca1\ud130 m1 = torch.rand(2, 2) # \ubb34\uc791\uc704 \ud589\ub82c m2 = torch.tensor([[3., 0.], [0., 3.]]) # \ub2e8\uc704 \ud589\ub82c\uc5d0 3\uc744 \uacf1\ud55c \uacb0\uacfc print(\u0027\\nVectors \u0026 Matrices:\u0027) print(torch.cross(v2, v1)) # z\ucd95 \ub2e8\uc704 \ubca1\ud130\uc758 \uc74c\uc218\uac12 (v1 x v2 == -v2 x v1) print(m1) m3 = torch.matmul(m1, m2) print(m3) # m1 \ud589\ub82c\uc744 3\ubc88 \uacf1\ud55c \uacb0\uacfc print(torch.svd(m3)) # \ud2b9\uc774\uac12 \ubd84\ud574 Common functions: tensor([[0.9238, 0.5724, 0.0791, 0.2629], [0.1986, 0.4439, 0.6434, 0.4776]]) tensor([[-0., -0., 1., -0.], [-0., 1., 1., -0.]]) tensor([[-1., -1., 0., -1.], [-1., 0., 0., -1.]]) tensor([[-0.5000, -0.5000, 0.0791, -0.2629], [-0.1986, 0.4439, 0.5000, -0.4776]]) Sine and arcsine: tensor([0.0000, 0.7854, 1.5708, 2.3562]) tensor([0.0000, 0.7071, 1.0000, 0.7071]) tensor([0.0000, 0.7854, 1.5708, 0.7854]) Bitwise XOR: tensor([3, 2, 1]) Broadcasted, element-wise equality comparison: tensor([[ True, False], [False, False]]) Reduction ops: tensor(4.) 4.0 tensor(2.5000) tensor(1.2910) tensor(24.) tensor([1, 2]) Vectors \u0026 Matrices: /workspace/tutorials-kr/beginner_source/introyt/tensors_deeper_tutorial.py:443: UserWarning: Using torch.cross without specifying the dim arg is deprecated. Please either pass the dim explicitly or simply use torch.linalg.cross. The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /pytorch/aten/src/ATen/native/Cross.cpp:63.) tensor([ 0., 0., -1.]) tensor([[0.7375, 0.8328], [0.8444, 0.2941]]) tensor([[2.2125, 2.4985], [2.5332, 0.8822]]) torch.return_types.svd( U=tensor([[-0.7889, -0.6145], [-0.6145, 0.7889]]), S=tensor([4.1498, 1.0548]), V=tensor([[-0.7957, 0.6056], [-0.6056, -0.7957]])) \uc774\uac83\uc740 \uc218\ub9ce\uc740 \uc5f0\uc0b0\uc758 \uc77c\ubd80\ubd84\uc785\ub2c8\ub2e4. \ub354 \uc790\uc138\ud55c \ub0b4\uc6a9\uc774\ub098 \uc218\ud559 \ud568\uc218\uc758 \uc804\uccb4\uc801\uc778 \ubaa9\ub85d\uc740, \ub2e4\uc74c documentation \ub97c \uc77d\uc5b4\uc8fc\uc138\uc694. Tensor\uc758 \uac12\uc744 \ubcc0\uacbd\ud558\uae30# \ub300\ubd80\ubd84 tensor\ub4e4\uc758 \uc774\uc9c4 \uc5f0\uc0b0\uc740 \uc81c3\uc790\uc758 \uc0c8\ub85c\uc6b4 tensor\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. c = a * b ( a \uc640 b \ub294 tensor)\uc5f0\uc0b0\uc744 \uc218\ud589\ud560 \ub54c, \uc0c8\ub85c\uc6b4 tensor c \ub294 \ub2e4\ub978 tensor\uc640 \uad6c\ubcc4\ub418\ub294 \uba54\ubaa8\ub9ac \uc601\uc5ed\uc744 \ucc28\uc9c0\ud558\uac8c \ub429\ub2c8\ub2e4. \uadf8\ub7fc\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 tensor\uc758 \uac12\uc744 \ubcc0\uacbd\ud558\uace0 \uc2f6\uc740 \uc21c\uac04\uc774 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4 - \uc608\ub97c \ub4e4\uc5b4, \uc911\uac04 \uc5f0\uc0b0 \uacb0\uacfc \uac12\uc744 \ubc84\ub9b4 \uc218 \uc788\ub294 \uac01 \uc6d0\uc18c \ub2e8\uc704 \uc5f0\uc0b0\uc744 \uc218\ud589\ud558\ub294 \uacbd\uc6b0\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7f0 \uc5f0\uc0b0\uc744 \uc704\ud574, \ub300\ubd80\ubd84\uc758 \uc218\ud559 \ud568\uc218\ub4e4\uc740 tensor \ub0b4\ubd80\uc758 \uac12\uc744 \ubcc0\uacbd\ud560 \uc218 \uc788\ub294 \ud568\uc218 \uc774\ub984 \ub9e8 \ub4a4\uc5d0 \ubc11\uc904 (_)\uc774 \ucd94\uac00\ub41c \ubc84\uc804\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc608\uc2dc: a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4]) print(\u0027a:\u0027) print(a) print(torch.sin(a)) # \uc774 \uc5f0\uc0b0\uc740 \uba54\ubaa8\ub9ac\uc5d0 \uc0c8\ub85c\uc6b4 tensor\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. print(a) # a\ub294 \ubcc0\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4]) print(\u0027\\nb:\u0027) print(b) print(torch.sin_(b)) # \ubc11\uc904\uc5d0 \uc8fc\ubaa9\ud558\uc138\uc694. print(b) # b\uac00 \ubcc0\ud569\ub2c8\ub2e4. a: tensor([0.0000, 0.7854, 1.5708, 2.3562]) tensor([0.0000, 0.7071, 1.0000, 0.7071]) tensor([0.0000, 0.7854, 1.5708, 2.3562]) b: tensor([0.0000, 0.7854, 1.5708, 2.3562]) tensor([0.0000, 0.7071, 1.0000, 0.7071]) tensor([0.0000, 0.7071, 1.0000, 0.7071]) \uc0b0\uc220 \uc5f0\uc0b0\uc5d0\uc11c, \ube44\uc2b7\ud55c \ud589\ub3d9\uc744 \ud558\ub294 \ud568\uc218\uac00 \uc788\uc2b5\ub2c8\ub2e4: a = torch.ones(2, 2) b = torch.rand(2, 2) print(\u0027Before:\u0027) print(a) print(b) print(\u0027\\nAfter adding:\u0027) print(a.add_(b)) print(a) print(b) print(\u0027\\nAfter multiplying\u0027) print(b.mul_(b)) print(b) Before: tensor([[1., 1.], [1., 1.]]) tensor([[0.3788, 0.4567], [0.0649, 0.6677]]) After adding: tensor([[1.3788, 1.4567], [1.0649, 1.6677]]) tensor([[1.3788, 1.4567], [1.0649, 1.6677]]) tensor([[0.3788, 0.4567], [0.0649, 0.6677]]) After multiplying tensor([[0.1435, 0.2086], [0.0042, 0.4459]]) tensor([[0.1435, 0.2086], [0.0042, 0.4459]]) \uc774\ub7ec\ud55c \ub0b4\ubd80\uc758 \uac12\uc744 \ubcc0\uacbd\ud558\ub294 \uc0b0\uc220 \ud568\uc218\ub294 \ub2e4\ub978 \ub9ce\uc740 \ud568\uc218\ub4e4 (e.g., torch.sin())\ucc98\ub7fc torch \ubaa8\ub4c8\uc758 \uba54\uc18c\ub4dc\uac00 \uc544\ub2c8\ub77c torch.Tensor \uac1d\uccb4\uc758 \uba54\uc18c\ub4dc\uc778 \uc810\uc5d0 \uc8fc\ubaa9\ud574\uc57c \ud569\ub2c8\ub2e4. a.add_(b) \uc640 \uac19\uc740 \uacbd\uc6b0\ucc98\ub7fc, \uba54\uc18c\ub4dc\ub97c \ud638\ucd9c\ud558\ub294 tensor\ub294 \uac12\uc774 \ubcc0\uacbd\ub429\ub2c8\ub2e4. \uc774\ubbf8 \uc874\uc7ac\ud558\uace0 \uc788\ub294 \uba54\ubaa8\ub9ac\uc5d0 \ud560\ub2f9\ub41c tensor\uc5d0 \uacc4\uc0b0 \uacb0\uacfc\uac12\uc744 \uc800\uc7a5\ud558\ub294 \ub610 \ub2e4\ub978 \uc635\uc158\uc774 \uc788\uc2b5\ub2c8\ub2e4. tensor\ub97c \uc0dd\uc131\ud558\ub294 \uba54\uc18c\ub4dc \ubfd0\ub9cc \uc544\ub2c8\ub77c \uc9c0\uae08\uae4c\uc9c0 \uc774 \ubb38\uc11c\uc5d0\uc11c \ubd24\ub358 \uc218\ub9ce\uc740 \ud568\uc218\ub098 \uba54\uc18c\ub4dc\ub294 \uacb0\uacfc \uac12\uc744 \ubc1b\ub294 \ud2b9\uc815 tensor\ub97c \uba85\uc2dc\ud558\ub294 out \uc774\ub77c\ub294 \uc778\uc790\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ub9cc\uc57d out tensor\uac00 \uc54c\ub9de\uc740 shape\uc640 dtype \uc744 \uac00\uc9c0\uace0 \uc788\ub2e4\uba74, \uc0c8\ub85c\uc6b4 \uba54\ubaa8\ub9ac \ud560\ub2f9 \uc5c6\uc774 \uacb0\uacfc\uac12\uc774 \uc800\uc7a5\ub429\ub2c8\ub2e4: a = torch.rand(2, 2) b = torch.rand(2, 2) c = torch.zeros(2, 2) old_id = id(c) print(c) d = torch.matmul(a, b, out=c) print(c) # c\uc758 \uac12\uc774 \ubcc0\uacbd\ub418\uc5c8\uc2b5\ub2c8\ub2e4. assert c is d # c\uc640 d\uac00 \uc11c\ub85c \ub2e8\uc21c\ud788 \uac19\uc740 \uac12\uc744 \uac00\uc9c0\ub294\uc9c0\uac00 \uc544\ub2c8\ub77c \uac19\uc740 \uac1d\uccb4\uc778\uc9c0 \ud14c\uc2a4\ud2b8\ud569\ub2c8\ub2e4. assert id(c) == old_id # \uc0c8\ub85c\uc6b4 c\ub294 \uc774\uc804 \uac1d\uccb4\uc640 \ud655\uc2e4\ud788 \uac19\uc740 \uac1d\uccb4\uc785\ub2c8\ub2e4. torch.rand(2, 2, out=c) # \ub2e4\uc2dc \ud55c\ubc88 \uc0dd\uc131\ud574\ubd05\uc2dc\ub2e4! print(c) # c\uc758 \uac12\uc774 \ub2e4\uc2dc \ubc14\ub00c\uc5c8\uc2b5\ub2c8\ub2e4. assert id(c) == old_id # \ud558\uc9c0\ub9cc \uc5ec\uc804\ud788 \uac19\uc740 \uac1d\uccb4\ub124\uc694! tensor([[0., 0.], [0., 0.]]) tensor([[0.3653, 0.8699], [0.2364, 0.3604]]) tensor([[0.0776, 0.4004], [0.9877, 0.0352]]) Tensor\ub97c \ubcf5\uc0ac\ud558\uae30# \ud30c\uc774\uc36c\uc758 \ub2e4\ub978 \uac1d\uccb4\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c \ubcc0\uc218\uc5d0 tensor\ub97c \ud560\ub2f9\ud558\ub294 \uac83\uc740 \ubcc0\uc218\uac00 tensor\uc758 label \uc774 \ub418\uace0 \uac12\uc744 \ubcf5\uc0ac\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \uc608\uc2dc\ub97c \ubcf4\uc2dc\uc8e0: a = torch.ones(2, 2) b = a a[0][1] = 561 # a\uc758 \uac12\uc744 \ubc14\uafb8\uba74... print(b) # ...b\uc758 \uac12\uc774 \ubc14\ub01d\ub2c8\ub2e4. tensor([[ 1., 561.], [ 1., 1.]]) \ud558\uc9c0\ub9cc \ub9cc\uc57d \uc6b0\ub9ac\uac00 \uc791\uc5c5\ud560 \ubcc4\ub3c4\uc758 \ub370\uc774\ud130 \ubcf5\uc0ac\ubcf8\uc744 \uc6d0\ud558\uba74 \uc5b4\ub5bb\uac8c \ud574\uc57c\ud560\uae4c\uc694? clone() \uba54\uc18c\ub4dc\uac00 \ub2f9\uc2e0\uc774 \ucc3e\ub358 \ud574\ub2f5\uc774 \ub420 \uac83\uc785\ub2c8\ub2e4: a = torch.ones(2, 2) b = a.clone() assert b is not a # \uba54\ubaa8\ub9ac \uc0c1\uc758 \ub2e4\ub978 \uac1d\uccb4\uc785\ub2c8\ub2e4... print(torch.eq(a, b)) # ...\ud558\uc9c0\ub9cc \uc5ec\uc804\ud788 \uac19\uc740 \uac12\uc744 \uac00\uc9c0\uace0 \uc788\ub124\uc694! a[0][1] = 561 # a\uac00 \ubcc0\uacbd\ub418\uc5c8\uc2b5\ub2c8\ub2e4... print(b) # ...\ud558\uc9c0\ub9cc \uc5ec\uc804\ud788 b\ub294 \uc774\uc804 \uac12\uc744 \uac00\uc9c0\uace0 \uc788\ub124\uc694. tensor([[True, True], [True, True]]) tensor([[1., 1.], [1., 1.]]) ``clone()`` \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud560 \ub54c \uc54c\uc544\uc57c \ud560 \uc911\uc694\ud55c \uc810\uc774 \uc788\uc2b5\ub2c8\ub2e4. \ub9cc\uc57d source tensor\uac00 autograd\ub97c \uac00\uc9c4\ub2e4\uba74 clone\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. \uc774 \ubd80\ubd84\uc740 autograd\uc640 \uad00\ub828\ub41c \ub3d9\uc601\uc0c1\uc5d0\uc11c \ub354 \uae4a\uc774 \ub2e4\ub8f0 \uac83 \uc785\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \ub9cc\uc57d \uc790\uc138\ud55c \ub0b4\uc6a9\uc744 \uac04\ub2e8\ud788 \uc54c\uace0 \uc2f6\ub2e4\uba74 \uacc4\uc18d \uc124\uba85\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ub300\ubd80\ubd84\uc758 \uacbd\uc6b0\uc5d0\uc11c \uc774\uac83\uc774 \ubc14\ub85c \uc5ec\ub7ec\ubd84\uc774 \uc6d0\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \ub9cc\uc57d \uc5ec\ub7ec\ubd84\uc758 \ubaa8\ub378\uc774 \uadf8 \ubaa8\ub378\uc758 forward() \uba54\uc18c\ub4dc\uc5d0 \uc5ec\ub7ec \uac08\ub798\uc758 \uacc4\uc0b0 \uacbd\ub85c\uac00 \uc788\uace0 \uc6d0\ubcf8 tensor\uc640 \uadf8\uac83\uc758 \ubcf5\uc81c\ubcf8 \ubaa8\ub450 \uac00 \ubaa8\ub378\uc758 \ucd9c\ub825\uc5d0 \uae30\uc5ec\ub97c \ud55c\ub2e4\uba74, \ub450 tensor\uc5d0 \ub300\ud55c autograd\ub97c \uc124\uc815\ud558\ub294 \ubaa8\ub378 \ud559\uc2b5\uc744 \ud65c\uc131\ud654 \ud569\ub2c8\ub2e4. \ub9cc\uc57d \uc5ec\ub7ec\ubd84\uc758 source tensor\uac00 autograd\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4\uba74 (\uc77c\ubc18\uc801\uc73c\ub85c \ud559\uc2b5 \uac00\uc911\uce58\uc758 \uc9d1\ud569\uc774\uac70\ub098, \uac00\uc911\uce58\ub97c \ud3ec\ud568\ud558\ub294 \uacc4\uc0b0\uc5d0\uc11c \ud30c\uc0dd\ub41c \uacbd\uc6b0), \uc5ec\ub7ec\ubd84\uc774 \uc6d0\ud558\ub294 \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubc18\uba74\uc5d0 \uc6d0\ubcf8 tensor\ub098 \uadf8\uac83\uc758 \ubcf5\uc81c\ubcf8 \ubaa8\ub450 \uac00 \ubcc0\ud654\ub3c4\ub97c \ucd94\uc801\ud560 \ud544\uc694\uac00 \uc5c6\ub2e4\uba74, source tensor\uc758 autograd\uac00 \uaebc\uc838\uc788\ub2e4\uba74 clone\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc138\ubc88\uc9f8 \uacbd\uc6b0 \uac00 \uc788\uc2b5\ub2c8\ub2e4: \uae30\ubcf8\uc801\uc73c\ub85c \ubcc0\ud654\ub3c4\uac00 \ubaa8\ub4e0 \uac83\uc744 \uc704\ud574 \ucf1c\uc838\uc788\uc9c0\ub9cc \uc77c\ubd80 \uc9c0\ud45c\ub97c \uc0dd\uc131\ud558\uae30 \uc704\ud574\uc11c \uc2a4\ud2b8\ub9bc \uc911\uac04\uc5d0\uc11c \uc77c\ubd80 \uac12\uc744 \uc0dd\uc131\ud558\uace0 \uc2f6\uc5b4 \ud558\ub294 \uc5ec\ub7ec\ubd84 \ubaa8\ub378\uc758 forward() \ud568\uc218\uc5d0\uc11c \uacc4\uc0b0\uc744 \uc218\ud589\ud55c\ub2e4\uace0 \uc0c1\uc0c1\ud574 \ubcf4\uc138\uc694. \uc774 \uacbd\uc6b0\uc5d0\ub294 \ubcc0\ud654\ub3c4\ub97c \ucd94\uc801\ud558\uae30 \uc704\ud574\uc11c source tensor\uc758 \ubcf5\uc81c\ubcf8\uc744 \uc6d0\ud558\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4 - \uc131\ub2a5\uc774 autograd\uc758 \ud788\uc2a4\ud1a0\ub9ac \ucd94\uc801 \uae30\ub2a5\uc744 \ub044\uba74\uc11c \ud5a5\uc0c1\ub429\ub2c8\ub2e4. \uc774 \uacbd\uc6b0\ub97c \uc704\ud574\uc11c\ub294 source tensor\uc5d0 .detach() \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: a = torch.rand(2, 2, requires_grad=True) # autograd\ub97c \ucf2d\ub2c8\ub2e4. print(a) b = a.clone() print(b) c = a.detach().clone() print(c) print(a) tensor([[0.0905, 0.4485], [0.8740, 0.2526]], requires_grad=True) tensor([[0.0905, 0.4485], [0.8740, 0.2526]], grad_fn=\u003cCloneBackward0\u003e) tensor([[0.0905, 0.4485], [0.8740, 0.2526]]) tensor([[0.0905, 0.4485], [0.8740, 0.2526]], requires_grad=True) \uc5ec\uae30\uc11c \ubb34\uc2a8 \uc77c\uc774 \uc77c\uc5b4\ub098\ub294\uac78\uae4c\uc694? a \ub97c requires_grad=True \uc635\uc158\uc744 \ud0a8 \uc0c1\ud0dc\ub85c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc544\uc9c1 \uc774 \uc120\ud0dd\uc801 \uc778\uc790\ub97c \ub2e4\ub8e8\uc9c0 \uc54a\uc558\uc9c0\ub9cc, autograd \ub2e8\uc6d0 \ub3d9\uc548\ub9cc \ub2e4\ub8f0 \uac83\uc785\ub2c8\ub2e4. a \ub97c \ucd9c\ub825\ud560 \ub54c, requires_grad=True \uc18d\uc131\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4\uace0 \uc54c\ub824\uc90d\ub2c8\ub2e4 - \uc774 \ub73b\uc740 autograd\uc640 \uacc4\uc0b0 \ud788\uc2a4\ud1a0\ub9ac \ucd94\uc801 \uae30\ub2a5\uc744 \ucf20\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. a \ub97c \ubcf5\uc81c\ud558\uace0 \uadf8\uac83\uc744 b \ub77c\uace0 \ub77c\ubca8\uc744 \ubd99\uc785\ub2c8\ub2e4. b \ub97c \ucd9c\ub825\ud560 \ub54c, \uadf8\uac83\uc758 \uacc4\uc0b0 \ud788\uc2a4\ud1a0\ub9ac\uac00 \ucd94\uc801\ub418\uace0 \uc788\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4 - a \uc758 autograd \uc124\uc815\uc5d0 \ub0b4\uc7a5\ub418\uc5b4 \uc788\ub294 \uae30\ub2a5\uc774\uba70, \uc774 \uc124\uc815\uc740 \uacc4\uc0b0 \ud788\uc2a4\ud1a0\ub9ac\uc5d0 \ucd94\uac00\ud569\ub2c8\ub2e4. a \ub97c c \uc5d0 \ubcf5\uc81c\ub97c \ud558\uc9c0\ub9cc detach() \ub97c \uba3c\uc800 \ud638\ucd9c\uc744 \ud569\ub2c8\ub2e4. c \ub97c \ucd9c\ub825\ud569\ub2c8\ub2e4. \uacc4\uc0b0 \ud788\uc2a4\ud1a0\ub9ac\uac00 \uc5c6\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uace0, requires_grad=True \uc635\uc158\uc774 \uc5c6\ub2e4\ub294 \uac83 \ub610\ud55c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. detach() \uba54\uc18c\ub4dc\ub294 tensor\uc758 \uacc4\uc0b0 \ud788\uc2a4\ud1a0\ub9ac\ub85c \ubd80\ud130 tensor\ub97c \ub5bc\uc5b4\ub0c5\ub2c8\ub2e4. \uc774 \uba54\uc18c\ub4dc\uc758 \uc758\ubbf8\ub294 \u201c\uba54\uc18c\ub4dc \ub4a4\uc5d0 \uc5b4\ub5a4 \uac83\uc774\ub4e0 \uc640\ub3c4 autograd\ub97c \ub048 \uac83\ucc98\ub7fc \uc791\ub3d9\ud558\ub77c.\u201c \ub77c\ub294 \ub73b\uc785\ub2c8\ub2e4. a \ub97c \ubcc0\uacbd\ud558\uc9c0 \uc54a\uace0 \uc774 \uba54\uc18c\ub4dc\ub97c \uc218\ud589\ud569\ub2c8\ub2e4 - \ub9c8\uc9c0\ub9c9\uc5d0 a \ub97c \ub2e4\uc2dc \ucd9c\ub825\ud560 \ub54c, \uc5ec\uc804\ud788 a \uac00 \uac00\uc9c4 requires_grad=True \uc18d\uc131\uc774 \ub0a8\uc544 \uc788\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. GPU \ud658\uacbd\uc73c\ub85c \uc774\ub3d9\ud558\uae30# PyTorch\uc758 \uc8fc\ub41c \uc7a5\uc810\uc911 \ud558\ub098\ub294 CUDA\uac00 \ud638\ud658\ub418\ub294 Nvidia GPU\uc5d0\uc11c\uc758 \uac15\ub825\ud55c \uc131\ub2a5 \uac00\uc18d\ud654\uc785\ub2c8\ub2e4. (\u201cCUDA\u201d \ub294 Compute Unified Device Architecture \uc758 \uc57d\uc790\uc774\uba70, \ubcd1\ub82c \ucef4\ud4e8\ud305\uc744 \uc704\ud55c Nvidia\uc758 \ud50c\ub7ab\ud3fc\uc785\ub2c8\ub2e4.) \uc9c0\uae08\uae4c\uc9c0 \ubaa8\ub4e0 \uc791\uc5c5\uc744 CPU\uc5d0\uc11c \ucc98\ub9ac\ud588\uc2b5\ub2c8\ub2e4. \uc5b4\ub5bb\uac8c \ub354 \ube60\ub978 \ud558\ub4dc\uc6e8\uc5b4\ub85c \uc774\ub3d9\ud560 \uc218 \uc788\uc744\uae4c\uc694? \uba3c\uc800 is_available() \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud574\uc11c GPU\uac00 \uc0ac\uc6a9 \uac00\ub2a5\ud55c\uc9c0 \uc544\ub2cc\uc9c0 \ud655\uc778\ud574\uc57c \ud569\ub2c8\ub2e4. \ucc38\uace0 \ub9cc\uc57d CUDA\uac00 \ud638\ud658\ub418\ub294 GPU\uac00 \uc5c6\uace0 CUDA \ub4dc\ub77c\uc774\ubc84\uac00 \uc124\uce58\ub418\uc5b4\uc788\uc9c0 \uc54a\ub2e4\uba74 \uc774 \uc139\uc158\uc5d0\uc11c\uc758 \uc2e4\ud589 \uac00\ub2a5\ud55c cell\uc740 \uc5b4\ub5a4 GPU\uc640 \uad00\ub828\ub41c \ucf54\ub4dc\ub3c4 \uc2e4\ud589\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. if torch.cuda.is_available(): print(\u0027We have a GPU!\u0027) else: print(\u0027Sorry, CPU only.\u0027) We have a GPU! \uc77c\ub2e8 1\uac1c \ud639\uc740 \uadf8 \uc774\uc0c1\uc758 GPU\uac00 \uc0ac\uc6a9 \uac00\ub2a5\ud558\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud588\ub2e4\uba74, \ub370\uc774\ud130\ub97c GPU\uac00 \ud655\uc778\ud560 \uc218 \uc788\ub294 \uc5b4\ub5a4 \uacf5\uac04\uc5d0 \uc800\uc7a5\ud560 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4. CPU\ub294 \ucef4\ud4e8\ud130\uc758 RAM\uc5d0\uc11c \ub370\uc774\ud130\ub97c \uc774\uc6a9\ud574\uc11c \uacc4\uc0b0\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. GPU\ub294 \uc804\uc6a9 \uba54\ubaa8\ub9ac\uac00 \uc5f0\uacb0\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ud574\ub2f9 \uc7a5\uce58\uc5d0\uc11c \uacc4\uc0b0\uc744 \uc218\ud589\ud558\uace0 \uc2f6\uc744 \ub54c\ub9c8\ub2e4 \uacc4\uc0b0\uc5d0 \ud544\uc694\ud55c \ubaa8\ub4e0 \ub370\uc774\ud130\ub97c GPU\uc7a5\uce58\uac00 \uc811\uadfc \uac00\ub2a5\ud55c \uba54\ubaa8\ub9ac\ub85c \uc774\ub3d9\ud574\uc57c \ud569\ub2c8\ub2e4. (\ud3c9\uc18c\uc5d0\ub294 \u201cGPU\uac00 \uc811\uadfc \uac00\ub2a5\ud55c \uba54\ubaa8\ub9ac\ub85c \ub370\uc774\ud130\ub97c \uc774\ub3d9\ud55c\ub2e4\u201c \ub97c \u201c\ub370\uc774\ud130\ub97c GPU\ub85c \uc62e\uae34\ub2e4\u201c \ub77c\uace0 \uc904\uc5ec\uc11c \ub9d0\ud569\ub2c8\ub2e4.) \ubaa9\uc801 \uc7a5\uce58\uc5d0\uc11c \ub370\uc774\ud130\ub97c \uac00\uc838\uc624\ub294 \ub2e4\uc591\ud55c \ubc29\ubc95\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uac1d\uccb4\ub97c \uc0dd\uc131\ud560 \ub54c \ub370\uc774\ud130\ub97c \uac00\uc838\uc62c \uc218 \uc788\uc2b5\ub2c8\ub2e4: if torch.cuda.is_available(): gpu_rand = torch.rand(2, 2, device=\u0027cuda\u0027) print(gpu_rand) else: print(\u0027Sorry, CPU only.\u0027) tensor([[0.3344, 0.2640], [0.2119, 0.0582]], device=\u0027cuda:0\u0027) \uae30\ubcf8\uc801\uc73c\ub85c \uc0c8\ub85c\uc6b4 tensor\ub294 CPU\uc5d0 \uc0dd\uc131\ub429\ub2c8\ub2e4. \ub530\ub77c\uc11c tensor\ub97c GPU\uc5d0 \uc0dd\uc131\ud558\uace0 \uc2f6\uc744 \ub54c device \uc120\ud0dd \uc778\uc790\ub97c \ubc18\ub4dc\uc2dc \uba85\uc2dc\ud574\uc918\uc57c \ud569\ub2c8\ub2e4. \uc0c8\ub85c\uc6b4 tensor\ub97c \ucd9c\ub825\ud560 \ub54c, (\ub9cc\uc57d CPU\uc5d0 \uc874\uc7ac\ud558\uc9c0 \uc54a\ub294\ub2e4\uba74) PyTorch\ub294 \uc5b4\ub290 \uc7a5\uce58\uc5d0 \uac1d\uccb4\uac00 \uc788\ub294\uc9c0 \uc54c\ub824\uc900\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. torch.cuda.device_count() \ub97c \uc0ac\uc6a9\ud574\uc11c GPU\uc758 \uac1c\uc218\ub97c \uc870\ud68c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9cc\uc57d 1\uac1c\ubcf4\ub2e4 \ub9ce\uc740 GPU\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4\uba74, \uac01 GPU\ub97c \uc778\ub371\uc2a4\ub85c \uc9c0\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: device=\u0027cuda:0\u0027, device=\u0027cuda:1\u0027, \uc640 \uac19\uc774 \ub9d0\uc774\uc8e0. \ucf54\ub529\uc744 \ud560 \ub54c, \uc5b4\ub514\uc5d0\uc11c\ub098 \uc7a5\uce58 \uc774\ub984\uc744 \ubb38\uc790\uc5f4 \uc0c1\uc218\ub85c \uc9c0\uc815\ud558\ub294 \uac83\uc740 \uc0c1\ub2f9\ud788 \uc720\uc9c0 \ubcf4\uc218\uc5d0 \ucde8\uc57d\ud569\ub2c8\ub2e4. CPU \ud558\ub4dc\uc6e8\uc5b4\ub098 GPU \ud558\ub4dc\uc6e8\uc5b4 \uc5b4\ub5a4 \uac83\uc744 \uc0ac\uc6a9\ud558\ub294\uc9c0\uc5d0 \uad00\uacc4\uc5c6\uc774 \uc5ec\ub7ec\ubd84\uc758 \ucf54\ub4dc\ub294 \uc798 \uc791\ub3d9\ud574\uc57c \ud569\ub2c8\ub2e4. \ubb38\uc790\uc5f4 \ub300\uc2e0\uc5d0 tensor\ub97c \uc800\uc7a5\ud560 \uc7a5\uce58 \ud578\ub4e4\ub7ec\ub97c \uc0dd\uc131\ud558\ub294 \uac83\uc73c\ub85c \uc720\uc9c0 \ubcf4\uc218\uac00 \uc26c\uc6b4 \ucf54\ub4dc\ub97c \uc791\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: if torch.cuda.is_available(): my_device = torch.device(\u0027cuda\u0027) else: my_device = torch.device(\u0027cpu\u0027) print(\u0027Device: {}\u0027.format(my_device)) x = torch.rand(2, 2, device=my_device) print(x) Device: cuda tensor([[0.0024, 0.6778], [0.2441, 0.6812]], device=\u0027cuda:0\u0027) \ub9cc\uc57d \ud55c \uc7a5\uce58\uc5d0 tensor\uac00 \uc788\uc744 \ub54c, to() \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud574\uc11c \ub2e4\ub978 \uc7a5\uce58\ub85c \uc774\ub3d9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \ucf54\ub4dc\ub294 CPU\uc5d0 tensor\ub97c \uc0dd\uc131\ud558\uace0, \uc774\uc804 cell\uc5d0\uc11c \uc5bb\uc740 \uc7a5\uce58 \ud578\ub4e4\ub7ec\ub85c tensor\ub97c \uc774\ub3d9\ud569\ub2c8\ub2e4. y = torch.rand(2, 2) y = y.to(my_device) 2\uac1c \ud639\uc740 \uadf8 \uc774\uc0c1\uc758 tensor\ub97c \ud3ec\ud568\ud55c \uacc4\uc0b0\uc744 \ud558\uae30 \uc704\ud574\uc11c\ub294 \ubaa8\ub4e0 tensor\uac00 \uac19\uc740 \uc7a5\uce58\uc5d0 \uc788\uc5b4\uc57c \ud55c\ub2e4 \ub294 \uac83\uc744 \uc544\ub294 \uac83\uc774 \uc911\uc694\ud569\ub2c8\ub2e4. \ub2e4\uc74c \ucf54\ub4dc\ub294 GPU \uc7a5\uce58\uc758 \uc0ac\uc6a9 \uac00\ub2a5 \uc5ec\ubd80\uc640 \uad00\uacc4\uc5c6\uc774 runtime error\ub97c \ubc1c\uc0dd\ud560 \uac83\uc785\ub2c8\ub2e4: x = torch.rand(2, 2) y = torch.rand(2, 2, device=\u0027gpu\u0027) z = x + y # \uc624\ub958\uac00 \ubc1c\uc0dd\ud560 \uac83\uc785\ub2c8\ub2e4. Tensor\uc758 shape \ub2e4\ub8e8\uae30# \ub54c\ub85c\ub294 tensor\uc758 shape\ub97c \ubcc0\ud658\ud560 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc544\ub798\uc5d0 \uc788\ub294 \uba87 \uac00\uc9c0 \ud754\ud55c \uacbd\uc6b0\uc640 \ud568\uaed8 tensor\uc758 shape\ub97c \ub2e4\ub8e8\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574 \uc54c\uc544\ubcfc \uac83 \uc785\ub2c8\ub2e4. \ucc28\uc6d0\uc758 \uac1c\uc218 \ubcc0\uacbd\ud558\uae30# \ucc28\uc6d0\uc758 \uac1c\uc218\ub97c \ubcc0\uacbd\ud560 \ud544\uc694\uac00 \uc788\ub294 \ud55c\uac00\uc9c0 \uacbd\uc6b0\ub294 \ubaa8\ub378\uc758 \uc785\ub825\uc5d0 \ub2e8\uc77c \uc778\uc2a4\ud134\uc2a4\ub97c \uc804\ub2ec\ud560 \ub54c \uc785\ub2c8\ub2e4. PyTorch \ubaa8\ub378\uc740 \uc77c\ubc18\uc801\uc73c\ub85c \uc785\ub825\uc5d0 \ubc30\uce58 \uac00 \ub4e4\uc5b4\uc624\uae30\ub97c \uae30\ub300\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, 3\uac1c\uc758 \uc0c9\uae54 \ucc44\ub110\uc744 \uac00\uc9c4 226\ud53d\uc140 \uc815\uc0ac\uac01\ud615 \uc774\ubbf8\uc9c0\uc778 3 x 226 x 226 \uac1c \ub370\uc774\ud130\uc640 \ud568\uaed8 \uc791\ub3d9\ud558\ub294 \ubaa8\ub378\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4\uace0 \uc0c1\uc0c1\ud574\ubcf4\uc138\uc694. \uc774\ubbf8\uc9c0\ub97c \ubd88\ub7ec\uc624\uace0 tensor\ub85c \ubcc0\ud658\ud558\uba74 (3, 226, 226) shape\ub97c \uac00\uc9c4 tensor\uac00 \ub429\ub2c8\ub2e4. \uadf8\ub7fc\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \uc774 \ubaa8\ub378\uc740 (N, 3, 226, 226) shape\ub97c \uac00\uc9c4 tensor\ub97c \uc785\ub825\uc73c\ub85c \uae30\ub300\ud569\ub2c8\ub2e4. \uc774\ub54c N \uc740 \ubc30\uce58\uc5d0 \ud3ec\ud568\ub41c \uc774\ubbf8\uc9c0\uc758 \uac1c\uc218\uc785\ub2c8\ub2e4. \uadf8\ub807\ub2e4\uba74 \uc5b4\ub5bb\uac8c \ud55c \ubc30\uce58\ub97c \ub9cc\ub4e4 \uc218 \uc788\uc744\uae4c\uc694? a = torch.rand(3, 226, 226) b = a.unsqueeze(0) print(a.shape) print(b.shape) torch.Size([3, 226, 226]) torch.Size([1, 3, 226, 226]) unsqueeze() \uba54\uc18c\ub4dc\ub294 \ud06c\uae30\uac00 1\uc778 \ucc28\uc6d0\uc744 \ucd94\uac00\ud569\ub2c8\ub2e4. unsqueeze(0) \ub294 \uc0c8\ub85c\uc6b4 0\ubc88\uc9f8 \ucc28\uc6d0\uc744 \ucd94\uac00\ud569\ub2c8\ub2e4 - \uc774\uc81c \ud55c \ubc30\uce58\ub97c \uac00\uc9c0\uac8c \ub418\uc5c8\uc2b5\ub2c8\ub2e4! \uc774\uac8c unsqueezing\uc774\uba74, squeezing\uc740 \ubb34\uc2a8 \ub73b \uc77c\uae4c\uc694? \uc5ec\uae30\uc11c\ub294 \ucc28\uc6d0\uc744 \ud558\ub098 \ud655\uc7a5\ud574\ub3c4 tensor\uc5d0 \uc788\ub294 \uc6d0\uc18c\uc758 \uac1c\uc218\ub294 \ubcc0\ud558\uc9c0 \uc54a\ub294\ub2e4 \ub294 \uc0ac\uc2e4\uc744 \uc774\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. c = torch.rand(1, 1, 1, 1, 1) print(c) tensor([[[[[0.2347]]]]]) \uc704\uc758 \uc608\uc81c\uc5d0 \uc774\uc5b4\uc11c \uac01 \uc785\ub825 \uac12\uc5d0 \ub300\ud55c \ubaa8\ub378\uc758 \ucd9c\ub825 \uac12\uc774 20\uac1c\uc758 \uc6d0\uc18c\ub97c \uac00\uc9c4 vector\ub77c\uace0 \uc0dd\uac01\ud574\ubd05\uc2dc\ub2e4. \uadf8\ub807\ub2e4\uba74 N \uc774 \uc785\ub825 \ubc30\uce58\uc5d0 \uc788\ub294 \uc778\uc2a4\ud134\uc2a4\uc758 \uac1c\uc218\ub77c\uace0 \ud560 \ub54c, \ucd9c\ub825 \uac12\uc758 shape\ub294 (N, 20) \ub77c\uace0 \uae30\ub300\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub73b\uc740 \uc785\ub825\uc73c\ub85c \ub2e8\uc77c \ubc30\uce58\uac00 \ub4e4\uc5b4\uc654\uc744 \ub54c, (1, 20) \uc758 shape\ub97c \uac00\uc9c4 \ucd9c\ub825 \uac12\uc744 \uc5bb\ub294\ub2e4\ub294 \uac83 \uc785\ub2c8\ub2e4. \ub9cc\uc57d \uadf8\uc800 20\uac1c\uc758 \uc6d0\uc18c\ub97c \uac00\uc9c4 \ubca1\ud130\uc640 \uac19\uc774 - \ubc30\uce58 shape\uac00 \uc544\ub2cc \uc5f0\uc0b0 \uacb0\uacfc\ub97c \uc5bb\uace0 \uc2f6\uc73c\uba74 \uc5b4\ub5bb\uac8c \ud574\uc57c\ud560\uae4c\uc694? a = torch.rand(1, 20) print(a.shape) print(a) b = a.squeeze(0) print(b.shape) print(b) c = torch.rand(2, 2) print(c.shape) d = c.squeeze(0) print(d.shape) torch.Size([1, 20]) tensor([[0.1899, 0.4067, 0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367, 0.8194, 0.4509, 0.2690, 0.8381, 0.8207, 0.6818, 0.5057, 0.9335, 0.9769, 0.2792, 0.3277]]) torch.Size([20]) tensor([0.1899, 0.4067, 0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367, 0.8194, 0.4509, 0.2690, 0.8381, 0.8207, 0.6818, 0.5057, 0.9335, 0.9769, 0.2792, 0.3277]) torch.Size([2, 2]) torch.Size([2, 2]) \uacb0\uacfc\ub85c \ub098\uc628 shape\ub85c \ubd80\ud130 2\ucc28\uc6d0 tensor\uac00 \uc774\uc81c 1\ucc28\uc6d0\uc73c\ub85c \ubc14\ub010 \uac83\uc744 \ubcfc \uc218 \uc788\uace0, \uc704\uc5d0 \uc788\ub294 cell\uc758 \uacb0\uacfc\ub97c \uc790\uc138\ud788 \ubcf4\uba74 \ucd94\uac00\uc801\uc778 \ucc28\uc6d0\uc744 \uac00\uc84c\uae30 \ub54c\ubb38\uc5d0 a \ub97c \ucd9c\ub825\ud558\ub294 \uac83\uc5d0\uc11c \u201c\ucd94\uac00\u201d \ub300\uad04\ud638 \uc9d1\ud569 [] \uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc624\uc9c1 \ucc28\uc6d0\uc758 \uac12\uc774 1\uc778 \uacbd\uc6b0\uc5d0\ub9cc squeeze() \ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. c \uc5d0\uc11c \ud06c\uae30\uac00 2\uc778 \ucc28\uc6d0\uc744 squeeze \ud558\ub824\uace0 \ud558\ub294 \uc704 \uc608\uc2dc\ub97c \ubcf4\uba74, \ucc98\uc74c \uadf8 shape\ub85c \ub2e4\uc2dc \ub3cc\uc544\uc628\ub2e4\ub294 \uc0ac\uc2e4\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. squeeze() \uc640 unsqueeze() \ub97c \ud638\ucd9c\ud558\ub294 \uac83\uc740 \uc624\uc9c1 \ucc28\uc6d0\uc758 \ud06c\uae30\uac00 1\uc77c \ub54c\ub9cc \uc791\ub3d9\ud569\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \uc774 \uacbd\uc6b0\uac00 \uc544\ub2c8\uba74 tensor\uc758 \uc6d0\uc18c \uac1c\uc218\uac00 \ubc14\ub00c\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. unsqueeze() \ub294 broadcasting\uc744 \uc27d\uac8c \ud558\ub294 \uacbd\uc6b0\uc5d0\ub3c4 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ub2e4\uc74c \ucf54\ub4dc\ub97c \ubcf4\uace0 \uc774\uc804 \uc608\uc2dc\ub97c \ub5a0\uc62c\ub824 \ubcf4\uc138\uc694: a = torch.ones(4, 3, 2) c = a * torch.rand( 3, 1) # 3\ubc88\uc9f8 \ucc28\uc6d0\uc740 1\uc774\uace0, 2\ubc88\uc9f8 \ucc28\uc6d0\uc740 a\uc640 \ub3d9\uc77c\ud569\ub2c8\ub2e4. print(c) broadcast\uc758 \uc21c\uc218\ud55c \ud6a8\uacfc\ub294 \ucc28\uc6d0 0\uacfc \ucc28\uc6d0 2\uc5d0 \ub300\ud55c \uc5f0\uc0b0\uc744 broadcast\ud574\uc11c \ubb34\uc791\uc704 3 x 1 shape\uc758 tensor\ub97c a \uc758 \uc6d0\uc18c \uac1c\uc218\uac00 3\uc778 \ubaa8\ub4e0 \uc5f4\uc5d0 \uacf1\ud558\ub294 \uac83\uc774\uc5c8\uc2b5\ub2c8\ub2e4. \ub9cc\uc57d \ubb34\uc791\uc704 \ubca1\ud130\uac00 \uc624\uc9c1 3\uac1c\uc758 \uc6d0\uc18c\ub9cc\uc744 \uac00\uc9c0\uba74 \uc5b4\ub5bb\uac8c \ub420\uae4c\uc694? broadcast\ub97c \ud560 \ub2a5\ub825\uc744 \uc783\uc5b4\ubc84\ub9ac\uac8c \ub429\ub2c8\ub2e4, \uc65c\ub0d0\ud558\uba74 \ub9c8\uc9c0\ub9c9 \ucc28\uc6d0\uc774 broadcasting \uaddc\uce59\uc5d0 \ub9de\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \ud558\uc9c0\ub9cc unsqueeze() \uac00 \ub3c4\uc640\uc90d\ub2c8\ub2e4: a = torch.ones(4, 3, 2) b = torch.rand( 3) # a * b\ub97c \uc2dc\ub3c4\ud558\ub294 \uac83\uc740 runtime error\uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4. c = b.unsqueeze(1) # \ub05d\uc5d0 \uc0c8\ub85c\uc6b4 \ucc28\uc6d0\uc744 \ucd94\uac00\ud574\uc11c 2\ucc28\uc6d0 tensor\ub85c \ubc14\uafc9\ub2c8\ub2e4. print(c.shape) print(a * c) # broadcasting\uc774 \ub2e4\uc2dc \uc791\ub3d9\ud569\ub2c8\ub2e4! torch.Size([3, 1]) tensor([[[0.1891, 0.1891], [0.3952, 0.3952], [0.9176, 0.9176]], [[0.1891, 0.1891], [0.3952, 0.3952], [0.9176, 0.9176]], [[0.1891, 0.1891], [0.3952, 0.3952], [0.9176, 0.9176]], [[0.1891, 0.1891], [0.3952, 0.3952], [0.9176, 0.9176]]]) squeeze() \uc640 unsqueeze() \uba54\uc18c\ub4dc\ub294 tensor \uc790\uccb4\uc758 \uac12\uc744 \ubcc0\uacbd\ud558\ub294 squeeze_() \uc640 unsqueeze_() \ub610\ud55c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. batch_me = torch.rand(3, 226, 226) print(batch_me.shape) batch_me.unsqueeze_(0) print(batch_me.shape) torch.Size([3, 226, 226]) torch.Size([1, 3, 226, 226]) \ub54c\ub85c\ub294 \uc6d0\uc18c\uc758 \uac1c\uc218\uc640 \uc6d0\uc18c\uc758 \uac12\uc744 \uc5ec\uc804\ud788 \uc720\uc9c0\ud558\uba74\uc11c tensor\uc758 shape\ub97c \ud55c\ubc88\uc5d0 \ubc14\uafb8\uace0 \uc2f6\uc744 \ub54c\uac00 \uc788\uc2b5\ub2c8\ub2e4. \ubaa8\ub378\uc758 \ud569\uc131\uacf1 \uacc4\uce35\uacfc \uc120\ud615 \uacc4\uce35 \uc0ac\uc774 \uc778\ud130\ud398\uc774\uc2a4\uc5d0\uc11c \uc774\ub7ec\ud55c \uc0c1\ud669\uc774 \ubc1c\uc0dd\ud569\ub2c8\ub2e4 - \uc774 \uc0c1\ud669\uc740 \uc774\ubbf8\uc9c0 \ubd84\ub958 \ubaa8\ub378\uc5d0\uc11c \ud754\ud788 \uc77c\uc5b4\ub098\ub294 \uc77c\uc785\ub2c8\ub2e4. \ud569\uc131\uacf1 \ucee4\ub110\uc740 \ud2b9\uc131\uc758 \uc218 x \ub108\ube44 x \ub192\uc774 shpae\uc758 tensor\ub97c \ucd9c\ub825 \uac12\uc73c\ub85c \uc0dd\uc131\ud558\uc9c0\ub9cc \uc774\ud6c4\uc5d0 \uc788\ub294 \uc120\ud615 \uacc4\uce35\uc740 \uc785\ub825 \uac12\uc73c\ub85c 1\ucc28\uc6d0\uc744 \uae30\ub300\ud569\ub2c8\ub2e4. \uc5ec\ub7ec\ubd84\uc774 \uc694\uccad\ud55c \ucc28\uc6d0\uc5d0 \uc785\ub825 tensor\uac00 \uac00\uc9c4 \uc6d0\uc18c\uc640 \uac19\uc740 \uac1c\uc218\ub97c \uc0dd\uc131\ud558\ub294 reshape() \ub97c \uc5ec\ub7ec\ubd84\uc744 \uc704\ud574\uc11c \uc81c\uacf5\ud569\ub2c8\ub2e4: output3d = torch.rand(6, 20, 20) print(output3d.shape) input1d = output3d.reshape(6 * 20 * 20) print(input1d.shape) # torch \ubaa8\ub4c8\uc5d0 \uc788\ub294 \uba54\uc18c\ub4dc\uc5d0 \ub300\ud574\uc11c\ub3c4 \ud638\ucd9c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. print(torch.reshape(output3d, (6 * 20 * 20,)).shape) torch.Size([6, 20, 20]) torch.Size([2400]) torch.Size([2400]) \ucc38\uace0 \uc704\uc5d0 \uc788\ub294 cell\uc758 \ub9c8\uc9c0\ub9c9 \uc904\uc5d0 \uc788\ub294 \uc778\uc790 (6 * 20 * 20,) \ub294 PyTorch\ub294 tensor shape\ub97c \ub098\ud0c0\ub0bc \ub54c tuple \uc744 \uae30\ub300\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \ud558\uc9c0\ub9cc shape\uac00 \uba54\uc18c\ub4dc\uc758 \uccab\ubc88\uc9f8 \uc778\uc790\ub77c\uba74 - \uc5f0\uc18d\ub41c \uc815\uc218\ub77c\uace0 \uc18d\uc5ec\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 \uba54\uc18c\ub4dc\uc5d0\uac8c \uc774 \uc778\uc790\uac00 \uc9c4\uc9dc 1\uac1c \uc6d0\uc18c\ub97c \uac00\uc9c4 \ud29c\ud50c\uc774\ub77c\uace0 \uc54c\ub824\uc8fc\uae30 \uc704\ud574\uc11c \ud3b8\uc758\uc0c1 \uc18c\uad04\ud638\uc640 \ucf64\ub9c8\ub97c \ucd94\uac00\ud574\uc57c \ud569\ub2c8\ub2e4. reshape() \ub294 tensor\ub97c \ubc14\ub77c\ubcf4\ub294 \uad00\uc810 \uc744 \ubcc0\uacbd\ud569\ub2c8\ub2e4. - \uc989, \uba54\ubaa8\ub9ac\uc758 \uac19\uc740 \uc9c0\uc5ed\uc744 \ubc14\ub77c\ubcf4\ub294 \uc11c\ub85c \ub2e4\ub978 \uad00\uc810\uc744 \uac00\uc9c4 tensor \uac1d\uccb4\ub77c\ub294 \ub73b\uc785\ub2c8\ub2e4. \uc774 \ub0b4\uc6a9\uc740 \uc815\ub9d0 \uc911\uc694\ud569\ub2c8\ub2e4: source tensor\uc5d0 \uc5b4\ub5a0\ud55c \ubcc0\ud654\uac00 \uc788\uc73c\uba74 clone() \uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub294 \ud55c, \ud574\ub2f9 tensor\ub97c \ubc14\ub77c\ubcf4\uace0 \uc788\ub294 \ub2e4\ub978 \uac1d\uccb4 \ub610\ud55c \uac12\uc774 \ubcc0\ud55c\ub2e4\ub294 \ub73b \uc785\ub2c8\ub2e4. \ud574\ub2f9 \uc18c\uac1c\uc758 \ubc94\uc704\ub97c \ubc97\uc5b4\ub09c \uc870\uac74 \ub4e4 \uc774 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\uac83\uc740 reshape() \uac00 data\uc758 \ubcf5\uc0ac\ubcf8\uc744 \uac00\uc9c4 tensor\ub97c \ubc18\ud658 \ud574\uc57c \ud55c\ub2e4\ub294 \uac83 \uc785\ub2c8\ub2e4. \ub354 \ub9ce\uc740 \uc815\ubcf4\ub294 \ub2e4\uc74c \ubb38\uc11c\ub97c \ucc38\uace0\ud558\uc138\uc694 docs. NumPy\ub85c \ubcc0\ud658# \uc704\uc5d0 \uc788\ub294 broadcasting \ubd80\ubd84\uc5d0\uc11c, PyTorch\uc758 broadcast \ubb38\ubc95\uc740 Numpy\uc640 \ud638\ud658 \uac00\ub2a5\ud558\ub2e4\uace0 \ub9d0\ud588\uc5c8\uc2b5\ub2c8\ub2e4 - \ud558\uc9c0\ub9cc PyTorch\uc640 NumPy \uc0ac\uc774 \uc720\uc0ac\uc131\uc740 \uc6b0\ub9ac\uac00 \uc0dd\uac01\ud55c \uac83 \ubcf4\ub2e4 \ub354\uc6b1 \uae4a\uc2b5\ub2c8\ub2e4. \ub9cc\uc57d NumPy\uc758 ndarrays\uc5d0 \uc800\uc7a5\ub418\uc5b4 \uc788\ub294 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\ub294 \uba38\uc2e0 \ub7ec\ub2dd \ud639\uc740 \uacfc\ud559 \ubd84\uc57c\uc640 \uad00\ub828\ub41c \ucf54\ub4dc\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4\uba74, \uac19\uc740 \ub370\uc774\ud130\ub97c PyTorch\uc758 GPU \uac00\uc18d\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uace0 \uba38\uc2e0 \ub7ec\ub2dd \ubaa8\ub378\uc744 \ub9cc\ub4dc\ub294\ub370 \ud544\uc694\ud55c \ud6a8\uacfc\uc801\uc778 \ucd94\uc0c1\ud654\ub97c \uc81c\uacf5\ud558\ub294 PyTorch tensor\ub85c \ud45c\ud604\ud558\uace0 \uc2f6\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. ndarray\uc640 PyTorch tensor\ub07c\ub9ac \ubc14\uafb8\ub294 \uac83\uc740 \uc27d\uc2b5\ub2c8\ub2e4: import numpy as np numpy_array = np.ones((2, 3)) print(numpy_array) pytorch_tensor = torch.from_numpy(numpy_array) print(pytorch_tensor) [[1. 1. 1.] [1. 1. 1.]] tensor([[1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) PyTorch\ub294 NumPy array\uc640 \uac19\uc740 shape\uc758 tensor\ub97c \uc0dd\uc131\ud558\uace0, \uac19\uc740 \ub370\uc774\ud130\ub97c \ud3ec\ud568\ud569\ub2c8\ub2e4. \uc2ec\uc9c0\uc5b4 NumPy\uc758 \uae30\ubcf8\uc801\uc778 64\ube44\ud2b8 \uc2e4\uc218 \ub370\uc774\ud130 \uc790\ub8cc\ud615\uc744 \uc720\uc9c0\ud569\ub2c8\ub2e4. PyTorch\uc5d0\uc11c NumPy\ub85c \ubcc0\ud658\uc740 \ub2e4\ub978 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud574\uc11c \uc27d\uac8c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: pytorch_rand = torch.rand(2, 3) print(pytorch_rand) numpy_rand = pytorch_rand.numpy() print(numpy_rand) tensor([[0.8716, 0.2459, 0.3499], [0.2853, 0.9091, 0.5695]]) [[0.87163675 0.2458961 0.34993553] [0.2853077 0.90905803 0.5695162 ]] \uc774\ub7ec\ud55c \ubcc0\ud658\ub41c \uac1d\uccb4\ub4e4\uc740 \ud574\ub2f9 \uac1d\uccb4\uc758 source \uac1d\uccb4\uac00 \uc704\uce58\ud55c \uba54\ubaa8\ub9ac\uc758 \uac19\uc740 \uacf5\uac04 \uc744 \uc0ac\uc6a9\ud55c\ub2e4\ub294 \uc810\uc744 \uc544\ub294 \uac83\uc774 \uc911\uc694\ud569\ub2c8\ub2e4. \uc774\uac83\uc740 \ud55c \uac1d\uccb4\uac00 \ubcc0\ud558\uba74 \ub2e4\ub978 \uac83\uc5d0 \uc601\ud5a5\uc744 \uc900\ub2e4\ub294 \uc758\ubbf8\uc785\ub2c8\ub2e4: numpy_array[1, 1] = 23 print(pytorch_tensor) pytorch_rand[1, 1] = 17 print(numpy_rand) tensor([[ 1., 1., 1.], [ 1., 23., 1.]], dtype=torch.float64) [[ 0.87163675 0.2458961 0.34993553] [ 0.2853077 17. 0.5695162 ]] Total running time of the script: (0 minutes 4.742 seconds) Download Jupyter notebook: tensors_deeper_tutorial.ipynb Download Python source code: tensors_deeper_tutorial.py Download zipped: tensors_deeper_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/beginner/introyt/tensors_deeper_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>