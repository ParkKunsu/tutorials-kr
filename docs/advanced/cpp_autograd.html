
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="C++ 프론트엔드의 자동 미분 (autograd)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/advanced/cpp_autograd.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="번역: 유용환 autograd 는 PyTorch로 유연하고 역동적인 신경망을 구축하기 위해 필수적인 패키지입니다. PyTorch 파이썬 프론트엔드의 자동 미분 API 대부분은 C++ 프론트엔드에서도 사용할 수 있으며, 파이썬에서 C++로 자동 미분 코드를 쉽게 변환할 수 있습니다. 이 튜토리얼에서는 PyTorch C++ 프론트엔드에서 자동 미분을 수행하는 몇 가지 예를 살펴보겠습니다. 이 튜토리얼은 여러분이 파이썬 프론트엔드의 자동 미분에 대해 기본적으로 이해하고 있다고 가정합니다. 그렇지 않은 경우 먼저 Autograd: A..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="번역: 유용환 autograd 는 PyTorch로 유연하고 역동적인 신경망을 구축하기 위해 필수적인 패키지입니다. PyTorch 파이썬 프론트엔드의 자동 미분 API 대부분은 C++ 프론트엔드에서도 사용할 수 있으며, 파이썬에서 C++로 자동 미분 코드를 쉽게 변환할 수 있습니다. 이 튜토리얼에서는 PyTorch C++ 프론트엔드에서 자동 미분을 수행하는 몇 가지 예를 살펴보겠습니다. 이 튜토리얼은 여러분이 파이썬 프론트엔드의 자동 미분에 대해 기본적으로 이해하고 있다고 가정합니다. 그렇지 않은 경우 먼저 Autograd: A..." />
<meta property="og:ignore_canonical" content="true" />

    <title>C++ 프론트엔드의 자동 미분 (autograd) &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'advanced/cpp_autograd';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/advanced/cpp_autograd.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="Extension" href="../extension.html" />
    <link rel="prev" title="PyTorch C++ 프론트엔드 사용하기" href="cpp_frontend.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../beginner/profiler.html">PyTorch 모듈 프로파일링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/parametrizations.html">Parametrizations Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/pruning_tutorial.html">가지치기 기법(Pruning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/scaled_dot_product_attention_tutorial.html">(Beta) Scaled Dot Product Attention (SDPA)로 고성능 트랜스포머(Transformers) 구현하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/knowledge_distillation_tutorial.html">Knowledge Distillation Tutorial</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frontend APIs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intermediate/memory_format_tutorial.html">(베타) PyTorch를 사용한 Channels Last 메모리 형식</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/forward_ad_usage.html">Forward-mode Automatic Differentiation (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing function transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/ensembling.html">모델 앙상블</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_frontend.html">PyTorch C++ 프론트엔드 사용하기</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">C++ 프론트엔드의 자동 미분 (autograd)</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../deep-dive.html" class="nav-link">Deep Dive</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">C++ 프론트엔드의...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../deep-dive.html">
        <meta itemprop="name" content="Deep Dive">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="C++ 프론트엔드의 자동 미분 (autograd)">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">advanced/cpp_autograd</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="c-autograd">
<h1>C++ 프론트엔드의 자동 미분 (autograd)<a class="headerlink" href="#c-autograd" title="Link to this heading">#</a></h1>
<p><strong>번역</strong>: <a class="reference external" href="https://github.com/yoosful">유용환</a></p>
<p><code class="docutils literal notranslate"><span class="pre">autograd</span></code> 는 PyTorch로 유연하고 역동적인 신경망을 구축하기 위해
필수적인 패키지입니다. PyTorch 파이썬 프론트엔드의 자동 미분 API 대부분은 C++ 프론트엔드에서도
사용할 수 있으며, 파이썬에서 C++로 자동 미분 코드를 쉽게 변환할 수 있습니다.</p>
<p>이 튜토리얼에서는 PyTorch C++ 프론트엔드에서 자동 미분을 수행하는 몇 가지 예를 살펴보겠습니다.
이 튜토리얼은 여러분이 파이썬 프론트엔드의 자동 미분에 대해 기본적으로 이해하고 있다고
가정합니다. 그렇지 않은 경우 먼저 <a class="reference external" href="https://tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html">Autograd: Automatic Differentiation</a> 을 읽어보세요.</p>
<section id="id2">
<h2>기초 자동 미분 연산<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>(<a class="reference external" href="https://tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html#autograd-automatic-differentiation">이 튜토리얼</a> 의 내용에 기반함)</p>
<p>텐서를 생성하고 그것의 계산을 추적하기 위해 <code class="docutils literal notranslate"><span class="pre">torch::requires_grad()</span></code> 를 실행해봅시다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">requires_grad</span><span class="p">());</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="m">1</span><span class="w"> </span><span class="m">1</span>
<span class="m">1</span><span class="w"> </span><span class="m">1</span>
<span class="o">[</span><span class="w"> </span>CPUFloatType<span class="o">{</span><span class="m">2</span>,2<span class="o">}</span><span class="w"> </span><span class="o">]</span>
</pre></div>
</div>
<p>텐서 연산을 수행해보겠습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="m">3</span><span class="w">  </span><span class="m">3</span>
<span class="w"> </span><span class="m">3</span><span class="w">  </span><span class="m">3</span>
<span class="o">[</span><span class="w"> </span>CPUFloatType<span class="o">{</span><span class="m">2</span>,2<span class="o">}</span><span class="w"> </span><span class="o">]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">y</span></code> 는 연산의 결과로 생성되었으므로 <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> 를 갖고 있습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">y</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>AddBackward1
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">y</span></code> 에 대해 더 많은 연산을 수행해봅시다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">z</span><span class="p">.</span><span class="n">mean</span><span class="p">();</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">z</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">z</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="m">27</span><span class="w">  </span><span class="m">27</span>
<span class="w"> </span><span class="m">27</span><span class="w">  </span><span class="m">27</span>
<span class="o">[</span><span class="w"> </span>CPUFloatType<span class="o">{</span><span class="m">2</span>,2<span class="o">}</span><span class="w"> </span><span class="o">]</span>
MulBackward1
<span class="m">27</span>
<span class="o">[</span><span class="w"> </span>CPUFloatType<span class="o">{}</span><span class="w"> </span><span class="o">]</span>
MeanBackward0
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">.requires_grad_(</span> <span class="pre">...</span> <span class="pre">)</span></code> 는 in-place로 텐서의 기존 <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 플래그를 바꿉니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">});</span>
<span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">((</span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">));</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="n">a</span><span class="p">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">a</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="k">auto</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">a</span><span class="p">).</span><span class="n">sum</span><span class="p">();</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">b</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">false</span>
<span class="nb">true</span>
SumBackward0
</pre></div>
</div>
<p>이제 역전파를 수행해봅시다. <code class="docutils literal notranslate"><span class="pre">out</span></code> 이 단일 스칼라만을 포함하므로, <code class="docutils literal notranslate"><span class="pre">out.backward()</span></code> 는
<code class="docutils literal notranslate"><span class="pre">out.backward(torch::tensor(1.))</span></code> 와 같습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">out</span><span class="p">.</span><span class="n">backward</span><span class="p">();</span>
</pre></div>
</div>
<p>변화도 d(out)/dx를 출력해보겠습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="m">4</span>.5000<span class="w">  </span><span class="m">4</span>.5000
<span class="w"> </span><span class="m">4</span>.5000<span class="w">  </span><span class="m">4</span>.5000
<span class="o">[</span><span class="w"> </span>CPUFloatType<span class="o">{</span><span class="m">2</span>,2<span class="o">}</span><span class="w"> </span><span class="o">]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">4.5</span></code> 행렬이 출력돼야 합니다. 이 값을 얻는 과정에 대한 설명은 <a class="reference external" href="https://tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html#gradients">이 튜토리얼의 해당 섹션</a> 에서 확인하세요.</p>
<p>이제 벡터-야코비안 곱의 예를 살펴보겠습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">requires_grad</span><span class="p">());</span>

<span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">norm</span><span class="p">().</span><span class="n">item</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">y</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>-1021.4020
<span class="w">  </span><span class="m">314</span>.6695
<span class="w"> </span>-613.4944
<span class="o">[</span><span class="w"> </span>CPUFloatType<span class="o">{</span><span class="m">3</span><span class="o">}</span><span class="w"> </span><span class="o">]</span>
MulBackward1
</pre></div>
</div>
<p>벡터-야코비안 곱을 얻기 위해 벡터를 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 의 인자로 넣어줍니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">tensor</span><span class="p">({</span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.0001</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">kFloat</span><span class="p">);</span>
<span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">v</span><span class="p">);</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="m">102</span>.4000
<span class="w"> </span><span class="m">1024</span>.0000
<span class="w">    </span><span class="m">0</span>.1024
<span class="o">[</span><span class="w"> </span>CPUFloatType<span class="o">{</span><span class="m">3</span><span class="o">}</span><span class="w"> </span><span class="o">]</span>
</pre></div>
</div>
<p>또한 코드에 <code class="docutils literal notranslate"><span class="pre">torch::NoGradGuard</span></code> 를 넣어주면 자동 미분으로 하여금 그래디언트가
필요한 텐서를 추적하지 않도록 할 수 있습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="p">{</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">NoGradGuard</span><span class="w"> </span><span class="n">no_grad</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">true</span>
<span class="nb">true</span>
<span class="nb">false</span>
</pre></div>
</div>
<p>혹은 <code class="docutils literal notranslate"><span class="pre">.detach()</span></code> 를 사용하여 내용은 동일하지만 그래디언트가 필요 없는
새 텐서를 얻을 수도 있습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">detach</span><span class="p">();</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">y</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="n">all</span><span class="p">().</span><span class="n">item</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">true</span>
<span class="nb">false</span>
<span class="nb">true</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">grad</span></code> / <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> / <code class="docutils literal notranslate"><span class="pre">is_leaf</span></code> / <code class="docutils literal notranslate"><span class="pre">backward</span></code> / <code class="docutils literal notranslate"><span class="pre">detach</span></code> / <code class="docutils literal notranslate"><span class="pre">detach_</span></code> /
<code class="docutils literal notranslate"><span class="pre">register_hook</span></code> / <code class="docutils literal notranslate"><span class="pre">retain_grad</span></code> 등 C++ 텐서 자동 미분 API에 대한 자세한 내용은 <a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html">해당 C++ API 문서</a> 에서 확인하세요.</p>
</section>
<section id="c">
<h2>C++로 고차원 그래디언트 계산하기<a class="headerlink" href="#c" title="Link to this heading">#</a></h2>
<p>고차원 그래디언트를 사용하는 사례로 그래디언트 패널티 계산이 있습니다.
<code class="docutils literal notranslate"><span class="pre">torch::autograd::grad</span></code> 를 사용하는 예를 살펴봅시다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/torch.h&gt;</span>

<span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>

<span class="k">auto</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">}).</span><span class="n">requires_grad_</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>

<span class="c1">// Calculate loss</span>
<span class="k">auto</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">});</span>
<span class="k">auto</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">MSELoss</span><span class="p">()(</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">target</span><span class="p">);</span>

<span class="c1">// Use norm of gradients as penalty</span>
<span class="k">auto</span><span class="w"> </span><span class="n">grad_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones_like</span><span class="p">(</span><span class="n">output</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">gradient</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">grad</span><span class="p">({</span><span class="n">output</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="n">input</span><span class="p">},</span><span class="w"> </span><span class="cm">/*grad_outputs=*/</span><span class="p">{</span><span class="n">grad_output</span><span class="p">},</span><span class="w"> </span><span class="cm">/*create_graph=*/</span><span class="nb">true</span><span class="p">)[</span><span class="mi">0</span><span class="p">];</span>
<span class="k">auto</span><span class="w"> </span><span class="n">gradient_penalty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">pow</span><span class="p">((</span><span class="n">gradient</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="mi">2</span><span class="p">).</span><span class="n">mean</span><span class="p">();</span>

<span class="c1">// Add gradient penalty to loss</span>
<span class="k">auto</span><span class="w"> </span><span class="n">combined_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">gradient_penalty</span><span class="p">;</span>
<span class="n">combined_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">();</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">input</span><span class="p">.</span><span class="n">grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>-0.1042<span class="w"> </span>-0.0638<span class="w">  </span><span class="m">0</span>.0103<span class="w">  </span><span class="m">0</span>.0723
-0.2543<span class="w"> </span>-0.1222<span class="w">  </span><span class="m">0</span>.0071<span class="w">  </span><span class="m">0</span>.0814
-0.1683<span class="w"> </span>-0.1052<span class="w">  </span><span class="m">0</span>.0355<span class="w">  </span><span class="m">0</span>.1024
<span class="o">[</span><span class="w"> </span>CPUFloatType<span class="o">{</span><span class="m">3</span>,4<span class="o">}</span><span class="w"> </span><span class="o">]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch::autograd::backward</span></code>
(<a class="reference external" href="https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1a1403bf65b1f4f8c8506a9e6e5312d030.html">링크</a>) 및
<code class="docutils literal notranslate"><span class="pre">torch::autograd::grad</span></code>
(<a class="reference external" href="https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1ab9fa15dc09a8891c26525fb61d33401a.html">링크</a>) 문서에서
이 함수들의 사용법에 대해 더 알아보세요.</p>
</section>
<section id="id7">
<h2>C++에서 사용자 지정 자동 미분 함수 사용하기<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<p>(<a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">이 튜토리얼</a> 의 내용에 기반함)</p>
<p><code class="docutils literal notranslate"><span class="pre">torch::autograd</span></code> 에 새로운 기본(elementary) 연산을 추가하려면 각 연산에 대해 새로운 <code class="docutils literal notranslate"><span class="pre">torch::autograd::Function</span></code>
하위 클래스(subclass)를 구현해야 합니다. <code class="docutils literal notranslate"><span class="pre">torch::autograd</span></code> 는 결과와 그래디언트를 계산하고 연산 기록을 인코딩하기 위해 위해
이 <code class="docutils literal notranslate"><span class="pre">torch::autograd::Function</span></code> 들을 사용합니다. 모든 새로운 함수에는 두 가지 방법, 즉 <code class="docutils literal notranslate"><span class="pre">forward</span></code> 와 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 를
구현해야 하며 자세한 요구사항은 <a class="reference external" href="https://pytorch.org/cppdocs/api/structtorch_1_1autograd_1_1_function.html">이 링크</a>
에서 확인하세요.</p>
<p>아래 코드는 <code class="docutils literal notranslate"><span class="pre">torch::nn</span></code> 의 <code class="docutils literal notranslate"><span class="pre">Linear</span></code> 함수를 사용합니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/torch.h&gt;</span>

<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">torch</span><span class="o">::</span><span class="nn">autograd</span><span class="p">;</span>

<span class="c1">// Inherit from Function</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LinearFunction</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">Function</span><span class="o">&lt;</span><span class="n">LinearFunction</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="c1">// Note that both forward and backward are static functions</span>

<span class="w">  </span><span class="c1">// bias is an optional argument</span>
<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span>
<span class="w">      </span><span class="n">AutogradContext</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">save_for_backward</span><span class="p">({</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p">});</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">.</span><span class="n">mm</span><span class="p">(</span><span class="n">weight</span><span class="p">.</span><span class="n">t</span><span class="p">());</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">bias</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">output</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">bias</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">expand_as</span><span class="p">(</span><span class="n">output</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">output</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">tensor_list</span><span class="w"> </span><span class="n">backward</span><span class="p">(</span><span class="n">AutogradContext</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_list</span><span class="w"> </span><span class="n">grad_outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">saved</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">get_saved_variables</span><span class="p">();</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">saved</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">saved</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">saved</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">grad_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grad_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">grad_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grad_output</span><span class="p">.</span><span class="n">mm</span><span class="p">(</span><span class="n">weight</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">grad_weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grad_output</span><span class="p">.</span><span class="n">t</span><span class="p">().</span><span class="n">mm</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">grad_bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">bias</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">grad_bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grad_output</span><span class="p">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">grad_input</span><span class="p">,</span><span class="w"> </span><span class="n">grad_weight</span><span class="p">,</span><span class="w"> </span><span class="n">grad_bias</span><span class="p">};</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
<p>이제 아래와 같이 <code class="docutils literal notranslate"><span class="pre">LinearFunction</span></code> 을 사용할 수 있습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">}).</span><span class="n">requires_grad_</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">({</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">}).</span><span class="n">requires_grad_</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">LinearFunction</span><span class="o">::</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">);</span>
<span class="n">y</span><span class="p">.</span><span class="n">sum</span><span class="p">().</span><span class="n">backward</span><span class="p">();</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="m">0</span>.5314<span class="w">  </span><span class="m">1</span>.2807<span class="w">  </span><span class="m">1</span>.4864
<span class="w"> </span><span class="m">0</span>.5314<span class="w">  </span><span class="m">1</span>.2807<span class="w">  </span><span class="m">1</span>.4864
<span class="o">[</span><span class="w"> </span>CPUFloatType<span class="o">{</span><span class="m">2</span>,3<span class="o">}</span><span class="w"> </span><span class="o">]</span>
<span class="w"> </span><span class="m">3</span>.7608<span class="w">  </span><span class="m">0</span>.9101<span class="w">  </span><span class="m">0</span>.0073
<span class="w"> </span><span class="m">3</span>.7608<span class="w">  </span><span class="m">0</span>.9101<span class="w">  </span><span class="m">0</span>.0073
<span class="w"> </span><span class="m">3</span>.7608<span class="w">  </span><span class="m">0</span>.9101<span class="w">  </span><span class="m">0</span>.0073
<span class="w"> </span><span class="m">3</span>.7608<span class="w">  </span><span class="m">0</span>.9101<span class="w">  </span><span class="m">0</span>.0073
<span class="o">[</span><span class="w"> </span>CPUFloatType<span class="o">{</span><span class="m">4</span>,3<span class="o">}</span><span class="w"> </span><span class="o">]</span>
</pre></div>
</div>
<p>여기서, 텐서가 아닌 인자를 매개변수로 갖는 또 다른 함수를 예로 들어 보겠습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/torch.h&gt;</span>

<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">torch</span><span class="o">::</span><span class="nn">autograd</span><span class="p">;</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MulConstant</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">Function</span><span class="o">&lt;</span><span class="n">MulConstant</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">AutogradContext</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">constant</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// ctx is a context object that can be used to stash information</span>
<span class="w">    </span><span class="c1">// for backward computation</span>
<span class="w">    </span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">saved_data</span><span class="p">[</span><span class="s">&quot;constant&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">constant</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">constant</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">tensor_list</span><span class="w"> </span><span class="n">backward</span><span class="p">(</span><span class="n">AutogradContext</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_list</span><span class="w"> </span><span class="n">grad_outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// We return as many input gradients as there were arguments.</span>
<span class="w">    </span><span class="c1">// Gradients of non-tensor arguments to forward must be `torch::Tensor()`.</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">grad_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">saved_data</span><span class="p">[</span><span class="s">&quot;constant&quot;</span><span class="p">].</span><span class="n">toDouble</span><span class="p">(),</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="p">()};</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
<p>이제 아래와 같이 <code class="docutils literal notranslate"><span class="pre">MulConstant</span></code> 를 사용할 수 있습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">({</span><span class="mi">2</span><span class="p">}).</span><span class="n">requires_grad_</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MulConstant</span><span class="o">::</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="mf">5.5</span><span class="p">);</span>
<span class="n">y</span><span class="p">.</span><span class="n">sum</span><span class="p">().</span><span class="n">backward</span><span class="p">();</span>

<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Out:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="m">5</span>.5000
<span class="w"> </span><span class="m">5</span>.5000
<span class="o">[</span><span class="w"> </span>CPUFloatType<span class="o">{</span><span class="m">2</span><span class="o">}</span><span class="w"> </span><span class="o">]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch::autograd::Function</span></code> 에 대한 더 많은 내용은
<a class="reference external" href="https://pytorch.org/cppdocs/api/structtorch_1_1autograd_1_1_function.html">이 문서</a> 에서 확인할 수 있습니다.</p>
</section>
<section id="id10">
<h2>파이썬 자동 미분 코드를 C++로 변환하기<a class="headerlink" href="#id10" title="Link to this heading">#</a></h2>
<p>개략적으로 말하면, C++에서 자동 미분을 사용하는 가장 쉬운 방법은 먼저
파이썬에서 동작하는 자동 미분 코드를 작성한 후, 아래 표를 참고해 C++ 코드로
변환하는 것입니다.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Python</p></th>
<th class="head"><p>C++</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">torch.autograd.backward</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::autograd::backward</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1a1403bf65b1f4f8c8506a9e6e5312d030.html">링크</a>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">torch.autograd.grad</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::autograd::grad</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1ab9fa15dc09a8891c26525fb61d33401a.html">링크</a>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.detach</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::Tensor::detach</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor6detachEv">링크</a>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.detach_</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::Tensor::detach_</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7detach_Ev">링크</a>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.backward</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::Tensor::backward</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor8backwardERK6Tensorbb">링크</a>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.register_hook</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::Tensor::register_hook</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4I0ENK2at6Tensor13register_hookE18hook_return_void_tI1TERR1T">링크</a>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.requires_grad</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::Tensor::requires_grad_</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor14requires_grad_Eb">링크</a>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.retain_grad</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::Tensor::retain_grad</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor11retain_gradEv">링크</a>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.grad</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::Tensor::grad</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4gradEv">링크</a>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.grad_fn</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::Tensor::grad_fn</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7grad_fnEv">링크</a>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.set_data</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::Tensor::set_data</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor8set_dataERK6Tensor">링크</a>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.data</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::Tensor::data</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4dataEv">링크</a>)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.output_nr</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::Tensor::output_nr</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor9output_nrEv">링크</a>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">torch.Tensor.is_leaf</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">torch::Tensor::is_leaf</span></code> (<a class="reference external" href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7is_leafEv">링크</a>)</p></td>
</tr>
</tbody>
</table>
</div>
<p>대부분의 변환된 파이썬 자동 미분 코드가 C++에서도 잘 동작할 것입니다.
동작하지 않을 경우, <a class="reference external" href="https://github.com/pytorch/pytorch/issues">GitHub issues</a> 에 버그 리포트를 제출해 주시면
최대한 빨리 고쳐드리겠습니다.</p>
</section>
<section id="id25">
<h2>결론<a class="headerlink" href="#id25" title="Link to this heading">#</a></h2>
<p>이제 PyTorch의 C++ 자동 미분 API에 대한 개괄적인 이해가 생겼을 것입니다.
여기서 사용된 코드 예제들은 <a class="reference external" href="https://github.com/pytorch/examples/tree/master/cpp/autograd">여기</a> 에서
확인할 수 있습니다. 언제나 그렇듯이 어떤 문제가 생기거나 질문이 있으면 저희
<a class="reference external" href="https://discuss.pytorch.org/">포럼</a> 을 이용하거나 <a class="reference external" href="https://github.com/pytorch/pytorch/issues">Github 이슈</a> 로 연락주세요.</p>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="cpp_frontend.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">PyTorch C++ 프론트엔드 사용하기</p>
      </div>
    </a>
    <a class="right-next"
       href="../extension.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Extension</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="cpp_frontend.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">PyTorch C++ 프론트엔드 사용하기</p>
      </div>
    </a>
    <a class="right-next"
       href="../extension.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Extension</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">기초 자동 미분 연산</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c">C++로 고차원 그래디언트 계산하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">C++에서 사용자 지정 자동 미분 함수 사용하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">파이썬 자동 미분 코드를 C++로 변환하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">결론</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 \uc790\ub3d9 \ubbf8\ubd84 (autograd)",
       "headline": "C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 \uc790\ub3d9 \ubbf8\ubd84 (autograd)",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/advanced/cpp_autograd.html",
       "articleBody": "C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 \uc790\ub3d9 \ubbf8\ubd84 (autograd)# \ubc88\uc5ed: \uc720\uc6a9\ud658 autograd \ub294 PyTorch\ub85c \uc720\uc5f0\ud558\uace0 \uc5ed\ub3d9\uc801\uc778 \uc2e0\uacbd\ub9dd\uc744 \uad6c\ucd95\ud558\uae30 \uc704\ud574 \ud544\uc218\uc801\uc778 \ud328\ud0a4\uc9c0\uc785\ub2c8\ub2e4. PyTorch \ud30c\uc774\uc36c \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 \uc790\ub3d9 \ubbf8\ubd84 API \ub300\ubd80\ubd84\uc740 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc5d0\uc11c\ub3c4 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc73c\uba70, \ud30c\uc774\uc36c\uc5d0\uc11c C++\ub85c \uc790\ub3d9 \ubbf8\ubd84 \ucf54\ub4dc\ub97c \uc27d\uac8c \ubcc0\ud658\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 PyTorch C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc5d0\uc11c \uc790\ub3d9 \ubbf8\ubd84\uc744 \uc218\ud589\ud558\ub294 \uba87 \uac00\uc9c0 \uc608\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 \uc5ec\ub7ec\ubd84\uc774 \ud30c\uc774\uc36c \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 \uc790\ub3d9 \ubbf8\ubd84\uc5d0 \ub300\ud574 \uae30\ubcf8\uc801\uc73c\ub85c \uc774\ud574\ud558\uace0 \uc788\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4. \uadf8\ub807\uc9c0 \uc54a\uc740 \uacbd\uc6b0 \uba3c\uc800 Autograd: Automatic Differentiation \uc744 \uc77d\uc5b4\ubcf4\uc138\uc694. \uae30\ucd08 \uc790\ub3d9 \ubbf8\ubd84 \uc5f0\uc0b0# (\uc774 \ud29c\ud1a0\ub9ac\uc5bc \uc758 \ub0b4\uc6a9\uc5d0 \uae30\ubc18\ud568) \ud150\uc11c\ub97c \uc0dd\uc131\ud558\uace0 \uadf8\uac83\uc758 \uacc4\uc0b0\uc744 \ucd94\uc801\ud558\uae30 \uc704\ud574 torch::requires_grad() \ub97c \uc2e4\ud589\ud574\ubd05\uc2dc\ub2e4. auto x = torch::ones({2, 2}, torch::requires_grad()); std::cout \u003c\u003c x \u003c\u003c std::endl; Out: 1 1 1 1 [ CPUFloatType{2,2} ] \ud150\uc11c \uc5f0\uc0b0\uc744 \uc218\ud589\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. auto y = x + 2; std::cout \u003c\u003c y \u003c\u003c std::endl; Out: 3 3 3 3 [ CPUFloatType{2,2} ] y \ub294 \uc5f0\uc0b0\uc758 \uacb0\uacfc\ub85c \uc0dd\uc131\ub418\uc5c8\uc73c\ubbc0\ub85c grad_fn \ub97c \uac16\uace0 \uc788\uc2b5\ub2c8\ub2e4. std::cout \u003c\u003c y.grad_fn()-\u003ename() \u003c\u003c std::endl; Out: AddBackward1 y \uc5d0 \ub300\ud574 \ub354 \ub9ce\uc740 \uc5f0\uc0b0\uc744 \uc218\ud589\ud574\ubd05\uc2dc\ub2e4. auto z = y * y * 3; auto out = z.mean(); std::cout \u003c\u003c z \u003c\u003c std::endl; std::cout \u003c\u003c z.grad_fn()-\u003ename() \u003c\u003c std::endl; std::cout \u003c\u003c out \u003c\u003c std::endl; std::cout \u003c\u003c out.grad_fn()-\u003ename() \u003c\u003c std::endl; Out: 27 27 27 27 [ CPUFloatType{2,2} ] MulBackward1 27 [ CPUFloatType{} ] MeanBackward0 .requires_grad_( ... ) \ub294 in-place\ub85c \ud150\uc11c\uc758 \uae30\uc874 requires_grad \ud50c\ub798\uadf8\ub97c \ubc14\uafc9\ub2c8\ub2e4. auto a = torch::randn({2, 2}); a = ((a * 3) / (a - 1)); std::cout \u003c\u003c a.requires_grad() \u003c\u003c std::endl; a.requires_grad_(true); std::cout \u003c\u003c a.requires_grad() \u003c\u003c std::endl; auto b = (a * a).sum(); std::cout \u003c\u003c b.grad_fn()-\u003ename() \u003c\u003c std::endl; Out: false true SumBackward0 \uc774\uc81c \uc5ed\uc804\ud30c\ub97c \uc218\ud589\ud574\ubd05\uc2dc\ub2e4. out \uc774 \ub2e8\uc77c \uc2a4\uce7c\ub77c\ub9cc\uc744 \ud3ec\ud568\ud558\ubbc0\ub85c, out.backward() \ub294 out.backward(torch::tensor(1.)) \uc640 \uac19\uc2b5\ub2c8\ub2e4. out.backward(); \ubcc0\ud654\ub3c4 d(out)/dx\ub97c \ucd9c\ub825\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. std::cout \u003c\u003c x.grad() \u003c\u003c std::endl; Out: 4.5000 4.5000 4.5000 4.5000 [ CPUFloatType{2,2} ] 4.5 \ud589\ub82c\uc774 \ucd9c\ub825\ub3fc\uc57c \ud569\ub2c8\ub2e4. \uc774 \uac12\uc744 \uc5bb\ub294 \uacfc\uc815\uc5d0 \ub300\ud55c \uc124\uba85\uc740 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \ud574\ub2f9 \uc139\uc158 \uc5d0\uc11c \ud655\uc778\ud558\uc138\uc694. \uc774\uc81c \ubca1\ud130-\uc57c\ucf54\ube44\uc548 \uacf1\uc758 \uc608\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. x = torch::randn(3, torch::requires_grad()); y = x * 2; while (y.norm().item\u003cdouble\u003e() \u003c 1000) { y = y * 2; } std::cout \u003c\u003c y \u003c\u003c std::endl; std::cout \u003c\u003c y.grad_fn()-\u003ename() \u003c\u003c std::endl; Out: -1021.4020 314.6695 -613.4944 [ CPUFloatType{3} ] MulBackward1 \ubca1\ud130-\uc57c\ucf54\ube44\uc548 \uacf1\uc744 \uc5bb\uae30 \uc704\ud574 \ubca1\ud130\ub97c backward \uc758 \uc778\uc790\ub85c \ub123\uc5b4\uc90d\ub2c8\ub2e4. auto v = torch::tensor({0.1, 1.0, 0.0001}, torch::kFloat); y.backward(v); std::cout \u003c\u003c x.grad() \u003c\u003c std::endl; Out: 102.4000 1024.0000 0.1024 [ CPUFloatType{3} ] \ub610\ud55c \ucf54\ub4dc\uc5d0 torch::NoGradGuard \ub97c \ub123\uc5b4\uc8fc\uba74 \uc790\ub3d9 \ubbf8\ubd84\uc73c\ub85c \ud558\uc5ec\uae08 \uadf8\ub798\ub514\uc5b8\ud2b8\uac00 \ud544\uc694\ud55c \ud150\uc11c\ub97c \ucd94\uc801\ud558\uc9c0 \uc54a\ub3c4\ub85d \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. std::cout \u003c\u003c x.requires_grad() \u003c\u003c std::endl; std::cout \u003c\u003c x.pow(2).requires_grad() \u003c\u003c std::endl; { torch::NoGradGuard no_grad; std::cout \u003c\u003c x.pow(2).requires_grad() \u003c\u003c std::endl; } Out: true true false \ud639\uc740 .detach() \ub97c \uc0ac\uc6a9\ud558\uc5ec \ub0b4\uc6a9\uc740 \ub3d9\uc77c\ud558\uc9c0\ub9cc \uadf8\ub798\ub514\uc5b8\ud2b8\uac00 \ud544\uc694 \uc5c6\ub294 \uc0c8 \ud150\uc11c\ub97c \uc5bb\uc744 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. std::cout \u003c\u003c x.requires_grad() \u003c\u003c std::endl; y = x.detach(); std::cout \u003c\u003c y.requires_grad() \u003c\u003c std::endl; std::cout \u003c\u003c x.eq(y).all().item\u003cbool\u003e() \u003c\u003c std::endl; Out: true false true grad / requires_grad / is_leaf / backward / detach / detach_ / register_hook / retain_grad \ub4f1 C++ \ud150\uc11c \uc790\ub3d9 \ubbf8\ubd84 API\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ud574\ub2f9 C++ API \ubb38\uc11c \uc5d0\uc11c \ud655\uc778\ud558\uc138\uc694. C++\ub85c \uace0\ucc28\uc6d0 \uadf8\ub798\ub514\uc5b8\ud2b8 \uacc4\uc0b0\ud558\uae30# \uace0\ucc28\uc6d0 \uadf8\ub798\ub514\uc5b8\ud2b8\ub97c \uc0ac\uc6a9\ud558\ub294 \uc0ac\ub840\ub85c \uadf8\ub798\ub514\uc5b8\ud2b8 \ud328\ub110\ud2f0 \uacc4\uc0b0\uc774 \uc788\uc2b5\ub2c8\ub2e4. torch::autograd::grad \ub97c \uc0ac\uc6a9\ud558\ub294 \uc608\ub97c \uc0b4\ud3b4\ubd05\uc2dc\ub2e4. #include \u003ctorch/torch.h\u003e auto model = torch::nn::Linear(4, 3); auto input = torch::randn({3, 4}).requires_grad_(true); auto output = model(input); // Calculate loss auto target = torch::randn({3, 3}); auto loss = torch::nn::MSELoss()(output, target); // Use norm of gradients as penalty auto grad_output = torch::ones_like(output); auto gradient = torch::autograd::grad({output}, {input}, /*grad_outputs=*/{grad_output}, /*create_graph=*/true)[0]; auto gradient_penalty = torch::pow((gradient.norm(2, /*dim=*/1) - 1), 2).mean(); // Add gradient penalty to loss auto combined_loss = loss + gradient_penalty; combined_loss.backward(); std::cout \u003c\u003c input.grad() \u003c\u003c std::endl; Out: -0.1042 -0.0638 0.0103 0.0723 -0.2543 -0.1222 0.0071 0.0814 -0.1683 -0.1052 0.0355 0.1024 [ CPUFloatType{3,4} ] torch::autograd::backward (\ub9c1\ud06c) \ubc0f torch::autograd::grad (\ub9c1\ud06c) \ubb38\uc11c\uc5d0\uc11c \uc774 \ud568\uc218\ub4e4\uc758 \uc0ac\uc6a9\ubc95\uc5d0 \ub300\ud574 \ub354 \uc54c\uc544\ubcf4\uc138\uc694. C++\uc5d0\uc11c \uc0ac\uc6a9\uc790 \uc9c0\uc815 \uc790\ub3d9 \ubbf8\ubd84 \ud568\uc218 \uc0ac\uc6a9\ud558\uae30# (\uc774 \ud29c\ud1a0\ub9ac\uc5bc \uc758 \ub0b4\uc6a9\uc5d0 \uae30\ubc18\ud568) torch::autograd \uc5d0 \uc0c8\ub85c\uc6b4 \uae30\ubcf8(elementary) \uc5f0\uc0b0\uc744 \ucd94\uac00\ud558\ub824\uba74 \uac01 \uc5f0\uc0b0\uc5d0 \ub300\ud574 \uc0c8\ub85c\uc6b4 torch::autograd::Function \ud558\uc704 \ud074\ub798\uc2a4(subclass)\ub97c \uad6c\ud604\ud574\uc57c \ud569\ub2c8\ub2e4. torch::autograd \ub294 \uacb0\uacfc\uc640 \uadf8\ub798\ub514\uc5b8\ud2b8\ub97c \uacc4\uc0b0\ud558\uace0 \uc5f0\uc0b0 \uae30\ub85d\uc744 \uc778\ucf54\ub529\ud558\uae30 \uc704\ud574 \uc704\ud574 \uc774 torch::autograd::Function \ub4e4\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ubaa8\ub4e0 \uc0c8\ub85c\uc6b4 \ud568\uc218\uc5d0\ub294 \ub450 \uac00\uc9c0 \ubc29\ubc95, \uc989 forward \uc640 backward \ub97c \uad6c\ud604\ud574\uc57c \ud558\uba70 \uc790\uc138\ud55c \uc694\uad6c\uc0ac\ud56d\uc740 \uc774 \ub9c1\ud06c \uc5d0\uc11c \ud655\uc778\ud558\uc138\uc694. \uc544\ub798 \ucf54\ub4dc\ub294 torch::nn \uc758 Linear \ud568\uc218\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. #include \u003ctorch/torch.h\u003e using namespace torch::autograd; // Inherit from Function class LinearFunction : public Function\u003cLinearFunction\u003e { public: // Note that both forward and backward are static functions // bias is an optional argument static torch::Tensor forward( AutogradContext *ctx, torch::Tensor input, torch::Tensor weight, torch::Tensor bias = torch::Tensor()) { ctx-\u003esave_for_backward({input, weight, bias}); auto output = input.mm(weight.t()); if (bias.defined()) { output += bias.unsqueeze(0).expand_as(output); } return output; } static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) { auto saved = ctx-\u003eget_saved_variables(); auto input = saved[0]; auto weight = saved[1]; auto bias = saved[2]; auto grad_output = grad_outputs[0]; auto grad_input = grad_output.mm(weight); auto grad_weight = grad_output.t().mm(input); auto grad_bias = torch::Tensor(); if (bias.defined()) { grad_bias = grad_output.sum(0); } return {grad_input, grad_weight, grad_bias}; } }; \uc774\uc81c \uc544\ub798\uc640 \uac19\uc774 LinearFunction \uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. auto x = torch::randn({2, 3}).requires_grad_(); auto weight = torch::randn({4, 3}).requires_grad_(); auto y = LinearFunction::apply(x, weight); y.sum().backward(); std::cout \u003c\u003c x.grad() \u003c\u003c std::endl; std::cout \u003c\u003c weight.grad() \u003c\u003c std::endl; Out: 0.5314 1.2807 1.4864 0.5314 1.2807 1.4864 [ CPUFloatType{2,3} ] 3.7608 0.9101 0.0073 3.7608 0.9101 0.0073 3.7608 0.9101 0.0073 3.7608 0.9101 0.0073 [ CPUFloatType{4,3} ] \uc5ec\uae30\uc11c, \ud150\uc11c\uac00 \uc544\ub2cc \uc778\uc790\ub97c \ub9e4\uac1c\ubcc0\uc218\ub85c \uac16\ub294 \ub610 \ub2e4\ub978 \ud568\uc218\ub97c \uc608\ub85c \ub4e4\uc5b4 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. #include \u003ctorch/torch.h\u003e using namespace torch::autograd; class MulConstant : public Function\u003cMulConstant\u003e { public: static torch::Tensor forward(AutogradContext *ctx, torch::Tensor tensor, double constant) { // ctx is a context object that can be used to stash information // for backward computation ctx-\u003esaved_data[\"constant\"] = constant; return tensor * constant; } static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) { // We return as many input gradients as there were arguments. // Gradients of non-tensor arguments to forward must be `torch::Tensor()`. return {grad_outputs[0] * ctx-\u003esaved_data[\"constant\"].toDouble(), torch::Tensor()}; } }; \uc774\uc81c \uc544\ub798\uc640 \uac19\uc774 MulConstant \ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. auto x = torch::randn({2}).requires_grad_(); auto y = MulConstant::apply(x, 5.5); y.sum().backward(); std::cout \u003c\u003c x.grad() \u003c\u003c std::endl; Out: 5.5000 5.5000 [ CPUFloatType{2} ] torch::autograd::Function \uc5d0 \ub300\ud55c \ub354 \ub9ce\uc740 \ub0b4\uc6a9\uc740 \uc774 \ubb38\uc11c \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud30c\uc774\uc36c \uc790\ub3d9 \ubbf8\ubd84 \ucf54\ub4dc\ub97c C++\ub85c \ubcc0\ud658\ud558\uae30# \uac1c\ub7b5\uc801\uc73c\ub85c \ub9d0\ud558\uba74, C++\uc5d0\uc11c \uc790\ub3d9 \ubbf8\ubd84\uc744 \uc0ac\uc6a9\ud558\ub294 \uac00\uc7a5 \uc26c\uc6b4 \ubc29\ubc95\uc740 \uba3c\uc800 \ud30c\uc774\uc36c\uc5d0\uc11c \ub3d9\uc791\ud558\ub294 \uc790\ub3d9 \ubbf8\ubd84 \ucf54\ub4dc\ub97c \uc791\uc131\ud55c \ud6c4, \uc544\ub798 \ud45c\ub97c \ucc38\uace0\ud574 C++ \ucf54\ub4dc\ub85c \ubcc0\ud658\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. Python C++ torch.autograd.backward torch::autograd::backward (\ub9c1\ud06c) torch.autograd.grad torch::autograd::grad (\ub9c1\ud06c) torch.Tensor.detach torch::Tensor::detach (\ub9c1\ud06c) torch.Tensor.detach_ torch::Tensor::detach_ (\ub9c1\ud06c) torch.Tensor.backward torch::Tensor::backward (\ub9c1\ud06c) torch.Tensor.register_hook torch::Tensor::register_hook (\ub9c1\ud06c) torch.Tensor.requires_grad torch::Tensor::requires_grad_ (\ub9c1\ud06c) torch.Tensor.retain_grad torch::Tensor::retain_grad (\ub9c1\ud06c) torch.Tensor.grad torch::Tensor::grad (\ub9c1\ud06c) torch.Tensor.grad_fn torch::Tensor::grad_fn (\ub9c1\ud06c) torch.Tensor.set_data torch::Tensor::set_data (\ub9c1\ud06c) torch.Tensor.data torch::Tensor::data (\ub9c1\ud06c) torch.Tensor.output_nr torch::Tensor::output_nr (\ub9c1\ud06c) torch.Tensor.is_leaf torch::Tensor::is_leaf (\ub9c1\ud06c) \ub300\ubd80\ubd84\uc758 \ubcc0\ud658\ub41c \ud30c\uc774\uc36c \uc790\ub3d9 \ubbf8\ubd84 \ucf54\ub4dc\uac00 C++\uc5d0\uc11c\ub3c4 \uc798 \ub3d9\uc791\ud560 \uac83\uc785\ub2c8\ub2e4. \ub3d9\uc791\ud558\uc9c0 \uc54a\uc744 \uacbd\uc6b0, GitHub issues \uc5d0 \ubc84\uadf8 \ub9ac\ud3ec\ud2b8\ub97c \uc81c\ucd9c\ud574 \uc8fc\uc2dc\uba74 \ucd5c\ub300\ud55c \ube68\ub9ac \uace0\uccd0\ub4dc\ub9ac\uaca0\uc2b5\ub2c8\ub2e4. \uacb0\ub860# \uc774\uc81c PyTorch\uc758 C++ \uc790\ub3d9 \ubbf8\ubd84 API\uc5d0 \ub300\ud55c \uac1c\uad04\uc801\uc778 \uc774\ud574\uac00 \uc0dd\uacbc\uc744 \uac83\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c \uc0ac\uc6a9\ub41c \ucf54\ub4dc \uc608\uc81c\ub4e4\uc740 \uc5ec\uae30 \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5b8\uc81c\ub098 \uadf8\ub807\ub4ef\uc774 \uc5b4\ub5a4 \ubb38\uc81c\uac00 \uc0dd\uae30\uac70\ub098 \uc9c8\ubb38\uc774 \uc788\uc73c\uba74 \uc800\ud76c \ud3ec\ub7fc \uc744 \uc774\uc6a9\ud558\uac70\ub098 Github \uc774\uc288 \ub85c \uc5f0\ub77d\uc8fc\uc138\uc694.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/advanced/cpp_autograd.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>