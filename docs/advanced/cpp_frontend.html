
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="PyTorch C++ 프론트엔드 사용하기" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/advanced/cpp_frontend.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="번역: 유용환 PyTorch C++ 프론트엔드는 PyTorch 머신러닝 프레임워크의 순수 C++ 인터페이스입니다. PyTorch의 주된 인터페이스는 물론 파이썬이지만 이 곳의 API는 텐서(tensor)나 자동 미분과 같은 기초적인 자료구조 및 기능을 제공하는 C++ 코드베이스 위에 구현되었습니다. C++ 프론트엔드는 이러한 기초적인 C++ 코드베이스를 비롯해 머신러닝 학습과 추론을 위해 필요한 도구들을 상속하는 순수 C++11 API를 제공합니다. 여기에는 신경망 모델링을 위해 필요한 공용 컴포넌트들의 빌트인 모음, 그것을 ..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="번역: 유용환 PyTorch C++ 프론트엔드는 PyTorch 머신러닝 프레임워크의 순수 C++ 인터페이스입니다. PyTorch의 주된 인터페이스는 물론 파이썬이지만 이 곳의 API는 텐서(tensor)나 자동 미분과 같은 기초적인 자료구조 및 기능을 제공하는 C++ 코드베이스 위에 구현되었습니다. C++ 프론트엔드는 이러한 기초적인 C++ 코드베이스를 비롯해 머신러닝 학습과 추론을 위해 필요한 도구들을 상속하는 순수 C++11 API를 제공합니다. 여기에는 신경망 모델링을 위해 필요한 공용 컴포넌트들의 빌트인 모음, 그것을 ..." />
<meta property="og:ignore_canonical" content="true" />

    <title>PyTorch C++ 프론트엔드 사용하기 &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'advanced/cpp_frontend';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/advanced/cpp_frontend.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="C++ 프론트엔드의 자동 미분 (autograd)" href="cpp_autograd.html" />
    <link rel="prev" title="Per-sample-gradients" href="../intermediate/per_sample_grads.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../beginner/profiler.html">PyTorch 모듈 프로파일링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/parametrizations.html">Parametrizations Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/pruning_tutorial.html">가지치기 기법(Pruning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/scaled_dot_product_attention_tutorial.html">(Beta) Scaled Dot Product Attention (SDPA)로 고성능 트랜스포머(Transformers) 구현하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/knowledge_distillation_tutorial.html">Knowledge Distillation Tutorial</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frontend APIs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intermediate/memory_format_tutorial.html">(베타) PyTorch를 사용한 Channels Last 메모리 형식</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/forward_ad_usage.html">Forward-mode Automatic Differentiation (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing function transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/ensembling.html">모델 앙상블</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">PyTorch C++ 프론트엔드 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_autograd.html">C++ 프론트엔드의 자동 미분 (autograd)</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../deep-dive.html" class="nav-link">Deep Dive</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">PyTorch C++...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../deep-dive.html">
        <meta itemprop="name" content="Deep Dive">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="PyTorch C++ 프론트엔드 사용하기">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">advanced/cpp_frontend</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="pytorch-c">
<h1>PyTorch C++ 프론트엔드 사용하기<a class="headerlink" href="#pytorch-c" title="Link to this heading">#</a></h1>
<p><strong>번역</strong>: <a class="reference external" href="https://github.com/yoosful">유용환</a></p>
<p>PyTorch C++ 프론트엔드는 PyTorch 머신러닝 프레임워크의 순수 C++
인터페이스입니다. PyTorch의 주된 인터페이스는 물론 파이썬이지만
이 곳의 API는 텐서(tensor)나 자동 미분과 같은 기초적인 자료구조
및 기능을 제공하는 C++ 코드베이스 위에 구현되었습니다. C++
프론트엔드는 이러한 기초적인 C++ 코드베이스를 비롯해 머신러닝 학습과 추론을
위해 필요한 도구들을 상속하는 순수 C++11 API를 제공합니다. 여기에는
신경망 모델링을 위해 필요한 공용 컴포넌트들의 빌트인 모음, 그것을
상속하기 위한 커스텀 모듈, 확률적 경사 하강법과 같은 유명한 최적화 알고리즘
라이브러리, 병렬 데이터 로더 및 데이터셋을 정의하고 불러오기 위한
API, 직렬화 루틴 등이 포함됩니다.</p>
<p>이 튜토리얼은 C++ 프론트엔드로 모델을 학습하는 엔드 투 엔드
예제를 안내합니다. 구체적으로, 우리는 생성 모델 중 하나인
<a class="reference external" href="https://arxiv.org/abs/1511.06434">DCGAN</a> 을 학습시켜
MNIST 숫자 이미지들을 생성할 것입니다. 개념적으로 쉬운 예시이지만,
여러분이 PyTorch C++ 프론트엔드에 대한 대략적인 개요를 파악하고 더
복잡한 모델을 학습시키고 싶은 욕구를 불러일으키기에 충분할 것입니다.
먼저 C++ 프론트엔드 사용에 대한 동기부여가 될 만한 이야기로 시작하고,
곧바로 모델을 정의하고 학습해 보도록 하겠습니다.</p>
<div class="admonition tip">
<p class="admonition-title">팁</p>
<p>C++ 프론트엔드에 대한 짧고 재미있는 발표를 보려면 <a class="reference external" href="https://www.youtube.com/watch?v=auRPXMMHJzc">CppCon 2018 라이트닝 토크</a> 를 시청하세요.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">팁</p>
<p><a class="reference external" href="https://pytorch.org/cppdocs/frontend.html">이 노트</a> 는 C++
프론트엔드의 컴포넌트와 디자인 철학의 전반적인 개요를 제공합니다.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">팁</p>
<p>PyTorch C++ 생태계에 대한 문서는 <a class="reference external" href="https://pytorch.org">https://pytorch.org</a>/cppdocs에서
확인할 수 있습니다. API 레벨의 문서뿐만 아니라 개괄적인 설명도
찾을 수 있을 것입니다.</p>
</div>
<section id="id3">
<h2>동기부여<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>GAN과 MNIST 숫자로의 설레는 여정을 시작하기에 앞서, 먼저
파이썬 대신 C++ 프론트엔드를 사용하는 이유에 대해
설명하겠습니다. 우리(PyTorch 팀)는 파이썬을 사용할 수 없거나
사용하기에 적합하지 않은 환경에서 연구를 가능하게 하기 위해
C++ 프론트엔드를 만들었습니다. 예를 들면 다음과 같습니다.</p>
<ul class="simple">
<li><p><strong>저지연 시스템</strong>: 초당 프레임 수가 높고 지연 시간이 짧은
순수 C++ 게임 엔진에서 강화 학습 연구를 수행할 수 있습니다.
그러한 환경에서는 파이썬 라이브러리보다 순수 C++ 라이브러리를
사용하는 것이 훨씬 더 적합합니다. 파이썬은 느린 인터프리터
때문에 다루기가 쉽지 않습니다.</p></li>
<li><p><strong>고도의 멀티쓰레딩 환경</strong>: 글로벌 인터프리터 락(GIL)으로 인해
파이썬은 동시에 둘 이상의 시스템 쓰레드를 실행할 수 없습니다.
대안으로 멀티프로세싱을 사용하면 확장성이 떨어지며 심각한 한계가
있습니다. C++는 이러한 제약 조건이 없으며 쓰레드를 쉽게 만들고
사용할 수 있습니다. <a class="reference external" href="https://www.uber.com/blog/deep-neuroevolution/">Deep Neuroevolution</a> 에
사용된 것과 같이 고도의 병렬화가 필요한 모델도 이를 활용할 수
있습니다.</p></li>
<li><p><strong>기존의 C++ 코드베이스</strong>: 백엔드 서버의 웹 페이지 서비스부터
사진 편집 소프트웨어의 3D 그래픽 렌더링에 이르기까지 어떠한
작업이라도 수행하는 기존 C++ 애플리케이션 소유자로서, 머신러닝
방법론을 시스템에 통합하고 싶을 수 있습니다. C++ 프론트엔드는
PyTorch (파이썬) 경험 본연의 높은 유연성과 직관성을 유지하면서,
파이썬과 C++를 앞뒤로 바인딩하는 번거로움 없이 C++를 사용할 수
있게 해줍니다.</p></li>
</ul>
<p>C++ 프론트엔드의 목적은 파이썬 프론트엔드와 경쟁하는 것이 아닌
보완하는 것입니다. 연구자와 엔지니어 모두가 PyTorch의 단순성,
유연성 및 직관적인 API를 매우 좋아합니다. 우리의 목표는 여러분이
위의 예시를 비롯한 모든 가능한 환경에서 이 핵심 디자인 원칙을
이용할 수 있도록 하는 것입니다. 이러한 시나리오 중 하나가 여러분의
사례에 해당하거나, 단순히 관심이 있거나 궁금하다면 아래 내용을 통해
C++ 프론트엔드에 대해 자세히 살펴보세요.</p>
<div class="admonition tip">
<p class="admonition-title">팁</p>
<blockquote>
<div><p>C++ 프론트엔드는 파이썬 프론트엔드와 최대한 유사한 API를</p>
</div></blockquote>
<p>제공하고자 합니다. 만일 파이썬 프론트엔드에 익숙한 사람이 “C++
프론트엔드로 X를 어떻게 해야 하는가?” 의문을 갖는다면, 많은 경우에
파이썬에서와 같은 방식으로 코드를 작성해 파이썬에서와 동일한 함수와
메서드를 사용할 수 있을 것입니다. (다만, 온점을 더블 콜론으로 바꾸는
것에 유의하세요.)</p>
</div>
</section>
<section id="id4">
<h2>기본 애플리케이션 작성하기<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<p>먼저 최소한의 C++ 애플리케이션을 작성해 우리의 설정과
빌드 환경이 동일한지 확인하겠습니다. 먼저, C++
프론트엔드를 사용하는 데 필요한 모든 관련 헤더, 라이브러리 및
CMake 빌드 파일을 패키징하는 <em>LibTorch</em> 배포판의 사본이
필요합니다. 리눅스, 맥OS, 윈도우용 LibTorch 배포판은
<a class="reference external" href="https://pytorch.org/get-started/locally/">PyTorch website</a> 에서
다운로드할 수 있습니다. 이 튜토리얼의 나머지 부분은 기본 우분투 리눅스
환경을 가정하지만 맥OS나 윈도우를 사용하셔도 괜찮습니다.</p>
<div class="admonition tip">
<p class="admonition-title">팁</p>
<p><a class="reference external" href="https://pytorch.org/cppdocs/installing.html">PyTorch C++ 배포판 설치</a>
의 설명에 다음의 과정이 더 자세히 안내되어
있습니다.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">팁</p>
<p>윈도우에서는 디버그 및 릴리스 빌드가 ABI와 호환되지 않습니다. 프로젝트를
디버그 모드로 빌드하려면 LibTorch의 디버그 버전을 사용해보세요.
아래의 <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">--build</span> <span class="pre">.</span></code> 에 올바른 설정을 지정하는 것도 잊지
마세요.</p>
</div>
<p>가장 먼저 할 것은 PyTorch 웹사이트에서 검색된 링크를 통해 LibTorch
배포판을 로컬에 다운로드하는 것입니다. 일반적 Ubuntu Linux 환경의 경우
다음 명령어를 실행합니다.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># CUDA 9.0 등에 대한 지원이 필요한 경우 아래 URL에서 &quot;cpu&quot;를 &quot;cu90&quot;로 바꾸세요.</span>
wget<span class="w"> </span>https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip
unzip<span class="w"> </span>libtorch-shared-with-deps-latest.zip
</pre></div>
</div>
<p>다음으로 <code class="docutils literal notranslate"><span class="pre">torch/torch.h</span></code> 를 호출하는 <code class="docutils literal notranslate"><span class="pre">dcgan.cpp</span></code> 라는 이름의 C++
파일 하나를 작성합시다. 우선은 아래와 같이 3x3 항등 행렬을 출력하기만 하면
됩니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/torch.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>이 작은 애플리케이션과 이후 완성할 학습용 스크립트를 빌드하기 위해 우리는 아래의 <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code> 를
사용할 것입니다:</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.0</span><span class="w"> </span><span class="s">FATAL_ERROR</span><span class="p">)</span>
<span class="nb">project</span><span class="p">(</span><span class="s">dcgan</span><span class="p">)</span>

<span class="nb">find_package</span><span class="p">(</span><span class="s">Torch</span><span class="w"> </span><span class="s">REQUIRED</span><span class="p">)</span>

<span class="nb">add_executable</span><span class="p">(</span><span class="s">dcgan</span><span class="w"> </span><span class="s">dcgan.cpp</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">dcgan</span><span class="w"> </span><span class="s2">&quot;${TORCH_LIBRARIES}&quot;</span><span class="p">)</span>
<span class="nb">set_property</span><span class="p">(</span><span class="s">TARGET</span><span class="w"> </span><span class="s">dcgan</span><span class="w"> </span><span class="s">PROPERTY</span><span class="w"> </span><span class="s">CXX_STANDARD</span><span class="w"> </span><span class="s">14</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>CMake는 LibTorch에 권장되는 빌드 시스템이지만 필수 요구
사항은 아닙니다. Visual Studio 프로젝트 파일, QMake, 일반
Make 파일 등 다른 빌드 환경을 사용해도 됩니다. 하지만
이에 대한 즉각적인 지원은 제공하지 않습니다.</p>
</div>
<p>위 CMake 파일 4번째 줄의 <code class="docutils literal notranslate"><span class="pre">find_package(Torch</span> <span class="pre">REQUIRED)</span></code> 는
CMake가 LibTorch 라이브러리 빌드 설정을 찾도록 안내합니다.
CMake가 해당 파일의 <em>위치</em> 를 찾을 수 있도록 하려면 <code class="docutils literal notranslate"><span class="pre">cmake</span></code> 호출 시
<code class="docutils literal notranslate"><span class="pre">CMAKE_PREFIX_PATH</span></code> 를 설정해야 합니다. 이에 앞서 <code class="docutils literal notranslate"><span class="pre">dcgan</span></code> 애플리케이션에
대해 디렉터리 구조를 다음과 같이 통일하도록 하겠습니다.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>dcgan/
<span class="w">  </span>CMakeLists.txt
<span class="w">  </span>dcgan.cpp
</pre></div>
</div>
<p>또한 앞으로 압축 해제된 LibTorch 배포판의 경로를 <code class="docutils literal notranslate"><span class="pre">/path/to/libtorch</span></code>
로 부르도록 하겠습니다. 이는 <strong>반드시 절대 경로여야</strong> 합니다. 특히
<code class="docutils literal notranslate"><span class="pre">CMAKE_PREFIX_PATH</span></code> 를 <code class="docutils literal notranslate"><span class="pre">../../libtorch</span></code> 와 같이 설정하면 예상치 못한
오류가 발생할 수 있습니다. 그보다는 <code class="docutils literal notranslate"><span class="pre">$PWD/../../libtorch</span></code> 와 같이 해당
절대 경로를 입력하세요. 이제 애플리케이션을 빌드할 준비가 되었습니다.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>root@fa350df05ecf:/home#<span class="w"> </span>mkdir<span class="w"> </span>build
root@fa350df05ecf:/home#<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
root@fa350df05ecf:/home/build#<span class="w"> </span>cmake<span class="w"> </span>-DCMAKE_PREFIX_PATH<span class="o">=</span>/path/to/libtorch<span class="w"> </span>..
--<span class="w"> </span>The<span class="w"> </span>C<span class="w"> </span>compiler<span class="w"> </span>identification<span class="w"> </span>is<span class="w"> </span>GNU<span class="w"> </span><span class="m">5</span>.4.0
--<span class="w"> </span>The<span class="w"> </span>CXX<span class="w"> </span>compiler<span class="w"> </span>identification<span class="w"> </span>is<span class="w"> </span>GNU<span class="w"> </span><span class="m">5</span>.4.0
--<span class="w"> </span>Check<span class="w"> </span><span class="k">for</span><span class="w"> </span>working<span class="w"> </span>C<span class="w"> </span>compiler:<span class="w"> </span>/usr/bin/cc
--<span class="w"> </span>Check<span class="w"> </span><span class="k">for</span><span class="w"> </span>working<span class="w"> </span>C<span class="w"> </span>compiler:<span class="w"> </span>/usr/bin/cc<span class="w"> </span>--<span class="w"> </span>works
--<span class="w"> </span>Detecting<span class="w"> </span>C<span class="w"> </span>compiler<span class="w"> </span>ABI<span class="w"> </span>info
--<span class="w"> </span>Detecting<span class="w"> </span>C<span class="w"> </span>compiler<span class="w"> </span>ABI<span class="w"> </span>info<span class="w"> </span>-<span class="w"> </span><span class="k">done</span>
--<span class="w"> </span>Detecting<span class="w"> </span>C<span class="w"> </span>compile<span class="w"> </span>features
--<span class="w"> </span>Detecting<span class="w"> </span>C<span class="w"> </span>compile<span class="w"> </span>features<span class="w"> </span>-<span class="w"> </span><span class="k">done</span>
--<span class="w"> </span>Check<span class="w"> </span><span class="k">for</span><span class="w"> </span>working<span class="w"> </span>CXX<span class="w"> </span>compiler:<span class="w"> </span>/usr/bin/c++
--<span class="w"> </span>Check<span class="w"> </span><span class="k">for</span><span class="w"> </span>working<span class="w"> </span>CXX<span class="w"> </span>compiler:<span class="w"> </span>/usr/bin/c++<span class="w"> </span>--<span class="w"> </span>works
--<span class="w"> </span>Detecting<span class="w"> </span>CXX<span class="w"> </span>compiler<span class="w"> </span>ABI<span class="w"> </span>info
--<span class="w"> </span>Detecting<span class="w"> </span>CXX<span class="w"> </span>compiler<span class="w"> </span>ABI<span class="w"> </span>info<span class="w"> </span>-<span class="w"> </span><span class="k">done</span>
--<span class="w"> </span>Detecting<span class="w"> </span>CXX<span class="w"> </span>compile<span class="w"> </span>features
--<span class="w"> </span>Detecting<span class="w"> </span>CXX<span class="w"> </span>compile<span class="w"> </span>features<span class="w"> </span>-<span class="w"> </span><span class="k">done</span>
--<span class="w"> </span>Looking<span class="w"> </span><span class="k">for</span><span class="w"> </span>pthread.h
--<span class="w"> </span>Looking<span class="w"> </span><span class="k">for</span><span class="w"> </span>pthread.h<span class="w"> </span>-<span class="w"> </span>found
--<span class="w"> </span>Looking<span class="w"> </span><span class="k">for</span><span class="w"> </span>pthread_create
--<span class="w"> </span>Looking<span class="w"> </span><span class="k">for</span><span class="w"> </span>pthread_create<span class="w"> </span>-<span class="w"> </span>not<span class="w"> </span>found
--<span class="w"> </span>Looking<span class="w"> </span><span class="k">for</span><span class="w"> </span>pthread_create<span class="w"> </span><span class="k">in</span><span class="w"> </span>pthreads
--<span class="w"> </span>Looking<span class="w"> </span><span class="k">for</span><span class="w"> </span>pthread_create<span class="w"> </span><span class="k">in</span><span class="w"> </span>pthreads<span class="w"> </span>-<span class="w"> </span>not<span class="w"> </span>found
--<span class="w"> </span>Looking<span class="w"> </span><span class="k">for</span><span class="w"> </span>pthread_create<span class="w"> </span><span class="k">in</span><span class="w"> </span>pthread
--<span class="w"> </span>Looking<span class="w"> </span><span class="k">for</span><span class="w"> </span>pthread_create<span class="w"> </span><span class="k">in</span><span class="w"> </span>pthread<span class="w"> </span>-<span class="w"> </span>found
--<span class="w"> </span>Found<span class="w"> </span>Threads:<span class="w"> </span>TRUE
--<span class="w"> </span>Found<span class="w"> </span>torch:<span class="w"> </span>/path/to/libtorch/lib/libtorch.so
--<span class="w"> </span>Configuring<span class="w"> </span><span class="k">done</span>
--<span class="w"> </span>Generating<span class="w"> </span><span class="k">done</span>
--<span class="w"> </span>Build<span class="w"> </span>files<span class="w"> </span>have<span class="w"> </span>been<span class="w"> </span>written<span class="w"> </span>to:<span class="w"> </span>/home/build
root@fa350df05ecf:/home/build#<span class="w"> </span>cmake<span class="w"> </span>--build<span class="w"> </span>.<span class="w"> </span>--config<span class="w"> </span>Release
Scanning<span class="w"> </span>dependencies<span class="w"> </span>of<span class="w"> </span>target<span class="w"> </span>dcgan
<span class="o">[</span><span class="w"> </span><span class="m">50</span>%<span class="o">]</span><span class="w"> </span>Building<span class="w"> </span>CXX<span class="w"> </span>object<span class="w"> </span>CMakeFiles/dcgan.dir/dcgan.cpp.o
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Linking<span class="w"> </span>CXX<span class="w"> </span>executable<span class="w"> </span>dcgan
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Built<span class="w"> </span>target<span class="w"> </span>dcgan
</pre></div>
</div>
<p>위에서 우리는 먼저 <code class="docutils literal notranslate"><span class="pre">dcgan</span></code> 디렉토리 안에 <code class="docutils literal notranslate"><span class="pre">build</span></code> 폴더를 만들고
이 폴더에 들어가서 필요한 빌드(Make) 파일을 생성하는 <code class="docutils literal notranslate"><span class="pre">cmake</span></code> 명령어를
실행한 후 <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">--build</span> <span class="pre">.</span> <span class="pre">--config</span> <span class="pre">Release</span></code> 를 실행하여 프로젝트를
성공적으로 컴파일했습니다. 이제 우리의 작은 바이너리를 실행하고 기본
프로젝트 설정에 대한 이 섹션을 완료할 준비가 됐습니다.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>root@fa350df05ecf:/home/build#<span class="w"> </span>./dcgan
<span class="m">1</span><span class="w">  </span><span class="m">0</span><span class="w">  </span><span class="m">0</span>
<span class="m">0</span><span class="w">  </span><span class="m">1</span><span class="w">  </span><span class="m">0</span>
<span class="m">0</span><span class="w">  </span><span class="m">0</span><span class="w">  </span><span class="m">1</span>
<span class="o">[</span><span class="w"> </span>Variable<span class="o">[</span>CPUFloatType<span class="o">]{</span><span class="m">3</span>,3<span class="o">}</span><span class="w"> </span><span class="o">]</span>
</pre></div>
</div>
<p>제가 보기엔 항등 행렬인 것 같군요!</p>
</section>
<section id="id6">
<h2>신경망 모델 정의하기<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<p>이제 기본적인 환경을 설정했으니, 이번 튜토리얼에서 훨씬
더 흥미로운 부분을 살펴봅시다. 먼저 C++ 프론트엔드에서 모듈을
정의하고 상호 작용하는 방법에 대해 논의하겠습니다. 기본적인
소규모 예제 모듈부터 시작하여 C++ 프론트엔드가 제공하는 다양한
내장 모듈 라이브러리를 사용하여 완성도 있는 GAN을 구현하겠습니다.</p>
<section id="api">
<h3>모듈 API 기초<a class="headerlink" href="#api" title="Link to this heading">#</a></h3>
<p>파이썬 인터페이스와 마찬가지로, C++ 프론트엔드에 기반을 둔 신경망도
<em>모듈</em> 이라 불리는 재사용 가능한 빌딩 블록으로 구성되어 있습니다. 파이썬에
다른 모든 모듈이 파생되는 <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> 라는 기본 모듈 클래스가
있듯이 C++에는 <code class="docutils literal notranslate"><span class="pre">torch::nn::Module</span></code> 클래스가 있습니다.
일반적으로 모듈에는 캡슐화된 알고리즘을 구현하는 <code class="docutils literal notranslate"><span class="pre">forward()</span></code>
메서드를 비롯해 매개변수, 버퍼 및 하위 모듈 세 가지 하위 객체가
포함됩니다.</p>
<p>매개변수와 버퍼는 텐서의 형태로 상태를 저장합니다. 매개변수는 그래디언트를
기록하지만 버퍼는 기록하지 않습니다. 매개변수는 일반적으로 신경망의 학습
가능한 가중치입니다. 버퍼의 예로는 배치 정규화를 위한 평균 및 분산이
있습니다. 특정 논리 및 상태 블록을 재사용하기 위해, PyTorch API는
모듈들이 중첩되는 것을 허용합니다. 중첩된 모듈은 <em>하위 모듈</em> 이라고
합니다.</p>
<p>매개변수, 버퍼 및 하위 모듈은 명시적으로 등록(register)을 해야 합니다.
등록이 되면 <code class="docutils literal notranslate"><span class="pre">parameters()</span></code> 나 <code class="docutils literal notranslate"><span class="pre">buffers()</span></code> 같은 메서드를 사용하여 (중첩을
포함한) 전체 모듈 계층 구조에서 모든 매개변수 묶음을 검색할 수 있습니다.
마찬가지로, <code class="docutils literal notranslate"><span class="pre">to(...)</span></code> 와 같은 메서드는 모듈 계층 구조 전체에 대한 메서드입니다.
예를 들어, <code class="docutils literal notranslate"><span class="pre">to(torch::kCUDA)</span></code> 는 모든 매개변수와 버퍼를 CPU에서 CUDA 메모리로
이동시킵니다.</p>
<section id="id7">
<h4>모듈 정의 및 매개변수 등록<a class="headerlink" href="#id7" title="Link to this heading">#</a></h4>
<p>이 내용을 코드로 구현하기 위해, 파이썬 인터페이스로 작성된 간단한 모듈 하나를
생각해 봅시다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">))</span>

  <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
</pre></div>
</div>
<p>이를 C++로 작성하면 다음과 같습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/torch.h&gt;</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">Net</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Net</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">M</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">W</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">register_parameter</span><span class="p">(</span><span class="s">&quot;W&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">({</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">}));</span>
<span class="w">    </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">register_parameter</span><span class="p">(</span><span class="s">&quot;b&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">addmm</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">W</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>파이썬에서와 마찬가지로 모듈 기본 클래스에서 파생한 <code class="docutils literal notranslate"><span class="pre">Net</span></code> 이라는 클래스를
정의합니다. (쉬운 설명을 위해 <code class="docutils literal notranslate"><span class="pre">class</span></code> 대신 <code class="docutils literal notranslate"><span class="pre">struct</span></code> 을 사용했습니다.)
파이썬에서 torch.randn을 사용하는 것처럼 생성자에서는 <code class="docutils literal notranslate"><span class="pre">torch::randn</span></code> 을
사용해 텐서를 만듭니다. 한 가지 흥미로운 차이점은 매개변수를 등록하는
방법입니다. 파이썬에서는 텐서를 <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> 으로 감싸는 것과 달리,
C++에서는 <code class="docutils literal notranslate"><span class="pre">register_parameter</span></code> 메서드를 통해 텐서를 전달해야
합니다. 이러한 차이의 원인은 파이썬 API의 경우, 어떤 속성(attirbute)이
<code class="docutils literal notranslate"><span class="pre">torch.nn.Parameter</span></code> 타입인지 감지해 그러한 텐서를 자동으로 등록할 수 있기
때문에 나타납니다. C++에서는 리플렉션(reflection)이 매우 제한적이므로 보다
전통적인 (그리하여 덜 마법적인) 방식이 제공됩니다.</p>
</section>
<section id="id8">
<h4>서브모듈 등록 및 모듈 계층 구조 탐색<a class="headerlink" href="#id8" title="Link to this heading">#</a></h4>
<p>매개변수 등록과 마찬가지 방법으로 서브모듈을 등록할 수 있습니다.
파이썬에서 서브모듈은 어떤 모듈의 속성으로 지정될 때 자동으로
감지되고 등록됩니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
      <span class="c1"># Registered as a submodule behind the scenes</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">another_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">M</span><span class="p">))</span>

  <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">another_bias</span>
</pre></div>
</div>
<p>예를 들어, <code class="docutils literal notranslate"><span class="pre">parameters()</span></code> 메서드를 사용하면 모듈 계층의 모든 매개변수에
재귀적으로 액세스할 수 있습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
<span class="go">[Parameter containing:</span>
<span class="go">tensor([0.0808, 0.8613, 0.2017, 0.5206, 0.5353], requires_grad=True), Parameter containing:</span>
<span class="go">tensor([[-0.3740, -0.0976, -0.4786, -0.4928],</span>
<span class="go">        [-0.1434,  0.4713,  0.1735, -0.3293],</span>
<span class="go">        [-0.3467, -0.3858,  0.1980,  0.1986],</span>
<span class="go">        [-0.1975,  0.4278, -0.1831, -0.2709],</span>
<span class="go">        [ 0.3730,  0.4307,  0.3236, -0.0629]], requires_grad=True), Parameter containing:</span>
<span class="go">tensor([ 0.2038,  0.4638, -0.2023,  0.1230, -0.0516], requires_grad=True)]</span>
</pre></div>
</div>
<p>C++에서 <code class="docutils literal notranslate"><span class="pre">torch::nn::Linear</span></code> 등의 모듈을 서브모듈로 등록하려면 이름에서
유추할 수 있듯이 <code class="docutils literal notranslate"><span class="pre">register_module()</span></code> 메서드를 사용합니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">Net</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Net</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">M</span><span class="p">)</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">linear</span><span class="p">(</span><span class="n">register_module</span><span class="p">(</span><span class="s">&quot;linear&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Linear</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">)))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">another_bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">register_parameter</span><span class="p">(</span><span class="s">&quot;b&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">linear</span><span class="p">(</span><span class="n">input</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">another_bias</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Linear</span><span class="w"> </span><span class="n">linear</span><span class="p">;</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">another_bias</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">팁</p>
<p><code class="docutils literal notranslate"><span class="pre">torch::nn</span></code> 에 대한 <a class="reference external" href="https://pytorch.org/cppdocs/api/namespace_torch__nn.html">이 문서</a>
에서 <code class="docutils literal notranslate"><span class="pre">torch::nn::Linear</span></code>, <code class="docutils literal notranslate"><span class="pre">torch::nn::Dropout</span></code>, <code class="docutils literal notranslate"><span class="pre">torch::nn::Conv2d</span></code>
등 사용 가능한 전체 빌트인 모듈 목록을 확인할 수
있습니다.</p>
</div>
<p>위 코드에서 한 가지 미묘한 사실은 서브모듈은 생성자의 이니셜라이저
목록에 작성되고 매개변수는 생성자의 바디(body)에 작성되었다는
것입니다. 여기에는 충분한 이유가 있으며 아래 C++ 프론트엔드의
<em>오너십 모델</em> 섹션에서 더 다룰 예정입니다. 그렇지만 최종 결론은
파이썬에서처럼 모듈 트리의 매개변수에 재귀적으로 액세스할 수
있다는 것입니다. <code class="docutils literal notranslate"><span class="pre">parameters()</span></code> 를 호출하면 순회가 가능한
<code class="docutils literal notranslate"><span class="pre">std::vector&lt;torch::Tensor&gt;</span></code> 가 반환됩니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Net</span><span class="w"> </span><span class="n">net</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">);</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>이를 실행한 결과는 다음과 같습니다.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>root@fa350df05ecf:/home/build#<span class="w"> </span>./dcgan
<span class="m">0</span>.0345
<span class="m">1</span>.4456
-0.6313
-0.3585
-0.4008
<span class="o">[</span><span class="w"> </span>Variable<span class="o">[</span>CPUFloatType<span class="o">]{</span><span class="m">5</span><span class="o">}</span><span class="w"> </span><span class="o">]</span>
-0.1647<span class="w">  </span><span class="m">0</span>.2891<span class="w">  </span><span class="m">0</span>.0527<span class="w"> </span>-0.0354
<span class="m">0</span>.3084<span class="w">  </span><span class="m">0</span>.2025<span class="w">  </span><span class="m">0</span>.0343<span class="w">  </span><span class="m">0</span>.1824
-0.4630<span class="w"> </span>-0.2862<span class="w">  </span><span class="m">0</span>.2500<span class="w"> </span>-0.0420
<span class="m">0</span>.3679<span class="w"> </span>-0.1482<span class="w"> </span>-0.0460<span class="w">  </span><span class="m">0</span>.1967
<span class="m">0</span>.2132<span class="w"> </span>-0.1992<span class="w">  </span><span class="m">0</span>.4257<span class="w">  </span><span class="m">0</span>.0739
<span class="o">[</span><span class="w"> </span>Variable<span class="o">[</span>CPUFloatType<span class="o">]{</span><span class="m">5</span>,4<span class="o">}</span><span class="w"> </span><span class="o">]</span>
<span class="m">0</span>.01<span class="w"> </span>*
<span class="m">3</span>.6861
-10.1166
-45.0333
<span class="m">7</span>.9983
-20.0705
<span class="o">[</span><span class="w"> </span>Variable<span class="o">[</span>CPUFloatType<span class="o">]{</span><span class="m">5</span><span class="o">}</span><span class="w"> </span><span class="o">]</span>
</pre></div>
</div>
<p>파이썬에서와 같이 세 개의 매개변수가 출력됐습니다. 이 매개변수들의 이름을
확인할 수 있도록 C++ API는 <code class="docutils literal notranslate"><span class="pre">named_parameters()</span></code> 메서드를 제공하며, 이는
파이썬에서와 같이 <code class="docutils literal notranslate"><span class="pre">Orderdict</span></code> 를 반환합니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Net</span><span class="w"> </span><span class="nf">net</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">);</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">pair</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">net</span><span class="p">.</span><span class="n">named_parameters</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">pair</span><span class="p">.</span><span class="n">key</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">pair</span><span class="p">.</span><span class="n">value</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>마찬가지로 코드를 실행하면 결과는 아래와 같습니다.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>root@fa350df05ecf:/home/build#<span class="w"> </span>make<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>./dcgan<span class="w">                                                                                                                                            </span><span class="m">11</span>:13:48
Scanning<span class="w"> </span>dependencies<span class="w"> </span>of<span class="w"> </span>target<span class="w"> </span>dcgan
<span class="o">[</span><span class="w"> </span><span class="m">50</span>%<span class="o">]</span><span class="w"> </span>Building<span class="w"> </span>CXX<span class="w"> </span>object<span class="w"> </span>CMakeFiles/dcgan.dir/dcgan.cpp.o
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Linking<span class="w"> </span>CXX<span class="w"> </span>executable<span class="w"> </span>dcgan
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Built<span class="w"> </span>target<span class="w"> </span>dcgan
b:<span class="w"> </span>-0.1863
-0.8611
-0.1228
<span class="m">1</span>.3269
<span class="m">0</span>.9858
<span class="o">[</span><span class="w"> </span>Variable<span class="o">[</span>CPUFloatType<span class="o">]{</span><span class="m">5</span><span class="o">}</span><span class="w"> </span><span class="o">]</span>
linear.weight:<span class="w">  </span><span class="m">0</span>.0339<span class="w">  </span><span class="m">0</span>.2484<span class="w">  </span><span class="m">0</span>.2035<span class="w"> </span>-0.2103
-0.0715<span class="w"> </span>-0.2975<span class="w"> </span>-0.4350<span class="w"> </span>-0.1878
-0.3616<span class="w">  </span><span class="m">0</span>.1050<span class="w"> </span>-0.4982<span class="w">  </span><span class="m">0</span>.0335
-0.1605<span class="w">  </span><span class="m">0</span>.4963<span class="w">  </span><span class="m">0</span>.4099<span class="w"> </span>-0.2883
<span class="m">0</span>.1818<span class="w"> </span>-0.3447<span class="w"> </span>-0.1501<span class="w"> </span>-0.0215
<span class="o">[</span><span class="w"> </span>Variable<span class="o">[</span>CPUFloatType<span class="o">]{</span><span class="m">5</span>,4<span class="o">}</span><span class="w"> </span><span class="o">]</span>
linear.bias:<span class="w"> </span>-0.0250
<span class="m">0</span>.0408
<span class="m">0</span>.3756
-0.2149
-0.3636
<span class="o">[</span><span class="w"> </span>Variable<span class="o">[</span>CPUFloatType<span class="o">]{</span><span class="m">5</span><span class="o">}</span><span class="w"> </span><span class="o">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p><code class="docutils literal notranslate"><span class="pre">torch::nn::Module</span></code> 에 대한 <a class="reference external" href="https://pytorch.org/cppdocs/api/classtorch_1_1nn_1_1_module.html#exhale-class-classtorch-1-1nn-1-1-module">문서</a> 는
모듈 계층 구조에 대한 메서드 목록 전체가 포함되어
있습니다.</p>
</div>
</section>
<section id="forward">
<h4>순전파(forward) 모드로 네트워크 실행<a class="headerlink" href="#forward" title="Link to this heading">#</a></h4>
<p>네트워크를 C++로 실행하기 위해서는, 우리가 정의한 <code class="docutils literal notranslate"><span class="pre">forward()</span></code> 메서드를
호출하기만 하면 됩니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Net</span><span class="w"> </span><span class="n">net</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">);</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">net</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">}))</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>출력은 대략 아래와 같을 것입니다</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>root@fa350df05ecf:/home/build#<span class="w"> </span>./dcgan
<span class="m">0</span>.8559<span class="w">  </span><span class="m">1</span>.1572<span class="w">  </span><span class="m">2</span>.1069<span class="w"> </span>-0.1247<span class="w">  </span><span class="m">0</span>.8060
<span class="m">0</span>.8559<span class="w">  </span><span class="m">1</span>.1572<span class="w">  </span><span class="m">2</span>.1069<span class="w"> </span>-0.1247<span class="w">  </span><span class="m">0</span>.8060
<span class="o">[</span><span class="w"> </span>Variable<span class="o">[</span>CPUFloatType<span class="o">]{</span><span class="m">2</span>,5<span class="o">}</span><span class="w"> </span><span class="o">]</span>
</pre></div>
</div>
</section>
<section id="ownership">
<h4>모듈 오너십 (Ownership)<a class="headerlink" href="#ownership" title="Link to this heading">#</a></h4>
<p>이제 우리는 C++에서 모듈을 정의하고, 매개변수를 등록하고, 하위 모듈을
등록하고, <code class="docutils literal notranslate"><span class="pre">parameters()</span></code> 등의 메서드를 통해 모듈 계층을 탐색하고,
모듈의 <code class="docutils literal notranslate"><span class="pre">forward()</span></code> 메서드를 실행하는 방법을 배웠습니다. C++ API에는
다른 메서드, 클래스, 그리고 주제가 많지만 전체 목록은 <a class="reference external" href="https://pytorch.org/cppdocs/api/namespace_torch__nn.html">문서</a> 를
참조하시기 바랍니다. 잠시 후에 DCGAN 모델과 엔드 투 엔드 학습
파이프라인을 구현하면서도 몇 가지 개념을 더 다룰 예정입니다. 그에 앞서
C++ 프론트엔드에서 <code class="docutils literal notranslate"><span class="pre">torch::nn::Module</span></code> 의 하위 클래스들에 대해 제공하는
<em>오너십 모델</em> 에 대해 간단히 설명하겠습니다.</p>
<p>이 논의에서 오너십 모델이란 모듈을 저장하고 전달하는 방식
(누가 혹은 무엇이 특정 모듈 인스턴스를 소유하는지)을 지칭합니다.
파이썬에서 객체는 항상 힙에 동적으로 할당되며 레퍼런스 시맨틱을
가지는데, 이는 다루고 이해하기가 매우 쉽습니다. 실제로 파이썬에서는
객체가 어디에 존재하고 어떻게 레퍼런스되는지 신경 쓰지 않고 하려는
일에만 집중할 수 있습니다.</p>
<p>저급 언어인 C++는 이 부분에서 더 많은 옵션을 제공합니다. 이는
C++ 프론트엔드의 복잡성을 증가시키며 그 설계와 인체공학적 요소에도
큰 영향을 줍니다. 특히, C++ 프론트엔드 모듈에서는 밸류 시맨틱
<em>또는</em> 레퍼런스 시맨틱을 사용할 수 있습니다. 전자가 지금까지의
사례에서 살펴본 가장 단순한 경우로, 모듈 객체가 스택에 할당되고
함수에 전달될 때 레퍼런스 혹은 포인터로 복사 및 이동(<code class="docutils literal notranslate"><span class="pre">std:move</span></code>)
시키거나 가져올 수 있습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">Net</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">};</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">a</span><span class="p">(</span><span class="n">Net</span><span class="w"> </span><span class="n">net</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">}</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">b</span><span class="p">(</span><span class="n">Net</span><span class="o">&amp;</span><span class="w"> </span><span class="n">net</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">}</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">Net</span><span class="o">*</span><span class="w"> </span><span class="n">net</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Net</span><span class="w"> </span><span class="n">net</span><span class="p">;</span>
<span class="w">  </span><span class="n">a</span><span class="p">(</span><span class="n">net</span><span class="p">);</span>
<span class="w">  </span><span class="n">a</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">net</span><span class="p">));</span>
<span class="w">  </span><span class="n">b</span><span class="p">(</span><span class="n">net</span><span class="p">);</span>
<span class="w">  </span><span class="n">c</span><span class="p">(</span><span class="o">&amp;</span><span class="n">net</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>후자(레퍼런스 시맨틱)의 경우, <code class="docutils literal notranslate"><span class="pre">std::shared_ptr</span></code> 를 사용할 수 있습니다.
모든 곳에서 <code class="docutils literal notranslate"><span class="pre">shared_ptr</span></code> 를 사용한다는 가정하에, 레퍼런스 시맨틱의
장점은 파이썬에서와 같이 모듈이 함수에 전달되고 인자가 선언되는 방식에
대해 생각할 부담을 덜어준다는 것입니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">Net</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="p">{};</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">a</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Net</span><span class="o">&gt;</span><span class="w"> </span><span class="n">net</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">net</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">Net</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="n">a</span><span class="p">(</span><span class="n">net</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>경험적으로, 동적 언어를 사용하던 연구자들은 비록 밸류 시맨틱이
더 C++에 “네이티브”함에도 불구하고 레퍼런스 시맨틱을 훨씬
선호합니다. 또한 <code class="docutils literal notranslate"><span class="pre">torch::nn::Module</span></code> 의 설계는
사용자 친화적인 파이썬 API를 유사하게 따르기 위해 shared 오너십에
의존합니다. 앞서 예시로 들었던 <code class="docutils literal notranslate"><span class="pre">Net</span></code> 의 정의를 축약해서 다시
살펴봅시다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">Net</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Net</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">M</span><span class="p">)</span>
<span class="w">    </span><span class="o">:</span><span class="w"> </span><span class="n">linear</span><span class="p">(</span><span class="n">register_module</span><span class="p">(</span><span class="s">&quot;linear&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Linear</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">)))</span>
<span class="w">  </span><span class="p">{</span><span class="w"> </span><span class="p">}</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Linear</span><span class="w"> </span><span class="n">linear</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>하위 모듈인 <code class="docutils literal notranslate"><span class="pre">linear</span></code> 를 사용하기 위해 이를 클래스에 직접 저장하고자
합니다. 그러나 동시에 모듈의 기초 클래스가 이 하위 모듈에 대해 알고 접근할
수 있기를 원합니다. 이를 위해서는 해당 하위 모듈에 대한 참조를 저장해야 합니다.
이 순간 이미 우리는 shared 오너십을 필요로 합니다. <code class="docutils literal notranslate"><span class="pre">torch::nn::Module</span></code>
클래스와 구상 클래스인 <code class="docutils literal notranslate"><span class="pre">Net</span></code> 모두에서 하위 모듈에 대한 레퍼런스가
필요합니다. 따라서 기초 클래스는 모듈을 <code class="docutils literal notranslate"><span class="pre">shared_ptr</span></code> 로 저장하며 이에
따라 구상 클래스 또한 마찬가지일 것입니다.</p>
<p>하지만 잠깐! 위의 코드에는 <code class="docutils literal notranslate"><span class="pre">shared_ptr</span></code> 에 대한 언급이 없습니다! 왜 그런
것일까요? 왜냐하면 <code class="docutils literal notranslate"><span class="pre">std::shared_ptr&lt;MyModule&gt;</span></code> 는 타이핑하기에 너무 길기 때문입니다.
연구원들의 생산성을 유지하기 위해, 우리는 레퍼런스 시맨틱을 유지하면서 밸류
시맨틱만의 장점인 <code class="docutils literal notranslate"><span class="pre">shared_ptr</span></code> 에 대한 언급을 숨기기 위한 정교한 계획을
세웠습니다. 그 작동 방식을 이해하기 위해 코어 라이브러리에 있는 <code class="docutils literal notranslate"><span class="pre">torch::nn::Linear</span></code>
모듈의 단순화된 정의를 살펴보겠습니다. (전체 정의는
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/torch/csrc/api/include/torch/nn/modules/linear.h">여기</a> 에서
확인할 수 있습니다.)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">LinearImpl</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">LinearImpl</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">in</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">out</span><span class="p">);</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">input</span><span class="p">);</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p">;</span>
<span class="p">};</span>

<span class="n">TORCH_MODULE</span><span class="p">(</span><span class="n">Linear</span><span class="p">);</span>
</pre></div>
</div>
<p>요약하자면 이 모듈은 <code class="docutils literal notranslate"><span class="pre">Linear</span></code> 가 아닌 <code class="docutils literal notranslate"><span class="pre">LinearImpl</span></code> 이라고 불립니다. 그리고
<code class="docutils literal notranslate"><span class="pre">TORCH_MODULE</span></code> 라는 매크로가 실제 <code class="docutils literal notranslate"><span class="pre">Linear</span></code> 클래스를 정의합니다. 이렇게 “생성된”
클래스는 <code class="docutils literal notranslate"><span class="pre">std::shared_ptr&lt;LinearImpl&gt;</span></code> 를 감싸는 래퍼(wrapper)입니다.
단순한 typedef가 아닌 래퍼이므로 생성자도 여전히 예상하는 대로 작동합니다.
즉, <code class="docutils literal notranslate"><span class="pre">std::make_shared&lt;LinearImpl&gt;(3,</span> <span class="pre">4)</span></code> 가 아닌 <code class="docutils literal notranslate"><span class="pre">torch::nn::Linear(3,</span> <span class="pre">4)</span></code>
라고 쓸 수 있습니다. 이렇게 매크로에 의해 생성된 클래스는 <em>holder</em> 모듈이라고
부릅니다. (shared) 포인터와 마찬가지로 화살표 연산자(즉,
<code class="docutils literal notranslate"><span class="pre">model-&gt;forward(...)</span></code>)를 사용해 기저 객체에 액세스합니다.
결론적으로 파이썬 API와 매우 유사한 오너십 모델을 얻었습니다.
기본적으로 레퍼런스 시맨틱을 따르지만, <code class="docutils literal notranslate"><span class="pre">std:shared_ptr</span></code> 나
<code class="docutils literal notranslate"><span class="pre">std::make_shared</span></code> 등을 타이핑할 필요가 없습니다. 우리의 <code class="docutils literal notranslate"><span class="pre">Net</span></code> 예시에서
모듈 holder API를 사용하면 아래와 같습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">NetImpl</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="p">{};</span>
<span class="n">TORCH_MODULE</span><span class="p">(</span><span class="n">Net</span><span class="p">);</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">a</span><span class="p">(</span><span class="n">Net</span><span class="w"> </span><span class="n">net</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Net</span><span class="w"> </span><span class="n">net</span><span class="p">;</span>
<span class="w">  </span><span class="n">a</span><span class="p">(</span><span class="n">net</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>여기서 언급할 만한 미묘한 문제가 하나 있습니다. 기본 생성자에 의해 만들어진
<code class="docutils literal notranslate"><span class="pre">std::shared_ptr</span></code> 는 “비어” 있습니다. 즉, null 포인터입니다. 기본 생성자로
만들어진 <code class="docutils literal notranslate"><span class="pre">Linear</span></code> 이나 <code class="docutils literal notranslate"><span class="pre">Net</span></code> 은 무엇이어야 할까요? 음, 이건 어려운 결정입니다.
빈 (null) <code class="docutils literal notranslate"><span class="pre">std::shared_ptr&lt;LinearImpl&gt;</span></code> 로 정할 수 있습니다. 하지만
<code class="docutils literal notranslate"><span class="pre">Linear(3,</span> <span class="pre">4)</span></code> 가 <code class="docutils literal notranslate"><span class="pre">std::make_shared&lt;LinearImpl&gt;(3,</span> <span class="pre">4)</span></code> 와 같다는 것을 기억합시다.
즉, <code class="docutils literal notranslate"><span class="pre">Linear</span> <span class="pre">linear;</span></code> 이 null 포인터여야 한다고 결정한다면
생성자에서 인자를 전혀 받지 않거나 모든 인자에 대해 기본값을 사용하는
모듈을 생성할 방법이 없어집니다. 이러한 이유로 현재
API에서 기본 생성자에 의해 만들어진 모듈 holder(<code class="docutils literal notranslate"><span class="pre">Linear()</span></code> 등)는
기저 모듈(<code class="docutils literal notranslate"><span class="pre">LinearImpl()</span></code>)의 기본 생성자를 호출합니다. 만약
기저 모듈에 기본 생성자가 없으면 컴파일러 오류가 발생합니다.
반대로 빈 holder를 생성하려면 holder 생성자에 <code class="docutils literal notranslate"><span class="pre">nullptr</span></code> 를
전달하면 됩니다.</p>
<p>실제로는 앞에서와 같이 하위 모듈을 사용해 모듈을 <em>이니셜라이저 (initializer) 목록</em> 에
등록 및 생성하거나,</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">Net</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Net</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">M</span><span class="p">)</span>
<span class="w">    </span><span class="o">:</span><span class="w"> </span><span class="n">linear</span><span class="p">(</span><span class="n">register_module</span><span class="p">(</span><span class="s">&quot;linear&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Linear</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">)))</span>
<span class="w">  </span><span class="p">{</span><span class="w"> </span><span class="p">}</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Linear</span><span class="w"> </span><span class="n">linear</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>파이썬 사용자들에게 더 친숙한 방법으로, 먼저 null 포인터로 홀더를 생성한 이후
생성자에서 값을 지정할 수 있습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">Net</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Net</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">M</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">linear</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">register_module</span><span class="p">(</span><span class="s">&quot;linear&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Linear</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">nn</span><span class="o">::</span><span class="n">Linear</span><span class="w"> </span><span class="n">linear</span><span class="p">{</span><span class="k">nullptr</span><span class="p">};</span><span class="w"> </span><span class="c1">// construct an empty holder</span>
<span class="p">};</span>
</pre></div>
</div>
<p>결론적으로 어떤 오너십 모델, 어떤 시맨틱을 사용하면 좋을까요? C++
프론트엔드 API는 모듈 holder가 제공하는 오너십 모델을 가장 잘 지원합니다.
이 메커니즘의 유일한 단점은 모듈 선언 아래에 boilerplate 한 줄이
추가된다는 것입니다. 즉, 가장 단순한 모델은 C++ 모듈의 기초를 배울 때
나오는 밸류 시맨틱 모델입니다. 작고 간단한 스크립트의 경우,
이것만으로 충분할 수 있습니다. 그러나 언젠가는 기술적 이유로 인해
이 기능이 항상 지원되지는 않는다는 사실을 알게 될 것입니다. 예를 들어 직렬화
API(<code class="docutils literal notranslate"><span class="pre">torch::save</span></code> 및 <code class="docutils literal notranslate"><span class="pre">torch::load</span></code>)는 모듈 holder(혹은 일반
<code class="docutils literal notranslate"><span class="pre">shared_ptr</span></code>)만을 지원합니다. 따라서 C++ 프론트엔드로 모듈을
정의할 때에는 모듈 holder API 방식이 권장되며, 앞으로 본 튜토리얼에서
이 API를 사용하겠습니다.</p>
</section>
</section>
<section id="id13">
<h3>DCGAN 모듈 정의하기<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>이제 이 글에서 해결하려는 머신러닝 태스크를 위한 모듈을 정의하는데
필요한 배경과 도입부 설명이 끝났습니다. 다시 상기하자면, 우리의 태스크는
<a class="reference external" href="https://huggingface.co/datasets/ylecun/mnist">MNIST 데이터셋</a> 의 숫자 이미지를
생성하는 것입니다. 우리는 이 태스크를 풀기 위해
<a class="reference external" href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">적대적 생성 신경망(GAN)</a> 을
사용하고자 합니다. 그 중에서도 우리는 <a class="reference external" href="https://arxiv.org/abs/1511.06434">DCGAN 아키텍처</a> 를 사용할 것입니다.
DCGAN은 가장 초기에 발표됐던 제일 간단한 GAN이지만 이 태스크를 위해서는
충분합니다.</p>
<div class="admonition tip">
<p class="admonition-title">팁</p>
<p>이 튜토리얼에 나온 소스 코드 전체는 <a class="reference external" href="https://github.com/pytorch/examples/tree/master/cpp/dcgan">이 저장소</a> 에서 확인할 수 있습니다.</p>
</div>
<section id="id16">
<h4>GAN이 뭐였죠?<a class="headerlink" href="#id16" title="Link to this heading">#</a></h4>
<p>GAN은 <em>생성기(generator)</em> 와 <em>판별기(discriminator)</em> 라는
두 가지 신경망 모델로 구성됩니다. 생성기는 노이즈 분포에서 샘플을 입력받고,
각 노이즈 샘플을 목표 분포(이 경우 MNIST 데이터셋)와 유사한 이미지로
변환하는 것이 목표입니다. 판별기는 MNIST 데이터셋의 <em>진짜</em>
이미지를 입력받거나 생성기로부터 <em>가짜</em> 이미지를 입력받습니다.
그리고 어떤 이미지가 얼마나 진짜같은지 (<code class="docutils literal notranslate"><span class="pre">1</span></code> 에 가까운 출력)
혹은 가짜같은 지 (<code class="docutils literal notranslate"><span class="pre">0</span></code> 에 가까운 출력) 판별합니다. 생성기가
만든 이미지가 얼마나 진짜같은 지 판별기가 피드백하고 이 피드백은 생성기
학습에 사용됩니다. 판별기가 진짜에 대한 안목이 얼마나 좋은 지에
대한 피드백은 판별기를 최적화하기 위해 사용됩니다. 이론적으로,
생성기와 판별기 사이의 섬세한 균형은 이 둘을 동시에 개선시킵니다.
이를 통해 생성기는 목표 분포와 구별할 수 없는 이미지를 생성하고,
(그때쯤이면) 잘 학습되어 있을 판별기의 안목을 속여 진짜와 가짜
이미지 모두에 대해 <code class="docutils literal notranslate"><span class="pre">0.5</span></code> 의 확률을 출력할 것입니다. 최종
결과물은 노이즈를 입력받아 실제 숫자의 이미지를 출력으로 생성하는
기계입니다.</p>
</section>
<section id="generator">
<h4>생성기 (Generator) 모듈<a class="headerlink" href="#generator" title="Link to this heading">#</a></h4>
<p>먼저 일련의 전치된 (transposed) 2D 합성곱, 배치 정규화 및
ReLU 활성화 유닛으로 구성된 생성기 모듈을 정의하겠습니다.
모듈의 <code class="docutils literal notranslate"><span class="pre">forward()</span></code> 메서드를 직접 정의하여 모듈 간 입력을
(함수형으로) 명시적으로 전달합니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">DCGANGeneratorImpl</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">nn</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">DCGANGeneratorImpl</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">kNoiseSize</span><span class="p">)</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">conv1</span><span class="p">(</span><span class="n">nn</span><span class="o">::</span><span class="n">ConvTranspose2dOptions</span><span class="p">(</span><span class="n">kNoiseSize</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span>
<span class="w">                  </span><span class="p">.</span><span class="n">bias</span><span class="p">(</span><span class="nb">false</span><span class="p">)),</span>
<span class="w">        </span><span class="n">batch_norm1</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
<span class="w">        </span><span class="n">conv2</span><span class="p">(</span><span class="n">nn</span><span class="o">::</span><span class="n">ConvTranspose2dOptions</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span>
<span class="w">                  </span><span class="p">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="w">                  </span><span class="p">.</span><span class="n">padding</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="w">                  </span><span class="p">.</span><span class="n">bias</span><span class="p">(</span><span class="nb">false</span><span class="p">)),</span>
<span class="w">        </span><span class="n">batch_norm2</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
<span class="w">        </span><span class="n">conv3</span><span class="p">(</span><span class="n">nn</span><span class="o">::</span><span class="n">ConvTranspose2dOptions</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span>
<span class="w">                  </span><span class="p">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="w">                  </span><span class="p">.</span><span class="n">padding</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="w">                  </span><span class="p">.</span><span class="n">bias</span><span class="p">(</span><span class="nb">false</span><span class="p">)),</span>
<span class="w">        </span><span class="n">batch_norm3</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>
<span class="w">        </span><span class="n">conv4</span><span class="p">(</span><span class="n">nn</span><span class="o">::</span><span class="n">ConvTranspose2dOptions</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span>
<span class="w">                  </span><span class="p">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="w">                  </span><span class="p">.</span><span class="n">padding</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="w">                  </span><span class="p">.</span><span class="n">bias</span><span class="p">(</span><span class="nb">false</span><span class="p">))</span>
<span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="c1">// register_module() is needed if we want to use the parameters() method later on</span>
<span class="w">   </span><span class="n">register_module</span><span class="p">(</span><span class="s">&quot;conv1&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">conv1</span><span class="p">);</span>
<span class="w">   </span><span class="n">register_module</span><span class="p">(</span><span class="s">&quot;conv2&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">conv2</span><span class="p">);</span>
<span class="w">   </span><span class="n">register_module</span><span class="p">(</span><span class="s">&quot;conv3&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">conv3</span><span class="p">);</span>
<span class="w">   </span><span class="n">register_module</span><span class="p">(</span><span class="s">&quot;conv4&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">conv4</span><span class="p">);</span>
<span class="w">   </span><span class="n">register_module</span><span class="p">(</span><span class="s">&quot;batch_norm1&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">batch_norm1</span><span class="p">);</span>
<span class="w">   </span><span class="n">register_module</span><span class="p">(</span><span class="s">&quot;batch_norm2&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">batch_norm2</span><span class="p">);</span>
<span class="w">   </span><span class="n">register_module</span><span class="p">(</span><span class="s">&quot;batch_norm3&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">batch_norm3</span><span class="p">);</span>
<span class="w"> </span><span class="p">}</span>

<span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">relu</span><span class="p">(</span><span class="n">batch_norm1</span><span class="p">(</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)));</span>
<span class="w">   </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">relu</span><span class="p">(</span><span class="n">batch_norm2</span><span class="p">(</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)));</span>
<span class="w">   </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">relu</span><span class="p">(</span><span class="n">batch_norm3</span><span class="p">(</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)));</span>
<span class="w">   </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">tanh</span><span class="p">(</span><span class="n">conv4</span><span class="p">(</span><span class="n">x</span><span class="p">));</span>
<span class="w">   </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="p">;</span>
<span class="w"> </span><span class="p">}</span>

<span class="w"> </span><span class="n">nn</span><span class="o">::</span><span class="n">ConvTranspose2d</span><span class="w"> </span><span class="n">conv1</span><span class="p">,</span><span class="w"> </span><span class="n">conv2</span><span class="p">,</span><span class="w"> </span><span class="n">conv3</span><span class="p">,</span><span class="w"> </span><span class="n">conv4</span><span class="p">;</span>
<span class="w"> </span><span class="n">nn</span><span class="o">::</span><span class="n">BatchNorm2d</span><span class="w"> </span><span class="n">batch_norm1</span><span class="p">,</span><span class="w"> </span><span class="n">batch_norm2</span><span class="p">,</span><span class="w"> </span><span class="n">batch_norm3</span><span class="p">;</span>
<span class="p">};</span>
<span class="n">TORCH_MODULE</span><span class="p">(</span><span class="n">DCGANGenerator</span><span class="p">);</span>

<span class="n">DCGANGenerator</span><span class="w"> </span><span class="nf">generator</span><span class="p">(</span><span class="n">kNoiseSize</span><span class="p">);</span>
</pre></div>
</div>
<p>이제 <code class="docutils literal notranslate"><span class="pre">DCGANGenerator</span></code> 의 <code class="docutils literal notranslate"><span class="pre">forward()</span></code> 를 호출해 노이즈 샘플을 이미지에 매핑할 수 있습니다.</p>
<p>여기서 사용한 <code class="docutils literal notranslate"><span class="pre">nn::ConvTranspose2d</span></code> 및 <code class="docutils literal notranslate"><span class="pre">nn::BatchNorm2d</span></code> 등의 모듈은
앞서 설명한 구조를 따릅니다. 상수 <code class="docutils literal notranslate"><span class="pre">kNoiseSize</span></code> 는 입력 노이즈 벡터의 크기를
결정하며 <code class="docutils literal notranslate"><span class="pre">100</span></code> 으로 설정됩니다. 하이퍼파라미터는 물론 대학원생들의 많은 노력을
통해 세팅됐습니다.</p>
<div class="admonition attention">
<p class="admonition-title">주의</p>
<p>하이퍼파라미터를 정하느라 다친 대학원생은 없었습니다. 그들은 서로서로 개사료를 먹이니까요.</p>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>C++ 프론트엔드의 <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> 와 같은 기본 제공 모듈에 옵션이 전달되는 방법에 대한
간단히 설명하자면, 모든 모듈은 몇 가지 필수 옵션을 갖고 있습니다. (예: <code class="docutils literal notranslate"><span class="pre">BatchNorm2d</span></code> 의
feature 개수) 만약 <code class="docutils literal notranslate"><span class="pre">BatchNorm2d(128)</span></code>, <code class="docutils literal notranslate"><span class="pre">Dropout(0.5)</span></code>, <code class="docutils literal notranslate"><span class="pre">Conv2d(8,</span> <span class="pre">4,</span> <span class="pre">2)</span></code> 와
같이 필수 옵션만 설정하려 한다면 모듈 생성자에 직접 전달할 수 있습니다.
(여기서는 각각 입력 채널 수, 출력 채널 수 및 커널 크기를 의미)
그러나 만약 <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> 의 <code class="docutils literal notranslate"><span class="pre">bias</span></code> 와 같이 일반적으로 기본값을 사용하는
다른 옵션을 수정해야 하는 경우, <em>options</em> 객체를 생성해 전달해야 합니다.
C++ 프론트엔드의  모듈은 <code class="docutils literal notranslate"><span class="pre">ModuleOptions</span></code> 이라고 하는 연관된 옵션 struct를
가지고 있습니다. 여기서 <code class="docutils literal notranslate"><span class="pre">Module</span></code> 은 해당 모듈의 이름으로, 예를 들어 <code class="docutils literal notranslate"><span class="pre">Linear</span></code>
의 경우 <code class="docutils literal notranslate"><span class="pre">LinearOptions</span></code> 와 같습니다. 우리는 위의 <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> 모듈에
대해 이를 수행한 것입니다.</p>
</div>
</section>
<section id="discriminator">
<h4>판별기(Discriminator) 모듈<a class="headerlink" href="#discriminator" title="Link to this heading">#</a></h4>
<p>판별기는 마찬가지로 합성곱, 배치 정규화 및 활성화의
연속입니다. 하지만 이번에 합성곱은 전치되지 않은 기본
합성곱이며, 일반적 ReLU 대신에 알파 값이 0.2인 leaky ReLU를
사용합니다. 또한 최종 활성화는 값을 0과 1 사이의 범위로 압축하는
Sigmoid가 됩니다. 그런 다음 이렇게 압축된 값을 판별자가
이미지에 대해 출력하는 확률로 해석할 수 있습니다.</p>
<p>판별기를 만들기 위해 <cite>Sequential</cite> 모듈이라는 다른 것을 시도해 보겠습니다.
파이썬에서와 같이, PyTorch는 모델 정의를 위해 두 가지 API를 제공합니다.
(생성기 모듈 예시와 같이) 입력이 연속적인 함수를 통해 전달되는 함수형 API와
전체 모델을 하위 모듈로 포함하는 <cite>Sequential</cite> 모듈을 생성하는 객체 지향형
API입니다. <cite>Sequential</cite> 을 사용하면 판별기는 대략 다음과 같습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">nn</span><span class="o">::</span><span class="n">Sequential</span><span class="w"> </span><span class="n">discriminator</span><span class="p">(</span>
<span class="w">  </span><span class="c1">// Layer 1</span>
<span class="w">  </span><span class="n">nn</span><span class="o">::</span><span class="n">Conv2d</span><span class="p">(</span>
<span class="w">      </span><span class="n">nn</span><span class="o">::</span><span class="n">Conv2dOptions</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">).</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">padding</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">bias</span><span class="p">(</span><span class="nb">false</span><span class="p">)),</span>
<span class="w">  </span><span class="n">nn</span><span class="o">::</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">nn</span><span class="o">::</span><span class="n">LeakyReLUOptions</span><span class="p">().</span><span class="n">negative_slope</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)),</span>
<span class="w">  </span><span class="c1">// Layer 2</span>
<span class="w">  </span><span class="n">nn</span><span class="o">::</span><span class="n">Conv2d</span><span class="p">(</span>
<span class="w">      </span><span class="n">nn</span><span class="o">::</span><span class="n">Conv2dOptions</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">).</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">padding</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">bias</span><span class="p">(</span><span class="nb">false</span><span class="p">)),</span>
<span class="w">  </span><span class="n">nn</span><span class="o">::</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
<span class="w">  </span><span class="n">nn</span><span class="o">::</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">nn</span><span class="o">::</span><span class="n">LeakyReLUOptions</span><span class="p">().</span><span class="n">negative_slope</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)),</span>
<span class="w">  </span><span class="c1">// Layer 3</span>
<span class="w">  </span><span class="n">nn</span><span class="o">::</span><span class="n">Conv2d</span><span class="p">(</span>
<span class="w">      </span><span class="n">nn</span><span class="o">::</span><span class="n">Conv2dOptions</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">).</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">padding</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">bias</span><span class="p">(</span><span class="nb">false</span><span class="p">)),</span>
<span class="w">  </span><span class="n">nn</span><span class="o">::</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
<span class="w">  </span><span class="n">nn</span><span class="o">::</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">nn</span><span class="o">::</span><span class="n">LeakyReLUOptions</span><span class="p">().</span><span class="n">negative_slope</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)),</span>
<span class="w">  </span><span class="c1">// Layer 4</span>
<span class="w">  </span><span class="n">nn</span><span class="o">::</span><span class="n">Conv2d</span><span class="p">(</span>
<span class="w">      </span><span class="n">nn</span><span class="o">::</span><span class="n">Conv2dOptions</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">).</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">padding</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">bias</span><span class="p">(</span><span class="nb">false</span><span class="p">)),</span>
<span class="w">  </span><span class="n">nn</span><span class="o">::</span><span class="n">Sigmoid</span><span class="p">());</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">팁</p>
<p><code class="docutils literal notranslate"><span class="pre">Sequential</span></code> 모듈은 단순한 함수 합성만을 수행합니다. 첫 번째 하위 모듈의 출력은
두 번째 하위 모듈의 입력이 되고 세 번째 하위 모듈의 출력은 네 번째 하위 모듈의 입력이
되고 이후에도 마찬가지입니다.</p>
</div>
</section>
</section>
</section>
<section id="id17">
<h2>데이터 불러오기<a class="headerlink" href="#id17" title="Link to this heading">#</a></h2>
<p>이제 생성기와 판별기 모델을 정의했으므로 이러한 모델을 학습시킬
데이터가 필요합니다. 파이썬과 마찬가지로 C++ 프론트엔드는
강력한 병렬 데이터 로더(data loader)를 제공한다. 이 데이터 로더는
사용자가 직접 정의할 수 있는 데이터셋에서 데이터 배치를 읽을 수 있으며
설정을 위한 많은 옵션을 제공합니다.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>파이썬 데이터 로더가 멀티 프로세싱을 사용하는 반면, C++ 데이터 로더는 실제로 멀티 스레딩을 사용해 어떠한 새로운 프로세스도 시작하지 않습니다.</p>
</div>
<p>데이터 로더는 <code class="docutils literal notranslate"><span class="pre">torch::data::</span></code> 네임스페이스에 포함된 C++ 프론트엔드의
<code class="docutils literal notranslate"><span class="pre">data</span></code> API의 일부입니다. 이 API는 다음과 같은 몇 가지 컴포넌트로 구성됩니다.</p>
<ul class="simple">
<li><p>데이터 로더 클래스</p></li>
<li><p>데이터셋을 정의하기 위한 API</p></li>
<li><p><em>변환</em> 을 정의하기 위한 API (데이터셋에 적용 가능)</p></li>
<li><p><em>샘플러</em> 를 정의하기 위한 API (데이터셋을 위한 인덱스를 생성)</p></li>
<li><p>기존 데이터셋, 변환, 샘플러들의 라이브러리</p></li>
</ul>
<p>이 튜토리얼에서는 C++ 프론트엔드와 함께 제공되는 <code class="docutils literal notranslate"><span class="pre">MNIST</span></code> 데이터셋을
사용합니다. <code class="docutils literal notranslate"><span class="pre">torch::data::datasets::MNIST</span></code> 인스턴스를 만들어
다음 두 가지 변환을 적용해봅시다. 첫째, 이미지를 정규화하여 <code class="docutils literal notranslate"><span class="pre">-1</span></code> 과
<code class="docutils literal notranslate"><span class="pre">+1</span></code> 사이에 있도록 합니다. (기존 범위는 <code class="docutils literal notranslate"><span class="pre">0</span></code> 과 <code class="docutils literal notranslate"><span class="pre">1</span></code> 사이)
둘째, 텐서 배치(batch)를 첫 번째 차원을 따라 단일 텐서로 쌓는 이른바
<code class="docutils literal notranslate"><span class="pre">Stack</span></code> <em>collation</em> 을 적용합니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">dataset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">data</span><span class="o">::</span><span class="n">datasets</span><span class="o">::</span><span class="n">MNIST</span><span class="p">(</span><span class="s">&quot;./mnist&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">map</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">data</span><span class="o">::</span><span class="n">transforms</span><span class="o">::</span><span class="n">Normalize</span><span class="o">&lt;&gt;</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5</span><span class="p">))</span>
<span class="w">    </span><span class="p">.</span><span class="n">map</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">data</span><span class="o">::</span><span class="n">transforms</span><span class="o">::</span><span class="n">Stack</span><span class="o">&lt;&gt;</span><span class="p">());</span>
</pre></div>
</div>
<p>MNIST 데이터셋은 학습 바이너리 실행 위치를 기준으로 <code class="docutils literal notranslate"><span class="pre">./mnist</span></code>
디렉토리에 위치해야 합니다. MNIST 데이터셋은 <a class="reference external" href="https://gist.github.com/goldsborough/6dd52a5e01ed73a642c1e772084bcd03">이 스크립트</a> 를
사용해 다운로드할 수 있습니다.</p>
<p>다음으로, 데이터 로더를 만들고 이 데이터셋을 전달합니다. 새로운 데이터
로더를 만들기 위해 <code class="docutils literal notranslate"><span class="pre">torch::data::make_data_loader</span></code> 를 사용합니다.
이 로더는 올바른 타입(데이터셋 타입, 샘플러 타입 및 기타 구현 세부사항에
따라 결정됨)의 <code class="docutils literal notranslate"><span class="pre">std::unique_ptr</span></code> 를 반환합니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">data_loader</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">data</span><span class="o">::</span><span class="n">make_data_loader</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">dataset</span><span class="p">));</span>
</pre></div>
</div>
<p>데이터 로더에는 많은 옵션이 제공됩니다. 전체 목록은 <a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/torch/csrc/api/include/torch/data/dataloader_options.h">여기</a>
에서 확인할 수 있습니다.
예를 들어 데이터 로딩 속도를 높이기 위해 작업자 수를 늘릴 수
있습니다. 기본값은 0이며, 이는 주 쓰레드가 사용됨을 의미합니다.
<code class="docutils literal notranslate"><span class="pre">workers</span></code> 를 <code class="docutils literal notranslate"><span class="pre">2</span></code> 로 설정하면 데이터를 동시에 로드하는 쓰레드가
두 개 생성됩니다. 또한 배치 크기를 기본값 <code class="docutils literal notranslate"><span class="pre">1</span></code> 에서 <code class="docutils literal notranslate"><span class="pre">64</span></code> (<code class="docutils literal notranslate"><span class="pre">kBatchSize</span></code> 값)
와 같이 더 적당한 값으로 늘려야 합니다. 그러면
<code class="docutils literal notranslate"><span class="pre">DataLoaderOptions</span></code> 객체를 만들어 적절한 속성을 설정해 보겠습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">data_loader</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">data</span><span class="o">::</span><span class="n">make_data_loader</span><span class="p">(</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">data</span><span class="o">::</span><span class="n">DataLoaderOptions</span><span class="p">().</span><span class="n">batch_size</span><span class="p">(</span><span class="n">kBatchSize</span><span class="p">).</span><span class="n">workers</span><span class="p">(</span><span class="mi">2</span><span class="p">));</span>
</pre></div>
</div>
<p>이제 데이터 배치를 로드하는 루프를 작성할 수 있습니다. 지금은
콘솔에만 출력할 것입니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">data</span><span class="o">::</span><span class="n">Example</span><span class="o">&lt;&gt;&amp;</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">*</span><span class="n">data_loader</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Batch size: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">batch</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; | Labels: &quot;</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">batch</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">batch</span><span class="p">.</span><span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">item</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>이 경우 데이터 로더가 반환하는 타입은 <code class="docutils literal notranslate"><span class="pre">torch::data::Example</span></code> 입니다.
이 타입은 데이터를 위한 <code class="docutils literal notranslate"><span class="pre">data</span></code> 필드와 레이블을 위한 <code class="docutils literal notranslate"><span class="pre">target</span></code> 필드가
있는 간단한 struct입니다. 앞서 <code class="docutils literal notranslate"><span class="pre">Stack</span></code> collation을 적용했기 때문에,
데이터 로더는 이 example을 하나만 반환합니다. 데이터 로더에 collation을
적용하지 않으면, <code class="docutils literal notranslate"><span class="pre">std::vector&lt;torch::data::Example&lt;&gt;&gt;</span></code> 를 yield하며,
각 배치의 example에는 하나의 element가 있을 것입니다.</p>
<p>이 코드를 다시 빌드하고 실행하면 대략 다음과 같은 내용을 얻을 것입니다.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>root@fa350df05ecf:/home/build#<span class="w"> </span>make
Scanning<span class="w"> </span>dependencies<span class="w"> </span>of<span class="w"> </span>target<span class="w"> </span>dcgan
<span class="o">[</span><span class="w"> </span><span class="m">50</span>%<span class="o">]</span><span class="w"> </span>Building<span class="w"> </span>CXX<span class="w"> </span>object<span class="w"> </span>CMakeFiles/dcgan.dir/dcgan.cpp.o
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Linking<span class="w"> </span>CXX<span class="w"> </span>executable<span class="w"> </span>dcgan
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Built<span class="w"> </span>target<span class="w"> </span>dcgan
root@fa350df05ecf:/home/build#<span class="w"> </span>make
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Built<span class="w"> </span>target<span class="w"> </span>dcgan
root@fa350df05ecf:/home/build#<span class="w"> </span>./dcgan
Batch<span class="w"> </span>size:<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>Labels:<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">9</span>
Batch<span class="w"> </span>size:<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>Labels:<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">2</span>
Batch<span class="w"> </span>size:<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>Labels:<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">0</span>
Batch<span class="w"> </span>size:<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>Labels:<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">7</span>
Batch<span class="w"> </span>size:<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>Labels:<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">0</span>
Batch<span class="w"> </span>size:<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>Labels:<span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">2</span>
Batch<span class="w"> </span>size:<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>Labels:<span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">7</span>
Batch<span class="w"> </span>size:<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>Labels:<span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">3</span>
Batch<span class="w"> </span>size:<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>Labels:<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">7</span>
Batch<span class="w"> </span>size:<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>Labels:<span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">4</span>
Batch<span class="w"> </span>size:<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>Labels:<span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">7</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">9</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="m">6</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="m">6</span>
...
</pre></div>
</div>
<p>즉, MNIST 데이터셋에서 데이터를 성공적으로 로드할 수 있습니다.</p>
</section>
<section id="id20">
<h2>학습 루프 작성하기<a class="headerlink" href="#id20" title="Link to this heading">#</a></h2>
<p>이제 예제의 알고리즘 부분을 마무리하고 생성기와 판별기 사이에서 일어나는 섬세한
작용을 구현해 보겠습니다. 먼저 생성기와 판별기 각각을 위해
총 두 개의 optimizer를 생성하겠습니다. 우리가 사용하는
optimizer는 <a class="reference external" href="https://arxiv.org/pdf/1412.6980.pdf">Adam</a> 알고리즘을 구현합니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">optim</span><span class="o">::</span><span class="n">Adam</span><span class="w"> </span><span class="nf">generator_optimizer</span><span class="p">(</span>
<span class="w">    </span><span class="n">generator</span><span class="o">-&gt;</span><span class="n">parameters</span><span class="p">(),</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">optim</span><span class="o">::</span><span class="n">AdamOptions</span><span class="p">(</span><span class="mf">2e-4</span><span class="p">).</span><span class="n">betas</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_tuple</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5</span><span class="p">)));</span>
<span class="n">torch</span><span class="o">::</span><span class="n">optim</span><span class="o">::</span><span class="n">Adam</span><span class="w"> </span><span class="nf">discriminator_optimizer</span><span class="p">(</span>
<span class="w">    </span><span class="n">discriminator</span><span class="o">-&gt;</span><span class="n">parameters</span><span class="p">(),</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">optim</span><span class="o">::</span><span class="n">AdamOptions</span><span class="p">(</span><span class="mf">5e-4</span><span class="p">).</span><span class="n">betas</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_tuple</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="w"> </span><span class="mf">0.5</span><span class="p">)));</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>이 글 작성 당시, C++ 프론트엔드가 Adagrad, Adam, LBFGS, RMSprop 및 SGD를 구현하는 옵티마이저를 제공합니다. 최신 리스트는 <a class="reference external" href="https://pytorch.org/cppdocs/api/namespace_torch__optim.html">docs</a> 에 있습니다.</p>
</div>
<p>다음으로, 우리의 학습 루프를 수정해야 합니다. 매 에폭마다 데이터 로더를 반복 실행하는
바깥 루프를 추가해 다음의 GAN 학습 코드를 작성합니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">kNumberOfEpochs</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">epoch</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">batch_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">data</span><span class="o">::</span><span class="n">Example</span><span class="o">&lt;&gt;&amp;</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">*</span><span class="n">data_loader</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Train discriminator with real images.</span>
<span class="w">    </span><span class="n">discriminator</span><span class="o">-&gt;</span><span class="n">zero_grad</span><span class="p">();</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">real_images</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batch</span><span class="p">.</span><span class="n">data</span><span class="p">;</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">real_labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">empty</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)).</span><span class="n">uniform_</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="p">);</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">real_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">discriminator</span><span class="o">-&gt;</span><span class="n">forward</span><span class="p">(</span><span class="n">real_images</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">real_labels</span><span class="p">.</span><span class="n">sizes</span><span class="p">());</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">d_loss_real</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">real_output</span><span class="p">,</span><span class="w"> </span><span class="n">real_labels</span><span class="p">);</span>
<span class="w">    </span><span class="n">d_loss_real</span><span class="p">.</span><span class="n">backward</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// Train discriminator with fake images.</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">noise</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">({</span><span class="n">batch</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="n">kNoiseSize</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">});</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">fake_images</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">generator</span><span class="o">-&gt;</span><span class="n">forward</span><span class="p">(</span><span class="n">noise</span><span class="p">);</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">fake_labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">fake_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">discriminator</span><span class="o">-&gt;</span><span class="n">forward</span><span class="p">(</span><span class="n">fake_images</span><span class="p">.</span><span class="n">detach</span><span class="p">()).</span><span class="n">reshape</span><span class="p">(</span><span class="n">fake_labels</span><span class="p">.</span><span class="n">sizes</span><span class="p">());</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">d_loss_fake</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">fake_output</span><span class="p">,</span><span class="w"> </span><span class="n">fake_labels</span><span class="p">);</span>
<span class="w">    </span><span class="n">d_loss_fake</span><span class="p">.</span><span class="n">backward</span><span class="p">();</span>

<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">d_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_loss_real</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">d_loss_fake</span><span class="p">;</span>
<span class="w">    </span><span class="n">discriminator_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">();</span>

<span class="w">    </span><span class="c1">// Train generator.</span>
<span class="w">    </span><span class="n">generator</span><span class="o">-&gt;</span><span class="n">zero_grad</span><span class="p">();</span>
<span class="w">    </span><span class="n">fake_labels</span><span class="p">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="n">fake_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">discriminator</span><span class="o">-&gt;</span><span class="n">forward</span><span class="p">(</span><span class="n">fake_images</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">fake_labels</span><span class="p">.</span><span class="n">sizes</span><span class="p">());</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">g_loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">fake_output</span><span class="p">,</span><span class="w"> </span><span class="n">fake_labels</span><span class="p">);</span>
<span class="w">    </span><span class="n">g_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">();</span>
<span class="w">    </span><span class="n">generator_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">();</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">printf</span><span class="p">(</span>
<span class="w">        </span><span class="s">&quot;</span><span class="se">\r</span><span class="s">[%2ld/%2ld][%3ld/%3ld] D_loss: %.4f | G_loss: %.4f&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="n">epoch</span><span class="p">,</span>
<span class="w">        </span><span class="n">kNumberOfEpochs</span><span class="p">,</span>
<span class="w">        </span><span class="o">++</span><span class="n">batch_index</span><span class="p">,</span>
<span class="w">        </span><span class="n">batches_per_epoch</span><span class="p">,</span>
<span class="w">        </span><span class="n">d_loss</span><span class="p">.</span><span class="n">item</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">        </span><span class="n">g_loss</span><span class="p">.</span><span class="n">item</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">());</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>위 코드는 먼저 진짜 (real) 이미지에 대해 판별기를 평가하는데, 이 때
판별기는 높은 확률을 출력해야 합니다. 이를 위해
<code class="docutils literal notranslate"><span class="pre">torch::empty(batch.data.size(0)).uniform_(0.8,</span> <span class="pre">1.0)</span></code> 를 목표 확률
값으로 사용합니다.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>판별기를 보다 견고하게 학습하기 위해 모든 곳에서 1.0이 아닌 0.8과 1.0 사이의 균일 분포에서 임의의 값을 선택합니다. 이 트릭을 <em>label smoothing</em> 이라고 합니다.</p>
</div>
<p>판별기를 평가하기에 앞서 매개변수의 그래디언트를 0으로 만듭니다.
손실을 계산한 후 <code class="docutils literal notranslate"><span class="pre">d_loss.backward()</span></code> 를 호출해 이를
네트워크에 역전파합니다. 가짜 (fake) 이미지들에 대해서 이 과정을
반복합니다. 데이터셋의 이미지를 사용하는 대신, 생성자에
무작위 노이즈를 입력하여 여기서 사용할 가짜 이미지를 만듭니다.
그리고 그 가짜 이미지들을 판별기에 전달합니다. 이번에는
판별기가 낮은 확률, 이상적으로는 모두 0을 출력하기를 바랍니다.
진짜 이미지와 가짜 이미지 배치 모두에 대한 판별기 손실을 계산한
후에는, 판별기의 optimizer 매개변수 업데이트를 한 단계씩
진행할 수 있습니다.</p>
<p>생성기를 학습시키기 위해 우선 그래디언트를 다시 한번 0으로 설정하고
다시 가짜 이미지로 판별기를 평가합니다. 그러나 이번에는 판별기가
확률 1에 매우 근접하게 출력하게 하여, 생성기가 판별기를
속여 실제 (데이터셋에 있는) 진짜라고 생각하는 이미지를 생성할 수
있도록 하려 합니다. 이를 위해 <code class="docutils literal notranslate"><span class="pre">fake_labels</span></code> 텐서를 모두
1로 채우겠습니다. 마지막으로 매개변수를 업데이트하기 위해
생성기의 optimzier 매개변수 업데이트를 진행합니다.</p>
<p>이제 CPU로 모델을 학습시킬 준비가 되었습니다. 상태나 샘플 출력을
캡처할 수 있는 코드는 아직 없지만 잠시 후에 추가하겠습니다. 지금은
모델이 <em>무언가</em> 를 수행하고 있다는 것만을 관찰하고, 나중에는 생성된
이미지를 기반으로 이 무언가가 의미 있는지 여부를 확인할 것입니다.
다시 빌드하고 실행하면 다음과 같은 내용이 출력돼야 합니다.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>root@3c0711f20896:/home/build#<span class="w"> </span>make<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>./dcgan
Scanning<span class="w"> </span>dependencies<span class="w"> </span>of<span class="w"> </span>target<span class="w"> </span>dcgan
<span class="o">[</span><span class="w"> </span><span class="m">50</span>%<span class="o">]</span><span class="w"> </span>Building<span class="w"> </span>CXX<span class="w"> </span>object<span class="w"> </span>CMakeFiles/dcgan.dir/dcgan.cpp.o
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Linking<span class="w"> </span>CXX<span class="w"> </span>executable<span class="w"> </span>dcgan
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Built<span class="w"> </span>target<span class="w"> </span>dcga
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/10<span class="o">][</span><span class="m">100</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.6876<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">4</span>.1304
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/10<span class="o">][</span><span class="m">200</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.3776<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">4</span>.3101
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/10<span class="o">][</span><span class="m">300</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.3652<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">4</span>.6626
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/10<span class="o">][</span><span class="m">400</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.8057<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">2</span>.2795
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/10<span class="o">][</span><span class="m">500</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.3531<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">4</span>.4452
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/10<span class="o">][</span><span class="m">600</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.3501<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">5</span>.0811
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/10<span class="o">][</span><span class="m">700</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.3581<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">4</span>.5623
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/10<span class="o">][</span><span class="m">800</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.6423<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">1</span>.7385
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/10<span class="o">][</span><span class="m">900</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.3592<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">4</span>.7333
<span class="o">[</span><span class="w"> </span><span class="m">2</span>/10<span class="o">][</span><span class="m">100</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.4660<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">2</span>.5242
<span class="o">[</span><span class="w"> </span><span class="m">2</span>/10<span class="o">][</span><span class="m">200</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.6364<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">2</span>.0886
<span class="o">[</span><span class="w"> </span><span class="m">2</span>/10<span class="o">][</span><span class="m">300</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.3717<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">3</span>.8103
<span class="o">[</span><span class="w"> </span><span class="m">2</span>/10<span class="o">][</span><span class="m">400</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">1</span>.0201<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">1</span>.3544
<span class="o">[</span><span class="w"> </span><span class="m">2</span>/10<span class="o">][</span><span class="m">500</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.4522<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">2</span>.6545
...
</pre></div>
</div>
</section>
<section id="gpu">
<h2>GPU로 이동하기<a class="headerlink" href="#gpu" title="Link to this heading">#</a></h2>
<p>이 스크립트는 CPU에서 잘 동작하지만, 합성곱 연산이 GPU에서 훨씬 빠르다는
것은 잘 알려진 사실입니다. 어떻게 학습을 GPU로 옮길 수 있을 지에 대해 빠르게 논의해
보겠습니다. 이를 위해 해야 할 일 두 가지로 GPU 장치(device) 사양을 우리가 직접 할당한
텐서에 전달하는 것과, C++ 프론트엔드의 모든 텐서와 모듈이 갖고 있는 <code class="docutils literal notranslate"><span class="pre">to()</span></code>
메서드를 사용해 다른 모든 텐서를 GPU에 명시적으로 복사하는 것이 있습니다.
두 가지를 모두 달성하는 가장 간단한 방법으로 학습 스크립트 최상위에
<code class="docutils literal notranslate"><span class="pre">torch::Device</span></code> 인스턴스를 만들어 <code class="docutils literal notranslate"><span class="pre">torch::zeros</span></code> 와 같은
텐서 팩토리 함수나 <code class="docutils literal notranslate"><span class="pre">to()</span></code> 메서드에 전달할 수 있습니다. 먼저 CPU device로
이를 구현해보겠습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// 학습 스크립트 최상단에 이 코드를 넣으세요.</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="nf">device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCPU</span><span class="p">);</span>
</pre></div>
</div>
<p>아래와 같은 새로운 텐서 할당의 경우,</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">fake_labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>
</pre></div>
</div>
<p>마지막 인자로 <code class="docutils literal notranslate"><span class="pre">device</span></code> 를 받도록 수정합니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">fake_labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="n">device</span><span class="p">);</span>
</pre></div>
</div>
<p>MNIST 데이터셋의 텐서처럼 우리가 직접 생성하지 않는 텐서에서는
명시적으로 <code class="docutils literal notranslate"><span class="pre">to()</span></code> 호출을 삽입해야 합니다. 따라서 아래 코드의 경우,</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">real_images</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batch</span><span class="p">.</span><span class="n">data</span><span class="p">;</span>
</pre></div>
</div>
<p>다음과 같이 변합니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">real_images</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batch</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
</pre></div>
</div>
<p>또한, 모델 매개변수를 올바른 장치로 옮겨야 합니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span><span class="o">-&gt;</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
<span class="n">discriminator</span><span class="o">-&gt;</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>만일 텐서가 이미 <code class="docutils literal notranslate"><span class="pre">to()</span></code> 에 전달된 장치 상에 있다면 그 호출은 아무 일도 하지 않습니다. 사본이 생성되지도 않습니다.</p>
</div>
<p>이제 CPU에서 실행되는 이전의 코드가 보다 명시적으로 바뀌었습니다.
하지만 이제는 장치를 CUDA 장치로 변경하는 것 또한 매우 쉽습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">)</span>
</pre></div>
</div>
<p>이제 모든 텐서가 GPU에 존재하며 어떠한 다운스트림 코드 변경 없이도
모든 연산을 위해 빠른 CUDA 커널을 호출합니다. 특정 인덱스의 장치를
지정하려면 <code class="docutils literal notranslate"><span class="pre">Device</span></code> 생성자의 두 번째 인자로 전달하면 됩니다.
서로 다른 장치에 서로 다른 텐서가 존재하기를 원하는 경우,
별도의 장치 인스턴스(예: CUDA 장치 0과 다른 CUDA 장치 1)를
전달할 수도 있습니다. 뿐만 아니라, 이러한 설정을 동적으로 수행할 수도
있어 다음과 같이 학습 스크립트의 휴대성을 높이는 데 종종 유용하게 사용됩니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">kCPU</span><span class="p">;</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">is_available</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;CUDA is available! Training on GPU.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>나아가 아래와 같은 코드도 가능합니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="nf">device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">is_available</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">kCPU</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="id21">
<h2>학습 상태 저장 및 복원하기<a class="headerlink" href="#id21" title="Link to this heading">#</a></h2>
<p>마지막으로 학습 스크립트에 추가해야 할 내용은 모델 매개변수 및
옵티마이저의 상태, 그리고 생성된 몇 개의 이미지 샘플을
주기적으로 저장하는 것입니다. 학습 과정 도중에 컴퓨터가 다운되면
이렇게 저장된 상태로부터 학습 상태를 복원할 수 있습니다.
이는 장시간 지속되는 학습을 위해 필수로 요구됩니다. 다행히도
C++ 프론트엔드는 개별 텐서뿐만 아니라 모델 및 옵티마이저 상태를
직렬화하고 역직렬화할 수 있는 API를 제공합니다.</p>
<p>이를 위한 핵심 API는 <code class="docutils literal notranslate"><span class="pre">torch::save(thing,filename)</span></code> 와
<code class="docutils literal notranslate"><span class="pre">torch::load(thing,filename)</span></code> 로, 여기서 <code class="docutils literal notranslate"><span class="pre">thing</span></code> 은
<code class="docutils literal notranslate"><span class="pre">torch::nn::Module</span></code> 의 하위 클래스 혹은 우리의 학습 스크립트의 <code class="docutils literal notranslate"><span class="pre">Adam</span></code>
객체와 같은 옵티마이저 인스턴스가 될 수 있습니다. 모델 및 옵티마이저 상태를
특정 주기마다 저장하도록 학습 루프를 수정해보겠습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">batch_index</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">kCheckpointEvery</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// 모델 및 옵티마이저 상태를 저장합니다.</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">save</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;generator-checkpoint.pt&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">save</span><span class="p">(</span><span class="n">generator_optimizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;generator-optimizer-checkpoint.pt&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">save</span><span class="p">(</span><span class="n">discriminator</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;discriminator-checkpoint.pt&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">save</span><span class="p">(</span><span class="n">discriminator_optimizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;discriminator-optimizer-checkpoint.pt&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// 생성기를 샘플링하고 이미지를 저장합니다.</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">samples</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">generator</span><span class="o">-&gt;</span><span class="n">forward</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">randn</span><span class="p">({</span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="n">kNoiseSize</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="n">device</span><span class="p">));</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">save</span><span class="p">((</span><span class="n">samples</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1.0</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">2.0</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">str</span><span class="p">(</span><span class="s">&quot;dcgan-sample-&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">checkpoint_counter</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;.pt&quot;</span><span class="p">));</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">-&gt; checkpoint &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="o">++</span><span class="n">checkpoint_counter</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="sc">&#39;\n&#39;</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>여기서 <code class="docutils literal notranslate"><span class="pre">100</span></code> 배치마다 상태를 저장하려면 <code class="docutils literal notranslate"><span class="pre">kCheckpointEvery</span></code> 를 <code class="docutils literal notranslate"><span class="pre">100</span></code>
과 같은 정수로 설정할 수 있으며, <code class="docutils literal notranslate"><span class="pre">checkpoint_counter</span></code> 는 상태를 저장할 때마다
증가하는 카운터입니다.</p>
<p>학습 상태를 복원하기 위해 모델 및 옵티마이저를 모두 생성한 후 학습 루프 앞에
다음 코드를 추가할 수 있습니다.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">::</span><span class="n">optim</span><span class="o">::</span><span class="n">Adam</span><span class="w"> </span><span class="nf">generator_optimizer</span><span class="p">(</span>
<span class="w">    </span><span class="n">generator</span><span class="o">-&gt;</span><span class="n">parameters</span><span class="p">(),</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">optim</span><span class="o">::</span><span class="n">AdamOptions</span><span class="p">(</span><span class="mf">2e-4</span><span class="p">).</span><span class="n">beta1</span><span class="p">(</span><span class="mf">0.5</span><span class="p">));</span>
<span class="n">torch</span><span class="o">::</span><span class="n">optim</span><span class="o">::</span><span class="n">Adam</span><span class="w"> </span><span class="nf">discriminator_optimizer</span><span class="p">(</span>
<span class="w">    </span><span class="n">discriminator</span><span class="o">-&gt;</span><span class="n">parameters</span><span class="p">(),</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">optim</span><span class="o">::</span><span class="n">AdamOptions</span><span class="p">(</span><span class="mf">2e-4</span><span class="p">).</span><span class="n">beta1</span><span class="p">(</span><span class="mf">0.5</span><span class="p">));</span>

<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">kRestoreFromCheckpoint</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">load</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;generator-checkpoint.pt&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">load</span><span class="p">(</span><span class="n">generator_optimizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;generator-optimizer-checkpoint.pt&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">load</span><span class="p">(</span><span class="n">discriminator</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;discriminator-checkpoint.pt&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">load</span><span class="p">(</span>
<span class="w">      </span><span class="n">discriminator_optimizer</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;discriminator-optimizer-checkpoint.pt&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int64_t</span><span class="w"> </span><span class="n">checkpoint_counter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">kNumberOfEpochs</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">epoch</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">batch_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">data</span><span class="o">::</span><span class="n">Example</span><span class="o">&lt;&gt;&amp;</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">*</span><span class="n">data_loader</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
</pre></div>
</div>
</section>
<section id="id22">
<h2>생성한 이미지 검사하기<a class="headerlink" href="#id22" title="Link to this heading">#</a></h2>
<p>학습 스크립트가 완성되어 CPU에서든 GPU에서든 GAN을 훈련시킬 준비가
됐습니다. 학습 과정의 중간 출력을 검사하기 위해
<code class="docutils literal notranslate"><span class="pre">&quot;dcgan-sample-xxx.pt&quot;</span></code> 에 주기적으로 이미지 샘플을 저장하는 코드를
추가했으니, 텐서들을 불러와 matplotlib로 시각화하는 간단한 파이썬
스크립트를 작성해보겠습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>


<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-i&quot;</span><span class="p">,</span> <span class="s2">&quot;--sample-file&quot;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-o&quot;</span><span class="p">,</span> <span class="s2">&quot;--out-file&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;out.png&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-d&quot;</span><span class="p">,</span> <span class="s2">&quot;--dimension&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">options</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="n">module</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">options</span><span class="o">.</span><span class="n">sample_file</span><span class="p">)</span>
<span class="n">images</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">options</span><span class="o">.</span><span class="n">dimension</span> <span class="o">*</span> <span class="n">options</span><span class="o">.</span><span class="n">dimension</span><span class="p">):</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
  <span class="n">array</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">options</span><span class="o">.</span><span class="n">dimension</span><span class="p">,</span> <span class="n">options</span><span class="o">.</span><span class="n">dimension</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">index</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
  <span class="n">axis</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
  <span class="n">axis</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">options</span><span class="o">.</span><span class="n">out_file</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Saved &quot;</span><span class="p">,</span> <span class="n">options</span><span class="o">.</span><span class="n">out_file</span><span class="p">)</span>
</pre></div>
</div>
<p>이제 모델을 약 30 에폭 정도 학습시킵시다.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>root@3c0711f20896:/home/build#<span class="w"> </span>make<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>./dcgan<span class="w">                                                                                                                                </span><span class="m">10</span>:17:57
Scanning<span class="w"> </span>dependencies<span class="w"> </span>of<span class="w"> </span>target<span class="w"> </span>dcgan
<span class="o">[</span><span class="w"> </span><span class="m">50</span>%<span class="o">]</span><span class="w"> </span>Building<span class="w"> </span>CXX<span class="w"> </span>object<span class="w"> </span>CMakeFiles/dcgan.dir/dcgan.cpp.o
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Linking<span class="w"> </span>CXX<span class="w"> </span>executable<span class="w"> </span>dcgan
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Built<span class="w"> </span>target<span class="w"> </span>dcgan
CUDA<span class="w"> </span>is<span class="w"> </span>available!<span class="w"> </span>Training<span class="w"> </span>on<span class="w"> </span>GPU.
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/30<span class="o">][</span><span class="m">200</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.4953<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">4</span>.0195
-&gt;<span class="w"> </span>checkpoint<span class="w"> </span><span class="m">1</span>
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/30<span class="o">][</span><span class="m">400</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.3610<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">4</span>.8148
-&gt;<span class="w"> </span>checkpoint<span class="w"> </span><span class="m">2</span>
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/30<span class="o">][</span><span class="m">600</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.4072<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">4</span>.36760
-&gt;<span class="w"> </span>checkpoint<span class="w"> </span><span class="m">3</span>
<span class="o">[</span><span class="w"> </span><span class="m">1</span>/30<span class="o">][</span><span class="m">800</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.4444<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">4</span>.0250
-&gt;<span class="w"> </span>checkpoint<span class="w"> </span><span class="m">4</span>
<span class="o">[</span><span class="w"> </span><span class="m">2</span>/30<span class="o">][</span><span class="m">200</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.3761<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">3</span>.8790
-&gt;<span class="w"> </span>checkpoint<span class="w"> </span><span class="m">5</span>
<span class="o">[</span><span class="w"> </span><span class="m">2</span>/30<span class="o">][</span><span class="m">400</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.3977<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">3</span>.3315
...
-&gt;<span class="w"> </span>checkpoint<span class="w"> </span><span class="m">120</span>
<span class="o">[</span><span class="m">30</span>/30<span class="o">][</span><span class="m">938</span>/938<span class="o">]</span><span class="w"> </span>D_loss:<span class="w"> </span><span class="m">0</span>.3610<span class="w"> </span><span class="p">|</span><span class="w"> </span>G_loss:<span class="w"> </span><span class="m">3</span>.8084
</pre></div>
</div>
<p>그리고 이미지들을 플롯에 시각화합니다.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>root@3c0711f20896:/home/build#<span class="w"> </span>python<span class="w"> </span>display.py<span class="w"> </span>-i<span class="w"> </span>dcgan-sample-100.pt
Saved<span class="w"> </span>out.png
</pre></div>
</div>
<p>그 결과는 아래와 같을 것입니다.</p>
<figure class="align-default">
<img alt="digits" src="../_images/digits.png" />
</figure>
<p>숫자네요! 만세! 이제 여러분 차례입니다. 숫자가 보다 나아 보이도록
모델을 개선할 수 있나요?</p>
</section>
<section id="id23">
<h2>결론<a class="headerlink" href="#id23" title="Link to this heading">#</a></h2>
<p>이 튜토리얼을 통해 PyTorch C++ 프론트엔드에 대한 어느 정도 이해도가 생기셨기
바랍니다. 필연적으로 PyTorch 같은 머신러닝 라이브러리는 매우 다양하고
광범위한 API를 가지고 있습니다. 따라서, 여기서 논의하기에 시간과 공간이
부족했던 개념들이 많습니다. 그러나 직접 API를 사용해보고,
<a class="reference external" href="https://pytorch.org/cppdocs/">문서</a>, 그 중에서도 특히
<a class="reference external" href="https://pytorch.org/cppdocs/api/library_root.html">라이브러리 API</a>
섹션을 참조해보는 것을 권장드립니다. 또한, C++ 프론트엔드가 파이썬
프론트엔드의 디자인과 시맨틱을 따른다는 사실을 잘 기억하면 보다 빠르게
학습할 수 있을 것입니다.</p>
<div class="admonition tip">
<p class="admonition-title">팁</p>
<p>본 튜토리얼에 대한 전체 소스코드는 <a class="reference external" href="https://github.com/pytorch/examples/tree/master/cpp/dcgan">이 저장소</a> 에 제공되어 있습니다.</p>
</div>
<p>언제나 그렇듯이 어떤 문제가 생기거나 질문이 있으면 저희
<a class="reference external" href="https://discuss.pytorch.org/">포럼</a> 을 이용하거나 <a class="reference external" href="https://github.com/pytorch/pytorch/issues">Github 이슈</a> 로 연락주세요.</p>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="../intermediate/per_sample_grads.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Per-sample-gradients</p>
      </div>
    </a>
    <a class="right-next"
       href="cpp_autograd.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">C++ 프론트엔드의 자동 미분 (autograd)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../intermediate/per_sample_grads.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Per-sample-gradients</p>
      </div>
    </a>
    <a class="right-next"
       href="cpp_autograd.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">C++ 프론트엔드의 자동 미분 (autograd)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">동기부여</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">기본 애플리케이션 작성하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">신경망 모델 정의하기</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#api">모듈 API 기초</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">모듈 정의 및 매개변수 등록</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">서브모듈 등록 및 모듈 계층 구조 탐색</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#forward">순전파(forward) 모드로 네트워크 실행</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ownership">모듈 오너십 (Ownership)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">DCGAN 모듈 정의하기</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">GAN이 뭐였죠?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generator">생성기 (Generator) 모듈</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#discriminator">판별기(Discriminator) 모듈</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">데이터 불러오기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">학습 루프 작성하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu">GPU로 이동하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">학습 상태 저장 및 복원하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">생성한 이미지 검사하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">결론</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "PyTorch C++ \ud504\ub860\ud2b8\uc5d4\ub4dc \uc0ac\uc6a9\ud558\uae30",
       "headline": "PyTorch C++ \ud504\ub860\ud2b8\uc5d4\ub4dc \uc0ac\uc6a9\ud558\uae30",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/advanced/cpp_frontend.html",
       "articleBody": "PyTorch C++ \ud504\ub860\ud2b8\uc5d4\ub4dc \uc0ac\uc6a9\ud558\uae30# \ubc88\uc5ed: \uc720\uc6a9\ud658 PyTorch C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\ub294 PyTorch \uba38\uc2e0\ub7ec\ub2dd \ud504\ub808\uc784\uc6cc\ud06c\uc758 \uc21c\uc218 C++ \uc778\ud130\ud398\uc774\uc2a4\uc785\ub2c8\ub2e4. PyTorch\uc758 \uc8fc\ub41c \uc778\ud130\ud398\uc774\uc2a4\ub294 \ubb3c\ub860 \ud30c\uc774\uc36c\uc774\uc9c0\ub9cc \uc774 \uacf3\uc758 API\ub294 \ud150\uc11c(tensor)\ub098 \uc790\ub3d9 \ubbf8\ubd84\uacfc \uac19\uc740 \uae30\ucd08\uc801\uc778 \uc790\ub8cc\uad6c\uc870 \ubc0f \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\ub294 C++ \ucf54\ub4dc\ubca0\uc774\uc2a4 \uc704\uc5d0 \uad6c\ud604\ub418\uc5c8\uc2b5\ub2c8\ub2e4. C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\ub294 \uc774\ub7ec\ud55c \uae30\ucd08\uc801\uc778 C++ \ucf54\ub4dc\ubca0\uc774\uc2a4\ub97c \ube44\ub86f\ud574 \uba38\uc2e0\ub7ec\ub2dd \ud559\uc2b5\uacfc \ucd94\ub860\uc744 \uc704\ud574 \ud544\uc694\ud55c \ub3c4\uad6c\ub4e4\uc744 \uc0c1\uc18d\ud558\ub294 \uc21c\uc218 C++11 API\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\ub294 \uc2e0\uacbd\ub9dd \ubaa8\ub378\ub9c1\uc744 \uc704\ud574 \ud544\uc694\ud55c \uacf5\uc6a9 \ucef4\ud3ec\ub10c\ud2b8\ub4e4\uc758 \ube4c\ud2b8\uc778 \ubaa8\uc74c, \uadf8\uac83\uc744 \uc0c1\uc18d\ud558\uae30 \uc704\ud55c \ucee4\uc2a4\ud140 \ubaa8\ub4c8, \ud655\ub960\uc801 \uacbd\uc0ac \ud558\uac15\ubc95\uacfc \uac19\uc740 \uc720\uba85\ud55c \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998 \ub77c\uc774\ube0c\ub7ec\ub9ac, \ubcd1\ub82c \ub370\uc774\ud130 \ub85c\ub354 \ubc0f \ub370\uc774\ud130\uc14b\uc744 \uc815\uc758\ud558\uace0 \ubd88\ub7ec\uc624\uae30 \uc704\ud55c API, \uc9c1\ub82c\ud654 \ub8e8\ud2f4 \ub4f1\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\ub85c \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294 \uc5d4\ub4dc \ud22c \uc5d4\ub4dc \uc608\uc81c\ub97c \uc548\ub0b4\ud569\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c, \uc6b0\ub9ac\ub294 \uc0dd\uc131 \ubaa8\ub378 \uc911 \ud558\ub098\uc778 DCGAN \uc744 \ud559\uc2b5\uc2dc\ucf1c MNIST \uc22b\uc790 \uc774\ubbf8\uc9c0\ub4e4\uc744 \uc0dd\uc131\ud560 \uac83\uc785\ub2c8\ub2e4. \uac1c\ub150\uc801\uc73c\ub85c \uc26c\uc6b4 \uc608\uc2dc\uc774\uc9c0\ub9cc, \uc5ec\ub7ec\ubd84\uc774 PyTorch C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc5d0 \ub300\ud55c \ub300\ub7b5\uc801\uc778 \uac1c\uc694\ub97c \ud30c\uc545\ud558\uace0 \ub354 \ubcf5\uc7a1\ud55c \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a4\uace0 \uc2f6\uc740 \uc695\uad6c\ub97c \ubd88\ub7ec\uc77c\uc73c\ud0a4\uae30\uc5d0 \ucda9\ubd84\ud560 \uac83\uc785\ub2c8\ub2e4. \uba3c\uc800 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc \uc0ac\uc6a9\uc5d0 \ub300\ud55c \ub3d9\uae30\ubd80\uc5ec\uac00 \ub420 \ub9cc\ud55c \uc774\uc57c\uae30\ub85c \uc2dc\uc791\ud558\uace0, \uace7\ubc14\ub85c \ubaa8\ub378\uc744 \uc815\uc758\ud558\uace0 \ud559\uc2b5\ud574 \ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. \ud301 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc5d0 \ub300\ud55c \uc9e7\uace0 \uc7ac\ubbf8\uc788\ub294 \ubc1c\ud45c\ub97c \ubcf4\ub824\uba74 CppCon 2018 \ub77c\uc774\ud2b8\ub2dd \ud1a0\ud06c \ub97c \uc2dc\uccad\ud558\uc138\uc694. \ud301 \uc774 \ub178\ud2b8 \ub294 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 \ucef4\ud3ec\ub10c\ud2b8\uc640 \ub514\uc790\uc778 \ucca0\ud559\uc758 \uc804\ubc18\uc801\uc778 \uac1c\uc694\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \ud301 PyTorch C++ \uc0dd\ud0dc\uacc4\uc5d0 \ub300\ud55c \ubb38\uc11c\ub294 https://pytorch.org/cppdocs\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. API \ub808\ubca8\uc758 \ubb38\uc11c\ubfd0\ub9cc \uc544\ub2c8\ub77c \uac1c\uad04\uc801\uc778 \uc124\uba85\ub3c4 \ucc3e\uc744 \uc218 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4. \ub3d9\uae30\ubd80\uc5ec# GAN\uacfc MNIST \uc22b\uc790\ub85c\uc758 \uc124\ub808\ub294 \uc5ec\uc815\uc744 \uc2dc\uc791\ud558\uae30\uc5d0 \uc55e\uc11c, \uba3c\uc800 \ud30c\uc774\uc36c \ub300\uc2e0 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\ub97c \uc0ac\uc6a9\ud558\ub294 \uc774\uc720\uc5d0 \ub300\ud574 \uc124\uba85\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac(PyTorch \ud300)\ub294 \ud30c\uc774\uc36c\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc5c6\uac70\ub098 \uc0ac\uc6a9\ud558\uae30\uc5d0 \uc801\ud569\ud558\uc9c0 \uc54a\uc740 \ud658\uacbd\uc5d0\uc11c \uc5f0\uad6c\ub97c \uac00\ub2a5\ud558\uac8c \ud558\uae30 \uc704\ud574 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\ub97c \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uba74 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. \uc800\uc9c0\uc5f0 \uc2dc\uc2a4\ud15c: \ucd08\ub2f9 \ud504\ub808\uc784 \uc218\uac00 \ub192\uace0 \uc9c0\uc5f0 \uc2dc\uac04\uc774 \uc9e7\uc740 \uc21c\uc218 C++ \uac8c\uc784 \uc5d4\uc9c4\uc5d0\uc11c \uac15\ud654 \ud559\uc2b5 \uc5f0\uad6c\ub97c \uc218\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ud55c \ud658\uacbd\uc5d0\uc11c\ub294 \ud30c\uc774\uc36c \ub77c\uc774\ube0c\ub7ec\ub9ac\ubcf4\ub2e4 \uc21c\uc218 C++ \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ud6e8\uc52c \ub354 \uc801\ud569\ud569\ub2c8\ub2e4. \ud30c\uc774\uc36c\uc740 \ub290\ub9b0 \uc778\ud130\ud504\ub9ac\ud130 \ub54c\ubb38\uc5d0 \ub2e4\ub8e8\uae30\uac00 \uc27d\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uace0\ub3c4\uc758 \uba40\ud2f0\uc4f0\ub808\ub529 \ud658\uacbd: \uae00\ub85c\ubc8c \uc778\ud130\ud504\ub9ac\ud130 \ub77d(GIL)\uc73c\ub85c \uc778\ud574 \ud30c\uc774\uc36c\uc740 \ub3d9\uc2dc\uc5d0 \ub458 \uc774\uc0c1\uc758 \uc2dc\uc2a4\ud15c \uc4f0\ub808\ub4dc\ub97c \uc2e4\ud589\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. \ub300\uc548\uc73c\ub85c \uba40\ud2f0\ud504\ub85c\uc138\uc2f1\uc744 \uc0ac\uc6a9\ud558\uba74 \ud655\uc7a5\uc131\uc774 \ub5a8\uc5b4\uc9c0\uba70 \uc2ec\uac01\ud55c \ud55c\uacc4\uac00 \uc788\uc2b5\ub2c8\ub2e4. C++\ub294 \uc774\ub7ec\ud55c \uc81c\uc57d \uc870\uac74\uc774 \uc5c6\uc73c\uba70 \uc4f0\ub808\ub4dc\ub97c \uc27d\uac8c \ub9cc\ub4e4\uace0 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Deep Neuroevolution \uc5d0 \uc0ac\uc6a9\ub41c \uac83\uacfc \uac19\uc774 \uace0\ub3c4\uc758 \ubcd1\ub82c\ud654\uac00 \ud544\uc694\ud55c \ubaa8\ub378\ub3c4 \uc774\ub97c \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uae30\uc874\uc758 C++ \ucf54\ub4dc\ubca0\uc774\uc2a4: \ubc31\uc5d4\ub4dc \uc11c\ubc84\uc758 \uc6f9 \ud398\uc774\uc9c0 \uc11c\ube44\uc2a4\ubd80\ud130 \uc0ac\uc9c4 \ud3b8\uc9d1 \uc18c\ud504\ud2b8\uc6e8\uc5b4\uc758 3D \uadf8\ub798\ud53d \ub80c\ub354\ub9c1\uc5d0 \uc774\ub974\uae30\uae4c\uc9c0 \uc5b4\ub5a0\ud55c \uc791\uc5c5\uc774\ub77c\ub3c4 \uc218\ud589\ud558\ub294 \uae30\uc874 C++ \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uc18c\uc720\uc790\ub85c\uc11c, \uba38\uc2e0\ub7ec\ub2dd \ubc29\ubc95\ub860\uc744 \uc2dc\uc2a4\ud15c\uc5d0 \ud1b5\ud569\ud558\uace0 \uc2f6\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\ub294 PyTorch (\ud30c\uc774\uc36c) \uacbd\ud5d8 \ubcf8\uc5f0\uc758 \ub192\uc740 \uc720\uc5f0\uc131\uacfc \uc9c1\uad00\uc131\uc744 \uc720\uc9c0\ud558\uba74\uc11c, \ud30c\uc774\uc36c\uacfc C++\ub97c \uc55e\ub4a4\ub85c \ubc14\uc778\ub529\ud558\ub294 \ubc88\uac70\ub85c\uc6c0 \uc5c6\uc774 C++\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uac8c \ud574\uc90d\ub2c8\ub2e4. C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 \ubaa9\uc801\uc740 \ud30c\uc774\uc36c \ud504\ub860\ud2b8\uc5d4\ub4dc\uc640 \uacbd\uc7c1\ud558\ub294 \uac83\uc774 \uc544\ub2cc \ubcf4\uc644\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc5f0\uad6c\uc790\uc640 \uc5d4\uc9c0\ub2c8\uc5b4 \ubaa8\ub450\uac00 PyTorch\uc758 \ub2e8\uc21c\uc131, \uc720\uc5f0\uc131 \ubc0f \uc9c1\uad00\uc801\uc778 API\ub97c \ub9e4\uc6b0 \uc88b\uc544\ud569\ub2c8\ub2e4. \uc6b0\ub9ac\uc758 \ubaa9\ud45c\ub294 \uc5ec\ub7ec\ubd84\uc774 \uc704\uc758 \uc608\uc2dc\ub97c \ube44\ub86f\ud55c \ubaa8\ub4e0 \uac00\ub2a5\ud55c \ud658\uacbd\uc5d0\uc11c \uc774 \ud575\uc2ec \ub514\uc790\uc778 \uc6d0\uce59\uc744 \uc774\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc2dc\ub098\ub9ac\uc624 \uc911 \ud558\ub098\uac00 \uc5ec\ub7ec\ubd84\uc758 \uc0ac\ub840\uc5d0 \ud574\ub2f9\ud558\uac70\ub098, \ub2e8\uc21c\ud788 \uad00\uc2ec\uc774 \uc788\uac70\ub098 \uad81\uae08\ud558\ub2e4\uba74 \uc544\ub798 \ub0b4\uc6a9\uc744 \ud1b5\ud574 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc5d0 \ub300\ud574 \uc790\uc138\ud788 \uc0b4\ud3b4\ubcf4\uc138\uc694. \ud301 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\ub294 \ud30c\uc774\uc36c \ud504\ub860\ud2b8\uc5d4\ub4dc\uc640 \ucd5c\ub300\ud55c \uc720\uc0ac\ud55c API\ub97c \uc81c\uacf5\ud558\uace0\uc790 \ud569\ub2c8\ub2e4. \ub9cc\uc77c \ud30c\uc774\uc36c \ud504\ub860\ud2b8\uc5d4\ub4dc\uc5d0 \uc775\uc219\ud55c \uc0ac\ub78c\uc774 \u201cC++ \ud504\ub860\ud2b8\uc5d4\ub4dc\ub85c X\ub97c \uc5b4\ub5bb\uac8c \ud574\uc57c \ud558\ub294\uac00?\u201d \uc758\ubb38\uc744 \uac16\ub294\ub2e4\uba74, \ub9ce\uc740 \uacbd\uc6b0\uc5d0 \ud30c\uc774\uc36c\uc5d0\uc11c\uc640 \uac19\uc740 \ubc29\uc2dd\uc73c\ub85c \ucf54\ub4dc\ub97c \uc791\uc131\ud574 \ud30c\uc774\uc36c\uc5d0\uc11c\uc640 \ub3d9\uc77c\ud55c \ud568\uc218\uc640 \uba54\uc11c\ub4dc\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4. (\ub2e4\ub9cc, \uc628\uc810\uc744 \ub354\ube14 \ucf5c\ub860\uc73c\ub85c \ubc14\uafb8\ub294 \uac83\uc5d0 \uc720\uc758\ud558\uc138\uc694.) \uae30\ubcf8 \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uc791\uc131\ud558\uae30# \uba3c\uc800 \ucd5c\uc18c\ud55c\uc758 C++ \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc791\uc131\ud574 \uc6b0\ub9ac\uc758 \uc124\uc815\uacfc \ube4c\ub4dc \ud658\uacbd\uc774 \ub3d9\uc77c\ud55c\uc9c0 \ud655\uc778\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uba3c\uc800, C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\ub97c \uc0ac\uc6a9\ud558\ub294 \ub370 \ud544\uc694\ud55c \ubaa8\ub4e0 \uad00\ub828 \ud5e4\ub354, \ub77c\uc774\ube0c\ub7ec\ub9ac \ubc0f CMake \ube4c\ub4dc \ud30c\uc77c\uc744 \ud328\ud0a4\uc9d5\ud558\ub294 LibTorch \ubc30\ud3ec\ud310\uc758 \uc0ac\ubcf8\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. \ub9ac\ub205\uc2a4, \ub9e5OS, \uc708\ub3c4\uc6b0\uc6a9 LibTorch \ubc30\ud3ec\ud310\uc740 PyTorch website \uc5d0\uc11c \ub2e4\uc6b4\ub85c\ub4dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \ub098\uba38\uc9c0 \ubd80\ubd84\uc740 \uae30\ubcf8 \uc6b0\ubd84\ud22c \ub9ac\ub205\uc2a4 \ud658\uacbd\uc744 \uac00\uc815\ud558\uc9c0\ub9cc \ub9e5OS\ub098 \uc708\ub3c4\uc6b0\ub97c \uc0ac\uc6a9\ud558\uc154\ub3c4 \uad1c\ucc2e\uc2b5\ub2c8\ub2e4. \ud301 PyTorch C++ \ubc30\ud3ec\ud310 \uc124\uce58 \uc758 \uc124\uba85\uc5d0 \ub2e4\uc74c\uc758 \uacfc\uc815\uc774 \ub354 \uc790\uc138\ud788 \uc548\ub0b4\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ud301 \uc708\ub3c4\uc6b0\uc5d0\uc11c\ub294 \ub514\ubc84\uadf8 \ubc0f \ub9b4\ub9ac\uc2a4 \ube4c\ub4dc\uac00 ABI\uc640 \ud638\ud658\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ud504\ub85c\uc81d\ud2b8\ub97c \ub514\ubc84\uadf8 \ubaa8\ub4dc\ub85c \ube4c\ub4dc\ud558\ub824\uba74 LibTorch\uc758 \ub514\ubc84\uadf8 \ubc84\uc804\uc744 \uc0ac\uc6a9\ud574\ubcf4\uc138\uc694. \uc544\ub798\uc758 cmake --build . \uc5d0 \uc62c\ubc14\ub978 \uc124\uc815\uc744 \uc9c0\uc815\ud558\ub294 \uac83\ub3c4 \uc78a\uc9c0 \ub9c8\uc138\uc694. \uac00\uc7a5 \uba3c\uc800 \ud560 \uac83\uc740 PyTorch \uc6f9\uc0ac\uc774\ud2b8\uc5d0\uc11c \uac80\uc0c9\ub41c \ub9c1\ud06c\ub97c \ud1b5\ud574 LibTorch \ubc30\ud3ec\ud310\uc744 \ub85c\uceec\uc5d0 \ub2e4\uc6b4\ub85c\ub4dc\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc77c\ubc18\uc801 Ubuntu Linux \ud658\uacbd\uc758 \uacbd\uc6b0 \ub2e4\uc74c \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\ud569\ub2c8\ub2e4. # CUDA 9.0 \ub4f1\uc5d0 \ub300\ud55c \uc9c0\uc6d0\uc774 \ud544\uc694\ud55c \uacbd\uc6b0 \uc544\ub798 URL\uc5d0\uc11c \"cpu\"\ub97c \"cu90\"\ub85c \ubc14\uafb8\uc138\uc694. wget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip unzip libtorch-shared-with-deps-latest.zip \ub2e4\uc74c\uc73c\ub85c torch/torch.h \ub97c \ud638\ucd9c\ud558\ub294 dcgan.cpp \ub77c\ub294 \uc774\ub984\uc758 C++ \ud30c\uc77c \ud558\ub098\ub97c \uc791\uc131\ud569\uc2dc\ub2e4. \uc6b0\uc120\uc740 \uc544\ub798\uc640 \uac19\uc774 3x3 \ud56d\ub4f1 \ud589\ub82c\uc744 \ucd9c\ub825\ud558\uae30\ub9cc \ud558\uba74 \ub429\ub2c8\ub2e4. #include \u003ctorch/torch.h\u003e #include \u003ciostream\u003e int main() { torch::Tensor tensor = torch::eye(3); std::cout \u003c\u003c tensor \u003c\u003c std::endl; } \uc774 \uc791\uc740 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uacfc \uc774\ud6c4 \uc644\uc131\ud560 \ud559\uc2b5\uc6a9 \uc2a4\ud06c\ub9bd\ud2b8\ub97c \ube4c\ub4dc\ud558\uae30 \uc704\ud574 \uc6b0\ub9ac\ub294 \uc544\ub798\uc758 CMakeLists.txt \ub97c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4: cmake_minimum_required(VERSION 3.0 FATAL_ERROR) project(dcgan) find_package(Torch REQUIRED) add_executable(dcgan dcgan.cpp) target_link_libraries(dcgan \"${TORCH_LIBRARIES}\") set_property(TARGET dcgan PROPERTY CXX_STANDARD 14) \ucc38\uace0 CMake\ub294 LibTorch\uc5d0 \uad8c\uc7a5\ub418\ub294 \ube4c\ub4dc \uc2dc\uc2a4\ud15c\uc774\uc9c0\ub9cc \ud544\uc218 \uc694\uad6c \uc0ac\ud56d\uc740 \uc544\ub2d9\ub2c8\ub2e4. Visual Studio \ud504\ub85c\uc81d\ud2b8 \ud30c\uc77c, QMake, \uc77c\ubc18 Make \ud30c\uc77c \ub4f1 \ub2e4\ub978 \ube4c\ub4dc \ud658\uacbd\uc744 \uc0ac\uc6a9\ud574\ub3c4 \ub429\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc774\uc5d0 \ub300\ud55c \uc989\uac01\uc801\uc778 \uc9c0\uc6d0\uc740 \uc81c\uacf5\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc704 CMake \ud30c\uc77c 4\ubc88\uc9f8 \uc904\uc758 find_package(Torch REQUIRED) \ub294 CMake\uac00 LibTorch \ub77c\uc774\ube0c\ub7ec\ub9ac \ube4c\ub4dc \uc124\uc815\uc744 \ucc3e\ub3c4\ub85d \uc548\ub0b4\ud569\ub2c8\ub2e4. CMake\uac00 \ud574\ub2f9 \ud30c\uc77c\uc758 \uc704\uce58 \ub97c \ucc3e\uc744 \uc218 \uc788\ub3c4\ub85d \ud558\ub824\uba74 cmake \ud638\ucd9c \uc2dc CMAKE_PREFIX_PATH \ub97c \uc124\uc815\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\uc5d0 \uc55e\uc11c dcgan \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \ub300\ud574 \ub514\ub809\ud130\ub9ac \uad6c\uc870\ub97c \ub2e4\uc74c\uacfc \uac19\uc774 \ud1b5\uc77c\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. dcgan/ CMakeLists.txt dcgan.cpp \ub610\ud55c \uc55e\uc73c\ub85c \uc555\ucd95 \ud574\uc81c\ub41c LibTorch \ubc30\ud3ec\ud310\uc758 \uacbd\ub85c\ub97c /path/to/libtorch \ub85c \ubd80\ub974\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ubc18\ub4dc\uc2dc \uc808\ub300 \uacbd\ub85c\uc5ec\uc57c \ud569\ub2c8\ub2e4. \ud2b9\ud788 CMAKE_PREFIX_PATH \ub97c ../../libtorch \uc640 \uac19\uc774 \uc124\uc815\ud558\uba74 \uc608\uc0c1\uce58 \ubabb\ud55c \uc624\ub958\uac00 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ubcf4\ub2e4\ub294 $PWD/../../libtorch \uc640 \uac19\uc774 \ud574\ub2f9 \uc808\ub300 \uacbd\ub85c\ub97c \uc785\ub825\ud558\uc138\uc694. \uc774\uc81c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \ube4c\ub4dc\ud560 \uc900\ube44\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4. root@fa350df05ecf:/home# mkdir build root@fa350df05ecf:/home# cd build root@fa350df05ecf:/home/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch .. -- The C compiler identification is GNU 5.4.0 -- The CXX compiler identification is GNU 5.4.0 -- Check for working C compiler: /usr/bin/cc -- Check for working C compiler: /usr/bin/cc -- works -- Detecting C compiler ABI info -- Detecting C compiler ABI info - done -- Detecting C compile features -- Detecting C compile features - done -- Check for working CXX compiler: /usr/bin/c++ -- Check for working CXX compiler: /usr/bin/c++ -- works -- Detecting CXX compiler ABI info -- Detecting CXX compiler ABI info - done -- Detecting CXX compile features -- Detecting CXX compile features - done -- Looking for pthread.h -- Looking for pthread.h - found -- Looking for pthread_create -- Looking for pthread_create - not found -- Looking for pthread_create in pthreads -- Looking for pthread_create in pthreads - not found -- Looking for pthread_create in pthread -- Looking for pthread_create in pthread - found -- Found Threads: TRUE -- Found torch: /path/to/libtorch/lib/libtorch.so -- Configuring done -- Generating done -- Build files have been written to: /home/build root@fa350df05ecf:/home/build# cmake --build . --config Release Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan \uc704\uc5d0\uc11c \uc6b0\ub9ac\ub294 \uba3c\uc800 dcgan \ub514\ub809\ud1a0\ub9ac \uc548\uc5d0 build \ud3f4\ub354\ub97c \ub9cc\ub4e4\uace0 \uc774 \ud3f4\ub354\uc5d0 \ub4e4\uc5b4\uac00\uc11c \ud544\uc694\ud55c \ube4c\ub4dc(Make) \ud30c\uc77c\uc744 \uc0dd\uc131\ud558\ub294 cmake \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\ud55c \ud6c4 cmake --build . --config Release \ub97c \uc2e4\ud589\ud558\uc5ec \ud504\ub85c\uc81d\ud2b8\ub97c \uc131\uacf5\uc801\uc73c\ub85c \ucef4\ud30c\uc77c\ud588\uc2b5\ub2c8\ub2e4. \uc774\uc81c \uc6b0\ub9ac\uc758 \uc791\uc740 \ubc14\uc774\ub108\ub9ac\ub97c \uc2e4\ud589\ud558\uace0 \uae30\ubcf8 \ud504\ub85c\uc81d\ud2b8 \uc124\uc815\uc5d0 \ub300\ud55c \uc774 \uc139\uc158\uc744 \uc644\ub8cc\ud560 \uc900\ube44\uac00 \ub410\uc2b5\ub2c8\ub2e4. root@fa350df05ecf:/home/build# ./dcgan 1 0 0 0 1 0 0 0 1 [ Variable[CPUFloatType]{3,3} ] \uc81c\uac00 \ubcf4\uae30\uc5d4 \ud56d\ub4f1 \ud589\ub82c\uc778 \uac83 \uac19\uad70\uc694! \uc2e0\uacbd\ub9dd \ubaa8\ub378 \uc815\uc758\ud558\uae30# \uc774\uc81c \uae30\ubcf8\uc801\uc778 \ud658\uacbd\uc744 \uc124\uc815\ud588\uc73c\ub2c8, \uc774\ubc88 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c \ud6e8\uc52c \ub354 \ud765\ubbf8\ub85c\uc6b4 \ubd80\ubd84\uc744 \uc0b4\ud3b4\ubd05\uc2dc\ub2e4. \uba3c\uc800 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc5d0\uc11c \ubaa8\ub4c8\uc744 \uc815\uc758\ud558\uace0 \uc0c1\ud638 \uc791\uc6a9\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574 \ub17c\uc758\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc778 \uc18c\uaddc\ubaa8 \uc608\uc81c \ubaa8\ub4c8\ubd80\ud130 \uc2dc\uc791\ud558\uc5ec C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uac00 \uc81c\uacf5\ud558\ub294 \ub2e4\uc591\ud55c \ub0b4\uc7a5 \ubaa8\ub4c8 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc644\uc131\ub3c4 \uc788\ub294 GAN\uc744 \uad6c\ud604\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ubaa8\ub4c8 API \uae30\ucd08# \ud30c\uc774\uc36c \uc778\ud130\ud398\uc774\uc2a4\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c, C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc5d0 \uae30\ubc18\uc744 \ub454 \uc2e0\uacbd\ub9dd\ub3c4 \ubaa8\ub4c8 \uc774\ub77c \ubd88\ub9ac\ub294 \uc7ac\uc0ac\uc6a9 \uac00\ub2a5\ud55c \ube4c\ub529 \ube14\ub85d\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ud30c\uc774\uc36c\uc5d0 \ub2e4\ub978 \ubaa8\ub4e0 \ubaa8\ub4c8\uc774 \ud30c\uc0dd\ub418\ub294 torch.nn.Module \ub77c\ub294 \uae30\ubcf8 \ubaa8\ub4c8 \ud074\ub798\uc2a4\uac00 \uc788\ub4ef\uc774 C++\uc5d0\ub294 torch::nn::Module \ud074\ub798\uc2a4\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c \ubaa8\ub4c8\uc5d0\ub294 \ucea1\uc290\ud654\ub41c \uc54c\uace0\ub9ac\uc998\uc744 \uad6c\ud604\ud558\ub294 forward() \uba54\uc11c\ub4dc\ub97c \ube44\ub86f\ud574 \ub9e4\uac1c\ubcc0\uc218, \ubc84\ud37c \ubc0f \ud558\uc704 \ubaa8\ub4c8 \uc138 \uac00\uc9c0 \ud558\uc704 \uac1d\uccb4\uac00 \ud3ec\ud568\ub429\ub2c8\ub2e4. \ub9e4\uac1c\ubcc0\uc218\uc640 \ubc84\ud37c\ub294 \ud150\uc11c\uc758 \ud615\ud0dc\ub85c \uc0c1\ud0dc\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4. \ub9e4\uac1c\ubcc0\uc218\ub294 \uadf8\ub798\ub514\uc5b8\ud2b8\ub97c \uae30\ub85d\ud558\uc9c0\ub9cc \ubc84\ud37c\ub294 \uae30\ub85d\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ub9e4\uac1c\ubcc0\uc218\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \uc2e0\uacbd\ub9dd\uc758 \ud559\uc2b5 \uac00\ub2a5\ud55c \uac00\uc911\uce58\uc785\ub2c8\ub2e4. \ubc84\ud37c\uc758 \uc608\ub85c\ub294 \ubc30\uce58 \uc815\uaddc\ud654\ub97c \uc704\ud55c \ud3c9\uade0 \ubc0f \ubd84\uc0b0\uc774 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\uc815 \ub17c\ub9ac \ubc0f \uc0c1\ud0dc \ube14\ub85d\uc744 \uc7ac\uc0ac\uc6a9\ud558\uae30 \uc704\ud574, PyTorch API\ub294 \ubaa8\ub4c8\ub4e4\uc774 \uc911\ucca9\ub418\ub294 \uac83\uc744 \ud5c8\uc6a9\ud569\ub2c8\ub2e4. \uc911\ucca9\ub41c \ubaa8\ub4c8\uc740 \ud558\uc704 \ubaa8\ub4c8 \uc774\ub77c\uace0 \ud569\ub2c8\ub2e4. \ub9e4\uac1c\ubcc0\uc218, \ubc84\ud37c \ubc0f \ud558\uc704 \ubaa8\ub4c8\uc740 \uba85\uc2dc\uc801\uc73c\ub85c \ub4f1\ub85d(register)\uc744 \ud574\uc57c \ud569\ub2c8\ub2e4. \ub4f1\ub85d\uc774 \ub418\uba74 parameters() \ub098 buffers() \uac19\uc740 \uba54\uc11c\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec (\uc911\ucca9\uc744 \ud3ec\ud568\ud55c) \uc804\uccb4 \ubaa8\ub4c8 \uacc4\uce35 \uad6c\uc870\uc5d0\uc11c \ubaa8\ub4e0 \ub9e4\uac1c\ubcc0\uc218 \ubb36\uc74c\uc744 \uac80\uc0c9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9c8\ucc2c\uac00\uc9c0\ub85c, to(...) \uc640 \uac19\uc740 \uba54\uc11c\ub4dc\ub294 \ubaa8\ub4c8 \uacc4\uce35 \uad6c\uc870 \uc804\uccb4\uc5d0 \ub300\ud55c \uba54\uc11c\ub4dc\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, to(torch::kCUDA) \ub294 \ubaa8\ub4e0 \ub9e4\uac1c\ubcc0\uc218\uc640 \ubc84\ud37c\ub97c CPU\uc5d0\uc11c CUDA \uba54\ubaa8\ub9ac\ub85c \uc774\ub3d9\uc2dc\ud0b5\ub2c8\ub2e4. \ubaa8\ub4c8 \uc815\uc758 \ubc0f \ub9e4\uac1c\ubcc0\uc218 \ub4f1\ub85d# \uc774 \ub0b4\uc6a9\uc744 \ucf54\ub4dc\ub85c \uad6c\ud604\ud558\uae30 \uc704\ud574, \ud30c\uc774\uc36c \uc778\ud130\ud398\uc774\uc2a4\ub85c \uc791\uc131\ub41c \uac04\ub2e8\ud55c \ubaa8\ub4c8 \ud558\ub098\ub97c \uc0dd\uac01\ud574 \ubd05\uc2dc\ub2e4. import torch class Net(torch.nn.Module): def __init__(self, N, M): super(Net, self).__init__() self.W = torch.nn.Parameter(torch.randn(N, M)) self.b = torch.nn.Parameter(torch.randn(M)) def forward(self, input): return torch.addmm(self.b, input, self.W) \uc774\ub97c C++\ub85c \uc791\uc131\ud558\uba74 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. #include \u003ctorch/torch.h\u003e struct Net : torch::nn::Module { Net(int64_t N, int64_t M) { W = register_parameter(\"W\", torch::randn({N, M})); b = register_parameter(\"b\", torch::randn(M)); } torch::Tensor forward(torch::Tensor input) { return torch::addmm(b, input, W); } torch::Tensor W, b; }; \ud30c\uc774\uc36c\uc5d0\uc11c\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c \ubaa8\ub4c8 \uae30\ubcf8 \ud074\ub798\uc2a4\uc5d0\uc11c \ud30c\uc0dd\ud55c Net \uc774\ub77c\ub294 \ud074\ub798\uc2a4\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. (\uc26c\uc6b4 \uc124\uba85\uc744 \uc704\ud574 class \ub300\uc2e0 struct \uc744 \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.) \ud30c\uc774\uc36c\uc5d0\uc11c torch.randn\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\ucc98\ub7fc \uc0dd\uc131\uc790\uc5d0\uc11c\ub294 torch::randn \uc744 \uc0ac\uc6a9\ud574 \ud150\uc11c\ub97c \ub9cc\ub4ed\ub2c8\ub2e4. \ud55c \uac00\uc9c0 \ud765\ubbf8\ub85c\uc6b4 \ucc28\uc774\uc810\uc740 \ub9e4\uac1c\ubcc0\uc218\ub97c \ub4f1\ub85d\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \ud30c\uc774\uc36c\uc5d0\uc11c\ub294 \ud150\uc11c\ub97c torch.nn \uc73c\ub85c \uac10\uc2f8\ub294 \uac83\uacfc \ub2ec\ub9ac, C++\uc5d0\uc11c\ub294 register_parameter \uba54\uc11c\ub4dc\ub97c \ud1b5\ud574 \ud150\uc11c\ub97c \uc804\ub2ec\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ucc28\uc774\uc758 \uc6d0\uc778\uc740 \ud30c\uc774\uc36c API\uc758 \uacbd\uc6b0, \uc5b4\ub5a4 \uc18d\uc131(attirbute)\uc774 torch.nn.Parameter \ud0c0\uc785\uc778\uc9c0 \uac10\uc9c0\ud574 \uadf8\ub7ec\ud55c \ud150\uc11c\ub97c \uc790\ub3d9\uc73c\ub85c \ub4f1\ub85d\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc5d0 \ub098\ud0c0\ub0a9\ub2c8\ub2e4. C++\uc5d0\uc11c\ub294 \ub9ac\ud50c\ub809\uc158(reflection)\uc774 \ub9e4\uc6b0 \uc81c\ud55c\uc801\uc774\ubbc0\ub85c \ubcf4\ub2e4 \uc804\ud1b5\uc801\uc778 (\uadf8\ub9ac\ud558\uc5ec \ub35c \ub9c8\ubc95\uc801\uc778) \ubc29\uc2dd\uc774 \uc81c\uacf5\ub429\ub2c8\ub2e4. \uc11c\ube0c\ubaa8\ub4c8 \ub4f1\ub85d \ubc0f \ubaa8\ub4c8 \uacc4\uce35 \uad6c\uc870 \ud0d0\uc0c9# \ub9e4\uac1c\ubcc0\uc218 \ub4f1\ub85d\uacfc \ub9c8\ucc2c\uac00\uc9c0 \ubc29\ubc95\uc73c\ub85c \uc11c\ube0c\ubaa8\ub4c8\uc744 \ub4f1\ub85d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud30c\uc774\uc36c\uc5d0\uc11c \uc11c\ube0c\ubaa8\ub4c8\uc740 \uc5b4\ub5a4 \ubaa8\ub4c8\uc758 \uc18d\uc131\uc73c\ub85c \uc9c0\uc815\ub420 \ub54c \uc790\ub3d9\uc73c\ub85c \uac10\uc9c0\ub418\uace0 \ub4f1\ub85d\ub429\ub2c8\ub2e4. class Net(torch.nn.Module): def __init__(self, N, M): super(Net, self).__init__() # Registered as a submodule behind the scenes self.linear = torch.nn.Linear(N, M) self.another_bias = torch.nn.Parameter(torch.rand(M)) def forward(self, input): return self.linear(input) + self.another_bias \uc608\ub97c \ub4e4\uc5b4, parameters() \uba54\uc11c\ub4dc\ub97c \uc0ac\uc6a9\ud558\uba74 \ubaa8\ub4c8 \uacc4\uce35\uc758 \ubaa8\ub4e0 \ub9e4\uac1c\ubcc0\uc218\uc5d0 \uc7ac\uadc0\uc801\uc73c\ub85c \uc561\uc138\uc2a4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \u003e\u003e\u003e net = Net(4, 5) \u003e\u003e\u003e print(list(net.parameters())) [Parameter containing: tensor([0.0808, 0.8613, 0.2017, 0.5206, 0.5353], requires_grad=True), Parameter containing: tensor([[-0.3740, -0.0976, -0.4786, -0.4928], [-0.1434, 0.4713, 0.1735, -0.3293], [-0.3467, -0.3858, 0.1980, 0.1986], [-0.1975, 0.4278, -0.1831, -0.2709], [ 0.3730, 0.4307, 0.3236, -0.0629]], requires_grad=True), Parameter containing: tensor([ 0.2038, 0.4638, -0.2023, 0.1230, -0.0516], requires_grad=True)] C++\uc5d0\uc11c torch::nn::Linear \ub4f1\uc758 \ubaa8\ub4c8\uc744 \uc11c\ube0c\ubaa8\ub4c8\ub85c \ub4f1\ub85d\ud558\ub824\uba74 \uc774\ub984\uc5d0\uc11c \uc720\ucd94\ud560 \uc218 \uc788\ub4ef\uc774 register_module() \uba54\uc11c\ub4dc\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. struct Net : torch::nn::Module { Net(int64_t N, int64_t M) : linear(register_module(\"linear\", torch::nn::Linear(N, M))) { another_bias = register_parameter(\"b\", torch::randn(M)); } torch::Tensor forward(torch::Tensor input) { return linear(input) + another_bias; } torch::nn::Linear linear; torch::Tensor another_bias; }; \ud301 torch::nn \uc5d0 \ub300\ud55c \uc774 \ubb38\uc11c \uc5d0\uc11c torch::nn::Linear, torch::nn::Dropout, torch::nn::Conv2d \ub4f1 \uc0ac\uc6a9 \uac00\ub2a5\ud55c \uc804\uccb4 \ube4c\ud2b8\uc778 \ubaa8\ub4c8 \ubaa9\ub85d\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc704 \ucf54\ub4dc\uc5d0\uc11c \ud55c \uac00\uc9c0 \ubbf8\ubb18\ud55c \uc0ac\uc2e4\uc740 \uc11c\ube0c\ubaa8\ub4c8\uc740 \uc0dd\uc131\uc790\uc758 \uc774\ub2c8\uc15c\ub77c\uc774\uc800 \ubaa9\ub85d\uc5d0 \uc791\uc131\ub418\uace0 \ub9e4\uac1c\ubcc0\uc218\ub294 \uc0dd\uc131\uc790\uc758 \ubc14\ub514(body)\uc5d0 \uc791\uc131\ub418\uc5c8\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\ub294 \ucda9\ubd84\ud55c \uc774\uc720\uac00 \uc788\uc73c\uba70 \uc544\ub798 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 \uc624\ub108\uc2ed \ubaa8\ub378 \uc139\uc158\uc5d0\uc11c \ub354 \ub2e4\ub8f0 \uc608\uc815\uc785\ub2c8\ub2e4. \uadf8\ub807\uc9c0\ub9cc \ucd5c\uc885 \uacb0\ub860\uc740 \ud30c\uc774\uc36c\uc5d0\uc11c\ucc98\ub7fc \ubaa8\ub4c8 \ud2b8\ub9ac\uc758 \ub9e4\uac1c\ubcc0\uc218\uc5d0 \uc7ac\uadc0\uc801\uc73c\ub85c \uc561\uc138\uc2a4\ud560 \uc218 \uc788\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. parameters() \ub97c \ud638\ucd9c\ud558\uba74 \uc21c\ud68c\uac00 \uac00\ub2a5\ud55c std::vector\u003ctorch::Tensor\u003e \uac00 \ubc18\ud658\ub429\ub2c8\ub2e4. int main() { Net net(4, 5); for (const auto\u0026 p : net.parameters()) { std::cout \u003c\u003c p \u003c\u003c std::endl; } } \uc774\ub97c \uc2e4\ud589\ud55c \uacb0\uacfc\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. root@fa350df05ecf:/home/build# ./dcgan 0.0345 1.4456 -0.6313 -0.3585 -0.4008 [ Variable[CPUFloatType]{5} ] -0.1647 0.2891 0.0527 -0.0354 0.3084 0.2025 0.0343 0.1824 -0.4630 -0.2862 0.2500 -0.0420 0.3679 -0.1482 -0.0460 0.1967 0.2132 -0.1992 0.4257 0.0739 [ Variable[CPUFloatType]{5,4} ] 0.01 * 3.6861 -10.1166 -45.0333 7.9983 -20.0705 [ Variable[CPUFloatType]{5} ] \ud30c\uc774\uc36c\uc5d0\uc11c\uc640 \uac19\uc774 \uc138 \uac1c\uc758 \ub9e4\uac1c\ubcc0\uc218\uac00 \ucd9c\ub825\ub410\uc2b5\ub2c8\ub2e4. \uc774 \ub9e4\uac1c\ubcc0\uc218\ub4e4\uc758 \uc774\ub984\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub3c4\ub85d C++ API\ub294 named_parameters() \uba54\uc11c\ub4dc\ub97c \uc81c\uacf5\ud558\uba70, \uc774\ub294 \ud30c\uc774\uc36c\uc5d0\uc11c\uc640 \uac19\uc774 Orderdict \ub97c \ubc18\ud658\ud569\ub2c8\ub2e4. Net net(4, 5); for (const auto\u0026 pair : net.named_parameters()) { std::cout \u003c\u003c pair.key() \u003c\u003c \": \" \u003c\u003c pair.value() \u003c\u003c std::endl; } \ub9c8\ucc2c\uac00\uc9c0\ub85c \ucf54\ub4dc\ub97c \uc2e4\ud589\ud558\uba74 \uacb0\uacfc\ub294 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4. root@fa350df05ecf:/home/build# make \u0026\u0026 ./dcgan 11:13:48 Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan b: -0.1863 -0.8611 -0.1228 1.3269 0.9858 [ Variable[CPUFloatType]{5} ] linear.weight: 0.0339 0.2484 0.2035 -0.2103 -0.0715 -0.2975 -0.4350 -0.1878 -0.3616 0.1050 -0.4982 0.0335 -0.1605 0.4963 0.4099 -0.2883 0.1818 -0.3447 -0.1501 -0.0215 [ Variable[CPUFloatType]{5,4} ] linear.bias: -0.0250 0.0408 0.3756 -0.2149 -0.3636 [ Variable[CPUFloatType]{5} ] \ucc38\uace0 torch::nn::Module \uc5d0 \ub300\ud55c \ubb38\uc11c \ub294 \ubaa8\ub4c8 \uacc4\uce35 \uad6c\uc870\uc5d0 \ub300\ud55c \uba54\uc11c\ub4dc \ubaa9\ub85d \uc804\uccb4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc21c\uc804\ud30c(forward) \ubaa8\ub4dc\ub85c \ub124\ud2b8\uc6cc\ud06c \uc2e4\ud589# \ub124\ud2b8\uc6cc\ud06c\ub97c C++\ub85c \uc2e4\ud589\ud558\uae30 \uc704\ud574\uc11c\ub294, \uc6b0\ub9ac\uac00 \uc815\uc758\ud55c forward() \uba54\uc11c\ub4dc\ub97c \ud638\ucd9c\ud558\uae30\ub9cc \ud558\uba74 \ub429\ub2c8\ub2e4. int main() { Net net(4, 5); std::cout \u003c\u003c net.forward(torch::ones({2, 4})) \u003c\u003c std::endl; } \ucd9c\ub825\uc740 \ub300\ub7b5 \uc544\ub798\uc640 \uac19\uc744 \uac83\uc785\ub2c8\ub2e4 root@fa350df05ecf:/home/build# ./dcgan 0.8559 1.1572 2.1069 -0.1247 0.8060 0.8559 1.1572 2.1069 -0.1247 0.8060 [ Variable[CPUFloatType]{2,5} ] \ubaa8\ub4c8 \uc624\ub108\uc2ed (Ownership)# \uc774\uc81c \uc6b0\ub9ac\ub294 C++\uc5d0\uc11c \ubaa8\ub4c8\uc744 \uc815\uc758\ud558\uace0, \ub9e4\uac1c\ubcc0\uc218\ub97c \ub4f1\ub85d\ud558\uace0, \ud558\uc704 \ubaa8\ub4c8\uc744 \ub4f1\ub85d\ud558\uace0, parameters() \ub4f1\uc758 \uba54\uc11c\ub4dc\ub97c \ud1b5\ud574 \ubaa8\ub4c8 \uacc4\uce35\uc744 \ud0d0\uc0c9\ud558\uace0, \ubaa8\ub4c8\uc758 forward() \uba54\uc11c\ub4dc\ub97c \uc2e4\ud589\ud558\ub294 \ubc29\ubc95\uc744 \ubc30\uc6e0\uc2b5\ub2c8\ub2e4. C++ API\uc5d0\ub294 \ub2e4\ub978 \uba54\uc11c\ub4dc, \ud074\ub798\uc2a4, \uadf8\ub9ac\uace0 \uc8fc\uc81c\uac00 \ub9ce\uc9c0\ub9cc \uc804\uccb4 \ubaa9\ub85d\uc740 \ubb38\uc11c \ub97c \ucc38\uc870\ud558\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4. \uc7a0\uc2dc \ud6c4\uc5d0 DCGAN \ubaa8\ub378\uacfc \uc5d4\ub4dc \ud22c \uc5d4\ub4dc \ud559\uc2b5 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uad6c\ud604\ud558\uba74\uc11c\ub3c4 \uba87 \uac00\uc9c0 \uac1c\ub150\uc744 \ub354 \ub2e4\ub8f0 \uc608\uc815\uc785\ub2c8\ub2e4. \uadf8\uc5d0 \uc55e\uc11c C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc5d0\uc11c torch::nn::Module \uc758 \ud558\uc704 \ud074\ub798\uc2a4\ub4e4\uc5d0 \ub300\ud574 \uc81c\uacf5\ud558\ub294 \uc624\ub108\uc2ed \ubaa8\ub378 \uc5d0 \ub300\ud574 \uac04\ub2e8\ud788 \uc124\uba85\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774 \ub17c\uc758\uc5d0\uc11c \uc624\ub108\uc2ed \ubaa8\ub378\uc774\ub780 \ubaa8\ub4c8\uc744 \uc800\uc7a5\ud558\uace0 \uc804\ub2ec\ud558\ub294 \ubc29\uc2dd (\ub204\uac00 \ud639\uc740 \ubb34\uc5c7\uc774 \ud2b9\uc815 \ubaa8\ub4c8 \uc778\uc2a4\ud134\uc2a4\ub97c \uc18c\uc720\ud558\ub294\uc9c0)\uc744 \uc9c0\uce6d\ud569\ub2c8\ub2e4. \ud30c\uc774\uc36c\uc5d0\uc11c \uac1d\uccb4\ub294 \ud56d\uc0c1 \ud799\uc5d0 \ub3d9\uc801\uc73c\ub85c \ud560\ub2f9\ub418\uba70 \ub808\ud37c\ub7f0\uc2a4 \uc2dc\ub9e8\ud2f1\uc744 \uac00\uc9c0\ub294\ub370, \uc774\ub294 \ub2e4\ub8e8\uace0 \uc774\ud574\ud558\uae30\uac00 \ub9e4\uc6b0 \uc27d\uc2b5\ub2c8\ub2e4. \uc2e4\uc81c\ub85c \ud30c\uc774\uc36c\uc5d0\uc11c\ub294 \uac1d\uccb4\uac00 \uc5b4\ub514\uc5d0 \uc874\uc7ac\ud558\uace0 \uc5b4\ub5bb\uac8c \ub808\ud37c\ub7f0\uc2a4\ub418\ub294\uc9c0 \uc2e0\uacbd \uc4f0\uc9c0 \uc54a\uace0 \ud558\ub824\ub294 \uc77c\uc5d0\ub9cc \uc9d1\uc911\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc800\uae09 \uc5b8\uc5b4\uc778 C++\ub294 \uc774 \ubd80\ubd84\uc5d0\uc11c \ub354 \ub9ce\uc740 \uc635\uc158\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774\ub294 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 \ubcf5\uc7a1\uc131\uc744 \uc99d\uac00\uc2dc\ud0a4\uba70 \uadf8 \uc124\uacc4\uc640 \uc778\uccb4\uacf5\ud559\uc801 \uc694\uc18c\uc5d0\ub3c4 \ud070 \uc601\ud5a5\uc744 \uc90d\ub2c8\ub2e4. \ud2b9\ud788, C++ \ud504\ub860\ud2b8\uc5d4\ub4dc \ubaa8\ub4c8\uc5d0\uc11c\ub294 \ubc38\ub958 \uc2dc\ub9e8\ud2f1 \ub610\ub294 \ub808\ud37c\ub7f0\uc2a4 \uc2dc\ub9e8\ud2f1\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc804\uc790\uac00 \uc9c0\uae08\uae4c\uc9c0\uc758 \uc0ac\ub840\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \uac00\uc7a5 \ub2e8\uc21c\ud55c \uacbd\uc6b0\ub85c, \ubaa8\ub4c8 \uac1d\uccb4\uac00 \uc2a4\ud0dd\uc5d0 \ud560\ub2f9\ub418\uace0 \ud568\uc218\uc5d0 \uc804\ub2ec\ub420 \ub54c \ub808\ud37c\ub7f0\uc2a4 \ud639\uc740 \ud3ec\uc778\ud130\ub85c \ubcf5\uc0ac \ubc0f \uc774\ub3d9(std:move) \uc2dc\ud0a4\uac70\ub098 \uac00\uc838\uc62c \uc218 \uc788\uc2b5\ub2c8\ub2e4. struct Net : torch::nn::Module { }; void a(Net net) { } void b(Net\u0026 net) { } void c(Net* net) { } int main() { Net net; a(net); a(std::move(net)); b(net); c(\u0026net); } \ud6c4\uc790(\ub808\ud37c\ub7f0\uc2a4 \uc2dc\ub9e8\ud2f1)\uc758 \uacbd\uc6b0, std::shared_ptr \ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubaa8\ub4e0 \uacf3\uc5d0\uc11c shared_ptr \ub97c \uc0ac\uc6a9\ud55c\ub2e4\ub294 \uac00\uc815\ud558\uc5d0, \ub808\ud37c\ub7f0\uc2a4 \uc2dc\ub9e8\ud2f1\uc758 \uc7a5\uc810\uc740 \ud30c\uc774\uc36c\uc5d0\uc11c\uc640 \uac19\uc774 \ubaa8\ub4c8\uc774 \ud568\uc218\uc5d0 \uc804\ub2ec\ub418\uace0 \uc778\uc790\uac00 \uc120\uc5b8\ub418\ub294 \ubc29\uc2dd\uc5d0 \ub300\ud574 \uc0dd\uac01\ud560 \ubd80\ub2f4\uc744 \ub35c\uc5b4\uc900\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. struct Net : torch::nn::Module {}; void a(std::shared_ptr\u003cNet\u003e net) { } int main() { auto net = std::make_shared\u003cNet\u003e(); a(net); } \uacbd\ud5d8\uc801\uc73c\ub85c, \ub3d9\uc801 \uc5b8\uc5b4\ub97c \uc0ac\uc6a9\ud558\ub358 \uc5f0\uad6c\uc790\ub4e4\uc740 \ube44\ub85d \ubc38\ub958 \uc2dc\ub9e8\ud2f1\uc774 \ub354 C++\uc5d0 \u201c\ub124\uc774\ud2f0\ube0c\u201d\ud568\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ub808\ud37c\ub7f0\uc2a4 \uc2dc\ub9e8\ud2f1\uc744 \ud6e8\uc52c \uc120\ud638\ud569\ub2c8\ub2e4. \ub610\ud55c torch::nn::Module \uc758 \uc124\uacc4\ub294 \uc0ac\uc6a9\uc790 \uce5c\ud654\uc801\uc778 \ud30c\uc774\uc36c API\ub97c \uc720\uc0ac\ud558\uac8c \ub530\ub974\uae30 \uc704\ud574 shared \uc624\ub108\uc2ed\uc5d0 \uc758\uc874\ud569\ub2c8\ub2e4. \uc55e\uc11c \uc608\uc2dc\ub85c \ub4e4\uc5c8\ub358 Net \uc758 \uc815\uc758\ub97c \ucd95\uc57d\ud574\uc11c \ub2e4\uc2dc \uc0b4\ud3b4\ubd05\uc2dc\ub2e4. struct Net : torch::nn::Module { Net(int64_t N, int64_t M) : linear(register_module(\"linear\", torch::nn::Linear(N, M))) { } torch::nn::Linear linear; }; \ud558\uc704 \ubaa8\ub4c8\uc778 linear \ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 \uc774\ub97c \ud074\ub798\uc2a4\uc5d0 \uc9c1\uc811 \uc800\uc7a5\ud558\uace0\uc790 \ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \ub3d9\uc2dc\uc5d0 \ubaa8\ub4c8\uc758 \uae30\ucd08 \ud074\ub798\uc2a4\uac00 \uc774 \ud558\uc704 \ubaa8\ub4c8\uc5d0 \ub300\ud574 \uc54c\uace0 \uc811\uadfc\ud560 \uc218 \uc788\uae30\ub97c \uc6d0\ud569\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574\uc11c\ub294 \ud574\ub2f9 \ud558\uc704 \ubaa8\ub4c8\uc5d0 \ub300\ud55c \ucc38\uc870\ub97c \uc800\uc7a5\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774 \uc21c\uac04 \uc774\ubbf8 \uc6b0\ub9ac\ub294 shared \uc624\ub108\uc2ed\uc744 \ud544\uc694\ub85c \ud569\ub2c8\ub2e4. torch::nn::Module \ud074\ub798\uc2a4\uc640 \uad6c\uc0c1 \ud074\ub798\uc2a4\uc778 Net \ubaa8\ub450\uc5d0\uc11c \ud558\uc704 \ubaa8\ub4c8\uc5d0 \ub300\ud55c \ub808\ud37c\ub7f0\uc2a4\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c \uae30\ucd08 \ud074\ub798\uc2a4\ub294 \ubaa8\ub4c8\uc744 shared_ptr \ub85c \uc800\uc7a5\ud558\uba70 \uc774\uc5d0 \ub530\ub77c \uad6c\uc0c1 \ud074\ub798\uc2a4 \ub610\ud55c \ub9c8\ucc2c\uac00\uc9c0\uc77c \uac83\uc785\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc7a0\uae50! \uc704\uc758 \ucf54\ub4dc\uc5d0\ub294 shared_ptr \uc5d0 \ub300\ud55c \uc5b8\uae09\uc774 \uc5c6\uc2b5\ub2c8\ub2e4! \uc65c \uadf8\ub7f0 \uac83\uc77c\uae4c\uc694? \uc65c\ub0d0\ud558\uba74 std::shared_ptr\u003cMyModule\u003e \ub294 \ud0c0\uc774\ud551\ud558\uae30\uc5d0 \ub108\ubb34 \uae38\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \uc5f0\uad6c\uc6d0\ub4e4\uc758 \uc0dd\uc0b0\uc131\uc744 \uc720\uc9c0\ud558\uae30 \uc704\ud574, \uc6b0\ub9ac\ub294 \ub808\ud37c\ub7f0\uc2a4 \uc2dc\ub9e8\ud2f1\uc744 \uc720\uc9c0\ud558\uba74\uc11c \ubc38\ub958 \uc2dc\ub9e8\ud2f1\ub9cc\uc758 \uc7a5\uc810\uc778 shared_ptr \uc5d0 \ub300\ud55c \uc5b8\uae09\uc744 \uc228\uae30\uae30 \uc704\ud55c \uc815\uad50\ud55c \uacc4\ud68d\uc744 \uc138\uc6e0\uc2b5\ub2c8\ub2e4. \uadf8 \uc791\ub3d9 \ubc29\uc2dd\uc744 \uc774\ud574\ud558\uae30 \uc704\ud574 \ucf54\uc5b4 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0 \uc788\ub294 torch::nn::Linear \ubaa8\ub4c8\uc758 \ub2e8\uc21c\ud654\ub41c \uc815\uc758\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. (\uc804\uccb4 \uc815\uc758\ub294 \uc5ec\uae30 \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.) struct LinearImpl : torch::nn::Module { LinearImpl(int64_t in, int64_t out); Tensor forward(const Tensor\u0026 input); Tensor weight, bias; }; TORCH_MODULE(Linear); \uc694\uc57d\ud558\uc790\uba74 \uc774 \ubaa8\ub4c8\uc740 Linear \uac00 \uc544\ub2cc LinearImpl \uc774\ub77c\uace0 \ubd88\ub9bd\ub2c8\ub2e4. \uadf8\ub9ac\uace0 TORCH_MODULE \ub77c\ub294 \ub9e4\ud06c\ub85c\uac00 \uc2e4\uc81c Linear \ud074\ub798\uc2a4\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. \uc774\ub807\uac8c \u201c\uc0dd\uc131\ub41c\u201d \ud074\ub798\uc2a4\ub294 std::shared_ptr\u003cLinearImpl\u003e \ub97c \uac10\uc2f8\ub294 \ub798\ud37c(wrapper)\uc785\ub2c8\ub2e4. \ub2e8\uc21c\ud55c typedef\uac00 \uc544\ub2cc \ub798\ud37c\uc774\ubbc0\ub85c \uc0dd\uc131\uc790\ub3c4 \uc5ec\uc804\ud788 \uc608\uc0c1\ud558\ub294 \ub300\ub85c \uc791\ub3d9\ud569\ub2c8\ub2e4. \uc989, std::make_shared\u003cLinearImpl\u003e(3, 4) \uac00 \uc544\ub2cc torch::nn::Linear(3, 4) \ub77c\uace0 \uc4f8 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub807\uac8c \ub9e4\ud06c\ub85c\uc5d0 \uc758\ud574 \uc0dd\uc131\ub41c \ud074\ub798\uc2a4\ub294 holder \ubaa8\ub4c8\uc774\ub77c\uace0 \ubd80\ub985\ub2c8\ub2e4. (shared) \ud3ec\uc778\ud130\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c \ud654\uc0b4\ud45c \uc5f0\uc0b0\uc790(\uc989, model-\u003eforward(...))\ub97c \uc0ac\uc6a9\ud574 \uae30\uc800 \uac1d\uccb4\uc5d0 \uc561\uc138\uc2a4\ud569\ub2c8\ub2e4. \uacb0\ub860\uc801\uc73c\ub85c \ud30c\uc774\uc36c API\uc640 \ub9e4\uc6b0 \uc720\uc0ac\ud55c \uc624\ub108\uc2ed \ubaa8\ub378\uc744 \uc5bb\uc5c8\uc2b5\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc73c\ub85c \ub808\ud37c\ub7f0\uc2a4 \uc2dc\ub9e8\ud2f1\uc744 \ub530\ub974\uc9c0\ub9cc, std:shared_ptr \ub098 std::make_shared \ub4f1\uc744 \ud0c0\uc774\ud551\ud560 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\uc758 Net \uc608\uc2dc\uc5d0\uc11c \ubaa8\ub4c8 holder API\ub97c \uc0ac\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4. struct NetImpl : torch::nn::Module {}; TORCH_MODULE(Net); void a(Net net) { } int main() { Net net; a(net); } \uc5ec\uae30\uc11c \uc5b8\uae09\ud560 \ub9cc\ud55c \ubbf8\ubb18\ud55c \ubb38\uc81c\uac00 \ud558\ub098 \uc788\uc2b5\ub2c8\ub2e4. \uae30\ubcf8 \uc0dd\uc131\uc790\uc5d0 \uc758\ud574 \ub9cc\ub4e4\uc5b4\uc9c4 std::shared_ptr \ub294 \u201c\ube44\uc5b4\u201d \uc788\uc2b5\ub2c8\ub2e4. \uc989, null \ud3ec\uc778\ud130\uc785\ub2c8\ub2e4. \uae30\ubcf8 \uc0dd\uc131\uc790\ub85c \ub9cc\ub4e4\uc5b4\uc9c4 Linear \uc774\ub098 Net \uc740 \ubb34\uc5c7\uc774\uc5b4\uc57c \ud560\uae4c\uc694? \uc74c, \uc774\uac74 \uc5b4\ub824\uc6b4 \uacb0\uc815\uc785\ub2c8\ub2e4. \ube48 (null) std::shared_ptr\u003cLinearImpl\u003e \ub85c \uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc Linear(3, 4) \uac00 std::make_shared\u003cLinearImpl\u003e(3, 4) \uc640 \uac19\ub2e4\ub294 \uac83\uc744 \uae30\uc5b5\ud569\uc2dc\ub2e4. \uc989, Linear linear; \uc774 null \ud3ec\uc778\ud130\uc5ec\uc57c \ud55c\ub2e4\uace0 \uacb0\uc815\ud55c\ub2e4\uba74 \uc0dd\uc131\uc790\uc5d0\uc11c \uc778\uc790\ub97c \uc804\ud600 \ubc1b\uc9c0 \uc54a\uac70\ub098 \ubaa8\ub4e0 \uc778\uc790\uc5d0 \ub300\ud574 \uae30\ubcf8\uac12\uc744 \uc0ac\uc6a9\ud558\ub294 \ubaa8\ub4c8\uc744 \uc0dd\uc131\ud560 \ubc29\ubc95\uc774 \uc5c6\uc5b4\uc9d1\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc774\uc720\ub85c \ud604\uc7ac API\uc5d0\uc11c \uae30\ubcf8 \uc0dd\uc131\uc790\uc5d0 \uc758\ud574 \ub9cc\ub4e4\uc5b4\uc9c4 \ubaa8\ub4c8 holder(Linear() \ub4f1)\ub294 \uae30\uc800 \ubaa8\ub4c8(LinearImpl())\uc758 \uae30\ubcf8 \uc0dd\uc131\uc790\ub97c \ud638\ucd9c\ud569\ub2c8\ub2e4. \ub9cc\uc57d \uae30\uc800 \ubaa8\ub4c8\uc5d0 \uae30\ubcf8 \uc0dd\uc131\uc790\uac00 \uc5c6\uc73c\uba74 \ucef4\ud30c\uc77c\ub7ec \uc624\ub958\uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4. \ubc18\ub300\ub85c \ube48 holder\ub97c \uc0dd\uc131\ud558\ub824\uba74 holder \uc0dd\uc131\uc790\uc5d0 nullptr \ub97c \uc804\ub2ec\ud558\uba74 \ub429\ub2c8\ub2e4. \uc2e4\uc81c\ub85c\ub294 \uc55e\uc5d0\uc11c\uc640 \uac19\uc774 \ud558\uc704 \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud574 \ubaa8\ub4c8\uc744 \uc774\ub2c8\uc15c\ub77c\uc774\uc800 (initializer) \ubaa9\ub85d \uc5d0 \ub4f1\ub85d \ubc0f \uc0dd\uc131\ud558\uac70\ub098, struct Net : torch::nn::Module { Net(int64_t N, int64_t M) : linear(register_module(\"linear\", torch::nn::Linear(N, M))) { } torch::nn::Linear linear; }; \ud30c\uc774\uc36c \uc0ac\uc6a9\uc790\ub4e4\uc5d0\uac8c \ub354 \uce5c\uc219\ud55c \ubc29\ubc95\uc73c\ub85c, \uba3c\uc800 null \ud3ec\uc778\ud130\ub85c \ud640\ub354\ub97c \uc0dd\uc131\ud55c \uc774\ud6c4 \uc0dd\uc131\uc790\uc5d0\uc11c \uac12\uc744 \uc9c0\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. struct Net : torch::nn::Module { Net(int64_t N, int64_t M) { linear = register_module(\"linear\", torch::nn::Linear(N, M)); } torch::nn::Linear linear{nullptr}; // construct an empty holder }; \uacb0\ub860\uc801\uc73c\ub85c \uc5b4\ub5a4 \uc624\ub108\uc2ed \ubaa8\ub378, \uc5b4\ub5a4 \uc2dc\ub9e8\ud2f1\uc744 \uc0ac\uc6a9\ud558\uba74 \uc88b\uc744\uae4c\uc694? C++ \ud504\ub860\ud2b8\uc5d4\ub4dc API\ub294 \ubaa8\ub4c8 holder\uac00 \uc81c\uacf5\ud558\ub294 \uc624\ub108\uc2ed \ubaa8\ub378\uc744 \uac00\uc7a5 \uc798 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \uc774 \uba54\ucee4\ub2c8\uc998\uc758 \uc720\uc77c\ud55c \ub2e8\uc810\uc740 \ubaa8\ub4c8 \uc120\uc5b8 \uc544\ub798\uc5d0 boilerplate \ud55c \uc904\uc774 \ucd94\uac00\ub41c\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \uc989, \uac00\uc7a5 \ub2e8\uc21c\ud55c \ubaa8\ub378\uc740 C++ \ubaa8\ub4c8\uc758 \uae30\ucd08\ub97c \ubc30\uc6b8 \ub54c \ub098\uc624\ub294 \ubc38\ub958 \uc2dc\ub9e8\ud2f1 \ubaa8\ub378\uc785\ub2c8\ub2e4. \uc791\uace0 \uac04\ub2e8\ud55c \uc2a4\ud06c\ub9bd\ud2b8\uc758 \uacbd\uc6b0, \uc774\uac83\ub9cc\uc73c\ub85c \ucda9\ubd84\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc5b8\uc820\uac00\ub294 \uae30\uc220\uc801 \uc774\uc720\ub85c \uc778\ud574 \uc774 \uae30\ub2a5\uc774 \ud56d\uc0c1 \uc9c0\uc6d0\ub418\uc9c0\ub294 \uc54a\ub294\ub2e4\ub294 \uc0ac\uc2e4\uc744 \uc54c\uac8c \ub420 \uac83\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uc9c1\ub82c\ud654 API(torch::save \ubc0f torch::load)\ub294 \ubaa8\ub4c8 holder(\ud639\uc740 \uc77c\ubc18 shared_ptr)\ub9cc\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\ub85c \ubaa8\ub4c8\uc744 \uc815\uc758\ud560 \ub54c\uc5d0\ub294 \ubaa8\ub4c8 holder API \ubc29\uc2dd\uc774 \uad8c\uc7a5\ub418\uba70, \uc55e\uc73c\ub85c \ubcf8 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c \uc774 API\ub97c \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4. DCGAN \ubaa8\ub4c8 \uc815\uc758\ud558\uae30# \uc774\uc81c \uc774 \uae00\uc5d0\uc11c \ud574\uacb0\ud558\ub824\ub294 \uba38\uc2e0\ub7ec\ub2dd \ud0dc\uc2a4\ud06c\ub97c \uc704\ud55c \ubaa8\ub4c8\uc744 \uc815\uc758\ud558\ub294\ub370 \ud544\uc694\ud55c \ubc30\uacbd\uacfc \ub3c4\uc785\ubd80 \uc124\uba85\uc774 \ub05d\ub0ac\uc2b5\ub2c8\ub2e4. \ub2e4\uc2dc \uc0c1\uae30\ud558\uc790\uba74, \uc6b0\ub9ac\uc758 \ud0dc\uc2a4\ud06c\ub294 MNIST \ub370\uc774\ud130\uc14b \uc758 \uc22b\uc790 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc774 \ud0dc\uc2a4\ud06c\ub97c \ud480\uae30 \uc704\ud574 \uc801\ub300\uc801 \uc0dd\uc131 \uc2e0\uacbd\ub9dd(GAN) \uc744 \uc0ac\uc6a9\ud558\uace0\uc790 \ud569\ub2c8\ub2e4. \uadf8 \uc911\uc5d0\uc11c\ub3c4 \uc6b0\ub9ac\ub294 DCGAN \uc544\ud0a4\ud14d\ucc98 \ub97c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. DCGAN\uc740 \uac00\uc7a5 \ucd08\uae30\uc5d0 \ubc1c\ud45c\ub410\ub358 \uc81c\uc77c \uac04\ub2e8\ud55c GAN\uc774\uc9c0\ub9cc \uc774 \ud0dc\uc2a4\ud06c\ub97c \uc704\ud574\uc11c\ub294 \ucda9\ubd84\ud569\ub2c8\ub2e4. \ud301 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0 \ub098\uc628 \uc18c\uc2a4 \ucf54\ub4dc \uc804\uccb4\ub294 \uc774 \uc800\uc7a5\uc18c \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. GAN\uc774 \ubb50\uc600\uc8e0?# GAN\uc740 \uc0dd\uc131\uae30(generator) \uc640 \ud310\ubcc4\uae30(discriminator) \ub77c\ub294 \ub450 \uac00\uc9c0 \uc2e0\uacbd\ub9dd \ubaa8\ub378\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \uc0dd\uc131\uae30\ub294 \ub178\uc774\uc988 \ubd84\ud3ec\uc5d0\uc11c \uc0d8\ud50c\uc744 \uc785\ub825\ubc1b\uace0, \uac01 \ub178\uc774\uc988 \uc0d8\ud50c\uc744 \ubaa9\ud45c \ubd84\ud3ec(\uc774 \uacbd\uc6b0 MNIST \ub370\uc774\ud130\uc14b)\uc640 \uc720\uc0ac\ud55c \uc774\ubbf8\uc9c0\ub85c \ubcc0\ud658\ud558\ub294 \uac83\uc774 \ubaa9\ud45c\uc785\ub2c8\ub2e4. \ud310\ubcc4\uae30\ub294 MNIST \ub370\uc774\ud130\uc14b\uc758 \uc9c4\uc9dc \uc774\ubbf8\uc9c0\ub97c \uc785\ub825\ubc1b\uac70\ub098 \uc0dd\uc131\uae30\ub85c\ubd80\ud130 \uac00\uc9dc \uc774\ubbf8\uc9c0\ub97c \uc785\ub825\ubc1b\uc2b5\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uc5b4\ub5a4 \uc774\ubbf8\uc9c0\uac00 \uc5bc\ub9c8\ub098 \uc9c4\uc9dc\uac19\uc740\uc9c0 (1 \uc5d0 \uac00\uae4c\uc6b4 \ucd9c\ub825) \ud639\uc740 \uac00\uc9dc\uac19\uc740 \uc9c0 (0 \uc5d0 \uac00\uae4c\uc6b4 \ucd9c\ub825) \ud310\ubcc4\ud569\ub2c8\ub2e4. \uc0dd\uc131\uae30\uac00 \ub9cc\ub4e0 \uc774\ubbf8\uc9c0\uac00 \uc5bc\ub9c8\ub098 \uc9c4\uc9dc\uac19\uc740 \uc9c0 \ud310\ubcc4\uae30\uac00 \ud53c\ub4dc\ubc31\ud558\uace0 \uc774 \ud53c\ub4dc\ubc31\uc740 \uc0dd\uc131\uae30 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \ud310\ubcc4\uae30\uac00 \uc9c4\uc9dc\uc5d0 \ub300\ud55c \uc548\ubaa9\uc774 \uc5bc\ub9c8\ub098 \uc88b\uc740 \uc9c0\uc5d0 \ub300\ud55c \ud53c\ub4dc\ubc31\uc740 \ud310\ubcc4\uae30\ub97c \ucd5c\uc801\ud654\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc774\ub860\uc801\uc73c\ub85c, \uc0dd\uc131\uae30\uc640 \ud310\ubcc4\uae30 \uc0ac\uc774\uc758 \uc12c\uc138\ud55c \uade0\ud615\uc740 \uc774 \ub458\uc744 \ub3d9\uc2dc\uc5d0 \uac1c\uc120\uc2dc\ud0b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uc0dd\uc131\uae30\ub294 \ubaa9\ud45c \ubd84\ud3ec\uc640 \uad6c\ubcc4\ud560 \uc218 \uc5c6\ub294 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud558\uace0, (\uadf8\ub54c\ucbe4\uc774\uba74) \uc798 \ud559\uc2b5\ub418\uc5b4 \uc788\uc744 \ud310\ubcc4\uae30\uc758 \uc548\ubaa9\uc744 \uc18d\uc5ec \uc9c4\uc9dc\uc640 \uac00\uc9dc \uc774\ubbf8\uc9c0 \ubaa8\ub450\uc5d0 \ub300\ud574 0.5 \uc758 \ud655\ub960\uc744 \ucd9c\ub825\ud560 \uac83\uc785\ub2c8\ub2e4. \ucd5c\uc885 \uacb0\uacfc\ubb3c\uc740 \ub178\uc774\uc988\ub97c \uc785\ub825\ubc1b\uc544 \uc2e4\uc81c \uc22b\uc790\uc758 \uc774\ubbf8\uc9c0\ub97c \ucd9c\ub825\uc73c\ub85c \uc0dd\uc131\ud558\ub294 \uae30\uacc4\uc785\ub2c8\ub2e4. \uc0dd\uc131\uae30 (Generator) \ubaa8\ub4c8# \uba3c\uc800 \uc77c\ub828\uc758 \uc804\uce58\ub41c (transposed) 2D \ud569\uc131\uacf1, \ubc30\uce58 \uc815\uaddc\ud654 \ubc0f ReLU \ud65c\uc131\ud654 \uc720\ub2db\uc73c\ub85c \uad6c\uc131\ub41c \uc0dd\uc131\uae30 \ubaa8\ub4c8\uc744 \uc815\uc758\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ubaa8\ub4c8\uc758 forward() \uba54\uc11c\ub4dc\ub97c \uc9c1\uc811 \uc815\uc758\ud558\uc5ec \ubaa8\ub4c8 \uac04 \uc785\ub825\uc744 (\ud568\uc218\ud615\uc73c\ub85c) \uba85\uc2dc\uc801\uc73c\ub85c \uc804\ub2ec\ud569\ub2c8\ub2e4. struct DCGANGeneratorImpl : nn::Module { DCGANGeneratorImpl(int kNoiseSize) : conv1(nn::ConvTranspose2dOptions(kNoiseSize, 256, 4) .bias(false)), batch_norm1(256), conv2(nn::ConvTranspose2dOptions(256, 128, 3) .stride(2) .padding(1) .bias(false)), batch_norm2(128), conv3(nn::ConvTranspose2dOptions(128, 64, 4) .stride(2) .padding(1) .bias(false)), batch_norm3(64), conv4(nn::ConvTranspose2dOptions(64, 1, 4) .stride(2) .padding(1) .bias(false)) { // register_module() is needed if we want to use the parameters() method later on register_module(\"conv1\", conv1); register_module(\"conv2\", conv2); register_module(\"conv3\", conv3); register_module(\"conv4\", conv4); register_module(\"batch_norm1\", batch_norm1); register_module(\"batch_norm2\", batch_norm2); register_module(\"batch_norm3\", batch_norm3); } torch::Tensor forward(torch::Tensor x) { x = torch::relu(batch_norm1(conv1(x))); x = torch::relu(batch_norm2(conv2(x))); x = torch::relu(batch_norm3(conv3(x))); x = torch::tanh(conv4(x)); return x; } nn::ConvTranspose2d conv1, conv2, conv3, conv4; nn::BatchNorm2d batch_norm1, batch_norm2, batch_norm3; }; TORCH_MODULE(DCGANGenerator); DCGANGenerator generator(kNoiseSize); \uc774\uc81c DCGANGenerator \uc758 forward() \ub97c \ud638\ucd9c\ud574 \ub178\uc774\uc988 \uc0d8\ud50c\uc744 \uc774\ubbf8\uc9c0\uc5d0 \ub9e4\ud551\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c \uc0ac\uc6a9\ud55c nn::ConvTranspose2d \ubc0f nn::BatchNorm2d \ub4f1\uc758 \ubaa8\ub4c8\uc740 \uc55e\uc11c \uc124\uba85\ud55c \uad6c\uc870\ub97c \ub530\ub985\ub2c8\ub2e4. \uc0c1\uc218 kNoiseSize \ub294 \uc785\ub825 \ub178\uc774\uc988 \ubca1\ud130\uc758 \ud06c\uae30\ub97c \uacb0\uc815\ud558\uba70 100 \uc73c\ub85c \uc124\uc815\ub429\ub2c8\ub2e4. \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub294 \ubb3c\ub860 \ub300\ud559\uc6d0\uc0dd\ub4e4\uc758 \ub9ce\uc740 \ub178\ub825\uc744 \ud1b5\ud574 \uc138\ud305\ub410\uc2b5\ub2c8\ub2e4. \uc8fc\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uc815\ud558\ub290\ub77c \ub2e4\uce5c \ub300\ud559\uc6d0\uc0dd\uc740 \uc5c6\uc5c8\uc2b5\ub2c8\ub2e4. \uadf8\ub4e4\uc740 \uc11c\ub85c\uc11c\ub85c \uac1c\uc0ac\ub8cc\ub97c \uba39\uc774\ub2c8\uae4c\uc694. \ucc38\uace0 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 Conv2d \uc640 \uac19\uc740 \uae30\ubcf8 \uc81c\uacf5 \ubaa8\ub4c8\uc5d0 \uc635\uc158\uc774 \uc804\ub2ec\ub418\ub294 \ubc29\ubc95\uc5d0 \ub300\ud55c \uac04\ub2e8\ud788 \uc124\uba85\ud558\uc790\uba74, \ubaa8\ub4e0 \ubaa8\ub4c8\uc740 \uba87 \uac00\uc9c0 \ud544\uc218 \uc635\uc158\uc744 \uac16\uace0 \uc788\uc2b5\ub2c8\ub2e4. (\uc608: BatchNorm2d \uc758 feature \uac1c\uc218) \ub9cc\uc57d BatchNorm2d(128), Dropout(0.5), Conv2d(8, 4, 2) \uc640 \uac19\uc774 \ud544\uc218 \uc635\uc158\ub9cc \uc124\uc815\ud558\ub824 \ud55c\ub2e4\uba74 \ubaa8\ub4c8 \uc0dd\uc131\uc790\uc5d0 \uc9c1\uc811 \uc804\ub2ec\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\uc5ec\uae30\uc11c\ub294 \uac01\uac01 \uc785\ub825 \ucc44\ub110 \uc218, \ucd9c\ub825 \ucc44\ub110 \uc218 \ubc0f \ucee4\ub110 \ud06c\uae30\ub97c \uc758\ubbf8) \uadf8\ub7ec\ub098 \ub9cc\uc57d Conv2d \uc758 bias \uc640 \uac19\uc774 \uc77c\ubc18\uc801\uc73c\ub85c \uae30\ubcf8\uac12\uc744 \uc0ac\uc6a9\ud558\ub294 \ub2e4\ub978 \uc635\uc158\uc744 \uc218\uc815\ud574\uc57c \ud558\ub294 \uacbd\uc6b0, options \uac1d\uccb4\ub97c \uc0dd\uc131\ud574 \uc804\ub2ec\ud574\uc57c \ud569\ub2c8\ub2e4. C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 \ubaa8\ub4c8\uc740 ModuleOptions \uc774\ub77c\uace0 \ud558\ub294 \uc5f0\uad00\ub41c \uc635\uc158 struct\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c Module \uc740 \ud574\ub2f9 \ubaa8\ub4c8\uc758 \uc774\ub984\uc73c\ub85c, \uc608\ub97c \ub4e4\uc5b4 Linear \uc758 \uacbd\uc6b0 LinearOptions \uc640 \uac19\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc704\uc758 Conv2d \ubaa8\ub4c8\uc5d0 \ub300\ud574 \uc774\ub97c \uc218\ud589\ud55c \uac83\uc785\ub2c8\ub2e4. \ud310\ubcc4\uae30(Discriminator) \ubaa8\ub4c8# \ud310\ubcc4\uae30\ub294 \ub9c8\ucc2c\uac00\uc9c0\ub85c \ud569\uc131\uacf1, \ubc30\uce58 \uc815\uaddc\ud654 \ubc0f \ud65c\uc131\ud654\uc758 \uc5f0\uc18d\uc785\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc774\ubc88\uc5d0 \ud569\uc131\uacf1\uc740 \uc804\uce58\ub418\uc9c0 \uc54a\uc740 \uae30\ubcf8 \ud569\uc131\uacf1\uc774\uba70, \uc77c\ubc18\uc801 ReLU \ub300\uc2e0\uc5d0 \uc54c\ud30c \uac12\uc774 0.2\uc778 leaky ReLU\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ub610\ud55c \ucd5c\uc885 \ud65c\uc131\ud654\ub294 \uac12\uc744 0\uacfc 1 \uc0ac\uc774\uc758 \ubc94\uc704\ub85c \uc555\ucd95\ud558\ub294 Sigmoid\uac00 \ub429\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c \uc774\ub807\uac8c \uc555\ucd95\ub41c \uac12\uc744 \ud310\ubcc4\uc790\uac00 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 \ucd9c\ub825\ud558\ub294 \ud655\ub960\ub85c \ud574\uc11d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud310\ubcc4\uae30\ub97c \ub9cc\ub4e4\uae30 \uc704\ud574 Sequential \ubaa8\ub4c8\uc774\ub77c\ub294 \ub2e4\ub978 \uac83\uc744 \uc2dc\ub3c4\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \ud30c\uc774\uc36c\uc5d0\uc11c\uc640 \uac19\uc774, PyTorch\ub294 \ubaa8\ub378 \uc815\uc758\ub97c \uc704\ud574 \ub450 \uac00\uc9c0 API\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. (\uc0dd\uc131\uae30 \ubaa8\ub4c8 \uc608\uc2dc\uc640 \uac19\uc774) \uc785\ub825\uc774 \uc5f0\uc18d\uc801\uc778 \ud568\uc218\ub97c \ud1b5\ud574 \uc804\ub2ec\ub418\ub294 \ud568\uc218\ud615 API\uc640 \uc804\uccb4 \ubaa8\ub378\uc744 \ud558\uc704 \ubaa8\ub4c8\ub85c \ud3ec\ud568\ud558\ub294 Sequential \ubaa8\ub4c8\uc744 \uc0dd\uc131\ud558\ub294 \uac1d\uccb4 \uc9c0\ud5a5\ud615 API\uc785\ub2c8\ub2e4. Sequential \uc744 \uc0ac\uc6a9\ud558\uba74 \ud310\ubcc4\uae30\ub294 \ub300\ub7b5 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. nn::Sequential discriminator( // Layer 1 nn::Conv2d( nn::Conv2dOptions(1, 64, 4).stride(2).padding(1).bias(false)), nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)), // Layer 2 nn::Conv2d( nn::Conv2dOptions(64, 128, 4).stride(2).padding(1).bias(false)), nn::BatchNorm2d(128), nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)), // Layer 3 nn::Conv2d( nn::Conv2dOptions(128, 256, 4).stride(2).padding(1).bias(false)), nn::BatchNorm2d(256), nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)), // Layer 4 nn::Conv2d( nn::Conv2dOptions(256, 1, 3).stride(1).padding(0).bias(false)), nn::Sigmoid()); \ud301 Sequential \ubaa8\ub4c8\uc740 \ub2e8\uc21c\ud55c \ud568\uc218 \ud569\uc131\ub9cc\uc744 \uc218\ud589\ud569\ub2c8\ub2e4. \uccab \ubc88\uc9f8 \ud558\uc704 \ubaa8\ub4c8\uc758 \ucd9c\ub825\uc740 \ub450 \ubc88\uc9f8 \ud558\uc704 \ubaa8\ub4c8\uc758 \uc785\ub825\uc774 \ub418\uace0 \uc138 \ubc88\uc9f8 \ud558\uc704 \ubaa8\ub4c8\uc758 \ucd9c\ub825\uc740 \ub124 \ubc88\uc9f8 \ud558\uc704 \ubaa8\ub4c8\uc758 \uc785\ub825\uc774 \ub418\uace0 \uc774\ud6c4\uc5d0\ub3c4 \ub9c8\ucc2c\uac00\uc9c0\uc785\ub2c8\ub2e4. \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30# \uc774\uc81c \uc0dd\uc131\uae30\uc640 \ud310\ubcc4\uae30 \ubaa8\ub378\uc744 \uc815\uc758\ud588\uc73c\ubbc0\ub85c \uc774\ub7ec\ud55c \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0ac \ub370\uc774\ud130\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. \ud30c\uc774\uc36c\uacfc \ub9c8\ucc2c\uac00\uc9c0\ub85c C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\ub294 \uac15\ub825\ud55c \ubcd1\ub82c \ub370\uc774\ud130 \ub85c\ub354(data loader)\ub97c \uc81c\uacf5\ud55c\ub2e4. \uc774 \ub370\uc774\ud130 \ub85c\ub354\ub294 \uc0ac\uc6a9\uc790\uac00 \uc9c1\uc811 \uc815\uc758\ud560 \uc218 \uc788\ub294 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub370\uc774\ud130 \ubc30\uce58\ub97c \uc77d\uc744 \uc218 \uc788\uc73c\uba70 \uc124\uc815\uc744 \uc704\ud55c \ub9ce\uc740 \uc635\uc158\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \ucc38\uace0 \ud30c\uc774\uc36c \ub370\uc774\ud130 \ub85c\ub354\uac00 \uba40\ud2f0 \ud504\ub85c\uc138\uc2f1\uc744 \uc0ac\uc6a9\ud558\ub294 \ubc18\uba74, C++ \ub370\uc774\ud130 \ub85c\ub354\ub294 \uc2e4\uc81c\ub85c \uba40\ud2f0 \uc2a4\ub808\ub529\uc744 \uc0ac\uc6a9\ud574 \uc5b4\ub5a0\ud55c \uc0c8\ub85c\uc6b4 \ud504\ub85c\uc138\uc2a4\ub3c4 \uc2dc\uc791\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ub370\uc774\ud130 \ub85c\ub354\ub294 torch::data:: \ub124\uc784\uc2a4\ud398\uc774\uc2a4\uc5d0 \ud3ec\ud568\ub41c C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 data API\uc758 \uc77c\ubd80\uc785\ub2c8\ub2e4. \uc774 API\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \uba87 \uac00\uc9c0 \ucef4\ud3ec\ub10c\ud2b8\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. \ub370\uc774\ud130 \ub85c\ub354 \ud074\ub798\uc2a4 \ub370\uc774\ud130\uc14b\uc744 \uc815\uc758\ud558\uae30 \uc704\ud55c API \ubcc0\ud658 \uc744 \uc815\uc758\ud558\uae30 \uc704\ud55c API (\ub370\uc774\ud130\uc14b\uc5d0 \uc801\uc6a9 \uac00\ub2a5) \uc0d8\ud50c\ub7ec \ub97c \uc815\uc758\ud558\uae30 \uc704\ud55c API (\ub370\uc774\ud130\uc14b\uc744 \uc704\ud55c \uc778\ub371\uc2a4\ub97c \uc0dd\uc131) \uae30\uc874 \ub370\uc774\ud130\uc14b, \ubcc0\ud658, \uc0d8\ud50c\ub7ec\ub4e4\uc758 \ub77c\uc774\ube0c\ub7ec\ub9ac \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc640 \ud568\uaed8 \uc81c\uacf5\ub418\ub294 MNIST \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. torch::data::datasets::MNIST \uc778\uc2a4\ud134\uc2a4\ub97c \ub9cc\ub4e4\uc5b4 \ub2e4\uc74c \ub450 \uac00\uc9c0 \ubcc0\ud658\uc744 \uc801\uc6a9\ud574\ubd05\uc2dc\ub2e4. \uccab\uc9f8, \uc774\ubbf8\uc9c0\ub97c \uc815\uaddc\ud654\ud558\uc5ec -1 \uacfc +1 \uc0ac\uc774\uc5d0 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. (\uae30\uc874 \ubc94\uc704\ub294 0 \uacfc 1 \uc0ac\uc774) \ub458\uc9f8, \ud150\uc11c \ubc30\uce58(batch)\ub97c \uccab \ubc88\uc9f8 \ucc28\uc6d0\uc744 \ub530\ub77c \ub2e8\uc77c \ud150\uc11c\ub85c \uc313\ub294 \uc774\ub978\ubc14 Stack collation \uc744 \uc801\uc6a9\ud569\ub2c8\ub2e4. auto dataset = torch::data::datasets::MNIST(\"./mnist\") .map(torch::data::transforms::Normalize\u003c\u003e(0.5, 0.5)) .map(torch::data::transforms::Stack\u003c\u003e()); MNIST \ub370\uc774\ud130\uc14b\uc740 \ud559\uc2b5 \ubc14\uc774\ub108\ub9ac \uc2e4\ud589 \uc704\uce58\ub97c \uae30\uc900\uc73c\ub85c ./mnist \ub514\ub809\ud1a0\ub9ac\uc5d0 \uc704\uce58\ud574\uc57c \ud569\ub2c8\ub2e4. MNIST \ub370\uc774\ud130\uc14b\uc740 \uc774 \uc2a4\ud06c\ub9bd\ud2b8 \ub97c \uc0ac\uc6a9\ud574 \ub2e4\uc6b4\ub85c\ub4dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c\uc73c\ub85c, \ub370\uc774\ud130 \ub85c\ub354\ub97c \ub9cc\ub4e4\uace0 \uc774 \ub370\uc774\ud130\uc14b\uc744 \uc804\ub2ec\ud569\ub2c8\ub2e4. \uc0c8\ub85c\uc6b4 \ub370\uc774\ud130 \ub85c\ub354\ub97c \ub9cc\ub4e4\uae30 \uc704\ud574 torch::data::make_data_loader \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774 \ub85c\ub354\ub294 \uc62c\ubc14\ub978 \ud0c0\uc785(\ub370\uc774\ud130\uc14b \ud0c0\uc785, \uc0d8\ud50c\ub7ec \ud0c0\uc785 \ubc0f \uae30\ud0c0 \uad6c\ud604 \uc138\ubd80\uc0ac\ud56d\uc5d0 \ub530\ub77c \uacb0\uc815\ub428)\uc758 std::unique_ptr \ub97c \ubc18\ud658\ud569\ub2c8\ub2e4. auto data_loader = torch::data::make_data_loader(std::move(dataset)); \ub370\uc774\ud130 \ub85c\ub354\uc5d0\ub294 \ub9ce\uc740 \uc635\uc158\uc774 \uc81c\uacf5\ub429\ub2c8\ub2e4. \uc804\uccb4 \ubaa9\ub85d\uc740 \uc5ec\uae30 \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \ub370\uc774\ud130 \ub85c\ub529 \uc18d\ub3c4\ub97c \ub192\uc774\uae30 \uc704\ud574 \uc791\uc5c5\uc790 \uc218\ub97c \ub298\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uae30\ubcf8\uac12\uc740 0\uc774\uba70, \uc774\ub294 \uc8fc \uc4f0\ub808\ub4dc\uac00 \uc0ac\uc6a9\ub428\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. workers \ub97c 2 \ub85c \uc124\uc815\ud558\uba74 \ub370\uc774\ud130\ub97c \ub3d9\uc2dc\uc5d0 \ub85c\ub4dc\ud558\ub294 \uc4f0\ub808\ub4dc\uac00 \ub450 \uac1c \uc0dd\uc131\ub429\ub2c8\ub2e4. \ub610\ud55c \ubc30\uce58 \ud06c\uae30\ub97c \uae30\ubcf8\uac12 1 \uc5d0\uc11c 64 (kBatchSize \uac12) \uc640 \uac19\uc774 \ub354 \uc801\ub2f9\ud55c \uac12\uc73c\ub85c \ub298\ub824\uc57c \ud569\ub2c8\ub2e4. \uadf8\ub7ec\uba74 DataLoaderOptions \uac1d\uccb4\ub97c \ub9cc\ub4e4\uc5b4 \uc801\uc808\ud55c \uc18d\uc131\uc744 \uc124\uc815\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. auto data_loader = torch::data::make_data_loader( std::move(dataset), torch::data::DataLoaderOptions().batch_size(kBatchSize).workers(2)); \uc774\uc81c \ub370\uc774\ud130 \ubc30\uce58\ub97c \ub85c\ub4dc\ud558\ub294 \ub8e8\ud504\ub97c \uc791\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc9c0\uae08\uc740 \ucf58\uc194\uc5d0\ub9cc \ucd9c\ub825\ud560 \uac83\uc785\ub2c8\ub2e4. for (torch::data::Example\u003c\u003e\u0026 batch : *data_loader) { std::cout \u003c\u003c \"Batch size: \" \u003c\u003c batch.data.size(0) \u003c\u003c \" | Labels: \"; for (int64_t i = 0; i \u003c batch.data.size(0); ++i) { std::cout \u003c\u003c batch.target[i].item\u003cint64_t\u003e() \u003c\u003c \" \"; } std::cout \u003c\u003c std::endl; } \uc774 \uacbd\uc6b0 \ub370\uc774\ud130 \ub85c\ub354\uac00 \ubc18\ud658\ud558\ub294 \ud0c0\uc785\uc740 torch::data::Example \uc785\ub2c8\ub2e4. \uc774 \ud0c0\uc785\uc740 \ub370\uc774\ud130\ub97c \uc704\ud55c data \ud544\ub4dc\uc640 \ub808\uc774\ube14\uc744 \uc704\ud55c target \ud544\ub4dc\uac00 \uc788\ub294 \uac04\ub2e8\ud55c struct\uc785\ub2c8\ub2e4. \uc55e\uc11c Stack collation\uc744 \uc801\uc6a9\ud588\uae30 \ub54c\ubb38\uc5d0, \ub370\uc774\ud130 \ub85c\ub354\ub294 \uc774 example\uc744 \ud558\ub098\ub9cc \ubc18\ud658\ud569\ub2c8\ub2e4. \ub370\uc774\ud130 \ub85c\ub354\uc5d0 collation\uc744 \uc801\uc6a9\ud558\uc9c0 \uc54a\uc73c\uba74, std::vector\u003ctorch::data::Example\u003c\u003e\u003e \ub97c yield\ud558\uba70, \uac01 \ubc30\uce58\uc758 example\uc5d0\ub294 \ud558\ub098\uc758 element\uac00 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4. \uc774 \ucf54\ub4dc\ub97c \ub2e4\uc2dc \ube4c\ub4dc\ud558\uace0 \uc2e4\ud589\ud558\uba74 \ub300\ub7b5 \ub2e4\uc74c\uacfc \uac19\uc740 \ub0b4\uc6a9\uc744 \uc5bb\uc744 \uac83\uc785\ub2c8\ub2e4. root@fa350df05ecf:/home/build# make Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan root@fa350df05ecf:/home/build# make [100%] Built target dcgan root@fa350df05ecf:/home/build# ./dcgan Batch size: 64 | Labels: 5 2 6 7 2 1 6 7 0 1 6 2 3 6 9 1 8 4 0 6 5 3 3 0 4 6 6 6 4 0 8 6 0 6 9 2 4 0 2 8 6 3 3 2 9 2 0 1 4 2 3 4 8 2 9 9 3 5 8 0 0 7 9 9 Batch size: 64 | Labels: 2 2 4 7 1 2 8 8 6 9 0 2 2 9 3 6 1 3 8 0 4 4 8 8 8 9 2 6 4 7 1 5 0 9 7 5 4 3 5 4 1 2 8 0 7 1 9 6 1 6 5 3 4 4 1 2 3 2 3 5 0 1 6 2 Batch size: 64 | Labels: 4 5 4 2 1 4 8 3 8 3 6 1 5 4 3 6 2 2 5 1 3 1 5 0 8 2 1 5 3 2 4 4 5 9 7 2 8 9 2 0 6 7 4 3 8 3 5 8 8 3 0 5 8 0 8 7 8 5 5 6 1 7 8 0 Batch size: 64 | Labels: 3 3 7 1 4 1 6 1 0 3 6 4 0 2 5 4 0 4 2 8 1 9 6 5 1 6 3 2 8 9 2 3 8 7 4 5 9 6 0 8 3 0 0 6 4 8 2 5 4 1 8 3 7 8 0 0 8 9 6 7 2 1 4 7 Batch size: 64 | Labels: 3 0 5 5 9 8 3 9 8 9 5 9 5 0 4 1 2 7 7 2 0 0 5 4 8 7 7 6 1 0 7 9 3 0 6 3 2 6 2 7 6 3 3 4 0 5 8 8 9 1 9 2 1 9 4 4 9 2 4 6 2 9 4 0 Batch size: 64 | Labels: 9 6 7 5 3 5 9 0 8 6 6 7 8 2 1 9 8 8 1 1 8 2 0 7 1 4 1 6 7 5 1 7 7 4 0 3 2 9 0 6 6 3 4 4 8 1 2 8 6 9 2 0 3 1 2 8 5 6 4 8 5 8 6 2 Batch size: 64 | Labels: 9 3 0 3 6 5 1 8 6 0 1 9 9 1 6 1 7 7 4 4 4 7 8 8 6 7 8 2 6 0 4 6 8 2 5 3 9 8 4 0 9 9 3 7 0 5 8 2 4 5 6 2 8 2 5 3 7 1 9 1 8 2 2 7 Batch size: 64 | Labels: 9 1 9 2 7 2 6 0 8 6 8 7 7 4 8 6 1 1 6 8 5 7 9 1 3 2 0 5 1 7 3 1 6 1 0 8 6 0 8 1 0 5 4 9 3 8 5 8 4 8 0 1 2 6 2 4 2 7 7 3 7 4 5 3 Batch size: 64 | Labels: 8 8 3 1 8 6 4 2 9 5 8 0 2 8 6 6 7 0 9 8 3 8 7 1 6 6 2 7 7 4 5 5 2 1 7 9 5 4 9 1 0 3 1 9 3 9 8 8 5 3 7 5 3 6 8 9 4 2 0 1 2 5 4 7 Batch size: 64 | Labels: 9 2 7 0 8 4 4 2 7 5 0 0 6 2 0 5 9 5 9 8 8 9 3 5 7 5 4 7 3 0 5 7 6 5 7 1 6 2 8 7 6 3 2 6 5 6 1 2 7 7 0 0 5 9 0 0 9 1 7 8 3 2 9 4 Batch size: 64 | Labels: 7 6 5 7 7 5 2 2 4 9 9 4 8 7 4 8 9 4 5 7 1 2 6 9 8 5 1 2 3 6 7 8 1 1 3 9 8 7 9 5 0 8 5 1 8 7 2 6 5 1 2 0 9 7 4 0 9 0 4 6 0 0 8 6 ... \uc989, MNIST \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub370\uc774\ud130\ub97c \uc131\uacf5\uc801\uc73c\ub85c \ub85c\ub4dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud559\uc2b5 \ub8e8\ud504 \uc791\uc131\ud558\uae30# \uc774\uc81c \uc608\uc81c\uc758 \uc54c\uace0\ub9ac\uc998 \ubd80\ubd84\uc744 \ub9c8\ubb34\ub9ac\ud558\uace0 \uc0dd\uc131\uae30\uc640 \ud310\ubcc4\uae30 \uc0ac\uc774\uc5d0\uc11c \uc77c\uc5b4\ub098\ub294 \uc12c\uc138\ud55c \uc791\uc6a9\uc744 \uad6c\ud604\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uba3c\uc800 \uc0dd\uc131\uae30\uc640 \ud310\ubcc4\uae30 \uac01\uac01\uc744 \uc704\ud574 \ucd1d \ub450 \uac1c\uc758 optimizer\ub97c \uc0dd\uc131\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud558\ub294 optimizer\ub294 Adam \uc54c\uace0\ub9ac\uc998\uc744 \uad6c\ud604\ud569\ub2c8\ub2e4. torch::optim::Adam generator_optimizer( generator-\u003eparameters(), torch::optim::AdamOptions(2e-4).betas(std::make_tuple(0.5, 0.5))); torch::optim::Adam discriminator_optimizer( discriminator-\u003eparameters(), torch::optim::AdamOptions(5e-4).betas(std::make_tuple(0.5, 0.5))); \ucc38\uace0 \uc774 \uae00 \uc791\uc131 \ub2f9\uc2dc, C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uac00 Adagrad, Adam, LBFGS, RMSprop \ubc0f SGD\ub97c \uad6c\ud604\ud558\ub294 \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \ucd5c\uc2e0 \ub9ac\uc2a4\ud2b8\ub294 docs \uc5d0 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c\uc73c\ub85c, \uc6b0\ub9ac\uc758 \ud559\uc2b5 \ub8e8\ud504\ub97c \uc218\uc815\ud574\uc57c \ud569\ub2c8\ub2e4. \ub9e4 \uc5d0\ud3ed\ub9c8\ub2e4 \ub370\uc774\ud130 \ub85c\ub354\ub97c \ubc18\ubcf5 \uc2e4\ud589\ud558\ub294 \ubc14\uae65 \ub8e8\ud504\ub97c \ucd94\uac00\ud574 \ub2e4\uc74c\uc758 GAN \ud559\uc2b5 \ucf54\ub4dc\ub97c \uc791\uc131\ud569\ub2c8\ub2e4. for (int64_t epoch = 1; epoch \u003c= kNumberOfEpochs; ++epoch) { int64_t batch_index = 0; for (torch::data::Example\u003c\u003e\u0026 batch : *data_loader) { // Train discriminator with real images. discriminator-\u003ezero_grad(); torch::Tensor real_images = batch.data; torch::Tensor real_labels = torch::empty(batch.data.size(0)).uniform_(0.8, 1.0); torch::Tensor real_output = discriminator-\u003eforward(real_images).reshape(real_labels.sizes()); torch::Tensor d_loss_real = torch::binary_cross_entropy(real_output, real_labels); d_loss_real.backward(); // Train discriminator with fake images. torch::Tensor noise = torch::randn({batch.data.size(0), kNoiseSize, 1, 1}); torch::Tensor fake_images = generator-\u003eforward(noise); torch::Tensor fake_labels = torch::zeros(batch.data.size(0)); torch::Tensor fake_output = discriminator-\u003eforward(fake_images.detach()).reshape(fake_labels.sizes()); torch::Tensor d_loss_fake = torch::binary_cross_entropy(fake_output, fake_labels); d_loss_fake.backward(); torch::Tensor d_loss = d_loss_real + d_loss_fake; discriminator_optimizer.step(); // Train generator. generator-\u003ezero_grad(); fake_labels.fill_(1); fake_output = discriminator-\u003eforward(fake_images).reshape(fake_labels.sizes()); torch::Tensor g_loss = torch::binary_cross_entropy(fake_output, fake_labels); g_loss.backward(); generator_optimizer.step(); std::printf( \"\\r[%2ld/%2ld][%3ld/%3ld] D_loss: %.4f | G_loss: %.4f\", epoch, kNumberOfEpochs, ++batch_index, batches_per_epoch, d_loss.item\u003cfloat\u003e(), g_loss.item\u003cfloat\u003e()); } } \uc704 \ucf54\ub4dc\ub294 \uba3c\uc800 \uc9c4\uc9dc (real) \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 \ud310\ubcc4\uae30\ub97c \ud3c9\uac00\ud558\ub294\ub370, \uc774 \ub54c \ud310\ubcc4\uae30\ub294 \ub192\uc740 \ud655\ub960\uc744 \ucd9c\ub825\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574 torch::empty(batch.data.size(0)).uniform_(0.8, 1.0) \ub97c \ubaa9\ud45c \ud655\ub960 \uac12\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ucc38\uace0 \ud310\ubcc4\uae30\ub97c \ubcf4\ub2e4 \uacac\uace0\ud558\uac8c \ud559\uc2b5\ud558\uae30 \uc704\ud574 \ubaa8\ub4e0 \uacf3\uc5d0\uc11c 1.0\uc774 \uc544\ub2cc 0.8\uacfc 1.0 \uc0ac\uc774\uc758 \uade0\uc77c \ubd84\ud3ec\uc5d0\uc11c \uc784\uc758\uc758 \uac12\uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4. \uc774 \ud2b8\ub9ad\uc744 label smoothing \uc774\ub77c\uace0 \ud569\ub2c8\ub2e4. \ud310\ubcc4\uae30\ub97c \ud3c9\uac00\ud558\uae30\uc5d0 \uc55e\uc11c \ub9e4\uac1c\ubcc0\uc218\uc758 \uadf8\ub798\ub514\uc5b8\ud2b8\ub97c 0\uc73c\ub85c \ub9cc\ub4ed\ub2c8\ub2e4. \uc190\uc2e4\uc744 \uacc4\uc0b0\ud55c \ud6c4 d_loss.backward() \ub97c \ud638\ucd9c\ud574 \uc774\ub97c \ub124\ud2b8\uc6cc\ud06c\uc5d0 \uc5ed\uc804\ud30c\ud569\ub2c8\ub2e4. \uac00\uc9dc (fake) \uc774\ubbf8\uc9c0\ub4e4\uc5d0 \ub300\ud574\uc11c \uc774 \uacfc\uc815\uc744 \ubc18\ubcf5\ud569\ub2c8\ub2e4. \ub370\uc774\ud130\uc14b\uc758 \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud558\ub294 \ub300\uc2e0, \uc0dd\uc131\uc790\uc5d0 \ubb34\uc791\uc704 \ub178\uc774\uc988\ub97c \uc785\ub825\ud558\uc5ec \uc5ec\uae30\uc11c \uc0ac\uc6a9\ud560 \uac00\uc9dc \uc774\ubbf8\uc9c0\ub97c \ub9cc\ub4ed\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uadf8 \uac00\uc9dc \uc774\ubbf8\uc9c0\ub4e4\uc744 \ud310\ubcc4\uae30\uc5d0 \uc804\ub2ec\ud569\ub2c8\ub2e4. \uc774\ubc88\uc5d0\ub294 \ud310\ubcc4\uae30\uac00 \ub0ae\uc740 \ud655\ub960, \uc774\uc0c1\uc801\uc73c\ub85c\ub294 \ubaa8\ub450 0\uc744 \ucd9c\ub825\ud558\uae30\ub97c \ubc14\ub78d\ub2c8\ub2e4. \uc9c4\uc9dc \uc774\ubbf8\uc9c0\uc640 \uac00\uc9dc \uc774\ubbf8\uc9c0 \ubc30\uce58 \ubaa8\ub450\uc5d0 \ub300\ud55c \ud310\ubcc4\uae30 \uc190\uc2e4\uc744 \uacc4\uc0b0\ud55c \ud6c4\uc5d0\ub294, \ud310\ubcc4\uae30\uc758 optimizer \ub9e4\uac1c\ubcc0\uc218 \uc5c5\ub370\uc774\ud2b8\ub97c \ud55c \ub2e8\uacc4\uc529 \uc9c4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc0dd\uc131\uae30\ub97c \ud559\uc2b5\uc2dc\ud0a4\uae30 \uc704\ud574 \uc6b0\uc120 \uadf8\ub798\ub514\uc5b8\ud2b8\ub97c \ub2e4\uc2dc \ud55c\ubc88 0\uc73c\ub85c \uc124\uc815\ud558\uace0 \ub2e4\uc2dc \uac00\uc9dc \uc774\ubbf8\uc9c0\ub85c \ud310\ubcc4\uae30\ub97c \ud3c9\uac00\ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc774\ubc88\uc5d0\ub294 \ud310\ubcc4\uae30\uac00 \ud655\ub960 1\uc5d0 \ub9e4\uc6b0 \uadfc\uc811\ud558\uac8c \ucd9c\ub825\ud558\uac8c \ud558\uc5ec, \uc0dd\uc131\uae30\uac00 \ud310\ubcc4\uae30\ub97c \uc18d\uc5ec \uc2e4\uc81c (\ub370\uc774\ud130\uc14b\uc5d0 \uc788\ub294) \uc9c4\uc9dc\ub77c\uace0 \uc0dd\uac01\ud558\ub294 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\ub3c4\ub85d \ud558\ub824 \ud569\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574 fake_labels \ud150\uc11c\ub97c \ubaa8\ub450 1\ub85c \ucc44\uc6b0\uaca0\uc2b5\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c \ub9e4\uac1c\ubcc0\uc218\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\uae30 \uc704\ud574 \uc0dd\uc131\uae30\uc758 optimzier \ub9e4\uac1c\ubcc0\uc218 \uc5c5\ub370\uc774\ud2b8\ub97c \uc9c4\ud589\ud569\ub2c8\ub2e4. \uc774\uc81c CPU\ub85c \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0ac \uc900\ube44\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc0c1\ud0dc\ub098 \uc0d8\ud50c \ucd9c\ub825\uc744 \ucea1\ucc98\ud560 \uc218 \uc788\ub294 \ucf54\ub4dc\ub294 \uc544\uc9c1 \uc5c6\uc9c0\ub9cc \uc7a0\uc2dc \ud6c4\uc5d0 \ucd94\uac00\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc9c0\uae08\uc740 \ubaa8\ub378\uc774 \ubb34\uc5b8\uac00 \ub97c \uc218\ud589\ud558\uace0 \uc788\ub2e4\ub294 \uac83\ub9cc\uc744 \uad00\ucc30\ud558\uace0, \ub098\uc911\uc5d0\ub294 \uc0dd\uc131\ub41c \uc774\ubbf8\uc9c0\ub97c \uae30\ubc18\uc73c\ub85c \uc774 \ubb34\uc5b8\uac00\uac00 \uc758\ubbf8 \uc788\ub294\uc9c0 \uc5ec\ubd80\ub97c \ud655\uc778\ud560 \uac83\uc785\ub2c8\ub2e4. \ub2e4\uc2dc \ube4c\ub4dc\ud558\uace0 \uc2e4\ud589\ud558\uba74 \ub2e4\uc74c\uacfc \uac19\uc740 \ub0b4\uc6a9\uc774 \ucd9c\ub825\ub3fc\uc57c \ud569\ub2c8\ub2e4. root@3c0711f20896:/home/build# make \u0026\u0026 ./dcgan Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcga [ 1/10][100/938] D_loss: 0.6876 | G_loss: 4.1304 [ 1/10][200/938] D_loss: 0.3776 | G_loss: 4.3101 [ 1/10][300/938] D_loss: 0.3652 | G_loss: 4.6626 [ 1/10][400/938] D_loss: 0.8057 | G_loss: 2.2795 [ 1/10][500/938] D_loss: 0.3531 | G_loss: 4.4452 [ 1/10][600/938] D_loss: 0.3501 | G_loss: 5.0811 [ 1/10][700/938] D_loss: 0.3581 | G_loss: 4.5623 [ 1/10][800/938] D_loss: 0.6423 | G_loss: 1.7385 [ 1/10][900/938] D_loss: 0.3592 | G_loss: 4.7333 [ 2/10][100/938] D_loss: 0.4660 | G_loss: 2.5242 [ 2/10][200/938] D_loss: 0.6364 | G_loss: 2.0886 [ 2/10][300/938] D_loss: 0.3717 | G_loss: 3.8103 [ 2/10][400/938] D_loss: 1.0201 | G_loss: 1.3544 [ 2/10][500/938] D_loss: 0.4522 | G_loss: 2.6545 ... GPU\ub85c \uc774\ub3d9\ud558\uae30# \uc774 \uc2a4\ud06c\ub9bd\ud2b8\ub294 CPU\uc5d0\uc11c \uc798 \ub3d9\uc791\ud558\uc9c0\ub9cc, \ud569\uc131\uacf1 \uc5f0\uc0b0\uc774 GPU\uc5d0\uc11c \ud6e8\uc52c \ube60\ub974\ub2e4\ub294 \uac83\uc740 \uc798 \uc54c\ub824\uc9c4 \uc0ac\uc2e4\uc785\ub2c8\ub2e4. \uc5b4\ub5bb\uac8c \ud559\uc2b5\uc744 GPU\ub85c \uc62e\uae38 \uc218 \uc788\uc744 \uc9c0\uc5d0 \ub300\ud574 \ube60\ub974\uac8c \ub17c\uc758\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574 \ud574\uc57c \ud560 \uc77c \ub450 \uac00\uc9c0\ub85c GPU \uc7a5\uce58(device) \uc0ac\uc591\uc744 \uc6b0\ub9ac\uac00 \uc9c1\uc811 \ud560\ub2f9\ud55c \ud150\uc11c\uc5d0 \uc804\ub2ec\ud558\ub294 \uac83\uacfc, C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 \ubaa8\ub4e0 \ud150\uc11c\uc640 \ubaa8\ub4c8\uc774 \uac16\uace0 \uc788\ub294 to() \uba54\uc11c\ub4dc\ub97c \uc0ac\uc6a9\ud574 \ub2e4\ub978 \ubaa8\ub4e0 \ud150\uc11c\ub97c GPU\uc5d0 \uba85\uc2dc\uc801\uc73c\ub85c \ubcf5\uc0ac\ud558\ub294 \uac83\uc774 \uc788\uc2b5\ub2c8\ub2e4. \ub450 \uac00\uc9c0\ub97c \ubaa8\ub450 \ub2ec\uc131\ud558\ub294 \uac00\uc7a5 \uac04\ub2e8\ud55c \ubc29\ubc95\uc73c\ub85c \ud559\uc2b5 \uc2a4\ud06c\ub9bd\ud2b8 \ucd5c\uc0c1\uc704\uc5d0 torch::Device \uc778\uc2a4\ud134\uc2a4\ub97c \ub9cc\ub4e4\uc5b4 torch::zeros \uc640 \uac19\uc740 \ud150\uc11c \ud329\ud1a0\ub9ac \ud568\uc218\ub098 to() \uba54\uc11c\ub4dc\uc5d0 \uc804\ub2ec\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uba3c\uc800 CPU device\ub85c \uc774\ub97c \uad6c\ud604\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. // \ud559\uc2b5 \uc2a4\ud06c\ub9bd\ud2b8 \ucd5c\uc0c1\ub2e8\uc5d0 \uc774 \ucf54\ub4dc\ub97c \ub123\uc73c\uc138\uc694. torch::Device device(torch::kCPU); \uc544\ub798\uc640 \uac19\uc740 \uc0c8\ub85c\uc6b4 \ud150\uc11c \ud560\ub2f9\uc758 \uacbd\uc6b0, torch::Tensor fake_labels = torch::zeros(batch.data.size(0)); \ub9c8\uc9c0\ub9c9 \uc778\uc790\ub85c device \ub97c \ubc1b\ub3c4\ub85d \uc218\uc815\ud569\ub2c8\ub2e4. torch::Tensor fake_labels = torch::zeros(batch.data.size(0), device); MNIST \ub370\uc774\ud130\uc14b\uc758 \ud150\uc11c\ucc98\ub7fc \uc6b0\ub9ac\uac00 \uc9c1\uc811 \uc0dd\uc131\ud558\uc9c0 \uc54a\ub294 \ud150\uc11c\uc5d0\uc11c\ub294 \uba85\uc2dc\uc801\uc73c\ub85c to() \ud638\ucd9c\uc744 \uc0bd\uc785\ud574\uc57c \ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc544\ub798 \ucf54\ub4dc\uc758 \uacbd\uc6b0, torch::Tensor real_images = batch.data; \ub2e4\uc74c\uacfc \uac19\uc774 \ubcc0\ud569\ub2c8\ub2e4. torch::Tensor real_images = batch.data.to(device); \ub610\ud55c, \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc62c\ubc14\ub978 \uc7a5\uce58\ub85c \uc62e\uaca8\uc57c \ud569\ub2c8\ub2e4. generator-\u003eto(device); discriminator-\u003eto(device); \ucc38\uace0 \ub9cc\uc77c \ud150\uc11c\uac00 \uc774\ubbf8 to() \uc5d0 \uc804\ub2ec\ub41c \uc7a5\uce58 \uc0c1\uc5d0 \uc788\ub2e4\uba74 \uadf8 \ud638\ucd9c\uc740 \uc544\ubb34 \uc77c\ub3c4 \ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc0ac\ubcf8\uc774 \uc0dd\uc131\ub418\uc9c0\ub3c4 \uc54a\uc2b5\ub2c8\ub2e4. \uc774\uc81c CPU\uc5d0\uc11c \uc2e4\ud589\ub418\ub294 \uc774\uc804\uc758 \ucf54\ub4dc\uac00 \ubcf4\ub2e4 \uba85\uc2dc\uc801\uc73c\ub85c \ubc14\ub00c\uc5c8\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc774\uc81c\ub294 \uc7a5\uce58\ub97c CUDA \uc7a5\uce58\ub85c \ubcc0\uacbd\ud558\ub294 \uac83 \ub610\ud55c \ub9e4\uc6b0 \uc27d\uc2b5\ub2c8\ub2e4. torch::Device device(torch::kCUDA) \uc774\uc81c \ubaa8\ub4e0 \ud150\uc11c\uac00 GPU\uc5d0 \uc874\uc7ac\ud558\uba70 \uc5b4\ub5a0\ud55c \ub2e4\uc6b4\uc2a4\ud2b8\ub9bc \ucf54\ub4dc \ubcc0\uacbd \uc5c6\uc774\ub3c4 \ubaa8\ub4e0 \uc5f0\uc0b0\uc744 \uc704\ud574 \ube60\ub978 CUDA \ucee4\ub110\uc744 \ud638\ucd9c\ud569\ub2c8\ub2e4. \ud2b9\uc815 \uc778\ub371\uc2a4\uc758 \uc7a5\uce58\ub97c \uc9c0\uc815\ud558\ub824\uba74 Device \uc0dd\uc131\uc790\uc758 \ub450 \ubc88\uc9f8 \uc778\uc790\ub85c \uc804\ub2ec\ud558\uba74 \ub429\ub2c8\ub2e4. \uc11c\ub85c \ub2e4\ub978 \uc7a5\uce58\uc5d0 \uc11c\ub85c \ub2e4\ub978 \ud150\uc11c\uac00 \uc874\uc7ac\ud558\uae30\ub97c \uc6d0\ud558\ub294 \uacbd\uc6b0, \ubcc4\ub3c4\uc758 \uc7a5\uce58 \uc778\uc2a4\ud134\uc2a4(\uc608: CUDA \uc7a5\uce58 0\uacfc \ub2e4\ub978 CUDA \uc7a5\uce58 1)\ub97c \uc804\ub2ec\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ubfd0\ub9cc \uc544\ub2c8\ub77c, \uc774\ub7ec\ud55c \uc124\uc815\uc744 \ub3d9\uc801\uc73c\ub85c \uc218\ud589\ud560 \uc218\ub3c4 \uc788\uc5b4 \ub2e4\uc74c\uacfc \uac19\uc774 \ud559\uc2b5 \uc2a4\ud06c\ub9bd\ud2b8\uc758 \ud734\ub300\uc131\uc744 \ub192\uc774\ub294 \ub370 \uc885\uc885 \uc720\uc6a9\ud558\uac8c \uc0ac\uc6a9\ub429\ub2c8\ub2e4. torch::Device device = torch::kCPU; if (torch::cuda::is_available()) { std::cout \u003c\u003c \"CUDA is available! Training on GPU.\" \u003c\u003c std::endl; device = torch::kCUDA; } \ub098\uc544\uac00 \uc544\ub798\uc640 \uac19\uc740 \ucf54\ub4dc\ub3c4 \uac00\ub2a5\ud569\ub2c8\ub2e4. torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU); \ud559\uc2b5 \uc0c1\ud0dc \uc800\uc7a5 \ubc0f \ubcf5\uc6d0\ud558\uae30# \ub9c8\uc9c0\ub9c9\uc73c\ub85c \ud559\uc2b5 \uc2a4\ud06c\ub9bd\ud2b8\uc5d0 \ucd94\uac00\ud574\uc57c \ud560 \ub0b4\uc6a9\uc740 \ubaa8\ub378 \ub9e4\uac1c\ubcc0\uc218 \ubc0f \uc635\ud2f0\ub9c8\uc774\uc800\uc758 \uc0c1\ud0dc, \uadf8\ub9ac\uace0 \uc0dd\uc131\ub41c \uba87 \uac1c\uc758 \uc774\ubbf8\uc9c0 \uc0d8\ud50c\uc744 \uc8fc\uae30\uc801\uc73c\ub85c \uc800\uc7a5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \ud559\uc2b5 \uacfc\uc815 \ub3c4\uc911\uc5d0 \ucef4\ud4e8\ud130\uac00 \ub2e4\uc6b4\ub418\uba74 \uc774\ub807\uac8c \uc800\uc7a5\ub41c \uc0c1\ud0dc\ub85c\ubd80\ud130 \ud559\uc2b5 \uc0c1\ud0dc\ub97c \ubcf5\uc6d0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc7a5\uc2dc\uac04 \uc9c0\uc18d\ub418\ub294 \ud559\uc2b5\uc744 \uc704\ud574 \ud544\uc218\ub85c \uc694\uad6c\ub429\ub2c8\ub2e4. \ub2e4\ud589\ud788\ub3c4 C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\ub294 \uac1c\ubcc4 \ud150\uc11c\ubfd0\ub9cc \uc544\ub2c8\ub77c \ubaa8\ub378 \ubc0f \uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc\ub97c \uc9c1\ub82c\ud654\ud558\uace0 \uc5ed\uc9c1\ub82c\ud654\ud560 \uc218 \uc788\ub294 API\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774\ub97c \uc704\ud55c \ud575\uc2ec API\ub294 torch::save(thing,filename) \uc640 torch::load(thing,filename) \ub85c, \uc5ec\uae30\uc11c thing \uc740 torch::nn::Module \uc758 \ud558\uc704 \ud074\ub798\uc2a4 \ud639\uc740 \uc6b0\ub9ac\uc758 \ud559\uc2b5 \uc2a4\ud06c\ub9bd\ud2b8\uc758 Adam \uac1d\uccb4\uc640 \uac19\uc740 \uc635\ud2f0\ub9c8\uc774\uc800 \uc778\uc2a4\ud134\uc2a4\uac00 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubaa8\ub378 \ubc0f \uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc\ub97c \ud2b9\uc815 \uc8fc\uae30\ub9c8\ub2e4 \uc800\uc7a5\ud558\ub3c4\ub85d \ud559\uc2b5 \ub8e8\ud504\ub97c \uc218\uc815\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. if (batch_index % kCheckpointEvery == 0) { // \ubaa8\ub378 \ubc0f \uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4. torch::save(generator, \"generator-checkpoint.pt\"); torch::save(generator_optimizer, \"generator-optimizer-checkpoint.pt\"); torch::save(discriminator, \"discriminator-checkpoint.pt\"); torch::save(discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\"); // \uc0dd\uc131\uae30\ub97c \uc0d8\ud50c\ub9c1\ud558\uace0 \uc774\ubbf8\uc9c0\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4. torch::Tensor samples = generator-\u003eforward(torch::randn({8, kNoiseSize, 1, 1}, device)); torch::save((samples + 1.0) / 2.0, torch::str(\"dcgan-sample-\", checkpoint_counter, \".pt\")); std::cout \u003c\u003c \"\\n-\u003e checkpoint \" \u003c\u003c ++checkpoint_counter \u003c\u003c \u0027\\n\u0027; } \uc5ec\uae30\uc11c 100 \ubc30\uce58\ub9c8\ub2e4 \uc0c1\ud0dc\ub97c \uc800\uc7a5\ud558\ub824\uba74 kCheckpointEvery \ub97c 100 \uacfc \uac19\uc740 \uc815\uc218\ub85c \uc124\uc815\ud560 \uc218 \uc788\uc73c\uba70, checkpoint_counter \ub294 \uc0c1\ud0dc\ub97c \uc800\uc7a5\ud560 \ub54c\ub9c8\ub2e4 \uc99d\uac00\ud558\ub294 \uce74\uc6b4\ud130\uc785\ub2c8\ub2e4. \ud559\uc2b5 \uc0c1\ud0dc\ub97c \ubcf5\uc6d0\ud558\uae30 \uc704\ud574 \ubaa8\ub378 \ubc0f \uc635\ud2f0\ub9c8\uc774\uc800\ub97c \ubaa8\ub450 \uc0dd\uc131\ud55c \ud6c4 \ud559\uc2b5 \ub8e8\ud504 \uc55e\uc5d0 \ub2e4\uc74c \ucf54\ub4dc\ub97c \ucd94\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. torch::optim::Adam generator_optimizer( generator-\u003eparameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); torch::optim::Adam discriminator_optimizer( discriminator-\u003eparameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); if (kRestoreFromCheckpoint) { torch::load(generator, \"generator-checkpoint.pt\"); torch::load(generator_optimizer, \"generator-optimizer-checkpoint.pt\"); torch::load(discriminator, \"discriminator-checkpoint.pt\"); torch::load( discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\"); } int64_t checkpoint_counter = 0; for (int64_t epoch = 1; epoch \u003c= kNumberOfEpochs; ++epoch) { int64_t batch_index = 0; for (torch::data::Example\u003c\u003e\u0026 batch : *data_loader) { \uc0dd\uc131\ud55c \uc774\ubbf8\uc9c0 \uac80\uc0ac\ud558\uae30# \ud559\uc2b5 \uc2a4\ud06c\ub9bd\ud2b8\uac00 \uc644\uc131\ub418\uc5b4 CPU\uc5d0\uc11c\ub4e0 GPU\uc5d0\uc11c\ub4e0 GAN\uc744 \ud6c8\ub828\uc2dc\ud0ac \uc900\ube44\uac00 \ub410\uc2b5\ub2c8\ub2e4. \ud559\uc2b5 \uacfc\uc815\uc758 \uc911\uac04 \ucd9c\ub825\uc744 \uac80\uc0ac\ud558\uae30 \uc704\ud574 \"dcgan-sample-xxx.pt\" \uc5d0 \uc8fc\uae30\uc801\uc73c\ub85c \uc774\ubbf8\uc9c0 \uc0d8\ud50c\uc744 \uc800\uc7a5\ud558\ub294 \ucf54\ub4dc\ub97c \ucd94\uac00\ud588\uc73c\ub2c8, \ud150\uc11c\ub4e4\uc744 \ubd88\ub7ec\uc640 matplotlib\ub85c \uc2dc\uac01\ud654\ud558\ub294 \uac04\ub2e8\ud55c \ud30c\uc774\uc36c \uc2a4\ud06c\ub9bd\ud2b8\ub97c \uc791\uc131\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. import argparse import matplotlib.pyplot as plt import torch parser = argparse.ArgumentParser() parser.add_argument(\"-i\", \"--sample-file\", required=True) parser.add_argument(\"-o\", \"--out-file\", default=\"out.png\") parser.add_argument(\"-d\", \"--dimension\", type=int, default=3) options = parser.parse_args() module = torch.jit.load(options.sample_file) images = list(module.parameters())[0] for index in range(options.dimension * options.dimension): image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8) array = image.numpy() axis = plt.subplot(options.dimension, options.dimension, 1 + index) plt.imshow(array, cmap=\"gray\") axis.get_xaxis().set_visible(False) axis.get_yaxis().set_visible(False) plt.savefig(options.out_file) print(\"Saved \", options.out_file) \uc774\uc81c \ubaa8\ub378\uc744 \uc57d 30 \uc5d0\ud3ed \uc815\ub3c4 \ud559\uc2b5\uc2dc\ud0b5\uc2dc\ub2e4. root@3c0711f20896:/home/build# make \u0026\u0026 ./dcgan 10:17:57 Scanning dependencies of target dcgan [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o [100%] Linking CXX executable dcgan [100%] Built target dcgan CUDA is available! Training on GPU. [ 1/30][200/938] D_loss: 0.4953 | G_loss: 4.0195 -\u003e checkpoint 1 [ 1/30][400/938] D_loss: 0.3610 | G_loss: 4.8148 -\u003e checkpoint 2 [ 1/30][600/938] D_loss: 0.4072 | G_loss: 4.36760 -\u003e checkpoint 3 [ 1/30][800/938] D_loss: 0.4444 | G_loss: 4.0250 -\u003e checkpoint 4 [ 2/30][200/938] D_loss: 0.3761 | G_loss: 3.8790 -\u003e checkpoint 5 [ 2/30][400/938] D_loss: 0.3977 | G_loss: 3.3315 ... -\u003e checkpoint 120 [30/30][938/938] D_loss: 0.3610 | G_loss: 3.8084 \uadf8\ub9ac\uace0 \uc774\ubbf8\uc9c0\ub4e4\uc744 \ud50c\ub86f\uc5d0 \uc2dc\uac01\ud654\ud569\ub2c8\ub2e4. root@3c0711f20896:/home/build# python display.py -i dcgan-sample-100.pt Saved out.png \uadf8 \uacb0\uacfc\ub294 \uc544\ub798\uc640 \uac19\uc744 \uac83\uc785\ub2c8\ub2e4. \uc22b\uc790\ub124\uc694! \ub9cc\uc138! \uc774\uc81c \uc5ec\ub7ec\ubd84 \ucc28\ub840\uc785\ub2c8\ub2e4. \uc22b\uc790\uac00 \ubcf4\ub2e4 \ub098\uc544 \ubcf4\uc774\ub3c4\ub85d \ubaa8\ub378\uc744 \uac1c\uc120\ud560 \uc218 \uc788\ub098\uc694? \uacb0\ub860# \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \ud1b5\ud574 PyTorch C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uc5d0 \ub300\ud55c \uc5b4\ub290 \uc815\ub3c4 \uc774\ud574\ub3c4\uac00 \uc0dd\uae30\uc168\uae30 \ubc14\ub78d\ub2c8\ub2e4. \ud544\uc5f0\uc801\uc73c\ub85c PyTorch \uac19\uc740 \uba38\uc2e0\ub7ec\ub2dd \ub77c\uc774\ube0c\ub7ec\ub9ac\ub294 \ub9e4\uc6b0 \ub2e4\uc591\ud558\uace0 \uad11\ubc94\uc704\ud55c API\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c, \uc5ec\uae30\uc11c \ub17c\uc758\ud558\uae30\uc5d0 \uc2dc\uac04\uacfc \uacf5\uac04\uc774 \ubd80\uc871\ud588\ub358 \uac1c\ub150\ub4e4\uc774 \ub9ce\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc9c1\uc811 API\ub97c \uc0ac\uc6a9\ud574\ubcf4\uace0, \ubb38\uc11c, \uadf8 \uc911\uc5d0\uc11c\ub3c4 \ud2b9\ud788 \ub77c\uc774\ube0c\ub7ec\ub9ac API \uc139\uc158\uc744 \ucc38\uc870\ud574\ubcf4\ub294 \uac83\uc744 \uad8c\uc7a5\ub4dc\ub9bd\ub2c8\ub2e4. \ub610\ud55c, C++ \ud504\ub860\ud2b8\uc5d4\ub4dc\uac00 \ud30c\uc774\uc36c \ud504\ub860\ud2b8\uc5d4\ub4dc\uc758 \ub514\uc790\uc778\uacfc \uc2dc\ub9e8\ud2f1\uc744 \ub530\ub978\ub2e4\ub294 \uc0ac\uc2e4\uc744 \uc798 \uae30\uc5b5\ud558\uba74 \ubcf4\ub2e4 \ube60\ub974\uac8c \ud559\uc2b5\ud560 \uc218 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4. \ud301 \ubcf8 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0 \ub300\ud55c \uc804\uccb4 \uc18c\uc2a4\ucf54\ub4dc\ub294 \uc774 \uc800\uc7a5\uc18c \uc5d0 \uc81c\uacf5\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc5b8\uc81c\ub098 \uadf8\ub807\ub4ef\uc774 \uc5b4\ub5a4 \ubb38\uc81c\uac00 \uc0dd\uae30\uac70\ub098 \uc9c8\ubb38\uc774 \uc788\uc73c\uba74 \uc800\ud76c \ud3ec\ub7fc \uc744 \uc774\uc6a9\ud558\uac70\ub098 Github \uc774\uc288 \ub85c \uc5f0\ub77d\uc8fc\uc138\uc694.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/advanced/cpp_frontend.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>