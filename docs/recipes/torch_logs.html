
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="(beta) torch.compile과 함께 TORCH_LOGS 파이썬 API 사용하기" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/recipes/torch_logs.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="저자: Michael Lazos 번역: 장효영 This tutorial introduces the TORCH_LOGS environment variable, as well as the Python API, and demonstrates how to apply it to observe the phases of torch.compile. 이 튜토리얼에서는 TORCH_LOGS 환경 변수와 함께 Python API를 소개하고, 이를 적용하여`` torch.compile``의 단계를 관찰하는 방법을 보여줍니다. 설정: In this e..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="저자: Michael Lazos 번역: 장효영 This tutorial introduces the TORCH_LOGS environment variable, as well as the Python API, and demonstrates how to apply it to observe the phases of torch.compile. 이 튜토리얼에서는 TORCH_LOGS 환경 변수와 함께 Python API를 소개하고, 이를 적용하여`` torch.compile``의 단계를 관찰하는 방법을 보여줍니다. 설정: In this e..." />
<meta property="og:ignore_canonical" content="true" />

    <title>(beta) torch.compile과 함께 TORCH_LOGS 파이썬 API 사용하기 &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'recipes/torch_logs';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/recipes/torch_logs.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="PyTorch에서 state_dict란 무엇인가요?" href="recipes/what_is_state_dict.html" />
    <link rel="prev" title="Pytorch를 사용해 신경망 정의하기" href="recipes/defining_a_neural_network.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="recipes/defining_a_neural_network.html">Pytorch를 사용해 신경망 정의하기</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">(beta) torch.compile과 함께 TORCH_LOGS 파이썬 API 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/what_is_state_dict.html">PyTorch에서 state_dict란 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/warmstarting_model_using_parameters_from_a_different_model.html">PyTorch에서 다른 모델의 매개변수를 사용하여 빠르게 모델 시작하기(warmstart)</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/zeroing_out_gradients.html">PyTorch에서 변화도를 0으로 만들기</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/profiler_recipe.html">PyTorch 프로파일러(Profiler)</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/Captum_Recipe.html">Captum을 사용하여 모델 해석하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/tensorboard_with_pytorch.html">PyTorch로 TensorBoard 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/amp_recipe.html">자동 혼합 정밀도(Automatic Mixed Precision) 가이드</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/tuning_guide.html">성능 튜닝 가이드</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_optimizer.html">(beta) Compiling the optimizer with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/timer_quick_start.html">Timer 빠르게 시작하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_backend_ipex.html">Intel® CPU에서의 Intel® Extension for PyTorch* 백엔드</a></li>
<li class="toctree-l1"><a class="reference internal" href="zero_redundancy_optimizer.html">Shard Optimizer States with ZeroRedundancyOptimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda_rpc.html">Direct Device-to-Device Communication with TensorPipe CUDA RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_comm_debug_mode.html">Getting Started with <code class="docutils literal notranslate"><span class="pre">CommDebugMode</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_export_challenges_solutions.html">Demonstration of torch.export flow, common challenges and the solutions to address them</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/benchmark.html">PyTorch Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/module_load_state_dict_tips.html">Tips for Loading an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> from a Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/reasoning_about_shapes.html">PyTorch의 Shape들에 대한 추론</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/swap_tensors.html">Extension points in <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> for <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> and tensor subclasses</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_export_aoti_python.html"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/tensorboard_with_pytorch.html">PyTorch로 TensorBoard 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference_tuning_on_aws_graviton.html">(Beta) PyTorch Inference Performance Tuning on AWS Graviton Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="amx.html">Leverage Intel® Advanced Matrix Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_torch_function_modes.html">(beta) Utilizing Torch Function modes with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_optimizer_lr_scheduler.html">(beta) Running the compiled optimizer with an LR Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="foreach_map.html">Explicit horizontal fusion with foreach_map and torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_user_defined_triton_kernel_tutorial.html">사용자 정의 Triton 커널을 ``torch.compile``과 함께 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_caching_tutorial.html">Compile Time Caching in <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_caching_configuration_tutorial.html">Compile Time Caching Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="regional_compilation.html">Reducing torch.compile cold start compilation time with regional compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="regional_aot.html">Reducing AoT cold start compilation time with regional compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="intel_neural_compressor_for_pytorch.html">Ease-of-use quantization for PyTorch with Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_device_mesh.html">Getting Started with DeviceMesh</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_checkpoint_recipe.html">Getting Started with Distributed Checkpoint (DCP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed_async_checkpoint_recipe.html">Asynchronous Saving with Distributed Checkpoint (DCP)</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../recipes_index.html" class="nav-link">Recipes</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">(beta)...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../recipes_index.html">
        <meta itemprop="name" content="Recipes">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="(beta) torch.compile과 함께 TORCH_LOGS 파이썬 API 사용하기">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">recipes/torch_logs</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-recipes-torch-logs-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="beta-torch-compile-torch-logs-api">
<span id="sphx-glr-recipes-torch-logs-py"></span><h1>(beta) torch.compile과 함께 TORCH_LOGS 파이썬 API 사용하기<a class="headerlink" href="#beta-torch-compile-torch-logs-api" title="Link to this heading">#</a></h1>
<p><strong>저자:</strong> <a class="reference external" href="https://github.com/mlazos">Michael Lazos</a>
<strong>번역:</strong> <a class="reference external" href="https://github.com/hyoyoung">장효영</a></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
</pre></div>
</div>
<p>This tutorial introduces the <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS</span></code> environment variable, as well as the Python API, and
demonstrates how to apply it to observe the phases  of <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>.
이 튜토리얼에서는 <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS</span></code> 환경 변수와 함께 Python API를 소개하고,
이를 적용하여 <a href="#id2"><span class="problematic" id="id3">``</span></a>torch.compile``의 단계를 관찰하는 방법을 보여줍니다.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>이 튜토리얼에는 PyTorch 2.2.0 이상 버전이 필요합니다.</p>
</div>
<section id="id4">
<h2>설정<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<p>In this example, we’ll set up a simple Python function which performs an elementwise
add and observe the compilation process with <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS</span></code> Python API.
이 예제에서는 요소별 덧셈을 수행하는 간단한 파이썬 함수를 설정하고
<code class="docutils literal notranslate"><span class="pre">TORCH_LOGS</span></code> 파이썬 API를 사용하여 컴파일 프로세스를 관찰해 보겠습니다.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>명령줄에서 로깅 설정을 변경하는 데 사용할 수 있는
환경 변수 <a href="#id5"><span class="problematic" id="id6">``</span></a>TORCH_LOGS``도 있습니다. 각 예제에 해당하는
환경 변수 설정이 표시되어 있습니다.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># torch.compile을 지원하지 않는 기기인 경우 완전히 종료합니다.</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_capability</span><span class="p">()</span> <span class="o">&lt;</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Skipping because torch.compile is not supported on this device.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">z</span> <span class="o">+</span> <span class="mi">2</span>


    <span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">))</span>


<span class="c1"># 각 예제 사이의 구분 기호를 출력하고 dynamo를 reset합니다</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">separator</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;===================</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">=========================&quot;</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>


    <span class="n">separator</span><span class="p">(</span><span class="s2">&quot;Dynamo Tracing&quot;</span><span class="p">)</span>
<span class="c1"># dynamo tracing 보기</span>
<span class="c1"># TORCH_LOGS=&quot;+dynamo&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">set_logs</span><span class="p">(</span><span class="n">dynamo</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>

    <span class="n">separator</span><span class="p">(</span><span class="s2">&quot;Traced Graph&quot;</span><span class="p">)</span>
<span class="c1"># traced 그래프 보기</span>
<span class="c1"># TORCH_LOGS=&quot;graph&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">set_logs</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>

    <span class="n">separator</span><span class="p">(</span><span class="s2">&quot;Fusion Decisions&quot;</span><span class="p">)</span>
<span class="c1"># fusion decision 보기</span>
<span class="c1"># TORCH_LOGS=&quot;fusion&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">set_logs</span><span class="p">(</span><span class="n">fusion</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>

    <span class="n">separator</span><span class="p">(</span><span class="s2">&quot;Output Code&quot;</span><span class="p">)</span>
<span class="c1"># inductor가 생성한 결과 코드 보기</span>
<span class="c1"># TORCH_LOGS=&quot;output_code&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">set_logs</span><span class="p">(</span><span class="n">output_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>

    <span class="n">separator</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>===================Dynamo Tracing=========================
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] torchdynamo start compiling fn /workspace/tutorials-kr/recipes_source/torch_logs.py:44, stack (elided 5 frames):
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/bin/sphinx-build&quot;, line 7, in &lt;module&gt;
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     sys.exit(main())
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 339, in main
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     return make_main(argv)
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 213, in make_main
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     return make_mode.run_make_mode(argv[1:])
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 181, in run_make_mode
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     return make.run_generic_build(args[0])
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 169, in run_generic_build
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     return build_main(args + opts)
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 293, in build_main
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 272, in __init__
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     self._init_builder()
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 343, in _init_builder
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     self.events.emit(&#39;builder-inited&#39;)
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/events.py&quot;, line 97, in emit
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     results.append(listener.handler(self.app, *args))
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py&quot;, line 757, in generate_gallery_rst
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     ) = generate_dir_rst(
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 606, in generate_dir_rst
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     results = parallel(
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 607, in &lt;genexpr&gt;
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/workspace/tutorials-kr/conf.py&quot;, line 86, in wrapper
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     p.start()
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 121, in start
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     self._popen = self._Popen(self)
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 224, in _Popen
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     return _default_context.get_context().Process._Popen(process_obj)
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 281, in _Popen
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     return Popen(process_obj)
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 19, in __init__
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     self._launch(process_obj)
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 71, in _launch
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     code = process_obj._bootstrap(parent_sentinel=child_r)
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 314, in _bootstrap
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     self.run()
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 108, in run
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     self._target(*self._args, **self._kwargs)
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/workspace/tutorials-kr/conf.py&quot;, line 74, in call_fn
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     result = func(*args, **kwargs)
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1374, in generate_file_rst
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     output_blocks, time_elapsed = execute_script(
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1192, in execute_script
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     execute_code_block(
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1048, in execute_code_block
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     is_last_expr, mem_max = _exec_and_get_memory(
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 876, in _exec_and_get_memory
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     mem_max, _ = call_memory(
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1725, in _sg_call_memory_noop
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     return 0.0, func()
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 794, in __call__
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     exec(self.code, self.fake_main.__dict__)
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]   File &quot;/workspace/tutorials-kr/recipes_source/torch_logs.py&quot;, line 63, in &lt;module&gt;
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]     fn(*inputs)
V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0]
I1004 00:38:40.536000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:3320] [0/0] Step 1: torchdynamo start tracing fn /workspace/tutorials-kr/recipes_source/torch_logs.py:44
I1004 00:38:40.537000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] [0/0] create_env
V1004 00:38:40.540000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1237] [0/0] [__trace_source] TRACE starts_line /workspace/tutorials-kr/recipes_source/torch_logs.py:44 in fn ()
V1004 00:38:40.540000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1237] [0/0] [__trace_source]         @torch.compile()
V1004 00:38:40.541000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE RESUME 0 []
V1004 00:38:40.541000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1237] [0/0] [__trace_source] TRACE starts_line /workspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn (fn)
V1004 00:38:40.541000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1237] [0/0] [__trace_source]             z = x + y
V1004 00:38:40.541000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE LOAD_FAST x []
V1004 00:38:40.541000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE LOAD_FAST y [LazyVariableTracker()]
V1004 00:38:40.542000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE BINARY_OP 0 [LazyVariableTracker(), LazyVariableTracker()]
V1004 00:38:40.543000 3764082 site-packages/torch/_dynamo/variables/builder.py:3373] [0/0] wrap_to_fake L[&#39;x&#39;] (2, 2) StatefulSymbolicContext(dynamic_sizes=[&lt;DimDynamic.STATIC: 2&gt;, &lt;DimDynamic.STATIC: 2&gt;], dynamic_strides=[&lt;DimDynamic.INFER_STRIDE: 4&gt;, &lt;DimDynamic.INFER_STRIDE: 4&gt;], constraint_sizes=[None, None], constraint_strides=[None, None], specialize_on=[[], []], view_base_context=None, tensor_source=LocalSource(local_name=&#39;x&#39;, is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) &lt;class &#39;torch.Tensor&#39;&gt;
V1004 00:38:40.544000 3764082 site-packages/torch/_dynamo/output_graph.py:2614] [0/0] create_graph_input L_x_ L[&#39;x&#39;] FakeTensor(..., device=&#39;cuda:0&#39;, size=(2, 2)) at debug_level 0 before=False
V1004 00:38:40.545000 3764082 site-packages/torch/_dynamo/variables/builder.py:3373] [0/0] wrap_to_fake L[&#39;y&#39;] (2, 2) StatefulSymbolicContext(dynamic_sizes=[&lt;DimDynamic.STATIC: 2&gt;, &lt;DimDynamic.STATIC: 2&gt;], dynamic_strides=[&lt;DimDynamic.INFER_STRIDE: 4&gt;, &lt;DimDynamic.INFER_STRIDE: 4&gt;], constraint_sizes=[None, None], constraint_strides=[None, None], specialize_on=[[], []], view_base_context=None, tensor_source=LocalSource(local_name=&#39;y&#39;, is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) &lt;class &#39;torch.Tensor&#39;&gt;
V1004 00:38:40.546000 3764082 site-packages/torch/_dynamo/output_graph.py:2614] [0/0] create_graph_input L_y_ L[&#39;y&#39;] FakeTensor(..., device=&#39;cuda:0&#39;, size=(2, 2)) at debug_level 0 before=False
V1004 00:38:40.547000 3764082 site-packages/torch/_dynamo/output_graph.py:2462] [0/0] [__trace_call] TRACE FX call add from /workspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn (fn)
V1004 00:38:40.547000 3764082 site-packages/torch/_dynamo/output_graph.py:2462] [0/0] [__trace_call]         z = x + y
V1004 00:38:40.547000 3764082 site-packages/torch/_dynamo/output_graph.py:2462] [0/0] [__trace_call]             ~~^~~
V1004 00:38:40.549000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE STORE_FAST z [TensorVariable()]
V1004 00:38:40.549000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1237] [0/0] [__trace_source] TRACE starts_line /workspace/tutorials-kr/recipes_source/torch_logs.py:47 in fn (fn)
V1004 00:38:40.549000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1237] [0/0] [__trace_source]             return z + 2
V1004 00:38:40.549000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE LOAD_FAST z []
V1004 00:38:40.549000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE LOAD_CONST 2 [TensorVariable()]
V1004 00:38:40.549000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE BINARY_OP 0 [TensorVariable(), ConstantVariable(int: 2)]
V1004 00:38:40.550000 3764082 site-packages/torch/_dynamo/output_graph.py:2462] [0/0] [__trace_call] TRACE FX call add_1 from /workspace/tutorials-kr/recipes_source/torch_logs.py:47 in fn (fn)
V1004 00:38:40.550000 3764082 site-packages/torch/_dynamo/output_graph.py:2462] [0/0] [__trace_call]         return z + 2
V1004 00:38:40.550000 3764082 site-packages/torch/_dynamo/output_graph.py:2462] [0/0] [__trace_call]                ~~^~~
V1004 00:38:40.551000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()]
I1004 00:38:40.551000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:3648] [0/0] Step 1: torchdynamo done tracing fn (RETURN_VALUE)
V1004 00:38:40.551000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:3652] [0/0] RETURN_VALUE triggered compile
V1004 00:38:40.552000 3764082 site-packages/torch/_dynamo/output_graph.py:1263] [0/0] COMPILING GRAPH due to GraphCompileReason(reason=&#39;return_value&#39;, user_stack=[&lt;FrameSummary file /workspace/tutorials-kr/recipes_source/torch_logs.py, line 47 in fn&gt;], graph_break=False)
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] TRACED GRAPH
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]  ===== __compiled_fn_1_2d30cbfe_4ba8_49a7_aef8_bc201073ee00 =====
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]  /opt/conda/lib/python3.11/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]     def forward(self, L_x_: &quot;f32[2, 2][2, 1]cuda:0&quot;, L_y_: &quot;f32[2, 2][2, 1]cuda:0&quot;):
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         l_x_ = L_x_
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         l_y_ = L_y_
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]          # File: /workspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn, code: z = x + y
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         z: &quot;f32[2, 2][2, 1]cuda:0&quot; = l_x_ + l_y_;  l_x_ = l_y_ = None
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]          # File: /workspace/tutorials-kr/recipes_source/torch_logs.py:47 in fn, code: return z + 2
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         add_1: &quot;f32[2, 2][2, 1]cuda:0&quot; = z + 2;  z = None
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]         return (add_1,)
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]
V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code]
I1004 00:38:40.555000 3764082 site-packages/torch/_dynamo/output_graph.py:1842] [0/0] Step 2: calling compiler function inductor
I1004 00:38:41.349000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] [0/0] produce_guards
I1004 00:38:41.351000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] [0/0] produce_guards
I1004 00:38:41.354000 3764082 site-packages/torch/_dynamo/output_graph.py:1847] [0/0] Step 2: done compiler function inductor
I1004 00:38:41.357000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] [0/0] produce_guards
V1004 00:38:41.357000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[&#39;x&#39;].size()[0] 2 None
V1004 00:38:41.357000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[&#39;x&#39;].size()[1] 2 None
V1004 00:38:41.357000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[&#39;x&#39;].stride()[0] 2 None
V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[&#39;x&#39;].stride()[1] 1 None
V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[&#39;x&#39;].storage_offset() 0 None
V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[&#39;y&#39;].size()[0] 2 None
V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[&#39;y&#39;].size()[1] 2 None
V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[&#39;y&#39;].stride()[0] 2 None
V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[&#39;y&#39;].stride()[1] 1 None
V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[&#39;y&#39;].storage_offset() 0 None
V1004 00:38:41.359000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[&#39;x&#39;].size()[0] == 2
V1004 00:38:41.359000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[&#39;x&#39;].size()[1] == 2
V1004 00:38:41.359000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[&#39;x&#39;].stride()[0] == 2
V1004 00:38:41.359000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[&#39;x&#39;].stride()[1] == 1
V1004 00:38:41.360000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[&#39;x&#39;].storage_offset() == 0
V1004 00:38:41.360000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[&#39;y&#39;].size()[0] == 2
V1004 00:38:41.360000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[&#39;y&#39;].size()[1] == 2
V1004 00:38:41.360000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[&#39;y&#39;].stride()[0] == 2
V1004 00:38:41.360000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[&#39;y&#39;].stride()[1] == 1
V1004 00:38:41.360000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[&#39;y&#39;].storage_offset() == 0
V1004 00:38:41.360000 3764082 site-packages/torch/_dynamo/guards.py:3064] [0/0] [__guards] GUARDS:
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards]
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] TREE_GUARD_MANAGER:
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] +- RootGuardManager
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:633 in init_ambient_guards
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:621 in init_ambient_guards
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- GuardManager: source=L[&#39;x&#39;], accessed_by=FrameLocalsGuardAccessor(key=&#39;x&#39;, framelocals_idx=0)
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L[&#39;x&#39;], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[2, 2], stride=[2, 1])  # z = x + y  # orkspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L[&#39;x&#39;], &#39;_dynamo_dynamic_indices&#39;) == False           # z = x + y  # orkspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L[&#39;x&#39;], L[&#39;y&#39;])
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- GuardManager: source=L[&#39;y&#39;], accessed_by=FrameLocalsGuardAccessor(key=&#39;y&#39;, framelocals_idx=1)
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L[&#39;y&#39;], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[2, 2], stride=[2, 1])  # z = x + y  # orkspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L[&#39;y&#39;], &#39;_dynamo_dynamic_indices&#39;) == False           # z = x + y  # orkspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_TENSOR_ALIASING
V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards]
V1004 00:38:41.379000 3764082 site-packages/torch/_dynamo/guards.py:2894] [0/0] [__guards] Guard eval latency = 9.69 us
I1004 00:38:41.380000 3764082 site-packages/torch/_dynamo/pgo.py:785] [0/0] put_code_state: no cache key, skipping
I1004 00:38:41.380000 3764082 site-packages/torch/_dynamo/convert_frame.py:1175] [0/0] run_gc_after_compile: running gc
V1004 00:38:41.383000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: inner (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_compile.py)
V1004 00:38:41.384000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: disable (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/decorators.py)
V1004 00:38:41.384000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: innermost_fn (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py)
V1004 00:38:41.384000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: __init__ (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py)
V1004 00:38:41.384000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: __init__ (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py)
V1004 00:38:41.385000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: nothing (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py)
V1004 00:38:41.385000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: __call__ (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py)
V1004 00:38:41.385000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: _fn (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py)
===================Traced Graph=========================
I1004 00:38:41.386000 3764082 site-packages/torch/_dynamo/__init__.py:118] torch._dynamo.reset
I1004 00:38:41.386000 3764082 site-packages/torch/_dynamo/__init__.py:151] torch._dynamo.reset_code_caches
===================Fusion Decisions=========================
===================Output Code=========================
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] Output code:
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # AOT ID: [&#39;0_inference&#39;]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import torch
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import math
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import random
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import os
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import tempfile
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from math import inf, nan
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from cmath import nanj
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch import device, empty_strided
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton.language as tl
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] aten = torch.ops.aten
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] _quantized = torch.ops._quantized
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] async_compile = AsyncCompile()
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # kernel path: /tmp/torchinductor_root/bc/cbcra7zetpvpubjqpgzmr2fv6olpakamutgikukl3oc6f6jb3qbh.py
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Topologically Sorted Source Nodes: [z, add_1], Original ATen: [aten.add]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Source node to ATen node mapping:
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   add_1 =&gt; add_1
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   z =&gt; add
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Graph fragment:
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %arg1_1), kwargs = {})
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] #   %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add, 2), kwargs = {})
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] triton_poi_fused_add_0 = async_compile.triton(&#39;triton_poi_fused_add_0&#39;, &#39;&#39;&#39;
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton.language as tl
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] @triton_heuristics.pointwise(
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     size_hints={&#39;x&#39;: 4},
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     filename=__file__,
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     triton_meta={&#39;signature&#39;: {&#39;in_ptr0&#39;: &#39;*fp32&#39;, &#39;in_ptr1&#39;: &#39;*fp32&#39;, &#39;out_ptr0&#39;: &#39;*fp32&#39;, &#39;xnumel&#39;: &#39;i32&#39;, &#39;XBLOCK&#39;: &#39;constexpr&#39;}, &#39;device&#39;: DeviceProperties(type=&#39;cuda&#39;, index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), &#39;constants&#39;: {}, &#39;configs&#39;: [{(0,): [[&#39;tt.divisibility&#39;, 16]], (1,): [[&#39;tt.divisibility&#39;, 16]], (2,): [[&#39;tt.divisibility&#39;, 16]]}]},
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     inductor_meta={&#39;grid_type&#39;: &#39;Grid1D&#39;, &#39;autotune_hints&#39;: set(), &#39;kernel_name&#39;: &#39;triton_poi_fused_add_0&#39;, &#39;mutated_arg_names&#39;: [], &#39;optimize_mem&#39;: True, &#39;no_x_dim&#39;: False, &#39;num_load&#39;: 2, &#39;num_reduction&#39;: 0, &#39;backend_hash&#39;: &#39;AD014388F727234BBC364D4F9312DA1C72DBEFEDA247CD785958FB6EB1138CAC&#39;, &#39;are_deterministic_algorithms_enabled&#39;: False, &#39;assert_indirect_indexing&#39;: True, &#39;autotune_local_cache&#39;: True, &#39;autotune_pointwise&#39;: True, &#39;autotune_remote_cache&#39;: None, &#39;force_disable_caches&#39;: False, &#39;dynamic_scale_rblock&#39;: True, &#39;max_autotune&#39;: False, &#39;max_autotune_pointwise&#39;: False, &#39;min_split_scan_rblock&#39;: 256, &#39;spill_threshold&#39;: 16, &#39;store_cubin&#39;: False, &#39;tiling_scores&#39;: {&#39;x&#39;: 32}},
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     min_elem_per_thread=0
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] )
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] @triton.jit
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def triton_poi_fused_add_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xnumel = 4
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     xmask = xindex &lt; xnumel
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     x0 = xindex
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), xmask)
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), xmask)
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp2 = tmp0 + tmp1
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp3 = 2.0
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tmp4 = tmp2 + tmp3
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp4, xmask)
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] &#39;&#39;&#39;, device_str=&#39;cuda&#39;)
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] async_compile.wait(globals())
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] del async_compile
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def call(args):
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     arg0_1, arg1_1 = args
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     args.clear()
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     assert_size_stride(arg0_1, (2, 2), (2, 1))
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     assert_size_stride(arg1_1, (2, 2), (2, 1))
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         torch.cuda.set_device(0)
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         buf0 = empty_strided_cuda((2, 2), (2, 1), torch.float32)
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [z, add_1], Original ATen: [aten.add]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         stream0 = get_raw_stream(0)
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         triton_poi_fused_add_0.run(arg0_1, arg1_1, buf0, 4, stream=stream0)
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         del arg0_1
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]         del arg1_1
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     return (buf0, )
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     arg0_1 = rand_strided((2, 2), (2, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     arg1_1 = rand_strided((2, 2), (2, 1), device=&#39;cuda:0&#39;, dtype=torch.float32)
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1])
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] if __name__ == &quot;__main__&quot;:
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]     compiled_module_main(&#39;None&#39;, benchmark_compiled_module)
V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code]
V1004 00:38:41.483000 3764082 site-packages/torch/_inductor/codecache.py:1237] [0/0] [__output_code] Output code written to: /tmp/torchinductor_root/5h/c5hssiemdwuea5ppzag4e75dgp4oh5pub4qbdo4lnqgzy5jc55jn.py
============================================
</pre></div>
</div>
</section>
<section id="id7">
<h2>결론<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<p>이 튜토리얼에서는 사용 가능한 몇 가지 로깅 옵션을 실험하여
TORCH_LOGS 환경 변수와 Python API를 소개했습니다.
사용 가능한 모든 옵션에 대한 설명을 보려면
파이썬 스크립트에서 import torch를 실행하고 TORCH_LOGS를 “help”로 설정하세요.</p>
<p>다른 방법으로는, <a class="reference external" href="https://pytorch.org/docs/main/logging.html">torch._logging 문서</a> 를 보면,
사용 가능한 모든 로깅 옵션에 대한 설명을 확인할 수 있습니다.</p>
<p>torch.compile에 관한 더 많은 정보는, <a href="#id8"><span class="problematic" id="id9">`</span></a>torch.compile 튜토리얼`_를 보세요.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 7.062 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-recipes-torch-logs-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/41526f38c5c72d94f024660d73cef185/torch_logs.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">torch_logs.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/aa116673383c7eeeacfb92b8c9beb97a/torch_logs.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">torch_logs.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/c276b15b15888a68dfb889e3ac6950af/torch_logs.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">torch_logs.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="recipes/defining_a_neural_network.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Pytorch를 사용해 신경망 정의하기</p>
      </div>
    </a>
    <a class="right-next"
       href="recipes/what_is_state_dict.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">PyTorch에서 state_dict란 무엇인가요?</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="recipes/defining_a_neural_network.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Pytorch를 사용해 신경망 정의하기</p>
      </div>
    </a>
    <a class="right-next"
       href="recipes/what_is_state_dict.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">PyTorch에서 state_dict란 무엇인가요?</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">설정</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">결론</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "(beta) torch.compile\uacfc \ud568\uaed8 TORCH_LOGS \ud30c\uc774\uc36c API \uc0ac\uc6a9\ud558\uae30",
       "headline": "(beta) torch.compile\uacfc \ud568\uaed8 TORCH_LOGS \ud30c\uc774\uc36c API \uc0ac\uc6a9\ud558\uae30",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/recipes/torch_logs.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. (beta) torch.compile\uacfc \ud568\uaed8 TORCH_LOGS \ud30c\uc774\uc36c API \uc0ac\uc6a9\ud558\uae30# \uc800\uc790: Michael Lazos \ubc88\uc5ed: \uc7a5\ud6a8\uc601 import logging This tutorial introduces the TORCH_LOGS environment variable, as well as the Python API, and demonstrates how to apply it to observe the phases of torch.compile. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 TORCH_LOGS \ud658\uacbd \ubcc0\uc218\uc640 \ud568\uaed8 Python API\ub97c \uc18c\uac1c\ud558\uace0, \uc774\ub97c \uc801\uc6a9\ud558\uc5ec ``torch.compile``\uc758 \ub2e8\uacc4\ub97c \uad00\ucc30\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ucc38\uace0 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\ub294 PyTorch 2.2.0 \uc774\uc0c1 \ubc84\uc804\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. \uc124\uc815# In this example, we\u2019ll set up a simple Python function which performs an elementwise add and observe the compilation process with TORCH_LOGS Python API. \uc774 \uc608\uc81c\uc5d0\uc11c\ub294 \uc694\uc18c\ubcc4 \ub367\uc148\uc744 \uc218\ud589\ud558\ub294 \uac04\ub2e8\ud55c \ud30c\uc774\uc36c \ud568\uc218\ub97c \uc124\uc815\ud558\uace0 TORCH_LOGS \ud30c\uc774\uc36c API\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucef4\ud30c\uc77c \ud504\ub85c\uc138\uc2a4\ub97c \uad00\ucc30\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \ucc38\uace0 \uba85\ub839\uc904\uc5d0\uc11c \ub85c\uae45 \uc124\uc815\uc744 \ubcc0\uacbd\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ud658\uacbd \ubcc0\uc218 ``TORCH_LOGS``\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \uc608\uc81c\uc5d0 \ud574\ub2f9\ud558\ub294 \ud658\uacbd \ubcc0\uc218 \uc124\uc815\uc774 \ud45c\uc2dc\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. import torch # torch.compile\uc744 \uc9c0\uc6d0\ud558\uc9c0 \uc54a\ub294 \uae30\uae30\uc778 \uacbd\uc6b0 \uc644\uc804\ud788 \uc885\ub8cc\ud569\ub2c8\ub2e4. if torch.cuda.get_device_capability() \u003c (7, 0): print(\"Skipping because torch.compile is not supported on this device.\") else: @torch.compile() def fn(x, y): z = x + y return z + 2 inputs = (torch.ones(2, 2, device=\"cuda\"), torch.zeros(2, 2, device=\"cuda\")) # \uac01 \uc608\uc81c \uc0ac\uc774\uc758 \uad6c\ubd84 \uae30\ud638\ub97c \ucd9c\ub825\ud558\uace0 dynamo\ub97c reset\ud569\ub2c8\ub2e4 def separator(name): print(f\"==================={name}=========================\") torch._dynamo.reset() separator(\"Dynamo Tracing\") # dynamo tracing \ubcf4\uae30 # TORCH_LOGS=\"+dynamo\" torch._logging.set_logs(dynamo=logging.DEBUG) fn(*inputs) separator(\"Traced Graph\") # traced \uadf8\ub798\ud504 \ubcf4\uae30 # TORCH_LOGS=\"graph\" torch._logging.set_logs(graph=True) fn(*inputs) separator(\"Fusion Decisions\") # fusion decision \ubcf4\uae30 # TORCH_LOGS=\"fusion\" torch._logging.set_logs(fusion=True) fn(*inputs) separator(\"Output Code\") # inductor\uac00 \uc0dd\uc131\ud55c \uacb0\uacfc \ucf54\ub4dc \ubcf4\uae30 # TORCH_LOGS=\"output_code\" torch._logging.set_logs(output_code=True) fn(*inputs) separator(\"\") ===================Dynamo Tracing========================= V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] torchdynamo start compiling fn /workspace/tutorials-kr/recipes_source/torch_logs.py:44, stack (elided 5 frames): V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/bin/sphinx-build\", line 7, in \u003cmodule\u003e V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] sys.exit(main()) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 339, in main V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] return make_main(argv) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 213, in make_main V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] return make_mode.run_make_mode(argv[1:]) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 181, in run_make_mode V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] return make.run_generic_build(args[0]) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 169, in run_generic_build V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] return build_main(args + opts) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 293, in build_main V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] app = Sphinx(args.sourcedir, args.confdir, args.outputdir, V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 272, in __init__ V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] self._init_builder() V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 343, in _init_builder V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] self.events.emit(\u0027builder-inited\u0027) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx/events.py\", line 97, in emit V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] results.append(listener.handler(self.app, *args)) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py\", line 757, in generate_gallery_rst V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] ) = generate_dir_rst( V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 606, in generate_dir_rst V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] results = parallel( V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 607, in \u003cgenexpr\u003e V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/workspace/tutorials-kr/conf.py\", line 86, in wrapper V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] p.start() V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 121, in start V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] self._popen = self._Popen(self) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] return _default_context.get_context().Process._Popen(process_obj) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 281, in _Popen V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] return Popen(process_obj) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__ V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] self._launch(process_obj) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 71, in _launch V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] code = process_obj._bootstrap(parent_sentinel=child_r) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] self.run() V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] self._target(*self._args, **self._kwargs) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/workspace/tutorials-kr/conf.py\", line 74, in call_fn V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] result = func(*args, **kwargs) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1374, in generate_file_rst V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] output_blocks, time_elapsed = execute_script( V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1192, in execute_script V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] execute_code_block( V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1048, in execute_code_block V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] is_last_expr, mem_max = _exec_and_get_memory( V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 876, in _exec_and_get_memory V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] mem_max, _ = call_memory( V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1725, in _sg_call_memory_noop V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] return 0.0, func() V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 794, in __call__ V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] exec(self.code, self.fake_main.__dict__) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] File \"/workspace/tutorials-kr/recipes_source/torch_logs.py\", line 63, in \u003cmodule\u003e V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] fn(*inputs) V1004 00:38:40.534000 3764082 site-packages/torch/_dynamo/convert_frame.py:1055] [0/0] I1004 00:38:40.536000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:3320] [0/0] Step 1: torchdynamo start tracing fn /workspace/tutorials-kr/recipes_source/torch_logs.py:44 I1004 00:38:40.537000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] [0/0] create_env V1004 00:38:40.540000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1237] [0/0] [__trace_source] TRACE starts_line /workspace/tutorials-kr/recipes_source/torch_logs.py:44 in fn () V1004 00:38:40.540000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1237] [0/0] [__trace_source] @torch.compile() V1004 00:38:40.541000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE RESUME 0 [] V1004 00:38:40.541000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1237] [0/0] [__trace_source] TRACE starts_line /workspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn (fn) V1004 00:38:40.541000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1237] [0/0] [__trace_source] z = x + y V1004 00:38:40.541000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE LOAD_FAST x [] V1004 00:38:40.541000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE LOAD_FAST y [LazyVariableTracker()] V1004 00:38:40.542000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE BINARY_OP 0 [LazyVariableTracker(), LazyVariableTracker()] V1004 00:38:40.543000 3764082 site-packages/torch/_dynamo/variables/builder.py:3373] [0/0] wrap_to_fake L[\u0027x\u0027] (2, 2) StatefulSymbolicContext(dynamic_sizes=[\u003cDimDynamic.STATIC: 2\u003e, \u003cDimDynamic.STATIC: 2\u003e], dynamic_strides=[\u003cDimDynamic.INFER_STRIDE: 4\u003e, \u003cDimDynamic.INFER_STRIDE: 4\u003e], constraint_sizes=[None, None], constraint_strides=[None, None], specialize_on=[[], []], view_base_context=None, tensor_source=LocalSource(local_name=\u0027x\u0027, is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) \u003cclass \u0027torch.Tensor\u0027\u003e V1004 00:38:40.544000 3764082 site-packages/torch/_dynamo/output_graph.py:2614] [0/0] create_graph_input L_x_ L[\u0027x\u0027] FakeTensor(..., device=\u0027cuda:0\u0027, size=(2, 2)) at debug_level 0 before=False V1004 00:38:40.545000 3764082 site-packages/torch/_dynamo/variables/builder.py:3373] [0/0] wrap_to_fake L[\u0027y\u0027] (2, 2) StatefulSymbolicContext(dynamic_sizes=[\u003cDimDynamic.STATIC: 2\u003e, \u003cDimDynamic.STATIC: 2\u003e], dynamic_strides=[\u003cDimDynamic.INFER_STRIDE: 4\u003e, \u003cDimDynamic.INFER_STRIDE: 4\u003e], constraint_sizes=[None, None], constraint_strides=[None, None], specialize_on=[[], []], view_base_context=None, tensor_source=LocalSource(local_name=\u0027y\u0027, is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) \u003cclass \u0027torch.Tensor\u0027\u003e V1004 00:38:40.546000 3764082 site-packages/torch/_dynamo/output_graph.py:2614] [0/0] create_graph_input L_y_ L[\u0027y\u0027] FakeTensor(..., device=\u0027cuda:0\u0027, size=(2, 2)) at debug_level 0 before=False V1004 00:38:40.547000 3764082 site-packages/torch/_dynamo/output_graph.py:2462] [0/0] [__trace_call] TRACE FX call add from /workspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn (fn) V1004 00:38:40.547000 3764082 site-packages/torch/_dynamo/output_graph.py:2462] [0/0] [__trace_call] z = x + y V1004 00:38:40.547000 3764082 site-packages/torch/_dynamo/output_graph.py:2462] [0/0] [__trace_call] ~~^~~ V1004 00:38:40.549000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE STORE_FAST z [TensorVariable()] V1004 00:38:40.549000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1237] [0/0] [__trace_source] TRACE starts_line /workspace/tutorials-kr/recipes_source/torch_logs.py:47 in fn (fn) V1004 00:38:40.549000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1237] [0/0] [__trace_source] return z + 2 V1004 00:38:40.549000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE LOAD_FAST z [] V1004 00:38:40.549000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE LOAD_CONST 2 [TensorVariable()] V1004 00:38:40.549000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE BINARY_OP 0 [TensorVariable(), ConstantVariable(int: 2)] V1004 00:38:40.550000 3764082 site-packages/torch/_dynamo/output_graph.py:2462] [0/0] [__trace_call] TRACE FX call add_1 from /workspace/tutorials-kr/recipes_source/torch_logs.py:47 in fn (fn) V1004 00:38:40.550000 3764082 site-packages/torch/_dynamo/output_graph.py:2462] [0/0] [__trace_call] return z + 2 V1004 00:38:40.550000 3764082 site-packages/torch/_dynamo/output_graph.py:2462] [0/0] [__trace_call] ~~^~~ V1004 00:38:40.551000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:1260] [0/0] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()] I1004 00:38:40.551000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:3648] [0/0] Step 1: torchdynamo done tracing fn (RETURN_VALUE) V1004 00:38:40.551000 3764082 site-packages/torch/_dynamo/symbolic_convert.py:3652] [0/0] RETURN_VALUE triggered compile V1004 00:38:40.552000 3764082 site-packages/torch/_dynamo/output_graph.py:1263] [0/0] COMPILING GRAPH due to GraphCompileReason(reason=\u0027return_value\u0027, user_stack=[\u003cFrameSummary file /workspace/tutorials-kr/recipes_source/torch_logs.py, line 47 in fn\u003e], graph_break=False) V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] TRACED GRAPH V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] ===== __compiled_fn_1_2d30cbfe_4ba8_49a7_aef8_bc201073ee00 ===== V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] /opt/conda/lib/python3.11/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module): V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] def forward(self, L_x_: \"f32[2, 2][2, 1]cuda:0\", L_y_: \"f32[2, 2][2, 1]cuda:0\"): V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] l_x_ = L_x_ V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] l_y_ = L_y_ V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] # File: /workspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn, code: z = x + y V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] z: \"f32[2, 2][2, 1]cuda:0\" = l_x_ + l_y_; l_x_ = l_y_ = None V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] # File: /workspace/tutorials-kr/recipes_source/torch_logs.py:47 in fn, code: return z + 2 V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] add_1: \"f32[2, 2][2, 1]cuda:0\" = z + 2; z = None V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] return (add_1,) V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] V1004 00:38:40.554000 3764082 site-packages/torch/_dynamo/output_graph.py:1667] [0/0] [__graph_code] I1004 00:38:40.555000 3764082 site-packages/torch/_dynamo/output_graph.py:1842] [0/0] Step 2: calling compiler function inductor I1004 00:38:41.349000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] [0/0] produce_guards I1004 00:38:41.351000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] [0/0] produce_guards I1004 00:38:41.354000 3764082 site-packages/torch/_dynamo/output_graph.py:1847] [0/0] Step 2: done compiler function inductor I1004 00:38:41.357000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] [0/0] produce_guards V1004 00:38:41.357000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[\u0027x\u0027].size()[0] 2 None V1004 00:38:41.357000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[\u0027x\u0027].size()[1] 2 None V1004 00:38:41.357000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[\u0027x\u0027].stride()[0] 2 None V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[\u0027x\u0027].stride()[1] 1 None V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[\u0027x\u0027].storage_offset() 0 None V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[\u0027y\u0027].size()[0] 2 None V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[\u0027y\u0027].size()[1] 2 None V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[\u0027y\u0027].stride()[0] 2 None V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[\u0027y\u0027].stride()[1] 1 None V1004 00:38:41.358000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [0/0] track_symint L[\u0027y\u0027].storage_offset() 0 None V1004 00:38:41.359000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[\u0027x\u0027].size()[0] == 2 V1004 00:38:41.359000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[\u0027x\u0027].size()[1] == 2 V1004 00:38:41.359000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[\u0027x\u0027].stride()[0] == 2 V1004 00:38:41.359000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[\u0027x\u0027].stride()[1] == 1 V1004 00:38:41.360000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[\u0027x\u0027].storage_offset() == 0 V1004 00:38:41.360000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[\u0027y\u0027].size()[0] == 2 V1004 00:38:41.360000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[\u0027y\u0027].size()[1] == 2 V1004 00:38:41.360000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[\u0027y\u0027].stride()[0] == 2 V1004 00:38:41.360000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[\u0027y\u0027].stride()[1] == 1 V1004 00:38:41.360000 3764082 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [0/0] Skipping guard L[\u0027y\u0027].storage_offset() == 0 V1004 00:38:41.360000 3764082 site-packages/torch/_dynamo/guards.py:3064] [0/0] [__guards] GUARDS: V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] TREE_GUARD_MANAGER: V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] +- RootGuardManager V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None # _dynamo/output_graph.py:633 in init_ambient_guards V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None # _dynamo/output_graph.py:621 in init_ambient_guards V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- GLOBAL_STATE: ___check_global_state() V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack() V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- GuardManager: source=L[\u0027x\u0027], accessed_by=FrameLocalsGuardAccessor(key=\u0027x\u0027, framelocals_idx=0) V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L[\u0027x\u0027], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[2, 2], stride=[2, 1]) # z = x + y # orkspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L[\u0027x\u0027], \u0027_dynamo_dynamic_indices\u0027) == False # z = x + y # orkspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L[\u0027x\u0027], L[\u0027y\u0027]) V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | +- GuardManager: source=L[\u0027y\u0027], accessed_by=FrameLocalsGuardAccessor(key=\u0027y\u0027, framelocals_idx=1) V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L[\u0027y\u0027], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[2, 2], stride=[2, 1]) # z = x + y # orkspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L[\u0027y\u0027], \u0027_dynamo_dynamic_indices\u0027) == False # z = x + y # orkspace/tutorials-kr/recipes_source/torch_logs.py:46 in fn V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] | | +- NO_TENSOR_ALIASING V1004 00:38:41.361000 3764082 site-packages/torch/_dynamo/guards.py:2863] [0/0] [__guards] V1004 00:38:41.379000 3764082 site-packages/torch/_dynamo/guards.py:2894] [0/0] [__guards] Guard eval latency = 9.69 us I1004 00:38:41.380000 3764082 site-packages/torch/_dynamo/pgo.py:785] [0/0] put_code_state: no cache key, skipping I1004 00:38:41.380000 3764082 site-packages/torch/_dynamo/convert_frame.py:1175] [0/0] run_gc_after_compile: running gc V1004 00:38:41.383000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: inner (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_compile.py) V1004 00:38:41.384000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: disable (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/decorators.py) V1004 00:38:41.384000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: innermost_fn (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py) V1004 00:38:41.384000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: __init__ (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py) V1004 00:38:41.384000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: __init__ (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py) V1004 00:38:41.385000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: nothing (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py) V1004 00:38:41.385000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: __call__ (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py) V1004 00:38:41.385000 3764082 site-packages/torch/_dynamo/convert_frame.py:1458] skipping: _fn (reason: in skipfiles, file: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py) ===================Traced Graph========================= I1004 00:38:41.386000 3764082 site-packages/torch/_dynamo/__init__.py:118] torch._dynamo.reset I1004 00:38:41.386000 3764082 site-packages/torch/_dynamo/__init__.py:151] torch._dynamo.reset_code_caches ===================Fusion Decisions========================= ===================Output Code========================= V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] Output code: V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # AOT ID: [\u00270_inference\u0027] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import torch V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import math V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import random V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import os V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import tempfile V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from math import inf, nan V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from cmath import nanj V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.utils import maybe_profile V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch import device, empty_strided V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton.language as tl V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] aten = torch.ops.aten V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] inductor_ops = torch.ops.inductor V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] _quantized = torch.ops._quantized V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] async_compile = AsyncCompile() V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # kernel path: /tmp/torchinductor_root/bc/cbcra7zetpvpubjqpgzmr2fv6olpakamutgikukl3oc6f6jb3qbh.py V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Topologically Sorted Source Nodes: [z, add_1], Original ATen: [aten.add] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Source node to ATen node mapping: V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # add_1 =\u003e add_1 V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # z =\u003e add V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Graph fragment: V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add, 2), kwargs = {}) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] triton_poi_fused_add_0 = async_compile.triton(\u0027triton_poi_fused_add_0\u0027, \u0027\u0027\u0027 V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] import triton.language as tl V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] triton_helpers.set_driver_to_gpu() V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] @triton_heuristics.pointwise( V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] size_hints={\u0027x\u0027: 4}, V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] filename=__file__, V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] triton_meta={\u0027signature\u0027: {\u0027in_ptr0\u0027: \u0027*fp32\u0027, \u0027in_ptr1\u0027: \u0027*fp32\u0027, \u0027out_ptr0\u0027: \u0027*fp32\u0027, \u0027xnumel\u0027: \u0027i32\u0027, \u0027XBLOCK\u0027: \u0027constexpr\u0027}, \u0027device\u0027: DeviceProperties(type=\u0027cuda\u0027, index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), \u0027constants\u0027: {}, \u0027configs\u0027: [{(0,): [[\u0027tt.divisibility\u0027, 16]], (1,): [[\u0027tt.divisibility\u0027, 16]], (2,): [[\u0027tt.divisibility\u0027, 16]]}]}, V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] inductor_meta={\u0027grid_type\u0027: \u0027Grid1D\u0027, \u0027autotune_hints\u0027: set(), \u0027kernel_name\u0027: \u0027triton_poi_fused_add_0\u0027, \u0027mutated_arg_names\u0027: [], \u0027optimize_mem\u0027: True, \u0027no_x_dim\u0027: False, \u0027num_load\u0027: 2, \u0027num_reduction\u0027: 0, \u0027backend_hash\u0027: \u0027AD014388F727234BBC364D4F9312DA1C72DBEFEDA247CD785958FB6EB1138CAC\u0027, \u0027are_deterministic_algorithms_enabled\u0027: False, \u0027assert_indirect_indexing\u0027: True, \u0027autotune_local_cache\u0027: True, \u0027autotune_pointwise\u0027: True, \u0027autotune_remote_cache\u0027: None, \u0027force_disable_caches\u0027: False, \u0027dynamic_scale_rblock\u0027: True, \u0027max_autotune\u0027: False, \u0027max_autotune_pointwise\u0027: False, \u0027min_split_scan_rblock\u0027: 256, \u0027spill_threshold\u0027: 16, \u0027store_cubin\u0027: False, \u0027tiling_scores\u0027: {\u0027x\u0027: 32}}, V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] min_elem_per_thread=0 V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] ) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] @triton.jit V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def triton_poi_fused_add_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr): V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] xnumel = 4 V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] xoffset = tl.program_id(0) * XBLOCK V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] xindex = xoffset + tl.arange(0, XBLOCK)[:] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] xmask = xindex \u003c xnumel V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] x0 = xindex V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] tmp0 = tl.load(in_ptr0 + (x0), xmask) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] tmp1 = tl.load(in_ptr1 + (x0), xmask) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] tmp2 = tmp0 + tmp1 V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] tmp3 = 2.0 V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] tmp4 = tmp2 + tmp3 V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] tl.store(out_ptr0 + (x0), tmp4, xmask) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] \u0027\u0027\u0027, device_str=\u0027cuda\u0027) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] async_compile.wait(globals()) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] del async_compile V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def call(args): V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] arg0_1, arg1_1 = args V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] args.clear() V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] assert_size_stride(arg0_1, (2, 2), (2, 1)) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] assert_size_stride(arg1_1, (2, 2), (2, 1)) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] with torch.cuda._DeviceGuard(0): V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] torch.cuda.set_device(0) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] buf0 = empty_strided_cuda((2, 2), (2, 1), torch.float32) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] # Topologically Sorted Source Nodes: [z, add_1], Original ATen: [aten.add] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] stream0 = get_raw_stream(0) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] triton_poi_fused_add_0.run(arg0_1, arg1_1, buf0, 4, stream=stream0) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] del arg0_1 V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] del arg1_1 V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] return (buf0, ) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10): V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._dynamo.testing import rand_strided V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.utils import print_performance V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] arg0_1 = rand_strided((2, 2), (2, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] arg1_1 = rand_strided((2, 2), (2, 1), device=\u0027cuda:0\u0027, dtype=torch.float32) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] fn = lambda: call([arg0_1, arg1_1]) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] return print_performance(fn, times=times, repeat=repeat) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] if __name__ == \"__main__\": V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] from torch._inductor.wrapper_benchmark import compiled_module_main V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] compiled_module_main(\u0027None\u0027, benchmark_compiled_module) V1004 00:38:41.476000 3764082 site-packages/torch/_inductor/codecache.py:1236] [0/0] [__output_code] V1004 00:38:41.483000 3764082 site-packages/torch/_inductor/codecache.py:1237] [0/0] [__output_code] Output code written to: /tmp/torchinductor_root/5h/c5hssiemdwuea5ppzag4e75dgp4oh5pub4qbdo4lnqgzy5jc55jn.py ============================================ \uacb0\ub860# \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 \uc0ac\uc6a9 \uac00\ub2a5\ud55c \uba87 \uac00\uc9c0 \ub85c\uae45 \uc635\uc158\uc744 \uc2e4\ud5d8\ud558\uc5ec TORCH_LOGS \ud658\uacbd \ubcc0\uc218\uc640 Python API\ub97c \uc18c\uac1c\ud588\uc2b5\ub2c8\ub2e4. \uc0ac\uc6a9 \uac00\ub2a5\ud55c \ubaa8\ub4e0 \uc635\uc158\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \ubcf4\ub824\uba74 \ud30c\uc774\uc36c \uc2a4\ud06c\ub9bd\ud2b8\uc5d0\uc11c import torch\ub97c \uc2e4\ud589\ud558\uace0 TORCH_LOGS\ub97c \u201chelp\u201d\ub85c \uc124\uc815\ud558\uc138\uc694. \ub2e4\ub978 \ubc29\ubc95\uc73c\ub85c\ub294, torch._logging \ubb38\uc11c \ub97c \ubcf4\uba74, \uc0ac\uc6a9 \uac00\ub2a5\ud55c \ubaa8\ub4e0 \ub85c\uae45 \uc635\uc158\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. torch.compile\uc5d0 \uad00\ud55c \ub354 \ub9ce\uc740 \uc815\ubcf4\ub294, `torch.compile \ud29c\ud1a0\ub9ac\uc5bc`_\ub97c \ubcf4\uc138\uc694. Total running time of the script: (0 minutes 7.062 seconds) Download Jupyter notebook: torch_logs.ipynb Download Python source code: torch_logs.py Download zipped: torch_logs.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/recipes/torch_logs.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>