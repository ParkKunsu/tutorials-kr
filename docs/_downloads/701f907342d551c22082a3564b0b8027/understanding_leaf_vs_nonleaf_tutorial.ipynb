{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors\n\n**Author:** [Justin Silver](https://github.com/j-silv)_\n\nThis tutorial explains the subtleties of ``requires_grad``,\n``retain_grad``, leaf, and non-leaf tensors using a simple example.\n\nBefore starting, make sure you understand [tensors and how to manipulate\nthem](https://docs.tutorials.pytorch.kr/beginner/basics/tensorqs_tutorial.html)_.\nA basic knowledge of [how autograd\nworks](https://docs.tutorials.pytorch.kr/beginner/basics/autogradqs_tutorial.html)_\nwould also be useful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nFirst, make sure [PyTorch is\ninstalled](https://pytorch.org/get-started/locally/)_ and then import\nthe necessary libraries.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we instantiate a simple network to focus on the gradients. This\nwill be an affine layer, followed by a ReLU activation, and ending with\na MSE loss between prediction and label tensors.\n\n\\begin{align}\\mathbf{y}_{\\text{pred}} = \\text{ReLU}(\\mathbf{x} \\mathbf{W} + \\mathbf{b})\\end{align}\n\n\\begin{align}L = \\text{MSE}(\\mathbf{y}_{\\text{pred}}, \\mathbf{y})\\end{align}\n\nNote that the ``requires_grad=True`` is necessary for the parameters\n(``W`` and ``b``) so that PyTorch tracks operations involving those\ntensors. We\u2019ll discuss more about this in a future\n[section](#requires-grad)_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# tensor setup\nx = torch.ones(1, 3)                      # input with shape: (1, 3)\nW = torch.ones(3, 2, requires_grad=True)  # weights with shape: (3, 2)\nb = torch.ones(1, 2, requires_grad=True)  # bias with shape: (1, 2)\ny = torch.ones(1, 2)                      # output with shape: (1, 2)\n\n# forward pass\nz = (x @ W) + b                           # pre-activation with shape: (1, 2)\ny_pred = F.relu(z)                        # activation with shape: (1, 2)\nloss = F.mse_loss(y_pred, y)              # scalar loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Leaf vs.\u00a0non-leaf tensors\n\nAfter running the forward pass, PyTorch autograd has built up a [dynamic\ncomputational\ngraph](https://docs.tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html#computational-graph)_\nwhich is shown below. This is a [Directed Acyclic Graph\n(DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph)_ which\nkeeps a record of input tensors (leaf nodes), all subsequent operations\non those tensors, and the intermediate/output tensors (non-leaf nodes).\nThe graph is used to compute gradients for each tensor starting from the\ngraph roots (outputs) to the leaves (inputs) using the [chain\nrule](https://en.wikipedia.org/wiki/Chain_rule)_ from calculus:\n\n\\begin{align}\\mathbf{y} = \\mathbf{f}_k\\bigl(\\mathbf{f}_{k-1}(\\dots \\mathbf{f}_1(\\mathbf{x}) \\dots)\\bigr)\\end{align}\n\n\\begin{align}\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} =\n   \\frac{\\partial \\mathbf{f}_k}{\\partial \\mathbf{f}_{k-1}} \\cdot\n   \\frac{\\partial \\mathbf{f}_{k-1}}{\\partial \\mathbf{f}_{k-2}} \\cdot\n   \\cdots \\cdot\n   \\frac{\\partial \\mathbf{f}_1}{\\partial \\mathbf{x}}\\end{align}\n\n.. figure:: /_static/img/understanding_leaf_vs_nonleaf/comp-graph-1.png\n   :alt: Computational graph after forward pass\n\n   Computational graph after forward pass\n\nPyTorch considers a node to be a *leaf* if it is not the result of a\ntensor operation with at least one input having ``requires_grad=True``\n(e.g.\u00a0``x``, ``W``, ``b``, and ``y``), and everything else to be\n*non-leaf* (e.g.\u00a0``z``, ``y_pred``, and ``loss``). You can verify this\nprogrammatically by probing the ``is_leaf`` attribute of the tensors:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# prints True because new tensors are leafs by convention\nprint(f\"{x.is_leaf=}\")\n\n# prints False because tensor is the result of an operation with at\n# least one input having requires_grad=True\nprint(f\"{z.is_leaf=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The distinction between leaf and non-leaf determines whether the\ntensor\u2019s gradient will be stored in the ``grad`` property after the\nbackward pass, and thus be usable for [gradient\ndescent](https://en.wikipedia.org/wiki/Gradient_descent)_. We\u2019ll cover\nthis some more in the [following section](#retain-grad)_.\n\nLet\u2019s now investigate how PyTorch calculates and stores gradients for\nthe tensors in its computational graph.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``requires_grad``\n\nTo build the computational graph which can be used for gradient\ncalculation, we need to pass in the ``requires_grad=True`` parameter to\na tensor constructor. By default, the value is ``False``, and thus\nPyTorch does not track gradients on any created tensors. To verify this,\ntry not setting ``requires_grad``, re-run the forward pass, and then run\nbackpropagation. You will see:\n\n::\n\n   >>> loss.backward()\n   RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n\nThis error means that autograd can\u2019t backpropagate to any leaf tensors\nbecause ``loss`` is not tracking gradients. If you need to change the\nproperty, you can call ``requires_grad_()`` on the tensor (notice the \\_\nsuffix).\n\nWe can sanity check which nodes require gradient calculation, just like\nwe did above with the ``is_leaf`` attribute:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"{x.requires_grad=}\") # prints False because requires_grad=False by default\nprint(f\"{W.requires_grad=}\") # prints True because we set requires_grad=True in constructor\nprint(f\"{z.requires_grad=}\") # prints True because tensor is a non-leaf node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It\u2019s useful to remember that a non-leaf tensor has\n``requires_grad=True`` by definition, since backpropagation would fail\notherwise. If the tensor is a leaf, then it will only have\n``requires_grad=True`` if it was specifically set by the user. Another\nway to phrase this is that if at least one of the inputs to a tensor\nrequires the gradient, then it will require the gradient as well.\n\nThere are two exceptions to this rule:\n\n1. Any ``nn.Module`` that has ``nn.Parameter`` will have\n   ``requires_grad=True`` for its parameters (see\n   [here](https://docs.tutorials.pytorch.kr/beginner/basics/quickstart_tutorial.html#creating-models)_)\n2. Locally disabling gradient computation with context managers (see\n   [here](https://docs.pytorch.org/docs/stable/notes/autograd.html#locally-disabling-gradient-computation)_)\n\nIn summary, ``requires_grad`` tells autograd which tensors need to have\ntheir gradients calculated for backpropagation to work. This is\ndifferent from which tensors have their ``grad`` field populated, which\nis the topic of the next section.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``retain_grad``\n\nTo actually perform optimization (e.g.\u00a0SGD, Adam, etc.), we need to run\nthe backward pass so that we can extract the gradients.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calling ``backward()`` populates the ``grad`` field of all leaf tensors\nwhich had ``requires_grad=True``. The ``grad`` is the gradient of the\nloss with respect to the tensor we are probing. Before running\n``backward()``, this attribute is set to ``None``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"{W.grad=}\")\nprint(f\"{b.grad=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You might be wondering about the other tensors in our network. Let\u2019s\ncheck the remaining leaf nodes:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# prints all None because requires_grad=False\nprint(f\"{x.grad=}\")\nprint(f\"{y.grad=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradients for these tensors haven\u2019t been populated because we did\nnot explicitly tell PyTorch to calculate their gradient\n(``requires_grad=False``).\n\nLet\u2019s now look at an intermediate non-leaf node:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"{z.grad=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch returns ``None`` for the gradient and also warns us that a\nnon-leaf node\u2019s ``grad`` attribute is being accessed. Although autograd\nhas to calculate intermediate gradients for backpropagation to work, it\nassumes you don\u2019t need to access the values afterwards. To change this\nbehavior, we can use the ``retain_grad()`` function on a tensor. This\ntells the autograd engine to populate that tensor\u2019s ``grad`` after\ncalling ``backward()``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# we have to re-run the forward pass\nz = (x @ W) + b\ny_pred = F.relu(z)\nloss = F.mse_loss(y_pred, y)\n\n# tell PyTorch to store the gradients after backward()\nz.retain_grad()\ny_pred.retain_grad()\nloss.retain_grad()\n\n# have to zero out gradients otherwise they would accumulate\nW.grad = None\nb.grad = None\n\n# backpropagation\nloss.backward()\n\n# print gradients for all tensors that have requires_grad=True\nprint(f\"{W.grad=}\")\nprint(f\"{b.grad=}\")\nprint(f\"{z.grad=}\")\nprint(f\"{y_pred.grad=}\")\nprint(f\"{loss.grad=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the same result for ``W.grad`` as before. Also note that because\nthe loss is scalar, the gradient of the loss with respect to itself is\nsimply ``1.0``.\n\nIf we look at the state of the computational graph now, we see that the\n``retains_grad`` attribute has changed for the intermediate tensors. By\nconvention, this attribute will print ``False`` for any leaf node, even\nif it requires its gradient.\n\n.. figure:: /_static/img/understanding_leaf_vs_nonleaf/comp-graph-2.png\n   :alt: Computational graph after backward pass\n\n   Computational graph after backward pass\n\nIf you call ``retain_grad()`` on a non-leaf node, it results in a no-op.\nIf we call ``retain_grad()`` on a node that has ``requires_grad=False``,\nPyTorch actually throws an error, since it can\u2019t store the gradient if\nit is never calculated.\n\n::\n\n   >>> x.retain_grad()\n   RuntimeError: can't retain_grad on Tensor that has requires_grad=False\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary table\n\nUsing ``retain_grad()`` and ``retains_grad`` only make sense for\nnon-leaf nodes, since the ``grad`` attribute will already be populated\nfor leaf tensors that have ``requires_grad=True``. By default, these\nnon-leaf nodes do not retain (store) their gradient after\nbackpropagation. We can change that by rerunning the forward pass,\ntelling PyTorch to store the gradients, and then performing\nbackpropagation.\n\nThe following table can be used as a reference which summarizes the\nabove discussions. The following scenarios are the only ones that are\nvalid for PyTorch tensors.\n\n\n\n+----------------+------------------------+------------------------+---------------------------------------------------+-------------------------------------+\n|  ``is_leaf``   |   ``requires_grad``    |   ``retains_grad``     |  ``require_grad()``                               |   ``retain_grad()``                 |\n+================+========================+========================+===================================================+=====================================+\n| ``True``       | ``False``              | ``False``              | sets ``requires_grad`` to ``True`` or ``False``   | no-op                               |\n+----------------+------------------------+------------------------+---------------------------------------------------+-------------------------------------+\n| ``True``       | ``True``               | ``False``              | sets ``requires_grad`` to ``True`` or ``False``   | no-op                               |\n+----------------+------------------------+------------------------+---------------------------------------------------+-------------------------------------+\n| ``False``      | ``True``               | ``False``              | no-op                                             | sets ``retains_grad`` to ``True``   |\n+----------------+------------------------+------------------------+---------------------------------------------------+-------------------------------------+\n| ``False``      | ``True``               | ``True``               | no-op                                             | no-op                               |\n+----------------+------------------------+------------------------+---------------------------------------------------+-------------------------------------+\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this tutorial, we covered when and how PyTorch computes gradients for\nleaf and non-leaf tensors. By using ``retain_grad``, we can access the\ngradients of intermediate tensors within autograd\u2019s computational graph.\n\nIf you would like to learn more about how PyTorch\u2019s autograd system\nworks, please visit the [references](#references)_ below. If you have\nany feedback for this tutorial (improvements, typo fixes, etc.) then\nplease use the [PyTorch Forums](https://discuss.pytorch.org/)_ and/or\nthe [issue tracker](https://github.com/pytorchkorea/tutorials-kr/issues)_ to\nreach out.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n-  [A Gentle Introduction to\n   torch.autograd](https://docs.tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html)_\n-  [Automatic Differentiation with\n   torch.autograd](https://docs.tutorials.pytorch.kr/beginner/basics/autogradqs_tutorial)_\n-  [Autograd\n   mechanics](https://docs.pytorch.org/docs/stable/notes/autograd.html)_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}