{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uae30 \uc704\ud55c \ud301\uc740 \ub2e4\uc74c\uc744 \ucc38\uc870\ud558\uc138\uc694:\n# https://tutorials.pytorch.kr/beginner/colab \n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# (beta) \ubc18\uad6c\uc870\uc801 (2:4) \ud76c\uc18c\uc131\uc744 \ud1b5\ud55c BERT \uac00\uc18d\ud654\n**\uc800\uc790**: [Jesse Cai](https://github.com/jcaip)\n**\ubc88\uc5ed**: [\uc774\ucc44\uc6b4](https://github.com/dlcodns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uac1c\uc694\n\n\ub2e4\ub978 \ud615\ud0dc\uc758 \ud76c\uc18c\uc131(sparsity)\ucc98\ub7fc, **\ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131**\uc740 \uc2e0\uacbd\ub9dd\uc758 \uba54\ubaa8\ub9ac \uc624\ubc84\ud5e4\ub4dc\uc640 \uc9c0\uc5f0 \uc2dc\uac04\uc744\n\uc904\uc774\uae30 \uc704\ud55c \ubaa8\ub378 \ucd5c\uc801\ud654 \uae30\ubc95\uc73c\ub85c, \uc77c\ubd80 \ubaa8\ub378 \uc815\ud655\ub3c4\ub294 \ud76c\uc0dd\ud558\uac8c \ub429\ub2c8\ub2e4. \uc774 \ubc29\ubc95\uc740\n**\uc138\ubd84\ud654\ub41c \uad6c\uc870\uc801 \ud76c\uc18c\uc131** \ub610\ub294 **2:4 \uad6c\uc870\uc801 \ud76c\uc18c\uc131**\uc73c\ub85c\ub3c4 \uc54c\ub824\uc838 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131\uc740 \uace0\uc720\ud55c \ud76c\uc18c\uc131 \ud328\ud134\uc5d0\uc11c \uc720\ub798\ud558\uba70, \uc5ec\uae30\uc11c 2n\uac1c\uc758 \uc694\uc18c \uc911 n\uac1c\uc758 \uc694\uc18c\uac00\n\uac00\uc9c0\uce58\uae30(prune)\ub429\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c n=2\uc778 \uacbd\uc6b0\uac00 \ub9ce\uc544 2:4 \ud76c\uc18c\uc131\uc774\ub77c\uace0 \ubd80\ub985\ub2c8\ub2e4.\n\ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131\uc740 GPU\uc5d0\uc11c \ud6a8\uc728\uc801\uc73c\ub85c \uac00\uc18d\ud654\ub420 \uc218 \uc788\uace0, \ub2e4\ub978 \ud76c\uc18c\uc131 \ud328\ud134\ub9cc\ud07c \ubaa8\ub378\n\uc815\ud655\ub3c4\ub97c \uc800\ud558\uc2dc\ud0a4\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 \ud2b9\ud788 \ud765\ubbf8\ub86d\uc2b5\ub2c8\ub2e4.\n\n[\ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131 \uc9c0\uc6d0](https://pytorch.org/docs/2.1/sparse.html#sparse-semi-structured-tensors),\n\uc774 \ub3c4\uc785\ub418\uba74\uc11c, PyTorch\ub97c \ubc97\uc5b4\ub098\uc9c0 \uc54a\uace0\ub3c4 \ubc18\uad6c\uc870\uc801 \ud76c\uc18c \ubaa8\ub378\uc744 \uac00\uc9c0\uce58\uae30\ud558\uace0 \uac00\uc18d\ud654\ud560\n\uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 \uc774 \uacfc\uc815\uc744 \uc124\uba85\ud560 \uac83\uc785\ub2c8\ub2e4.\n\n<img src=\"file://../../_static/img/pruning_flow.jpg\">\n\n\ud29c\ud1a0\ub9ac\uc5bc\uc774 \ub05d\ub098\uba74 BERT \uc9c8\ubb38-\uc751\ub2f5 \ubaa8\ub378\uc744 2:4 \ud76c\uc18c\ud654\ud558\uc5ec \uac70\uc758 \ubaa8\ub4e0 F1 \uc190\uc2e4\uc744 \ud68c\ubcf5\ud55c\n\uc0c1\ud0dc(86.92\uc758 \ubc00\uc9d1 \ubaa8\ub378 vs 86.48\uc758 \ud76c\uc18c \ubaa8\ub378)\ub85c \ubbf8\uc138 \uc870\uc815\ud560 \uac83\uc785\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9\uc73c\ub85c\n\uc774 2:4 \ud76c\uc18c \ubaa8\ub378\uc744 \ucd94\ub860\uc744 \uc704\ud574 \uac00\uc18d\ud654\ud558\uc5ec 1.3\ubc30 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud560 \uac83\uc785\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uc694\uad6c\uc0ac\ud56d\n\n-  PyTorch >= 2.1.\n-  \ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131\uc744 \uc9c0\uc6d0\ud558\ub294 NVIDIA GPU(Compute Capability 8.0+)\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 \ucd08\ubcf4\uc790\uc5d0\uac8c \ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131 \ubc0f \uc77c\ubc18\uc801\uc778 \ud76c\uc18c\uc131\uc744 \ub9de\ucda4 \uc124\uba85\ud569\ub2c8\ub2e4.\n\uc774\ubbf8 2:4 \ud76c\uc18c \ubaa8\ub378\uc744 \ubcf4\uc720\ud55c \uc0ac\uc6a9\uc790\uc5d0\uac8c\ub294 ``to_sparse_semi_structured``\ub97c \uc0ac\uc6a9\ud558\uc5ec\n\ucd94\ub860\uc744 \uc704\ud55c ``nn.Linear`` \ub808\uc774\uc5b4\ub97c \uac00\uc18d\ud654\ud558\ub294 \uac83\uc774 \ub9e4\uc6b0 \uac04\ub2e8\ud569\ub2c8\ub2e4. \ub2e4\uc74c\uc740 \uadf8 \uc608\uc2dc\uc785\ub2c8\ub2e4:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensor\nfrom torch.utils.benchmark import Timer\nSparseSemiStructuredTensor._FORCE_CUTLASS = True\n\n# Linear \uac00\uc911\uce58\ub97c 2:4 \ud76c\uc18c\uc131\uc73c\ub85c \ub9c8\uc2a4\ud0b9\nmask = torch.Tensor([0, 0, 1, 1]).tile((3072, 2560)).cuda().bool()\nlinear = torch.nn.Linear(10240, 3072).half().cuda().eval()\nlinear.weight = torch.nn.Parameter(mask * linear.weight)\n\nx = torch.rand(3072, 10240).half().cuda()\n\nwith torch.inference_mode():\n    dense_output = linear(x)\n    dense_t = Timer(stmt=\"linear(x)\",\n                    globals={\"linear\": linear,\n                             \"x\": x}).blocked_autorange().median * 1e3\n\n    # SparseSemiStructuredTensor\ub97c \ud1b5\ud574 \uac00\uc18d\ud654\n    linear.weight = torch.nn.Parameter(to_sparse_semi_structured(linear.weight))\n\n    sparse_output = linear(x)\n    sparse_t = Timer(stmt=\"linear(x)\",\n                    globals={\"linear\": linear,\n                             \"x\": x}).blocked_autorange().median * 1e3\n\n    # \ud76c\uc18c \ubc0f \ubc00\uc9d1 \ud589\ub82c \uacf1\uc148\uc740 \uc218\uce58\uc801\uc73c\ub85c \ub3d9\uc77c\ud568\n    # A100 80GB\uc5d0\uc11c, \ub2e4\uc74c\uacfc \uac19\uc740 \uacb0\uacfc\ub97c \ud655\uc778: `Dense: 0.870ms Sparse: 0.630ms | Speedup: 1.382x`\n    assert torch.allclose(sparse_output, dense_output, atol=1e-3)\n    print(f\"Dense: {dense_t:.3f}ms Sparse: {sparse_t:.3f}ms | Speedup: {(dense_t / sparse_t):.3f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131\uc740 \uc5b4\ub5a4 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294\uac00?\n\n\ud76c\uc18c\uc131\uc758 \uc77c\ubc18\uc801\uc778 \ubaa9\uc801\uc740 \uac04\ub2e8\ud569\ub2c8\ub2e4: \ub124\ud2b8\uc6cc\ud06c \ub0b4\uc5d0 0\uc774 \uc788\ub294 \uacbd\uc6b0,\n\ud574\ub2f9 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc800\uc7a5\ud558\uac70\ub098 \uacc4\uc0b0\ud558\uc9c0 \uc54a\uc74c\uc73c\ub85c\uc368 \ud6a8\uc728\uc131\uc744 \ucd5c\uc801\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uadf8\ub7ec\ub098 \ud76c\uc18c\uc131\uc758 \uad6c\uccb4\uc801\uc778 \uad6c\ud604\uc740 \uae4c\ub2e4\ub86d\uc2b5\ub2c8\ub2e4. \ub9e4\uac1c\ubcc0\uc218\ub97c 0\uc73c\ub85c \ub9cc\ub4dc\ub294 \uac83\ub9cc\uc73c\ub85c\ub294\n\uae30\ubcf8\uc801\uc73c\ub85c \ubaa8\ub378\uc758 \uc9c0\uc5f0 \uc2dc\uac04 / \uba54\ubaa8\ub9ac \uc624\ubc84\ud5e4\ub4dc\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\n\uadf8 \uc774\uc720\ub294 dense tensor\uac00 \uc5ec\uc804\ud788 \uac00\uc9c0\uce58\uae30\ub41c(0\uc778) \uc694\uc18c\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uc73c\uba70,\n\ubc00\uc9d1 \ud589\ub82c \uacf1\uc148 \ucee4\ub110\uc774 \uc774\ub7ec\ud55c \uc694\uc18c\uc5d0 \ub300\ud574 \uacc4\uc18d \uc5f0\uc0b0\uc744 \uc218\ud589\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \uc131\ub2a5 \ud5a5\uc0c1\uc744\n\uc2e4\ud604\ud558\ub824\uba74, \ubc00\uc9d1 \ucee4\ub110\uc744 \uac00\uc9c0\uce58\uae30\ub41c \uc694\uc18c\uc758 \uacc4\uc0b0\uc744 \uac74\ub108\ub6f0\ub294 \ud76c\uc18c \ucee4\ub110\ub85c \uad50\uccb4\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n\uc774\ub97c \uc704\ud574, \ud76c\uc18c \ucee4\ub110\uc740 \uac00\uc9c0\uce58\uae30\ub41c \uc694\uc18c\ub97c \uc800\uc7a5\ud558\uc9c0 \uc54a\uace0, \uc9c0\uc815\ub41c \uc694\uc18c\ub97c \uc555\ucd95\ub41c \ud615\uc2dd\uc73c\ub85c\n\uc800\uc7a5\ud558\ub294 \ud76c\uc18c \ud589\ub82c\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131\uc758 \uacbd\uc6b0, \uc6d0\ub798 \ub9e4\uac1c\ubcc0\uc218\uc758 \uc815\ud655\ud788 \uc808\ubc18\uacfc \uc694\uc18c\uac00 \uc5b4\ub5bb\uac8c\n\ubc30\uc5f4\ub418\uc5c8\ub294\uc9c0\uc5d0 \ub300\ud55c \uc555\ucd95\ub41c \uba54\ud0c0\ub370\uc774\ud130\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4.\n\n<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2023/06/2-4-structured-sparsity-pattern.png\" align=\"center\" width=\"80%\">\n\n   Image sourced from [NVIDIA blog post](https://developer.nvidia.com/blog/structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines/) on semi-structured sparsity.\n\n\ud76c\uc18c \ub808\uc774\uc544\uc6c3\uc5d0\ub294 \uac01\uae30 \ub2e4\ub978 \uc7a5\uc810\uacfc \ub2e8\uc810\uc744 \uac00\uc9c4 \uc5ec\ub7ec \uac00\uc9c0\uac00 \uc788\uc2b5\ub2c8\ub2e4. 2:4 \ubc18\uad6c\uc870\uc801\n\ud76c\uc18c \ub808\uc774\uc544\uc6c3\uc740 \ud765\ubbf8\ub85c\uc6b4 \ub450 \uac00\uc9c0 \uc774\uc720\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\n* \uc774\uc804\uc758 \ud76c\uc18c \ud615\uc2dd\uacfc \ub2ec\ub9ac \ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131\uc740 GPU\uc5d0\uc11c \ud6a8\uc728\uc801\uc73c\ub85c \uac00\uc18d\ub418\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n  2020\ub144 NVIDIA\ub294 Ampere \uc544\ud0a4\ud14d\ucc98\ub97c \ud1b5\ud574 \ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131\uc744 \uc704\ud55c \ud558\ub4dc\uc6e8\uc5b4 \uc9c0\uc6d0\uc744 \ub3c4\uc785\ud588\uc73c\uba70,\n  CUTLASS [cuSPARSELt](https://docs.nvidia.com/cuda/cusparselt/index.html)_\n  cuSPARSELt\ub97c \ud1b5\ud574 \ube60\ub978 \ud76c\uc18c \ucee4\ub110\ub3c4 \ucd9c\uc2dc\ud588\uc2b5\ub2c8\ub2e4.\n\n* \ub3d9\uc2dc\uc5d0 \ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131\uc740 \ub2e4\ub978 \ud76c\uc18c \ud615\uc2dd\uc5d0 \ube44\ud574 \ubaa8\ub378 \uc815\ud655\ub3c4\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc774 \ub35c\ud55c \uacbd\ud5a5\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n  \ud2b9\ud788 \ub354 \ubc1c\uc804\ub41c \uac00\uc9c0\uce58\uae30 \ubc0f \ubbf8\uc138 \uc870\uc815 \ubc29\ubc95\uc744 \uace0\ub824\ud560 \ub54c \uadf8\ub807\uc2b5\ub2c8\ub2e4.\n  NVIDIA\uac00 \uacf5\uac1c\ud55c [\ubc31\uc11c](https://arxiv.org/abs/2104.08378)\n  \uc5d0\uc11c 2:4 \ud76c\uc18c\uc131\uc744 \ubaa9\ud45c\ub85c \ud55c \ub2e8\uc21c\ud55c \ud06c\uae30 \uae30\uc900 \uac00\uc9c0\uce58\uae30(magnitude pruning) \ud6c4 \ubaa8\ub378\uc744\n  \uc7ac\ud559\uc2b5\ud558\uba74 \uac70\uc758 \ub3d9\uc77c\ud55c \ubaa8\ub378 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4.\n\n\ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131\uc740 \uc774\ub860\uc801\uc73c\ub85c 2\ubc30\uc758 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \uc81c\uacf5\ud558\uba74\uc11c\ub3c4 \ud76c\uc18c\uc131 \uc218\uc900\uc774 \ub0ae\uace0(50%), \ubaa8\ub378\n\uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud558\uae30\uc5d0 \ucda9\ubd84\ud788 \uc138\ubc00\ud55c \uc801\uc808\ud55c \uade0\ud615\uc810\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\n+---------------------+-------------+--------+------------+-------------+\n| \ub124\ud2b8\uc6cc\ud06c             | \ub370\uc774\ud130 \uc138\ud2b8  | \uba54\ud2b8\ub9ad | Dense FP16 | Sparse FP16 |\n+=====================+=============+========+============+=============+\n| ResNet-50           | ImageNet    | Top-1  | 76.1       | 76.2        |\n+---------------------+-------------+--------+------------+-------------+\n| ResNeXt-101_32x8d   | ImageNet    | Top-1  | 79.3       | 79.3        |\n+---------------------+-------------+--------+------------+-------------+\n| Xception            | ImageNet    | Top-1  | 79.2       | 79.2        |\n+---------------------+-------------+--------+------------+-------------+\n| SSD-RN50            | COCO2017    | bbAP   | 24.8       | 24.8        |\n+---------------------+-------------+--------+------------+-------------+\n| MaskRCNN-RN50       | COCO2017    | bbAP   | 37.9       | 37.9        |\n+---------------------+-------------+--------+------------+-------------+\n| FairSeq Transformer | EN-DE WMT14 | BLEU   | 28.2       | 28.5        |\n+---------------------+-------------+--------+------------+-------------+\n| BERT-Large          | SQuAD v1.1  | F1     | 91.9       | 91.9        |\n+---------------------+-------------+--------+------------+-------------+\n\n\ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131\uc740 \uc6cc\ud06c\ud50c\ub85c \uad00\uc810\uc5d0\uc11c \ucd94\uac00\uc801\uc778 \uc7a5\uc810\uc774 \uc788\uc2b5\ub2c8\ub2e4. \ud76c\uc18c\uc131 \uc218\uc900\uc774 50%\ub85c \uace0\uc815\ub418\uc5b4\n\uc788\uc5b4 \ubaa8\ub378\uc744 \ud76c\uc18c\ud654\ud558\ub294 \ubb38\uc81c\ub97c \ub450 \uac00\uc9c0 \ubcc4\uac1c\uc758 \ud558\uc704 \ubb38\uc81c\ub85c \ubd84\ud574\ud558\uae30\uac00 \ub354 \uc27d\uc2b5\ub2c8\ub2e4.\n\n- \uc815\ud655\ub3c4 - 2:4 \ud76c\uc18c \uac00\uc911\uce58 \uc138\ud2b8\ub97c \ucc3e\uc544 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4 \uc800\ud558\ub97c \ucd5c\uc18c\ud654\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc740 \ubb34\uc5c7\uc778\uac00\uc694?\n\n- \uc131\ub2a5: \ucd94\ub860 \ubc0f \uba54\ubaa8\ub9ac \uc624\ubc84\ud5e4\ub4dc\ub97c \uc904\uc774\uae30 \uc704\ud574 2:4 \ud76c\uc18c \uac00\uc911\uce58\ub97c \uc5b4\ub5bb\uac8c \uac00\uc18d\ud654\ud560 \uc218 \uc788\ub294\uac00?\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\begin{align}\\begin{bmatrix}\n      1 & 1 & 0 & 0 \\\\\n      0 & 0 & 1 & 1 \\\\\n      1 & 0 & 0 & 0 \\\\\n      0 & 0 & 1 & 1 \\\\\n      \\end{bmatrix}\\end{align}\n\n\uc774 \ub450 \ubb38\uc81c \uc0ac\uc774\uc758 \uc790\uc5f0\uc2a4\ub7ec\uc6b4 \uc5f0\uacb0\uc810\uc740 0\uc73c\ub85c \ub41c \ubc00\uc9d1 \ud150\uc11c\uc785\ub2c8\ub2e4. \uc6b0\ub9ac\uc758 \ucd94\ub860 \uc194\ub8e8\uc158\uc740 \uc774\ub7ec\ud55c \ud615\uc2dd\uc758\n\ud150\uc11c\ub97c \uc555\ucd95\ud558\uace0 \uac00\uc18d\ud558\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ud65c\ubc1c\ud55c \uc5f0\uad6c \ubd84\uc57c\uc774\uae30 \ub54c\ubb38\uc5d0 \ub9ce\uc740 \uc0ac\uc6a9\uc790\uac00 \ub9de\ucda4\ud615 \ub9c8\uc2a4\ud0b9\n\uc194\ub8e8\uc158\uc744 \uace0\uc548\ud560 \uac83\uc73c\ub85c \uc608\uc0c1\ub429\ub2c8\ub2e4.\n\n\ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131\uc5d0 \ub300\ud574 \uc870\uae08 \ub354 \ubc30\uc6e0\uc73c\ub2c8, \uc774\uc81c \uc9c8\ubb38 \uc751\ub2f5 \uc791\uc5c5\uc778 SQuAD\uc5d0 \ub300\ud574 \ud559\uc2b5\ub41c BERT \ubaa8\ub378\uc5d0 \uc774\ub97c\n\uc801\uc6a9\ud574 \ubd05\uc2dc\ub2e4.\n\n## \uc18c\uac1c & \uc124\uc815\n\n\ud544\uc694\ud55c \ubaa8\ub4e0 \ud328\ud0a4\uc9c0\ub97c \ubd88\ub7ec\uc624\ub294 \uac83\uc73c\ub85c \uc2dc\uc791\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ub9cc\uc57d Google Colab\uc5d0\uc11c \uc2e4\ud589 \uc911\uc774\ub77c\uba74, \ub2e4\uc74c \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\ud558\uc138\uc694:\n\n# .. code-block: python\n#\n#    !pip install datasets transformers evaluate accelerate pandas\n#\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nimport collections\nimport datasets\nimport evaluate\nimport numpy as np\nimport torch\nimport torch.utils.benchmark as benchmark\nfrom torch import nn\nfrom torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensor\nfrom torch.ao.pruning import WeightNormSparsifier\nimport transformers\n\n# ``cuSPARSELt``\uac00 \uc0ac\uc6a9 \ubd88\uac00\ub2a5\ud55c \uacbd\uc6b0, \uac15\uc81c\ub85c CUTLASS\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\nSparseSemiStructuredTensor._FORCE_CUTLASS = True\ntorch.manual_seed(100)\n\n# \uae30\ubcf8 \uc7a5\uce58\ub97c \"cuda:0\"\uc73c\ub85c \uc124\uc815\ud569\ub2c8\ub2e4.\ntorch.set_default_device(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc6b0\ub9ac\uac00 \ub2e4\ub8e8\uace0 \uc788\ub294 \ub370\uc774\ud130\uc14b/\uc791\uc5c5\uc5d0 \ud2b9\ud654\ub41c \uba87 \uac00\uc9c0 \ubcf4\uc870 \ud568\uc218\ub3c4\n\uc815\uc758\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ud568\uc218\ub4e4\uc740 Hugging Face \ucf54\uc2a4\uc758\n[\uc774 \uc790\ub8cc](https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt)_\n\ub97c \ucc38\uace0\ud558\uc5ec \uc218\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def preprocess_validation_function(examples, tokenizer):\n    inputs = tokenizer(\n        [q.strip() for q in examples[\"question\"]],\n        examples[\"context\"],\n        max_length=384,\n        truncation=\"only_second\",\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    example_ids = []\n\n    for i in range(len(inputs[\"input_ids\"])):\n        sample_idx = sample_map[i]\n        example_ids.append(examples[\"id\"][sample_idx])\n        sequence_ids = inputs.sequence_ids(i)\n        offset = inputs[\"offset_mapping\"][i]\n        inputs[\"offset_mapping\"][i] = [\n            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n        ]\n\n    inputs[\"example_id\"] = example_ids\n    return inputs\n\n\ndef preprocess_train_function(examples, tokenizer):\n    inputs = tokenizer(\n        [q.strip() for q in examples[\"question\"]],\n        examples[\"context\"],\n        max_length=384,\n        truncation=\"only_second\",\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs[\"offset_mapping\"]\n    answers = examples[\"answers\"]\n    start_positions = []\n    end_positions = []\n\n    for i, (offset, answer) in enumerate(zip(offset_mapping, answers)):\n        start_char = answer[\"answer_start\"][0]\n        end_char = start_char + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # \ubb38\ub9e5\uc758 \uc2dc\uc791\uacfc \ub05d\uc744 \ucc3e\uae30\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n\n        # \ub2f5\ubcc0\uc774 \ubb38\ub9e5 \uc548\uc5d0 \uc644\uc804\ud788 \ud3ec\ud568\ub418\uc9c0 \uc54a\ub294 \uacbd\uc6b0, (0, 0)\uc73c\ub85c \ub808\uc774\ube14 \uc9c0\uc815\n        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 \uc2dc\uc791 \ubc0f \ub05d \ud1a0\ud070 \uc704\uce58\ub97c \uc9c0\uc815\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\n\ndef compute_metrics(start_logits, end_logits, features, examples):\n    n_best = 20\n    max_answer_length = 30\n    metric = evaluate.load(\"squad\")\n\n    example_to_features = collections.defaultdict(list)\n    for idx, feature in enumerate(features):\n        example_to_features[feature[\"example_id\"]].append(idx)\n\n    predicted_answers = []\n    # \uc608\ub97c \ub4e4\uc5b4 ``tqdm``(examples)\uc5d0\uc11c:\n    for example in examples:\n        example_id = example[\"id\"]\n        context = example[\"context\"]\n        answers = []\n\n        # \ud574\ub2f9 \uc608\uc81c\uc640 \uad00\ub828\ub41c \ubaa8\ub4e0 \ud2b9\uc131(feature)\uc744 \ubc18\ubcf5\n        for feature_index in example_to_features[example_id]:\n            start_logit = start_logits[feature_index]\n            end_logit = end_logits[feature_index]\n            offsets = features[feature_index][\"offset_mapping\"]\n\n            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # \ubb38\ub9e5\uc5d0 \uc644\uc804\ud788 \ud3ec\ud568\ub418\uc9c0 \uc54a\uc740 \ub2f5\ubcc0\uc740 \uac74\ub108\ub700\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    # \uae38\uc774\uac00 0\ubcf4\ub2e4 \uc791\uac70\ub098\n                    # max_answer_length\ubcf4\ub2e4 \ud070 \ub2f5\ubcc0\uc740 \uac74\ub108\ub700\n                    if (\n                        end_index < start_index\n                        or end_index - start_index + 1 > max_answer_length\n                    ):\n                        continue\n\n                    answer = {\n                        \"text\": context[\n                            offsets[start_index][0] : offsets[end_index][1]\n                        ],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                    answers.append(answer)\n\n        # \uac00\uc7a5 \ub192\uc740 \uc810\uc218\ub97c \uac00\uc9c4 \ub2f5\ubcc0\uc744 \uc120\ud0dd\n        if len(answers) > 0:\n            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n            predicted_answers.append(\n                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n            )\n        else:\n            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n\n    theoretical_answers = [\n        {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples\n    ]\n    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc81c \uc774\ub7ec\ud55c \ud568\uc218\ub4e4\uc774 \uc815\uc758\ub418\uc5c8\uc73c\ubbc0\ub85c, \ubaa8\ub378\uc758 \ubca4\uce58\ub9c8\ud06c\ub97c \ub3c4\uc640\uc904 \ucd94\uac00\uc801\uc778 \ubcf4\uc870 \ud568\uc218 \ud558\ub098\ub9cc \ub354 \ud544\uc694\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def measure_execution_time(model, batch_sizes, dataset):\n    dataset_for_model = dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n    dataset_for_model.set_format(\"torch\")\n    batch_size_to_time_sec = {}\n    for batch_size in batch_sizes:\n        batch = {\n            k: dataset_for_model[k][:batch_size].cuda()\n            for k in dataset_for_model.column_names\n        }\n\n        with torch.no_grad():\n            baseline_predictions = model(**batch)\n            timer = benchmark.Timer(\n                stmt=\"model(**batch)\", globals={\"model\": model, \"batch\": batch}\n            )\n            p50 = timer.blocked_autorange().median * 1000\n            batch_size_to_time_sec[batch_size] = p50\n\n            model_c = torch.compile(model, fullgraph=True)\n            timer = benchmark.Timer(\n                stmt=\"model(**batch)\", globals={\"model\": model_c, \"batch\": batch}\n            )\n            p50 = timer.blocked_autorange().median * 1000\n            batch_size_to_time_sec[f\"{batch_size}_compile\"] = p50\n            new_predictions = model_c(**batch)\n\n    return batch_size_to_time_sec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378\uacfc \ud1a0\ud06c\ub098\uc774\uc800\ub97c \ub85c\ub4dc\ud55c \ud6c4, \ub370\uc774\ud130\uc14b\uc744 \uc124\uc815\ud558\uba74\uc11c \uc2dc\uc791\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ubaa8\ub378 \ubd88\ub7ec\uc624\uae30\nmodel_name = \"bert-base-cased\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\nmodel = transformers.AutoModelForQuestionAnswering.from_pretrained(model_name)\nprint(f\"Loading tokenizer: {model_name}\")\nprint(f\"Loading model: {model_name}\")\n\n# \ud559\uc2b5 \ubc0f \uac80\uc99d \ub370\uc774\ud130\uc14b \uc124\uc815\nsquad_dataset = datasets.load_dataset(\"squad\")\ntokenized_squad_dataset = {}\ntokenized_squad_dataset[\"train\"] = squad_dataset[\"train\"].map(\n    lambda x: preprocess_train_function(x, tokenizer), batched=True\n)\ntokenized_squad_dataset[\"validation\"] = squad_dataset[\"validation\"].map(\n    lambda x: preprocess_validation_function(x, tokenizer),\n    batched=True,\n    remove_columns=squad_dataset[\"train\"].column_names,\n)\ndata_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \uae30\uc900 \uc131\ub2a5 \uc124\uc815\n\n\ub2e4\uc74c\uc73c\ub85c, SQuAD \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ubaa8\ub378\uc758 \ube60\ub978 \uae30\uc900 \uc131\ub2a5\uc744 \ud559\uc2b5\uc2dc\ucf1c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc774 \uc791\uc5c5\uc740 \ubaa8\ub378\uc774 \uc8fc\uc5b4\uc9c4\n\ubb38\ub9e5(\uc704\ud0a4\ud53c\ub514\uc544 \uae30\uc0ac)\uc5d0\uc11c \uc8fc\uc5b4\uc9c4 \uc9c8\ubb38\uc5d0 \ub300\ud55c \ub2f5\ubcc0\uc774 \ub418\ub294 \ud14d\uc2a4\ud2b8\uc758 \ubc94\uc704 \ub610\ub294 \uad6c\uac04\uc744 \uc2dd\ubcc4\ud558\ub3c4\ub85d\n\uc694\uad6c\ud569\ub2c8\ub2e4. \ub2e4\uc74c \ucf54\ub4dc\ub97c \uc2e4\ud589\ud558\uba74 F1 \uc810\uc218\ub294 86.9\uac00 \ub098\uc635\ub2c8\ub2e4. \uc774\ub294 \ubcf4\uace0\ub41c NVIDIA \uc810\uc218\uc640 \ub9e4\uc6b0 \uac00\uae5d\uace0,\n\ucc28\uc774\ub294 \uc544\ub9c8\ub3c4 BERT-base\uc640 BERT-large \ub610\ub294 \ubbf8\uc138 \uc870\uc815 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ub54c\ubb38\uc77c \uac83\uc785\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training_args = transformers.TrainingArguments(\n    \"trainer\",\n    num_train_epochs=1,\n    lr_scheduler_type=\"constant\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=256,\n    logging_steps=50,\n    # \ud29c\ud1a0\ub9ac\uc5bc \uc2e4\ud589\uc744 \uc704\ud55c \ucd5c\ub300 \ub2e8\uacc4 \uc81c\ud55c. \ubcf4\uace0\ub41c \uc815\ud655\ub3c4 \uc218\uce58\ub97c \ubcf4\ub824\uba74 \uc544\ub798 \uc904\uc744 \uc0ad\uc81c\ud558\uc138\uc694.\n    max_steps=500,\n    report_to=None,\n)\n\ntrainer = transformers.Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_squad_dataset[\"train\"],\n    eval_dataset=tokenized_squad_dataset[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\n\n# \ud3c9\uac00\ub97c \uc704\ud55c \ube44\uad50 \ubc30\uce58 \ud06c\uae30\nbatch_sizes = [4, 16, 64, 256]\n# 2:4 \ud76c\uc18c\uc131\uc740 fp16\uc744 \ud544\uc694\ub85c \ud558\ubbc0\ub85c, \uacf5\uc815\ud55c \ube44\uad50\ub97c \uc704\ud574 \uc5ec\uae30\uc5d0\uc11c \uce90\uc2a4\ud305\ud568\nwith torch.autocast(\"cuda\"):\n    with torch.no_grad():\n        predictions = trainer.predict(tokenized_squad_dataset[\"validation\"])\n        start_logits, end_logits = predictions.predictions\n        fp16_baseline = compute_metrics(\n            start_logits,\n            end_logits,\n            tokenized_squad_dataset[\"validation\"],\n            squad_dataset[\"validation\"],\n        )\n        fp16_time = measure_execution_time(\n            model,\n            batch_sizes,\n            tokenized_squad_dataset[\"validation\"],\n        )\n\nprint(\"fp16\", fp16_baseline)\nprint(\"cuda_fp16 time\", fp16_time)\n\nimport pandas as pd\ndf = pd.DataFrame(trainer.state.log_history)\ndf.plot.line(x='step', y='loss', title=\"Loss vs. # steps\", ylabel=\"loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT\ub97c 2:4 \ud76c\uc18c\uc131\uc73c\ub85c \uac00\uc9c0\uce58\uae30\n\n\uc774\uc81c \uae30\uc900 \uc131\ub2a5\uc744 \uc124\uc815\ud588\uc73c\ub2c8, BERT\ub97c \uac00\uc9c0\uce58\uae30\ud560 \ucc28\ub840\uc785\ub2c8\ub2e4. \uac00\uc9c0\uce58\uae30\uc5d0\ub294 \uc5ec\ub7ec \uac00\uc9c0 \uc804\ub7b5\uc774 \uc788\uc9c0\ub9cc,\n\uac00\uc7a5 \uc77c\ubc18\uc801\uc778 \ubc29\ubc95 \uc911 \ud558\ub098\ub294 **\ud06c\uae30 \uae30\ubc18 \uac00\uc9c0\uce58\uae30**\ub85c, \uc774\ub294 L1 norm\uc774 \uac00\uc7a5\n\ub0ae\uc740 \uac00\uc911\uce58\ub97c \uc81c\uac70\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. NVIDIA\ub294 \ubaa8\ub4e0 \uacb0\uacfc\uc5d0\uc11c \ud06c\uae30 \uae30\ubc18 \uac00\uc9c0\uce58\uae30\ub97c \uc0ac\uc6a9\ud588\uc73c\uba70, \uc774\ub294\n\uc77c\ubc18\uc801\uc778 \uae30\uc900 \ubc29\ubc95\uc785\ub2c8\ub2e4.\n\n\uc774\ub97c \uc704\ud574 \uc6b0\ub9ac\ub294 ``torch.ao.pruning`` \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \uc774 \ud328\ud0a4\uc9c0\uc5d0\ub294 \uac00\uc911\uce58-norm \ud76c\uc18c\ud654\n\ub3c4\uad6c\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ud76c\uc18c\ud654 \ub3c4\uad6c\ub294 \ubaa8\ub378\uc758 \uac00\uc911\uce58 \ud150\uc11c\uc5d0 \ub9c8\uc2a4\ud06c \ub9e4\uac1c\ubcc0\uc218\ud654\ub97c \uc801\uc6a9\ud558\uc5ec\n\uc791\ub3d9\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uac00\uc9c0\uce58\uae30\ub41c \uac00\uc911\uce58\ub97c \ub9c8\uc2a4\ud0b9\ud558\uc5ec \ud76c\uc18c\uc131\uc744 \uc2dc\ubbac\ub808\uc774\uc158\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ub610\ud55c, \ubaa8\ub378\uc758 \uc5b4\ub290 \ub808\uc774\uc5b4\uc5d0 \ud76c\uc18c\uc131\uc744 \uc801\uc6a9\ud560\uc9c0 \uacb0\uc815\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774 \uacbd\uc6b0\uc5d0\ub294 \uac01\uac01\uc758 \ud14c\uc2a4\ud06c \ud5e4\ub4dc \ucd9c\ub825\uc744\n\uc81c\uc678\ud55c \ubaa8\ub4e0 ``nn.Linear`` \ub808\uc774\uc5b4\uc5d0 \uc801\uc6a9\ud569\ub2c8\ub2e4. \uc774\ub294 \ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131(semi-structured sparsity)\uc774\n[\ud615\uc0c1 \uc81c\uc57d](https://pytorch.org/docs/2.1/sparse.html#constructing-sparse-semi-structured-tensors)\n\uc744 \uac00\uc9c0\uae30 \ub54c\ubb38\uc774\uba70, \uac01\uac01\uc758 task ``nn.Linear`` \ub808\uc774\uc5b4\ub294 \uc774\ub7ec\ud55c \uc81c\uc57d\uc744 \ucda9\uc871\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sparsifier = WeightNormSparsifier(\n    # \ubaa8\ub4e0 \ube14\ub85d\uc5d0 \ud76c\uc18c\uc131 \uc801\uc6a9\n    sparsity_level=1.0,\n    # 4\uac1c\uc758 \uc694\uc18c\uac00 \ud558\ub098\uc758 \ube14\ub85d\uc758 \ud615\ud0dc\n    sparse_block_shape=(1, 4),\n    # 4\uac1c\uc758 \ube14\ub85d\ub9c8\ub2e4 \ub450 \uac1c\uc758 0\uc774 \ud3ec\ud568\ub428\n    zeros_per_block=2\n)\n\n# BERT \ubaa8\ub378\uc5d0 ``nn.Linear``\uac00 \uc788\ub294 \uacbd\uc6b0 \uc124\uc815\uc5d0 \ucd94\uac00\nsparse_config = [\n    {\"tensor_fqn\": f\"{fqn}.weight\"}\n    for fqn, module in model.named_modules()\n    if isinstance(module, nn.Linear) and \"layer\" in fqn\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218\ud654\ub294 \uccab \ubc88\uc9f8 \ub2e8\uacc4\ub294 \ubaa8\ub378\uc758 \uac00\uc911\uce58\ub97c \ub9c8\uc2a4\ud0b9\ud558\uae30 \uc704\ud55c \ub9e4\uac1c\ubcc0\uc218\ud654\ub97c \uc0bd\uc785\ud558\ub294 \uac83\uc785\ub2c8\ub2e4.\n\uc774\ub294 \uc900\ube44 \ub2e8\uacc4\uc5d0\uc11c \uc218\ud589\ub429\ub2c8\ub2e4. \uc774\ub807\uac8c \ud558\uba74 ``.weight``\uc5d0 \uc811\uadfc\ud560 \ub54c\ub9c8\ub2e4 \ub300\uc2e0 ``mask * weight``\ub97c\n\uc5bb\uac8c \ub429\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ubaa8\ub378\uc744 \uc900\ube44\ud558\uace0, \ud559\uc2b5\uc744 \uc704\ud55c \uac00\uc9dc \ud76c\uc18c\uc131 \ub9e4\uac1c\ubcc0\uc218\uc218\ud654\ub97c \uc0bd\uc785\ud569\ub2c8\ub2e4.\nsparsifier.prepare(model, sparse_config)\nprint(model.bert.encoder.layer[0].output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uadf8 \ub2e4\uc74c, \ub2e8\uc77c \uac00\uc9c0\uce58\uae30 \ub2e8\uacc4\ub97c \uc218\ud589\ud569\ub2c8\ub2e4. \ubaa8\ub4e0 \uac00\uc9c0\uce58\uae30 \ub3c4\uad6c(pruner)\ub294 \uac00\uc9c0\uce58\uae30 \ub3c4\uad6c\uc758 \uad6c\ud604\n\ub17c\ub9ac\uc5d0 \ub530\ub77c \ub9c8\uc2a4\ud06c\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\ub294 ``update_mask()`` \uba54\uc11c\ub4dc\ub97c \uad6c\ud604\ud569\ub2c8\ub2e4. \uc774 \ub2e8\uacc4 \uba54\uc11c\ub4dc\ub294\n\ud76c\uc18c\uc131 \uc124\uc815(sparse config)\uc5d0\uc11c \uc9c0\uc815\ub41c \uac00\uc911\uce58\uc5d0 \ub300\ud574 \uc774 ``update_mask`` \ud568\uc218\ub97c \ud638\ucd9c\ud569\ub2c8\ub2e4.\n\n\ub610\ud55c \ubaa8\ub378\uc744 \ud3c9\uac00\ud558\uc5ec \ubbf8\uc138 \uc870\uc815/\uc7ac\ud559\uc2b5 \uc5c6\uc774 \uac00\uc9c0\uce58\uae30(zero-shot) \ub610\ub294 \uac00\uc9c0\uce58\uae30\uc758 \uc815\ud655\ub3c4 \uc800\ud558\ub97c \ubcf4\uc5ec\uc904 \uac83\uc785\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sparsifier.step()\nwith torch.autocast(\"cuda\"):\n    with torch.no_grad():\n        predictions = trainer.predict(tokenized_squad_dataset[\"validation\"])\n    pruned = compute_metrics(\n        *predictions.predictions,\n        tokenized_squad_dataset[\"validation\"],\n        squad_dataset[\"validation\"],\n    )\nprint(\"pruned eval metrics:\", pruned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774 \uc0c1\ud0dc\uc5d0\uc11c \ubaa8\ub378\uc744 \ubbf8\uc138 \uc870\uc815(fine-tuning)\ud558\uc5ec \uac00\uc9c0\uce58\uae30\ub418\uc9c0 \uc54a\ub294 \uc694\uc18c\ub4e4\uc744 \uc5c5\ub370\uc774\ud2b8\ud558\uace0, \uc815\ud655\ub3c4 \uc190\uc2e4\uc744\n\ubcf4\uc644\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9cc\uc871\ud560 \ub9cc\ud55c \uc0c1\ud0dc\uc5d0 \ub3c4\ub2ec\ud558\uba74, ``squash_mask``\ub97c \ud638\ucd9c\ud558\uc5ec \ub9c8\uc2a4\ud06c\uc640\n\uac00\uc911\uce58\ub97c \ud558\ub098\ub85c \uacb0\ud569\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub807\uac8c \ud558\uba74 \ub9e4\uac1c\ubcc0\uc218\ud654\uac00 \uc81c\uac70\ub418\uace0, 0\uc73c\ub85c \ub41c 2:4 \ubc00\uc9d1\n\ubaa8\ub378\uc774 \ub0a8\uac8c \ub429\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.train()\nsparsifier.squash_mask()\ntorch.set_printoptions(edgeitems=4)\nprint(model.bert.encoder.layer[0].intermediate.dense.weight[:8, :8])\n\ndf[\"sparse_loss\"] = pd.DataFrame(trainer.state.log_history)[\"loss\"]\ndf.plot.line(x='step', y=[\"loss\", \"sparse_loss\"], title=\"Loss vs. # steps\", ylabel=\"loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ucd94\ub860\uc744 \uc704\ud55c 2:4 \ud76c\uc18c \ubaa8\ub378 \uac00\uc18d\ud654\n\n\uc774\uc81c \uc774 \ud615\uc2dd\uc758 \ubaa8\ub378\uc744 \uc5bb\uc5c8\uc73c\ubbc0\ub85c, QuickStart \uac00\uc774\ub4dc\uc5d0\uc11c\ucc98\ub7fc \ucd94\ub860\uc744 \uc704\ud574 \uac00\uc18d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = model.cuda().half()\n# \ud76c\uc18c\uc131\uc744 \uc704\ud574 \uac00\uc18d\ud654\nfor fqn, module in model.named_modules():\n    if isinstance(module, nn.Linear) and \"layer\" in fqn:\n        module.weight = nn.Parameter(to_sparse_semi_structured(module.weight))\n\nwith torch.no_grad():\n    predictions = trainer.predict(tokenized_squad_dataset[\"validation\"])\nstart_logits, end_logits = predictions.predictions\nmetrics_sparse = compute_metrics(\n    start_logits,\n    end_logits,\n    tokenized_squad_dataset[\"validation\"],\n    squad_dataset[\"validation\"],\n)\nprint(\"sparse eval metrics: \", metrics_sparse)\nsparse_perf = measure_execution_time(\n    model,\n    batch_sizes,\n    tokenized_squad_dataset[\"validation\"],\n)\nprint(\"sparse perf metrics: \", sparse_perf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud06c\uae30 \uae30\ubc18 \uac00\uc9c0\uce58\uae30 \ud6c4 \ubaa8\ub378\uc744 \uc7ac\ud559\uc2b5\ud55c \uacb0\uacfc, \uac00\uc9c0\uce58\uae30 \uc2dc \uc190\uc2e4\ub418\uc5c8\ub358 F1\n\uc810\uc218\uc758 \uac70\uc758 \ub300\ubd80\ubd84\uc774 \ud68c\ubcf5\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ub3d9\uc2dc\uc5d0 ``bs=16``\uc758 \ubc30\uce58 \ud06c\uae30\uc5d0\uc11c 1.28\ubc30\uc758 \uc18d\ub3c4 \ud5a5\uc0c1\uc744\n\ub2ec\uc131\ud588\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \ubaa8\ub4e0 \ud615\uc0c1\uc774 \uc131\ub2a5 \ud5a5\uc0c1\uc5d0 \uc801\ud569\ud55c \uac83\uc740 \uc544\ub2d9\ub2c8\ub2e4. \ubc30\uce58 \ud06c\uae30\uac00 \uc791\uace0 \uacc4\uc0b0\uc5d0\n\uc0ac\uc6a9\ub418\ub294 \uc2dc\uac04\uc774 \uc81c\ud55c\uc801\uc77c \ub54c\ub294 \ud76c\uc18c \ucee4\ub110\uc774 \ubc00\uc9d1 \ucee4\ub110\ubcf4\ub2e4 \ub354 \ub290\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ubc18\uad6c\uc870\uc801 \ud76c\uc18c\uc131(semi-structured sparsity)\uc740 \ud150\uc11c\uc758 \ud558\uc704 \ud074\ub798\uc2a4(subclass)\ub85c \uad6c\ud604\ub418\uc5b4 \uc788\uae30 \ub54c\ubb38\uc5d0,\n``torch.compile``\uacfc \ud638\ud658\ub429\ub2c8\ub2e4. ``to_sparse_semi_structured``\uc640 \ud568\uaed8 \uc0ac\uc6a9\ud558\uba74 BERT\uc5d0\uc11c \ucd1d\n2\ubc30\uc758 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n.. table::\n\n    +--------------------+--------+--------------+-------------------+-----------+\n    | Metrics            | fp16   | 2:4 \ud76c\uc18c\uc131    | \ubcc0\ud654 / \uc18d\ub3c4 \uc99d\uac00\uc728 | \ucef4\ud30c\uc77c\ub428   |\n    +====================+========+==============+===================+===========+\n    | \uc815\ud655\ub3c4 \uc77c\uce58  (%)    | 78.53  | 78.44        | -0.09             |           |\n    +--------------------+--------+--------------+-------------------+-----------+\n    | F1 (%)             | 86.93  | 86.49        | -0.44             |           |\n    +--------------------+--------+--------------+-------------------+-----------+\n    | Time (bs=4)        | 11.10  | 15.54        | 0.71x             | no        |\n    +--------------------+--------+--------------+-------------------+-----------+\n    | Time (bs=16)       | 19.35  | 15.74        | 1.23x             | no        |\n    +--------------------+--------+--------------+-------------------+-----------+\n    | Time (bs=64)       | 72.71  | 59.41        | 1.22x             | no        |\n    +--------------------+--------+--------------+-------------------+-----------+\n    | Time (bs=256)      | 286.65 | 247.63       | 1.14x             | no        |\n    +--------------------+--------+--------------+-------------------+-----------+\n    | Time (bs=4)        | 7.59   | 7.46         | 1.02x             | yes       |\n    +--------------------+--------+--------------+-------------------+-----------+\n    | Time (bs=16)       | 11.47  | 9.68         | 1.18x             | yes       |\n    +--------------------+--------+--------------+-------------------+-----------+\n    | Time (bs=64)       | 41.57  | 36.92        | 1.13x             | yes       |\n    +--------------------+--------+--------------+-------------------+-----------+\n    | Time (bs=256)      | 159.22 | 142.23       | 1.12x             | yes       |\n    +--------------------+--------+--------------+-------------------+-----------+\n\n# \uacb0\ub860\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 BERT\ub97c 2:4 \ud76c\uc18c\uc131\uc73c\ub85c \uac00\uc9c0\uce58\uae30\ud558\ub294 \ubc29\ubc95\uacfc 2:4 \ud76c\uc18c \ubaa8\ub378\uc744 \ucd94\ub860\uc6a9\uc73c\ub85c \uac00\uc18d\ud558\ub294\n\ubc29\ubc95\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4. ``SparseSemiStructuredTensor`` \ud558\uc704 \ud074\ub798\uc2a4\ub97c \ud65c\uc6a9\ud558\uc5ec fp16 \uae30\uc900 \uc131\ub2a5\uc5d0\n\ube44\ud574 1.3\ubc30\uc758 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \ub2ec\uc131\ud588\uc73c\uba70, ``torch.compile``\uc744 \uc0ac\uc6a9\ud558\uba74 \ucd5c\ub300 2\ubc30\uae4c\uc9c0 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \uc774\ub8f0\n\uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \ub610\ud55c, BERT\ub97c \ubbf8\uc138 \uc870\uc815\ud558\uc5ec \uc190\uc2e4\ub41c F1 \uc810\uc218(\ubc00\uc9d1 \ubaa8\ub378: 86.92 vs \ud76c\uc18c \ubaa8\ub378: 86.48)\ub97c\n\ud68c\ubcf5\ud558\ub294 \uacfc\uc815\uc5d0\uc11c 2:4 \ud76c\uc18c\uc131\uc758 \uc774\uc810\uc744 \uc785\uc99d\ud588\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}