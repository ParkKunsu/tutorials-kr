{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uae30 \uc704\ud55c \ud301\uc740 \ub2e4\uc74c\uc744 \ucc38\uc870\ud558\uc138\uc694:\n# https://tutorials.pytorch.kr/beginner/colab \n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# (Beta) Scaled Dot Product Attention (SDPA)\ub85c \uace0\uc131\ub2a5 \ud2b8\ub79c\uc2a4\ud3ec\uba38(Transformers) \uad6c\ud604\ud558\uae30\n\n**Author:** [Driss Guessous](https://github.com/drisspg)\n  **\ubc88\uc5ed:** [\uc774\uac15\ud76c](https://github.com/khleexv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uc694\uc57d\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c, \ud2b8\ub79c\uc2a4\ud3ec\uba38(Transformer) \uc544\ud0a4\ud14d\ucc98 \uad6c\ud604\uc5d0 \ub3c4\uc6c0\uc774 \ub418\ub294 \uc0c8\ub85c\uc6b4\n``torch.nn.functional`` \ubaa8\ub4c8\uc758 \ud568\uc218\ub97c \uc18c\uac1c\ud569\ub2c8\ub2e4. \uc774 \ud568\uc218\uc758 \uc774\ub984\uc740 ``torch.nn.functional.scaled_dot_product_attention``\n\uc785\ub2c8\ub2e4. \ud568\uc218\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uc124\uba85\uc740 [PyTorch \ubb38\uc11c](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention)_\n\ub97c \ucc38\uace0\ud558\uc138\uc694. \uc774 \ud568\uc218\ub294 \uc774\ubbf8 ``torch.nn.MultiheadAttention`` \uacfc ``torch.nn.TransformerEncoderLayer``\n\uc5d0\uc11c \uc0ac\uc6a9\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n## \uac1c\uc694\n\uace0\uc218\uc900\uc5d0\uc11c, \uc774 PyTorch \ud568\uc218\ub294 \ucffc\ub9ac(query), \ud0a4(key), \uac12(value) \uc0ac\uc774\uc758\nscaled dot product attention (SDPA)\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n\uc774 \ud568\uc218\uc758 \uc815\uc758\ub294 [Attention is all you need](https://arxiv.org/abs/1706.03762)_\n\ub17c\ubb38\uc5d0\uc11c \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud568\uc218\ub294 \uae30\uc874 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec PyTorch\ub85c \uc791\uc131\ud560 \uc218 \uc788\uc9c0\ub9cc,\n\ud4e8\uc988\ub4dc(fused) \uad6c\ud604\uc740 \ub2e8\uc21c\ud55c \uad6c\ud604\ubcf4\ub2e4 \ud070 \uc131\ub2a5 \uc774\uc810\uc744 \uc81c\uacf5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n## \ud4e8\uc988\ub4dc \uad6c\ud604\n\n\uc774 \ud568\uc218\ub294 CUDA tensor \uc785\ub825\uc744 \ub2e4\uc74c \uc911 \ud558\ub098\uc758 \uad6c\ud604\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\uad6c\ud604:\n\n* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)_\n* [Memory-Efficient Attention](https://github.com/facebookresearch/xformers)_\n* A PyTorch implementation defined in C++\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 PyTorch \ubc84\uc804 2.0.0 \uc774\uc0c1\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# \uc0ac\uc6a9 \uc608\uc2dc:\nquery, key, value = torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device)\nF.scaled_dot_product_attention(query, key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uba85\uc2dc\uc801 Dispatcher \uc81c\uc5b4\n\n\uc774 \ud568\uc218\ub294 \uc554\uc2dc\uc801\uc73c\ub85c \uc138 \uac00\uc9c0 \uad6c\ud604 \uc911 \ud558\ub098\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \ucee8\ud14d\uc2a4\ud2b8 \ub9e4\ub2c8\uc800\ub97c\n\uc0ac\uc6a9\ud558\uba74 \uba85\uc2dc\uc801\uc73c\ub85c \uc5b4\ub5a4 \uad6c\ud604\uc744 \uc0ac\uc6a9\ud560 \uc9c0 \uc81c\uc5b4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ucee8\ud14d\uc2a4\ud2b8 \ub9e4\ub2c8\uc800\ub97c \ud1b5\ud574\n\ud2b9\uc815 \uad6c\ud604\uc744 \uba85\uc2dc\uc801\uc73c\ub85c \ube44\ud65c\uc131\ud654 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\uc815 \uc785\ub825\uc5d0 \ub300\ud55c \uac00\uc7a5 \ube60\ub978 \uad6c\ud604\uc744 \ucc3e\uace0\uc790\n\ud55c\ub2e4\uba74, \ucee8\ud14d\uc2a4\ud2b8 \ub9e4\ub2c8\uc800\ub85c \ubaa8\ub4e0 \uad6c\ud604\uc758 \uc131\ub2a5\uc744 \uce21\uc815\ud574\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ubca4\uce58\ub9c8\ud06c \ud568\uc218\ub97c \uc815\uc758\ud569\ub2c8\ub2e4\nimport torch.utils.benchmark as benchmark\ndef benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n    t0 = benchmark.Timer(\n        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n    )\n    return t0.blocked_autorange().mean * 1e6\n\n# \uc785\ub825\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uc815\uc758\ud569\ub2c8\ub2e4\nbatch_size = 32\nmax_sequence_len = 1024\nnum_heads = 32\nembed_dimension = 32\n\ndtype = torch.float16\n\nquery = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\nkey = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\nvalue = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n\nprint(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n\n# \uc138 \uac00\uc9c0 \uad6c\ud604\uc758 \uc18d\ub3c4\ub97c \uce21\uc815\ud569\ub2c8\ub2e4\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\n\n\nwith sdpa_kernel(SDPBackend.MATH):\n    math_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n    print(f\"The math implementation runs in {math_time:.3f} microseconds\")\n\nwith sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n    try:\n        flash_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n        print(f\"The flash attention implementation runs in {flash_time:.3f} microseconds\")\n    except RuntimeError:\n        print(\"FlashAttention is not supported. See warnings for reasons.\")\n\nwith sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION):\n    try:\n        efficient_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value)\n        print(f\"The memory efficient implementation runs in {efficient_time:.3f} microseconds\")\n    except RuntimeError:\n        print(\"EfficientAttention is not supported. See warnings for reasons.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud558\ub4dc\uc6e8\uc5b4 \uc758\uc874\uc131\n\n\uc704 \uc140\uc744 \uc5b4\ub5a4 \uba38\uc2e0\uc5d0\uc11c \uc2e4\ud589\ud588\ub294\uc9c0\uc640 \uc0ac\uc6a9 \uac00\ub2a5\ud55c \ud558\ub4dc\uc6e8\uc5b4\uc5d0 \ub530\ub77c \uacb0\uacfc\uac00 \ub2e4\ub97c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n- GPU\uac00 \uc5c6\uace0 FP32 \uc9c0\uc6d0 CPU\uc5d0\uc11c \uc2e4\ud589 \uc911\uc774\ub77c\uba74 \ucee8\ud14d\uc2a4\ud2b8 \ub9e4\ub2c8\uc800\ub294 \ud6a8\uacfc\uac00 \uc5c6\uace0 \uc138 \uac00\uc9c0 \uc2e4\ud589 \ubaa8\ub450\n\uc720\uc0ac\ud55c \uc2dc\uac04\uc744 \ubc18\ud658\ud560 \uac83\uc785\ub2c8\ub2e4.\n- \uadf8\ub798\ud53d \uce74\ub4dc\uac00 \uc9c0\uc6d0\ud558\ub294 \ucef4\ud4e8\ud305 \ub2a5\ub825\uc5d0 \ub530\ub77c flash attention \ub610\ub294\nmemory efficient \uad6c\ud604\uc774 \ub3d9\uc791\ud558\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Causal Self Attention\n\n\uc544\ub798\ub294 multi-head causal self attention \ube14\ub85d\uc758 \uad6c\ud604 \uc608\uc2dc\uc785\ub2c8\ub2e4.\n[Andrej Karpathy NanoGPT](https://github.com/karpathy/nanoGPT)_ \uc800\uc7a5\uc18c\ub97c \ucc38\uace0\ud588\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n\n    def __init__(self, num_heads: int, embed_dimension: int, bias: bool=False, is_causal: bool=False, dropout:float=0.0):\n        super().__init__()\n        assert embed_dimension % num_heads == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(embed_dimension, 3 * embed_dimension, bias=bias)\n        # output projection\n        self.c_proj = nn.Linear(embed_dimension, embed_dimension, bias=bias)\n        # regularization\n        self.dropout = dropout\n        self.resid_dropout = nn.Dropout(dropout)\n        self.num_heads = num_heads\n        self.embed_dimension = embed_dimension\n        # Perform causal masking\n        self.is_causal = is_causal\n\n    def forward(self, x):\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        query_projected = self.c_attn(x)\n\n        batch_size = query_projected.size(0)\n        embed_dim = query_projected.size(2)\n        head_dim = embed_dim // (self.num_heads * 3)\n\n        query, key, value = query_projected.chunk(3, -1)\n        query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n\n        if self.training:\n            dropout = self.dropout\n            is_causal = self.is_causal\n        else:\n            dropout = 0.0\n            is_causal = False\n\n        y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n        y = y.transpose(1, 2).view(batch_size, -1, self.num_heads * head_dim)\n\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\n\nnum_heads = 8\nheads_per_dim = 64\nembed_dimension = num_heads * heads_per_dim\ndtype = torch.float16\nmodel = CausalSelfAttention(num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_causal=True, dropout=0.1).to(\"cuda\").to(dtype).eval()\nprint(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ``NestedTensor`` \ubc0f Dense tensor \uc9c0\uc6d0\n\nSDPA\ub294 ``NestedTensor`` \uc640 Dense tensor \uc785\ub825\uc744 \ubaa8\ub450 \uc9c0\uc6d0\ud569\ub2c8\ub2e4.\n``NestedTensors`` \ub294 \uc785\ub825\uc774 \uac00\ubcc0 \uae38\uc774 \uc2dc\ud000\uc2a4\ub85c \uad6c\uc131\ub41c \ubc30\uce58\uc778 \uacbd\uc6b0\uc5d0\n\ubc30\uce58 \ub0b4 \uc2dc\ud000\uc2a4\uc758 \ucd5c\ub300 \uae38\uc774\uc5d0 \ub9de\ucdb0 \uac01 \uc2dc\ud000\uc2a4\ub97c \ud328\ub529\ud560 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4. ``NestedTensors`` \uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740\n[torch.nested](https://pytorch.org/docs/stable/nested.html)_ \uc640 [NestedTensors \ud29c\ud1a0\ub9ac\uc5bc](https://tutorials.pytorch.kr/prototype/nestedtensor.html)_ \uc744 \ucc38\uace0\ud558\uc138\uc694.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\ndef generate_rand_batch(\n    batch_size,\n    max_sequence_len,\n    embed_dimension,\n    pad_percentage=None,\n    dtype=torch.float16,\n    device=\"cuda\",\n):\n    if not pad_percentage:\n        return (\n            torch.randn(\n                batch_size,\n                max_sequence_len,\n                embed_dimension,\n                dtype=dtype,\n                device=device,\n            ),\n            None,\n        )\n    # Random sequence lengths\n    seq_len_list = [\n        int(max_sequence_len * (1 - random.gauss(pad_percentage, 0.01)))\n        for _ in range(batch_size)\n    ]\n    # Make random entry in the batch have max sequence length\n    seq_len_list[random.randint(0, batch_size - 1)] = max_sequence_len\n    return (\n        torch.nested.nested_tensor(\n            [\n                torch.randn(seq_len, embed_dimension,\n                            dtype=dtype, device=device)\n                for seq_len in seq_len_list\n            ]\n        ),\n        seq_len_list,\n    )\n\nrandom_nt, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=0.5, dtype=dtype, device=device)\nrandom_dense, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=None, dtype=dtype, device=device)\n\n# \ud604\uc7ac \ud4e8\uc988\ub4dc(fused) \uad6c\ud604\uc740 ``NestedTensor`` \ub85c \ud559\uc2b5\ud558\ub294 \uac83\uc744 \uc9c0\uc6d0\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\nmodel.eval()\n\nwith sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n    try:\n        print(f\"Random NT runs in {benchmark_torch_function_in_microseconds(model, random_nt):.3f} microseconds\")\n        print(f\"Random Dense runs in {benchmark_torch_function_in_microseconds(model, random_dense):.3f} microseconds\")\n    except RuntimeError:\n        print(\"FlashAttention is not supported. See warnings for reasons.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``torch.compile`` \uacfc \ud568\uaed8 SDPA \uc0ac\uc6a9\ud558\uae30\n\nPyTorch 2.0 \ub9b4\ub9ac\uc988\uc640 \ud568\uaed8 ``torch.compile()`` \ub77c\ub294 \uc0c8\ub85c\uc6b4 \uae30\ub2a5\uc774 \ucd94\uac00\ub418\uc5c8\ub294\ub370,\n\uc774\ub294 eager mode\ubcf4\ub2e4 \uc0c1\ub2f9\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc81c\uacf5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nScaled dot product attention\uc740 ``torch.compile()`` \ub85c \uc644\uc804\ud788 \uad6c\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc774\ub97c \ud655\uc778\ud558\uae30 \uc704\ud574 ``torch.compile()`` \uc744 \ud1b5\ud574 ``CausalSelfAttention`` \ubaa8\ub4c8\uc744 \ucef4\ud30c\uc77c\ud558\uace0\n\uacb0\uacfc\uc801\uc73c\ub85c \uc5bb\uc5b4\uc9c0\ub294 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc54c\uc544\ubd05\uc2dc\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 32\nmax_sequence_len = 256\nx = torch.rand(batch_size, max_sequence_len,\n               embed_dimension, device=device, dtype=dtype)\nprint(\n    f\"The non compiled module runs in  {benchmark_torch_function_in_microseconds(model, x):.3f} microseconds\")\n\n\ncompiled_model = torch.compile(model)\n# Let's compile it\ncompiled_model(x)\nprint(\n    f\"The compiled module runs in  {benchmark_torch_function_in_microseconds(compiled_model, x):.3f} microseconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc815\ud655\ud55c \uc2e4\ud589 \uc2dc\uac04\uc740 \ud658\uacbd\uc5d0 \ub530\ub77c \ub2e4\ub974\uc9c0\ub9cc, \ub2e4\uc74c\uc740 \uc800\uc790\uc758 \uacb0\uacfc\uc785\ub2c8\ub2e4.\n\ucef4\ud30c\uc77c \ub418\uc9c0 \uc54a\uc740 \ubaa8\ub4c8\uc740 \uc2e4\ud589\uc5d0 166.616ms \uac00 \uc18c\uc694\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\ucef4\ud30c\uc77c \ub41c \ubaa8\ub4c8\uc740 \uc2e4\ud589\uc5d0 166.726ms \uac00 \uc18c\uc694\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\uc774\ub294 \uc6b0\ub9ac\uc758 \uc608\uc0c1\uacfc\ub294 \ub2e4\ub985\ub2c8\ub2e4. \uc880 \ub354 \uc790\uc138\ud788 \uc54c\uc544\ubd05\uc2dc\ub2e4.\nPyTorch\ub294 \ucf54\ub4dc\uc758 \uc131\ub2a5 \ud2b9\uc131\uc744 \uc810\uac80\ud560 \uc218 \uc788\ub294 \ub180\ub77c\uc6b4 \ub0b4\uc7a5(built-in) \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.profiler import profile, record_function, ProfilerActivity\nactivities = [ProfilerActivity.CPU]\nif device == 'cuda':\n    activities.append(ProfilerActivity.CUDA)\n\nwith profile(activities=activities, record_shapes=False) as prof:\n    with record_function(\" Non-Compilied Causal Attention\"):\n        for _ in range(25):\n            model(x)\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n\nwith profile(activities=activities, record_shapes=False) as prof:\n    with record_function(\"Compiled Causal Attention\"):\n        for _ in range(25):\n            compiled_model(x)\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub354 \ub9ce\uc740 \uc815\ubcf4\ub97c \uc5bb\uae30 \uc704\ud574 \ucd94\uc801(trace)\ub97c \ub0b4\ubcf4\ub0b4\uace0 ``chrome://tracing``\uc744 \uc0ac\uc6a9\ud558\uc5ec \uacb0\uacfc\ub97c \ud655\uc778\ud574\ubcf4\uc138\uc694.\n\n```python\nprof.export_chrome_trace(\"compiled_causal_attention_trace.json\").\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc804 \ucf54\ub4dc \uc870\uac01(snippet)\uc740 \ucef4\ud30c\uc77c \ub41c \ubaa8\ub4c8\uacfc \ucef4\ud30c\uc77c\ub418\uc9c0 \uc54a\uc740 \ubaa8\ub4c8 \ubaa8\ub450\uc5d0 \ub300\ud574\n\uac00\uc7a5 \ub9ce\uc740 GPU \uc2e4\ud589 \uc2dc\uac04\uc744 \ucc28\uc9c0\ud55c \uc0c1\uc704 10\uac1c\uc758 PyTorch \ud568\uc218\uc5d0 \ub300\ud55c \ubcf4\uace0\uc11c\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n\ubd84\uc11d \uacb0\uacfc, \ub450 \ubaa8\ub4c8 \ubaa8\ub450 GPU\uc5d0\uc11c \uc18c\uc694\ub41c \uc2dc\uac04\uc758 \ub300\ubd80\ubd84\uc774\n\ub3d9\uc77c\ud55c \ud568\uc218\ub4e4\uc5d0 \uc9d1\uc911\ub418\uc5b4 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\nPyTorch\uac00 \ud504\ub808\uc784\uc6cc\ud06c \uc624\ubc84\ud5e4\ub4dc\ub97c \uc81c\uac70\ud558\ub294 \ub370 \ub9e4\uc6b0 \ud0c1\uc6d4\ud55c ``torch.compile`` \ub97c\n\uc81c\uacf5\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. ``CausalSelfAttention`` \uac19\uc740 \uacbd\uc6b0\ucc98\ub7fc \ud06c\uace0, \ud6a8\uc728\uc801\uc778 CUDA \ucee4\ub110\uc744\n\uc0ac\uc6a9\ud558\ub294 \ubaa8\ub378\uc5d0\uc11c PyTorch \uc624\ubc84\ud5e4\ub4dc\ub294 \uc791\uc544\uc9c8 \uac83\uc785\ub2c8\ub2e4.\n\n\uc0ac\uc2e4, \ubaa8\ub4c8\uc740 \ubcf4\ud1b5 ``CausalSelfAttention`` \ube14\ub7ed \ud558\ub098\ub9cc\uc73c\ub85c \uad6c\uc131\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n[Andrej Karpathy NanoGPT](https://github.com/karpathy/nanoGPT)_ \uc800\uc7a5\uc18c\uc5d0\uc11c \uc2e4\ud5d8\ud55c \uacbd\uc6b0,\n\ubaa8\ub4c8\uc744 \ucef4\ud30c\uc77c \ud558\ub294 \uac83\uc740 \ud559\uc2b5\uc758 \uac01 \ub2e8\uacc4\ubcc4 \uc18c\uc694 \uc2dc\uac04\uc744 ``6090.49ms`` \uc5d0\uc11c ``3273.17ms`` \ub85c\n\uc904\uc77c \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \uc2e4\ud5d8\uc740 NanoGPT \uc800\uc7a5\uc18c\uc758 ``ae3a8d5`` \ucee4\ubc0b\uc5d0\uc11c Shakespeare\n\ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc9c4\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SDPA\ub97c ``atteition.bias`` \ud558\uc704 \ud074\ub798\uc2a4\uc640 \uc0ac\uc6a9\ud558\uae30\n\nPyTorch 2.3\ubd80\ud130 \ud150\uc11c \ud558\uc704 \ud074\ub798\uc2a4\ub97c \ud3ec\ud568\ud558\ub294 \uc0c8\ub85c\uc6b4 \uc11c\ube0c\ubaa8\ub4c8\uc744 \ucd94\uac00\ud588\uc2b5\ub2c8\ub2e4.\n\ucd94\uac00\ub41c \ubaa8\ub4c8\uc758 \uc774\ub984\uc740 ``torch.nn.attention.bias`` \uc774\uba70, ``torch.nn.functional.scaled_dot_product_attention``\n\uc640 \ud568\uaed8 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uc778\uacfc\uc801 \uc5b4\ud150\uc158 \ubcc0\ud615(Causal Attention Variants)\uc744 \uc0dd\uc131\ud558\uae30\n\uc704\ud55c \ub2e4\uc74c 2\uac00\uc9c0 \uae30\ub2a5(utilities)\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4:\n\n- ``torch.nn.attention.bias.causal_upper_left``\n- ``torch.nn.attention.bias.causal_lower_right``\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>\ud604\uc7ac ``torch.nn.functional.scaled_dot_product_attention`` \uc758 ``is_causal`` \uc778\uc790(argument)\ub294\n   ``torch.nn.attention.bias.causal_upper_left`` \ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uacfc \ub3d9\uc77c\ud569\ub2c8\ub2e4.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.nn.attention.bias import causal_lower_right, causal_upper_left\n\nbatch_size = 32\nsequence_length_q = 2\nsequence_length_kv = 10\nnum_heads = 16\nembed_dimension = 32\n\ndtype = torch.float16\n\nquery = torch.rand(batch_size, num_heads, sequence_length_q, embed_dimension, device=device, dtype=dtype)\nkey = torch.rand(batch_size, num_heads, sequence_length_kv, embed_dimension, device=device, dtype=dtype)\nvalue = torch.rand(batch_size, num_heads, sequence_length_kv, embed_dimension, device=device, dtype=dtype)\n\nupper_left_bias = causal_upper_left(sequence_length_q, sequence_length_kv)\nlower_right_bias = causal_lower_right(sequence_length_q, sequence_length_kv)\n\nprint(type(upper_left_bias))\nprint(type(lower_right_bias))\n\nassert type(upper_left_bias) == type(lower_right_bias)\nassert issubclass(type(upper_left_bias), torch.Tensor)\n\n# \uc704\uc758 \ucd9c\ub825\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef, \ub450 \uac1d\uccb4\ub294 \uac19\uc740 \ud0c0\uc785\uc778 ``torch.nn.attention.bias.CausalBias`` \uc774\uba70,\n# ``torch.Tensor`` \uc758 \ud558\uc704 \ud074\ub798\uc2a4(subclass)\uc785\ub2c8\ub2e4.\n\n# \uac01 \ud150\uc11c\ub4e4\uc774 \uc5b4\ub5bb\uac8c \uc0dd\uacbc\ub294\uc9c0 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\nprint(upper_left_bias)\nprint(lower_right_bias)\n\n# Upper Left Bias\ub294 \uc778\uacfc\uc801 \uc5b4\ud150\uc158 \ub9c8\uc2a4\ud06c(causal attention mask)\ub97c \uc5b4\ud150\uc158 \uc810\uc218 \ud589\ub82c(attention scores matrix)\uc758 \uc67c\ucabd \uc0c1\ub2e8\uc5d0 \uc815\ub82c\ud569\ub2c8\ub2e4.\n# \uc774\ub294 \uc5b4\ud150\uc158 \uc810\uc218 \ud589\ub82c\uc774 \uc815\uc0ac\uac01\ud615\uc774 \uc544\ub2cc \uacbd\uc6b0\uc5d0\ub9cc \uc601\ud5a5\uc744 \ubbf8\uce58\uba70, \uc774\ub294 \ub514\ucf54\ub529 \uc0c1\ud669\uc5d0\uc11c \uc77c\ubc18\uc801\uc778 \uacbd\uc6b0\uc785\ub2c8\ub2e4.\n# \uc774 \uac1c\ub150\uc744 \ub2e4\ub978 \ubc29\uc2dd\uc73c\ub85c \uc0dd\uac01\ud558\ub294 \ubc29\ubc95\uc740, upper left bias\ub97c \uc0ac\uc6a9\ud560 \ub54c\ub294 \ucffc\ub9ac(query)\uc758 0\ubc88\uc9f8 \ud1a0\ud070\uc774 \ud0a4(key)\uc758 0\ubc88\uc9f8 \ud1a0\ud070\uacfc \uc815\ub82c\ub41c\ub2e4\uace0\n# \uc0dd\uac01\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc989, \uc5b4\ud150\uc158 \uc810\uc218 \ud589\ub82c(attention score matrix)\uc774 2\ucc28\uc6d0\uc774\ub77c\uace0 \uac00\uc815\ud560 \ub54c, ``attn_score[0][0]`` \uc774 \ucffc\ub9ac\uc758 0\ubc88\uc9f8 \ud1a0\ud070\uacfc\n# \ud0a4\uc758 0\ubc88\uc9f8 \ud1a0\ud070 \uc0ac\uc774\uc758 \uc5b4\ud150\uc158 \uc810\uc218\uc778 \uac83\uc785\ub2c8\ub2e4.\n# Lower Right Bias\uc758 \uacbd\uc6b0\uc5d0\ub294 \ucffc\ub9ac(query)\uc758 \ub9c8\uc9c0\ub9c9 \ud1a0\ud070\uc774 \ud0a4(key)\uc758 \ub9c8\uc9c0\ub9c9 \ud1a0\ud070\uacfc \uc815\ub82c\ub418\ub3c4\ub85d \ucffc\ub9ac(query)\uc758 \uc2dc\ud000\uc2a4\ub97c \uc815\ub82c\ud569\ub2c8\ub2e4.\n# \uc608\ub97c \ub4e4\uc5b4, ``attn_score[-1][-1]`` \uc740 \ucffc\ub9ac\uc640 \ud0a4\uc758 \uae38\uc774\uac00 \uc11c\ub85c \ub2e4\ub974\ub354\ub77c\ub3c4 \ucffc\ub9ac\uc758 \ub9c8\uc9c0\ub9c9 \ud1a0\ud070\uacfc \ud0a4\uc758 \ub9c8\uc9c0\ub9c9 \ud1a0\ud070\uc774 \uac19\uc740 \uc704\uce58\uc5d0 \uc788\uae30 \ub54c\ubb38\uc5d0\n# \ubaa8\ub450 True\uc785\ub2c8\ub2e4.\n\n# SDPA\uc640 \ud568\uaed8 \uc0ac\uc6a9\ud558\uae30 \uc704\ud55c \uac1d\uccb4\ub4e4\uc785\ub2c8\ub2e4.\nout_upper_left = F.scaled_dot_product_attention(query, key, value, upper_left_bias)\nout_lower_right = F.scaled_dot_product_attention(query, key, value, lower_right_bias)\nout_is_causal = F.scaled_dot_product_attention(query, key, value, is_causal=True)\n\nassert torch.allclose(out_upper_left, out_is_causal)\nassert not torch.allclose(out_upper_left, out_lower_right)\n\n# \uc544\ub798 \uc5b4\ud150\uc158 \ud3b8\ud5a5(attention bias)\ub4e4\uc740 torch.compile\uacfc \ud638\ud658\ub429\ub2c8\ub2e4.\ncompiled_sdpa = torch.compile(F.scaled_dot_product_attention, fullgraph=True)\nout_upper_left = compiled_sdpa(query, key, value, upper_left_bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uacb0\ub860\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c, ``torch.nn.functional.scaled_dot_product_attention`` \uc758 \uae30\ubcf8\uc801\uc778\n\uc0ac\uc6a9\ubc95\uc744 \uc0b4\ud3b4\ubd24\uc2b5\ub2c8\ub2e4. ``sdpa_kernel`` \ucee8\ud14d\uc2a4\ud2b8 \ub9e4\ub2c8\uc800\ub85c GPU\uac00 \ud2b9\uc815 \uad6c\ud604\uc744\n\uc0ac\uc6a9\ud558\ub3c4\ub85d \ud560 \uc218 \uc788\ub2e4\ub294 \uac83\uc744 \ubcf4\uc558\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uac04\ub2e8\ud55c ``NestedTensor`` \uc5d0\uc11c \uc791\ub3d9\ud558\uace0\n\ucef4\ud30c\uc77c \uac00\ub2a5\ud55c ``CausalSelfAttention`` \ubaa8\ub4c8\uc744 \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4.\n\uc774 \uacfc\uc815\uc5d0\uc11c \ud504\ub85c\ud30c\uc77c\ub9c1 \ub3c4\uad6c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc720\uc800\uac00 \uc815\uc758\ud55c \ubaa8\ub4c8\uc758 \uc131\ub2a5 \ud2b9\uc131\uc744 \uc5b4\ub5bb\uac8c\n\ud655\uc778\ud560 \uc218 \uc788\ub294\uc9c0\ub3c4 \uc0b4\ud3b4\ubd24\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}