{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uae30 \uc704\ud55c \ud301\uc740 \ub2e4\uc74c\uc744 \ucc38\uc870\ud558\uc138\uc694:\n# https://tutorials.pytorch.kr/beginner/colab \n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Building a Convolution/Batch Norm fuser with torch.compile\n\n**Author:** [Horace He](https://github.com/chillee), [Will Feng](https://github.com/yf225)\n\n.. grid:: 2\n\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\n       :class-card: card-prerequisites\n\n       * How to register custom fusion patterns with torch.compile's pattern matcher\n\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\n       :class-card: card-prerequisites\n\n       * PyTorch v2.7.0\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This optimization only works for models in inference mode (i.e. ``model.eval()``).\n   However, torch.compile's pattern matching system works for both training and inference.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's get some imports out of the way (we will be using all\nof these later in the code).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Type, Dict, Any, Tuple, Iterable\nimport copy\nimport torch\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this tutorial, we are going to create a model consisting of convolutions\nand batch norms. Note that this model has some tricky components - some of\nthe conv/batch norm patterns are hidden within Sequentials and one of the\n``BatchNorms`` is wrapped in another Module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class WrappedBatchNorm(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mod = nn.BatchNorm2d(1)\n    def forward(self, x):\n        return self.mod(x)\n\nclass M(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 1, 1)\n        self.bn1 = nn.BatchNorm2d(1)\n        self.conv2 = nn.Conv2d(1, 1, 1)\n        self.nested = nn.Sequential(\n            nn.BatchNorm2d(1),\n            nn.Conv2d(1, 1, 1),\n        )\n        self.wrapped = WrappedBatchNorm()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.conv2(x)\n        x = self.nested(x)\n        x = self.wrapped(x)\n        return x\n\nmodel = M().to(device)\nmodel.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fusing Convolution with Batch Norm\nOne of the primary challenges with trying to automatically fuse convolution\nand batch norm in PyTorch is that PyTorch does not provide an easy way of\naccessing the computational graph. torch.compile resolves this problem by\ncapturing the computational graph during compilation, allowing us to apply\npattern-based optimizations across the entire model, including operations\nnested within Sequential modules or wrapped in custom modules.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch._inductor.pattern_matcher as pm\nfrom torch._inductor.pattern_matcher import register_replacement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "torch.compile will capture a graph representation of our model. During\ncompilation, modules hidden within Sequential containers and wrapped\nmodules are all inlined into the graph, making them available for\npattern matching and optimization.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fusing Convolution with Batch Norm\nUnlike some other fusions, fusion of convolution with batch norm does not\nrequire any new operators. Instead, as batch norm during inference\nconsists of a pointwise add and multiply, these operations can be \"baked\"\ninto the preceding convolution's weights. This allows us to remove the batch\nnorm entirely from our model! Read\nhttps://nenadmarkus.com/p/fusing-batchnorm-and-conv/ for further details. The\ncode here is copied from\nhttps://github.com/pytorch/pytorch/blob/orig/release/1.8/torch/nn/utils/fusion.py\nclarity purposes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fuse_conv_bn_eval(conv, bn):\n    \"\"\"\n    Given a conv Module `A` and an batch_norm module `B`, returns a conv\n    module `C` such that C(x) == B(A(x)) in inference mode.\n    \"\"\"\n    assert(not (conv.training or bn.training)), \"Fusion only for eval!\"\n    fused_conv = copy.deepcopy(conv)\n\n    fused_conv.weight, fused_conv.bias = \\\n        fuse_conv_bn_weights(fused_conv.weight, fused_conv.bias,\n                             bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)\n\n    return fused_conv\n\ndef fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):\n    if conv_b is None:\n        conv_b = torch.zeros_like(bn_rm)\n    if bn_w is None:\n        bn_w = torch.ones_like(bn_rm)\n    if bn_b is None:\n        bn_b = torch.zeros_like(bn_rm)\n    bn_var_rsqrt = torch.rsqrt(bn_rv + bn_eps)\n\n    conv_w = conv_w * (bn_w * bn_var_rsqrt).reshape([-1] + [1] * (len(conv_w.shape) - 1))\n    conv_b = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b\n\n    return torch.nn.Parameter(conv_w), torch.nn.Parameter(conv_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pattern Matching with torch.compile\nNow that we have our fusion logic, we need to register a pattern that\ntorch.compile's pattern matcher will recognize and replace during\ncompilation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define the pattern we want to match: conv2d followed by batch_norm\ndef conv_bn_pattern(x, conv_weight, conv_bias, bn_mean, bn_var, bn_weight, bn_bias):\n    conv_out = torch.nn.functional.conv2d(x, conv_weight, conv_bias)\n    bn_out = torch.nn.functional.batch_norm(\n        conv_out, bn_mean, bn_var, bn_weight, bn_bias,\n        training=False, eps=1e-5\n    )\n    return bn_out\n\ndef conv_bn_replacement(x, conv_weight, conv_bias, bn_mean, bn_var, bn_weight, bn_bias):\n    fused_weight, fused_bias = fuse_conv_bn_weights(\n        conv_weight, conv_bias, bn_mean, bn_var, 1e-5, bn_weight, bn_bias\n    )\n    return torch.nn.functional.conv2d(x, fused_weight, fused_bias)\n\n# Example inputs are needed to trace the pattern functions.\n# The inputs should match the function signatures of conv_bn_pattern and conv_bn_replacement.\n# These are used to trace the pattern functions to create the match template.\n# IMPORTANT: The pattern matcher is shape-agnostic! The specific shapes you use here\n# don't limit what shapes will be matched - any valid conv2d->batch_norm sequence\n# will be matched regardless of channels, kernel size, or spatial dimensions.\n# - x: input tensor (batch_size, channels, height, width)\n# - conv_weight: (out_channels, in_channels, kernel_h, kernel_w)\n# - conv_bias: (out_channels,)\n# - bn_mean, bn_var, bn_weight, bn_bias: all have shape (num_features,) matching out_channels\nexample_inputs = [\n    torch.randn(1, 1, 4, 4).to(device),  # x: input tensor\n    torch.randn(1, 1, 1, 1).to(device),  # conv_weight: 1 output channel, 1 input channel, 1x1 kernel\n    torch.randn(1).to(device),           # conv_bias: 1 output channel\n    torch.randn(1).to(device),           # bn_mean: batch norm running mean\n    torch.randn(1).to(device),           # bn_var: batch norm running variance\n    torch.randn(1).to(device),           # bn_weight: batch norm weight (gamma)\n    torch.randn(1).to(device),           # bn_bias: batch norm bias (beta)\n]\n\nfrom torch._inductor.pattern_matcher import PatternMatcherPass\nfrom torch._inductor import config\n\n# Create a pattern matcher pass and register our pattern\npatterns = PatternMatcherPass()\n\nregister_replacement(\n    conv_bn_pattern,\n    conv_bn_replacement,\n    example_inputs,\n    pm.fwd_only,\n    patterns,\n)\n\n# Create a custom pass function that applies our patterns\ndef conv_bn_fusion_pass(graph):\n    return patterns.apply(graph)\n\n# Set our custom pass in the config\nconfig.post_grad_custom_post_pass = conv_bn_fusion_pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>We make some simplifications here for demonstration purposes, such as only\n      matching 2D convolutions. The pattern matcher in torch.compile\n      can handle more complex patterns.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing out our Fusion Pass\nWe can now run this fusion pass on our initial toy model and verify that our\nresults are identical. In addition, we can print out the code for our fused\nmodel and verify that there are no more batch norms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch._dynamo.utils import counters\n\n# Clear the counters before compilation\ncounters.clear()\n\n# Ensure pattern matcher is enabled\nconfig.pattern_matcher = True\n\nfused_model = torch.compile(model, backend=\"inductor\")\ninp = torch.randn(5, 1, 1, 1).to(device)\n\n# Run the model to trigger compilation and pattern matching\nwith torch.no_grad():\n    output = fused_model(inp)\n    expected = model(inp)\n    torch.testing.assert_close(output, expected)\n\n# Check how many patterns were matched\nassert counters['inductor']['pattern_matcher_count'] == 3, \"Expected 3 conv-bn patterns to be matched\"\n\n# Create a model with different shapes than our example_inputs\ntest_model_diff_shape = nn.Sequential(\n    nn.Conv2d(3, 16, 5),\n    nn.BatchNorm2d(16),\n    nn.ReLU(),\n    nn.Conv2d(16, 32, 7),\n    nn.BatchNorm2d(32),\n).to(device).eval()\n\ncounters.clear()\ncompiled_diff_shape = torch.compile(test_model_diff_shape, backend=\"inductor\")\ntest_input_diff_shape = torch.randn(1, 3, 28, 28).to(device)\nwith torch.no_grad():\n    compiled_diff_shape(test_input_diff_shape)\n\n# Check how many patterns were matched\nassert counters['inductor']['pattern_matcher_count'] == 2, \"Expected 2 conv-bn patterns to be matched\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmarking our Fusion on ResNet18\nWe can test our fusion pass on a larger model like ResNet18 and see how much\nthis pass improves inference performance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\nimport time\n\nrn18 = models.resnet18().to(device)\nrn18.eval()\n\ninp = torch.randn(10, 3, 224, 224).to(device)\noutput = rn18(inp)\n\ndef benchmark(model, iters=20):\n    with torch.no_grad():\n        for _ in range(10):\n            model(inp)\n        begin = time.time()\n        for _ in range(iters):\n            model(inp)\n        return str(time.time()-begin)\n\n# Benchmark original model\nprint(\"Original model time: \", benchmark(rn18))\n\n# Compile with our custom pattern\ncompiled_with_pattern_matching = torch.compile(rn18, backend=\"inductor\")\n\n# Benchmark compiled model\nprint(\"\\ntorch.compile (with conv-bn pattern matching and other fusions): \", benchmark(compiled_with_pattern_matching))\n\n\n############\n# Conclusion\n# ----------\n# As we can see, torch.compile provides a powerful way to implement\n# graph transformations and optimizations through pattern matching.\n# By registering custom patterns, we can extend torch.compile's\n# optimization capabilities to handle domain-specific transformations.\n#\n# The conv-bn fusion demonstrated here is just one example of what's\n# possible with torch.compile's pattern matching system."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}