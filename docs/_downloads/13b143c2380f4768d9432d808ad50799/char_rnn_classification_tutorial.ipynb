{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uae30 \uc704\ud55c \ud301\uc740 \ub2e4\uc74c\uc744 \ucc38\uc870\ud558\uc138\uc694:\n# https://tutorials.pytorch.kr/beginner/colab \n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \uae30\ucd08\ubd80\ud130 \uc2dc\uc791\ud558\ub294 NLP: \ubb38\uc790-\ub2e8\uc704 RNN\uc73c\ub85c \uc774\ub984 \ubd84\ub958\ud558\uae30\n\n**Author**: [Sean Robertson](https://github.com/spro)\n  **\ubc88\uc5ed**: [\ud669\uc131\uc218](https://github.com/adonisues), [\uae40\uc81c\ud544](https://github.com/garlicvread)\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 3\ubd80\ub85c \uad6c\uc131\ub41c \uc2dc\ub9ac\uc988\uc758 \uc77c\ubd80\uc785\ub2c8\ub2e4:\n\n* [\uae30\ucd08\ubd80\ud130 \uc2dc\uc791\ud558\ub294 NLP: \ubb38\uc790-\ub2e8\uc704 RNN\uc73c\ub85c \uc774\ub984 \ubd84\ub958\ud558\uae30](https://tutorials.pytorch.kr/intermediate/char_rnn_classification_tutorial.html)_\n* [\uae30\ucd08\ubd80\ud130 \uc2dc\uc791\ud558\ub294 NLP: \ubb38\uc790-\ub2e8\uc704 RNN\uc73c\ub85c \uc774\ub984 \uc0dd\uc131\ud558\uae30](https://tutorials.pytorch.kr/intermediate/char_rnn_generation_tutorial.html)_\n* [\uae30\ucd08\ubd80\ud130 \uc2dc\uc791\ud558\ub294 NLP: Sequence to Sequence \ub124\ud2b8\uc6cc\ud06c\uc640 Attention\uc744 \uc774\uc6a9\ud55c \ubc88\uc5ed](https://tutorials.pytorch.kr/intermediate/seq2seq_translation_tutorial.html)_\n\n\uc5ec\uae30\uc5d0\uc11c\ub294 \ub2e8\uc5b4\ub97c \ubd84\ub958\ud558\uae30 \uc704\ud574 \uae30\ucd08\uc801\uc778 \ubb38\uc790-\ub2e8\uc704\uc758 \uc21c\ud658 \uc2e0\uacbd\ub9dd(RNN, Recurrent Nueral Network)\uc744\n\uad6c\ucd95\ud558\uace0 \ud559\uc2b5\ud560 \uc608\uc815\uc785\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc \ubc0f \uc774\ud6c4 2\uac1c \ud29c\ud1a0\ub9ac\uc5bc\uc778 :doc:`/intermediate/char_rnn_generation_tutorial`\n\ubc0f :doc:`/intermediate/seq2seq_translation_tutorial` \uc5d0\uc11c\ub294 \uc790\uc5f0\uc5b4 \ucc98\ub9ac(NLP, Natural Language Processing)\n\ubd84\uc57c\uc5d0\uc11c \uc5b4\ub5bb\uac8c \ub370\uc774\ud130\ub97c \uc804\ucc98\ub9ac\ud558\uace0 NLP \ubaa8\ub378\uc744 \uad6c\ucd95\ud558\ub294\uc9c0\ub97c \ubc11\ubc14\ub2e5\ubd80\ud130(from scratch) \uc124\uba85\ud569\ub2c8\ub2e4.\n\uc774\ub97c \uc704\ud574 \uc774 \ud29c\ud1a0\ub9ac\uc5bc \uc2dc\ub9ac\uc988\uc5d0\uc11c\ub294 NLP \ubaa8\ub378\ub9c1\uc744 \uc704\ud55c \ub370\uc774\ud130 \uc804\ucc98\ub9ac\uac00 \ubc11\ubc14\ub2e5(low-level)\uc5d0\uc11c \uc5b4\ub5bb\uac8c \uc9c4\ud589\ub418\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ubb38\uc790-\ub2e8\uc704 RNN\uc740 \ub2e8\uc5b4\ub97c \ubb38\uc790\uc758 \uc5f0\uc18d\uc73c\ub85c \uc77d\uc5b4 \ub4e4\uc5ec\uc11c \uac01 \ub2e8\uacc4\uc758 \uc608\uce21\uacfc\n\"\uc740\ub2c9 \uc0c1\ud0dc(Hidden State)\"\ub97c \ucd9c\ub825\ud558\uace0, \ub2e4\uc74c \ub2e8\uacc4\uc5d0 \uc774\uc804 \ub2e8\uacc4\uc758 \uc740\ub2c9 \uc0c1\ud0dc\ub97c \uc804\ub2ec\ud569\ub2c8\ub2e4.\n\ub2e8\uc5b4\uac00 \uc18d\ud55c \ud074\ub798\uc2a4\ub85c \ucd9c\ub825\ub418\ub3c4\ub85d \ucd5c\uc885 \uc608\uce21\uc73c\ub85c \uc120\ud0dd\ud569\ub2c8\ub2e4.\n\n\uad6c\uccb4\uc801\uc73c\ub85c, 18\uac1c \uc5b8\uc5b4\ub85c \ub41c \uc218\ucc9c \uac1c\uc758 \uc131(\u59d3)\uc744 \ud6c8\ub828\uc2dc\ud0a4\uace0,\n\ucca0\uc790\uc5d0 \ub530\ub77c \uc774\ub984\uc774 \uc5b4\ub5a4 \uc5b8\uc5b4\uc778\uc9c0 \uc608\uce21\ud569\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Torch \uc900\ube44\n\n\ud558\ub4dc\uc6e8\uc5b4(CPU \ub610\ub294 CUDA)\uc5d0 \ub9de\ucdb0 GPU \uac00\uc18d\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \uc801\uc808\ud55c \uc7a5\uce58\ub97c \uae30\ubcf8 \uc7a5\uce58\ub85c \uc124\uc815\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\n# Check if CUDA is available\ndevice = torch.device('cpu')\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n\ntorch.set_default_device(device)\nprint(f\"Using device = {torch.get_default_device()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ub370\uc774\ud130 \uc900\ube44\n\n[\uc5ec\uae30](https://download.pytorch.org/tutorial/data.zip)_ \uc5d0\uc11c \ub370\uc774\ud130\ub97c \ub2e4\uc6b4\ub85c\ub4dc \ubc1b\uace0\n\ud604\uc7ac \ub514\ub809\ud1a0\ub9ac\uc5d0 \uc555\ucd95\uc744 \ud489\ub2c8\ub2e4.\n\n``data/names`` \ub514\ub809\ud1a0\ub9ac\uc5d0\ub294 ``[Language].txt`` \ub77c\ub294 18 \uac1c\uc758 \ud14d\uc2a4\ud2b8 \ud30c\uc77c\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\uac01 \ud30c\uc77c\uc5d0\ub294 \ud55c \uc904\uc5d0 \ud558\ub098\uc758 \uc774\ub984\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70 \ub300\ubd80\ubd84 \ub85c\ub9c8\uc790\ub85c \ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\n(\ud558\uc9c0\ub9cc \uc720\ub2c8\ucf54\ub4dc\uc5d0\uc11c ASCII\ub85c \ubcc0\ud658\uc740 \ud574\uc57c \ud569\ub2c8\ub2e4)\n\n\uccab\ubc88\uc9f8 \ub2e8\uacc4\ub294 \ub370\uc774\ud130\ub97c \uc815\uc758\ud558\uace0 \uc815\ub9ac\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \ucd08\uae30\uc5d0\ub294 \uc720\ub2c8\ucf54\ub4dc\ub97c \uc77c\ubc18 ASCII\ub85c \ubcc0\ud658\ud558\uc5ec\nRNN \uc785\ub825 \ub808\uc774\uc5b4\ub97c \uc81c\ud55c\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\ub294 \uc720\ub2c8\ucf54\ub4dc \ubb38\uc790\uc5f4\uc744 ASCII\ub85c \ubcc0\ud658\ud558\uace0 \ud5c8\uc6a9\ub41c \ubb38\uc790\uc758 \uc791\uc740 \uc9d1\ud569\ub9cc\uc744 \ud5c8\uc6a9\ud558\uc5ec \uc774\ub8e8\uc5b4\uc9d1\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import string\nimport unicodedata\n\n# \"_\" \ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5b4\ud718\uc9d1(Vocabulary)\uc5d0 \uc5c6\ub294 \ubb38\uc790\ub97c \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc989, \ubaa8\ub378\uc5d0\uc11c \ucc98\ub9ac\ud558\uc9c0 \uc54a\ub294 \ubaa8\ub4e0 \ubb38\uc790\ub97c \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nallowed_characters = string.ascii_letters + \" .,;'\" + \"_\"\nn_letters = len(allowed_characters)\n\n# \uc720\ub2c8\ucf54\ub4dc \ubb38\uc790\uc5f4\uc744 \uc77c\ubc18 ASCII\ub85c \ubcc0\ud658\ud558\uae30: https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in allowed_characters\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc720\ub2c8\ucf54\ub4dc \uc54c\ud30c\ubcb3 \uc774\ub984\uc744 \uc77c\ubc18 ASCII\ub85c \ubcc0\ud658\ud558\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uc774\ub807\uac8c \ud558\uba74 \uc785\ub825 \ub808\uc774\uc5b4\ub97c \ub2e8\uc21c\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print (f\"converting '\u015alus\u00e0rski' to {unicodeToAscii('\u015alus\u00e0rski')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uc774\ub984\uc744 Tensor\ub85c \ubcc0\uacbd\n\n\uc774\uc81c \ubaa8\ub4e0 \uc774\ub984\uc744 \uccb4\uacc4\ud654\ud588\uc73c\ubbc0\ub85c, \uc774\ub97c \ud65c\uc6a9\ud558\uae30 \uc704\ud574 Tensor\ub85c\n\ubcc0\ud658\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n\ud558\ub098\uc758 \ubb38\uc790\ub97c \ud45c\ud604\ud558\uae30 \uc704\ud574 \ud06c\uae30\uac00 ``<1 x n_letters>`` \uc778\n\"One-Hot \ubca1\ud130\"\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. One-Hot \ubca1\ud130\ub294 \ud604\uc7ac \ubb38\uc790\uc758\n\uc8fc\uc18c\uc5d0\ub294 1\uc774, \uadf8 \uc678 \ub098\uba38\uc9c0 \uc8fc\uc18c\uc5d0\ub294 0\uc774 \ucc44\uc6cc\uc9c4 \ubca1\ud130\uc785\ub2c8\ub2e4.\n\uc608\uc2dc ``\"b\" = <0 1 0 0 0 ...>`` .\n\n\ub2e8\uc5b4\ub97c \ub9cc\ub4e4\uae30 \uc704\ud574 One-Hot \ubca1\ud130\ub4e4\uc744 2\ucc28\uc6d0 \ud589\ub82c\n``<line_length x 1 x n_letters>`` \uc5d0 \uacb0\ud569\uc2dc\ud0b5\ub2c8\ub2e4.\n\n\uc704\uc5d0\uc11c \ubcf4\uc774\ub294 \ucd94\uac00\uc801\uc778 1\ucc28\uc6d0\uc740 PyTorch\uc5d0\uc11c \ubaa8\ub4e0 \uac83\uc774 \ubc30\uce58(batch)\uc5d0 \uc788\ub2e4\uace0 \uac00\uc815\ud558\uae30\n\ub54c\ubb38\uc5d0 \ubc1c\uc0dd\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 \ubc30\uce58 \ud06c\uae30 1\uc744 \uc0ac\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# .. note::\n#    \uc5ed\uc790 \uc8fc: One-Hot \ubca1\ud130\ub294 \uc5b8\uc5b4 \ubc0f \ubc94\uc8fc\ud615 \ub370\uc774\ud130\ub97c \ub2e4\ub8f0 \ub54c \uc8fc\ub85c \uc0ac\uc6a9\ud558\uba70,\n#    \ub2e8\uc5b4, \uae00\uc790 \ub4f1\uc744 \ubca1\ud130\ub85c \ud45c\ud604\ud560 \ub54c \ub2e8\uc5b4, \uae00\uc790 \uc0ac\uc774\uc758 \uc0c1\uad00 \uad00\uacc4\ub97c \ubbf8\ub9ac \uc54c \uc218 \uc5c6\uc744 \uacbd\uc6b0,\n#    One-Hot\uc73c\ub85c \ud45c\ud604\ud558\uc5ec \uc11c\ub85c \uc9c1\uad50\ud55c\ub2e4\uace0 \uac00\uc815\ud558\uace0 \ud559\uc2b5\uc744 \uc2dc\uc791\ud569\ub2c8\ub2e4.\n#    \uc774\uc640 \ub3d9\uc77c\ud558\uac8c, \uc0c1\uad00 \uad00\uacc4\ub97c \uc54c \uc218 \uc5c6\ub294 \ub2e4\ub978 \ub370\uc774\ud130\uc758 \uacbd\uc6b0\uc5d0\ub3c4 One-Hot \ubca1\ud130\ub97c \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n#\n\nimport torch\n\n# all_letters \ub85c \ubb38\uc790\uc758 \uc8fc\uc18c \ucc3e\uae30, \uc608\uc2dc \"a\" = 0\ndef letterToIndex(letter):\n    # \ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \uae00\uc790\ub97c \ub9cc\ub098\uba74, \uc5b4\ud718\uc9d1\uc5d0 \uc874\uc7ac\ud558\uc9c0 \uc54a\ub294 \ubb38\uc790(\"_\")\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4.\n    if letter not in allowed_characters:\n        return allowed_characters.find(\"_\")\n    else:\n        return allowed_characters.find(letter)\n\n# \uac80\uc99d\uc744 \uc704\ud574\uc11c \ud55c \uac1c\uc758 \ubb38\uc790\ub97c <1 x n_letters> Tensor\ub85c \ubcc0\ud658\ndef letterToTensor(letter):\n    tensor = torch.zeros(1, n_letters)\n    tensor[0][letterToIndex(letter)] = 1\n    return tensor\n\n# \ud55c \uc904(\uc774\ub984)\uc744  <line_length x 1 x n_letters>,\n# \ub610\ub294 One-Hot \ubb38\uc790 \ubca1\ud130\uc758 Array\ub85c \ubcc0\uacbd\ndef lineToTensor(line):\n    tensor = torch.zeros(len(line), 1, n_letters)\n    for li, letter in enumerate(line):\n        tensor[li][0][letterToIndex(letter)] = 1\n    return tensor\n\nprint(letterToTensor('J'))\n\nprint(lineToTensor('Jones').size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are some examples of how to use ``lineToTensor()`` for a single and multiple character string.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print (f\"The letter 'a' becomes {lineToTensor('a')}\") #notice that the first position in the tensor = 1\nprint (f\"The name 'Ahn' becomes {lineToTensor('Ahn')}\") #notice 'A' sets the 27th index to 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Congratulations, you have built the foundational tensor objects for this learning task! You can use a similar approach\nfor other RNN tasks with text.\n\nNext, we need to combine all our examples into a dataset so we can train, test and validate our models. For this,\nwe will use the [Dataset and DataLoader](https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html)_ classes\nto hold our dataset. Each Dataset needs to implement three functions: ``__init__``, ``__len__``, and ``__getitem__``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from io import open\nimport glob\nimport os\nimport time\n\nimport torch\nfrom torch.utils.data import Dataset\n\nclass NamesDataset(Dataset):\n\n    def __init__(self, data_dir):\n        self.data_dir = data_dir #for provenance of the dataset\n        self.load_time = time.localtime #for provenance of the dataset\n        labels_set = set() #set of all classes\n\n        self.data = []\n        self.data_tensors = []\n        self.labels = []\n        self.labels_tensors = []\n\n        #read all the ``.txt`` files in the specified directory\n        text_files = glob.glob(os.path.join(data_dir, '*.txt'))\n        for filename in text_files:\n            label = os.path.splitext(os.path.basename(filename))[0]\n            labels_set.add(label)\n            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n            for name in lines:\n                self.data.append(name)\n                self.data_tensors.append(lineToTensor(name))\n                self.labels.append(label)\n\n        #Cache the tensor representation of the labels\n        self.labels_uniq = list(labels_set)\n        for idx in range(len(self.labels)):\n            temp_tensor = torch.tensor([self.labels_uniq.index(self.labels[idx])], dtype=torch.long)\n            self.labels_tensors.append(temp_tensor)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data_item = self.data[idx]\n        data_label = self.labels[idx]\n        data_tensor = self.data_tensors[idx]\n        label_tensor = self.labels_tensors[idx]\n\n        return label_tensor, data_tensor, data_label, data_item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can load our example data into the ``NamesDataset``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alldata = NamesDataset(\"data/names\")\nprint(f\"loaded {len(alldata)} items of data\")\nprint(f\"example = {alldata[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the dataset object allows us to easily split the data into train and test sets. Here we create a 80/20\nsplit but the ``torch.utils.data`` has more useful utilities. Here we specify a generator since we need to use the\nsame device as PyTorch defaults to above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], generator=torch.Generator(device=device).manual_seed(2024))\n\nprint(f\"train examples = {len(train_set)}, validation examples = {len(test_set)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have a basic dataset containing **20074** examples where each example is a pairing of label and name. We have also\nsplit the dataset into training and testing so we can validate the model that we build.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ub124\ud2b8\uc6cc\ud06c \uc0dd\uc131\n\nAutograd \uc804\uc5d0, Torch\uc5d0\uc11c RNN(recurrent neural network) \uc0dd\uc131\uc740\n\uc5ec\ub7ec \uc2dc\uac04 \ub2e8\uacc4 \uac78\uccd0\uc11c \uacc4\uce35\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \ubcf5\uc81c\ud558\ub294 \uc791\uc5c5\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4.\n\uacc4\uce35\uc740 \uc740\ub2c9 \uc0c1\ud0dc\uc640 \ubcc0\ud654\ub3c4(Gradient)\ub97c \uac00\uc9c0\uba70, \uc774\uc81c \uc774\uac83\ub4e4\uc740 \uadf8\ub798\ud504 \uc790\uccb4\uc5d0\uc11c\n\uc644\uc804\ud788 \ucc98\ub9ac\ub429\ub2c8\ub2e4. \uc774\ub294 feed-forward \uacc4\uce35\uacfc\n\uac19\uc740 \ub9e4\uc6b0 \"\uc21c\uc218\ud55c\" \ubc29\ubc95\uc73c\ub85c RNN\uc744 \uad6c\ud604\ud560 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>\uc5ed\uc790 \uc8fc: \uc5ec\uae30\uc11c\ub294 \ud559\uc2b5 \ubaa9\uc801\uc73c\ub85c nn.RNN \ub300\uc2e0 \uc9c1\uc811 RNN\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.</p></div>\n\n\uc774 RNN \ubaa8\ub4c8\uc740 \"\uae30\ubcf8(vanilla)\uc801\uc778 RNN\"\uc744 \uad6c\ud604\ud558\uba70, \uc785\ub825\uacfc \uc740\ub2c9 \uc0c1\ud0dc(hidden state),\n\uadf8\ub9ac\uace0 \ucd9c\ub825 \ub4a4 \ub3d9\uc791\ud558\ub294 ``LogSoftmax`` \uacc4\uce35\uc774 \uc788\ub294 3\uac1c\uc758 \uc120\ud615 \uacc4\uce35\ub9cc\uc744 \uac00\uc9d1\ub2c8\ub2e4.\n\nThis CharRNN class implements an RNN with three components.\nFirst, we use the [nn.RNN implementation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)_.\nNext, we define a layer that maps the RNN hidden layers to our output. And finally, we apply a ``softmax`` function. Using ``nn.RNN``\nleads to a significant improvement in performance, such as cuDNN-accelerated kernels, versus implementing\neach layer as a ``nn.Linear``. It also simplifies the implementation in ``forward()``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\nimport torch.nn.functional as F\n\nclass CharRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(CharRNN, self).__init__()\n\n        self.rnn = nn.RNN(input_size, hidden_size)\n        self.h2o = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, line_tensor):\n        rnn_out, hidden = self.rnn(line_tensor)\n        output = self.h2o(hidden[0])\n        output = self.softmax(output)\n\n        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then create an RNN with 58 input nodes, 128 hidden nodes, and 18 outputs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_hidden = 128\nrnn = CharRNN(n_letters, n_hidden, len(alldata.labels_uniq))\nprint(rnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After that we can pass our Tensor to the RNN to obtain a predicted output. Subsequently,\nwe use a helper function, ``label_from_output``, to derive a text label for the class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def label_from_output(output, output_labels):\n    top_n, top_i = output.topk(1)\n    label_i = top_i[0].item()\n    return output_labels[label_i], label_i\n\ninput = lineToTensor('Albert')\noutput = rnn(input) #this is equivalent to ``output = rnn.forward(input)``\nprint(output)\nprint(label_from_output(output, alldata.labels_uniq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud559\uc2b5\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \uc2e0\uacbd\ub9dd \ud559\uc2b5\n\n\uc774\uc81c \uc774 \ub124\ud2b8\uc6cc\ud06c\ub97c \ud559\uc2b5\ud558\ub294 \ub370 \ud544\uc694\ud55c \uc608\uc2dc(\ud559\uc2b5 \ub370\uc774\ud130)\ub97c \ubcf4\uc5ec\uc8fc\uace0 \ucd94\uc815\ud569\ub2c8\ub2e4.\n\ub9cc\uc77c \ud2c0\ub838\ub2e4\uba74 \uc54c\ub824 \uc90d\ub2c8\ub2e4.\n\nWe do this by defining a ``train()`` function which trains the model on a given dataset using minibatches. RNNs\nRNNs are trained similarly to other networks; therefore, for completeness, we include a batched training method here.\nThe loop (``for i in batch``) computes the losses for each of the items in the batch before adjusting the\nweights. This operation is repeated until the number of epochs is reached.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\nimport numpy as np\n\ndef train(rnn, training_data, n_epoch = 10, n_batch_size = 64, report_every = 50, learning_rate = 0.2, criterion = nn.NLLLoss()):\n    \"\"\"\n    Learn on a batch of training_data for a specified number of iterations and reporting thresholds\n    \"\"\"\n    # Keep track of losses for plotting\n    current_loss = 0\n    all_losses = []\n    rnn.train()\n    optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n\n    start = time.time()\n    print(f\"training on data set with n = {len(training_data)}\")\n\n    for iter in range(1, n_epoch + 1):\n        rnn.zero_grad() # clear the gradients\n\n        # create some minibatches\n        # we cannot use dataloaders because each of our names is a different length\n        batches = list(range(len(training_data)))\n        random.shuffle(batches)\n        batches = np.array_split(batches, len(batches) //n_batch_size )\n\n        for idx, batch in enumerate(batches):\n            batch_loss = 0\n            for i in batch: #for each example in this batch\n                (label_tensor, text_tensor, label, text) = training_data[i]\n                output = rnn.forward(text_tensor)\n                loss = criterion(output, label_tensor)\n                batch_loss += loss\n\n            # optimize parameters\n            batch_loss.backward()\n            nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n            optimizer.step()\n            optimizer.zero_grad()\n\n            current_loss += batch_loss.item() / len(batch)\n\n        all_losses.append(current_loss / len(batches) )\n        if iter % report_every == 0:\n            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n        current_loss = 0\n\n    return all_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now train a dataset with minibatches for a specified number of epochs. The number of epochs for this\nexample is reduced to speed up the build. You can get better results with different parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "start = time.time()\nall_losses = train(rnn, train_set, n_epoch=27, learning_rate=0.15, report_every=5)\nend = time.time()\nprint(f\"training took {end-start}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \uacb0\uacfc \ub3c4\uc2dd\ud654\n\n``all_losses`` \ub97c \uc774\uc6a9\ud55c \uc190\uc2e4 \ub3c4\uc2dd\ud654\ub294\n\ub124\ud2b8\uc6cc\ud06c\uc758 \ud559\uc2b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uacb0\uacfc \ud3c9\uac00\n\n\ub124\ud2b8\uc6cc\ud06c\uac00 \ub2e4\ub978 \uce74\ud14c\uace0\ub9ac\uc5d0\uc11c \uc5bc\ub9c8\ub098 \uc798 \uc791\ub3d9\ud558\ub294\uc9c0 \ubcf4\uae30 \uc704\ud574\n\ubaa8\ub4e0 \uc2e4\uc81c \uc5b8\uc5b4(\ud589)\uac00 \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c \uc5b4\ub5a4 \uc5b8\uc5b4\ub85c \ucd94\uce21(\uc5f4)\ub418\ub294\uc9c0 \ub098\ud0c0\ub0b4\ub294\n\ud63c\ub780 \ud589\ub82c(confusion matrix)\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. \ud63c\ub780 \ud589\ub82c\uc744 \uacc4\uc0b0\ud558\uae30 \uc704\ud574\n``evaluate()`` \ub85c \ub9ce\uc740 \uc218\uc758 \uc0d8\ud50c\uc744 \ub124\ud2b8\uc6cc\ud06c\uc5d0 \uc2e4\ud589\ud569\ub2c8\ub2e4.\n``evaluate()`` \uc740 ``train ()`` \uacfc \uc5ed\uc804\ud30c\ub97c \ube7c\uba74 \ub3d9\uc77c\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate(rnn, testing_data, classes):\n    confusion = torch.zeros(len(classes), len(classes))\n\n    rnn.eval() #set to eval mode\n    with torch.no_grad(): # do not record the gradients during eval phase\n        for i in range(len(testing_data)):\n            (label_tensor, text_tensor, label, text) = testing_data[i]\n            output = rnn(text_tensor)\n            guess, guess_i = label_from_output(output, classes)\n            label_i = classes.index(label)\n            confusion[label_i][guess_i] += 1\n\n    # Normalize by dividing every row by its sum\n    for i in range(len(classes)):\n        denom = confusion[i].sum()\n        if denom > 0:\n            confusion[i] = confusion[i] / denom\n\n    # Set up plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(confusion.cpu().numpy()) #numpy uses cpu here so we need to use a cpu version\n    fig.colorbar(cax)\n\n    # Set up axes\n    ax.set_xticks(np.arange(len(classes)), labels=classes, rotation=90)\n    ax.set_yticks(np.arange(len(classes)), labels=classes)\n\n    # Force label at every tick\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    # sphinx_gallery_thumbnail_number = 2\n    plt.show()\n\n\n\nevaluate(rnn, test_set, classes=alldata.labels_uniq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc8fc\ucd95\uc5d0\uc11c \ubc97\uc5b4\ub09c \ubc1d\uc740 \uc810\uc744 \uc120\ud0dd\ud558\uc5ec \uc798\ubabb \ucd94\uce21\ud55c \uc5b8\uc5b4\ub97c \ud45c\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc608\ub97c \ub4e4\uc5b4 \ud55c\uad6d\uc5b4\ub294 \uc911\uad6d\uc5b4\ub85c \uc774\ud0c8\ub9ac\uc544\uc5b4\ub85c \uc2a4\ud398\uc778\uc5b4\ub85c.\n\uadf8\ub9ac\uc2a4\uc5b4\ub294 \ub9e4\uc6b0 \uc798\ub418\ub294 \uac83\uc73c\ub85c \uc601\uc5b4\ub294 \ub9e4\uc6b0 \ub098\uc05c \uac83\uc73c\ub85c \ubcf4\uc785\ub2c8\ub2e4.\n(\ub2e4\ub978 \uc5b8\uc5b4\ub4e4\uacfc\uc758 \uc911\ucca9 \ub54c\ubb38\uc73c\ub85c \ucd94\uc815)\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uc5f0\uc2b5\n\n-  Get better results with a bigger and/or better shaped network\n\n   -  Adjust the hyperparameters to enhance performance, such as changing the number of epochs, batch size, and learning rate\n   -  Try the ``nn.LSTM`` and ``nn.GRU`` layers\n   -  Modify the size of the layers, such as increasing or decreasing the number of hidden nodes or adding additional linear layers\n   -  Combine multiple of these RNNs as a higher level network\n\n-  \"line -> label\" \uc758 \ub2e4\ub978 \ub370\uc774\ud130 \uc9d1\ud569\uc73c\ub85c \uc2dc\ub3c4\ud574 \ubcf4\uc2ed\uc2dc\uc624, \uc608\ub97c \ub4e4\uc5b4:\n\n   -  \ub2e8\uc5b4 -> \uc5b8\uc5b4\n   -  \uc774\ub984 -> \uc131\ubcc4\n   -  \uce90\ub9ad\ud130 \uc774\ub984 -> \uc791\uac00\n   -  \ud398\uc774\uc9c0 \uc81c\ubaa9 -> \ube14\ub85c\uadf8 \ub610\ub294 \uc11c\ube0c\ub808\ub527\n\n-  \ub354 \ud06c\uace0 \ub354 \ub098\uc740 \ubaa8\uc591\uc758 \ub124\ud2b8\uc6cc\ud06c\ub85c \ub354 \ub098\uc740 \uacb0\uacfc\ub97c \uc5bb\uc73c\uc2ed\uc2dc\uc624.\n\n   -  \ub354 \ub9ce\uc740 \uc120\ud615 \uacc4\uce35\uc744 \ucd94\uac00\ud574 \ubcf4\uc2ed\uc2dc\uc624.\n   -  ``nn.LSTM`` \uacfc ``nn.GRU`` \uacc4\uce35\uc744 \ucd94\uac00\ud574 \ubcf4\uc2ed\uc2dc\uc624.\n   -  \uc704\uc640 \uac19\uc740 RNN \uc5ec\ub7ec \uac1c\ub97c \uc0c1\uc704 \uc218\uc900 \ub124\ud2b8\uc6cc\ud06c\ub85c \uacb0\ud569\ud574 \ubcf4\uc2ed\uc2dc\uc624.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}