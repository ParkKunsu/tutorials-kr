{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uae30 \uc704\ud55c \ud301\uc740 \ub2e4\uc74c\uc744 \ucc38\uc870\ud558\uc138\uc694:\n# https://tutorials.pytorch.kr/beginner/colab \n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \uc0ac\uc6a9\uc790 \uc815\uc758 Triton \ucee4\ub110\uc744 ``torch.compile``\uacfc \ud568\uaed8 \uc0ac\uc6a9\ud558\uae30\n**\uc800\uc790:** [Oguz Ulgen](https://github.com/oulgen)\n**\ubc88\uc5ed:** [\uad6c\uacbd\uc120](https://github.com/kookyungseon), [\uc774\ucc44\uc6b4](https://github.com/dlcodns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc0ac\uc6a9\uc790 \uc815\uc758 Triton \ucee4\ub110\uc744 \uc0ac\uc6a9\ud558\uba74 \ubaa8\ub378\uc758 \ud2b9\uc815 \ubd80\ubd84\uc758 \uacc4\uc0b0\uc744 \ucd5c\uc801\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc774 \ucee4\ub110\ub4e4\uc740 Triton\uc758 \uc5b8\uc5b4\ub85c \uc791\uc131\ub41c \uac83\uc73c\ub85c \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\uc0ac\uc6a9\uc790 \uc815\uc758 Triton\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud558\ub4dc\uc6e8\uc5b4 \uc131\ub2a5\uc744 \ucd5c\uace0\ub85c \ud5a5\uc0c1\uc2dc\ud0b5\ub2c8\ub2e4.\n``torch.compile``\ub97c \uc0ac\uc6a9\ud558\ub294 \ucee4\ub110\uc740 \uc774\ub7ec\ud55c \ucd5c\uc801\ud654\ub41c \uacc4\uc0b0\uc744 \ud1b5\ud569\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nPyTorch \ubaa8\ub378\uc744 \ud1b5\ud574 \uc0c1\ub2f9\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc2e4\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc774 \ub808\uc2dc\ud53c\ub294 \uc0ac\uc6a9\uc790 \uc815\uc758 Triton \ucee4\ub110\uc744  ``torch.compile``\uacfc \ud568\uaed8 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\n\n## \uc804\uc81c\uc870\uac74\n\n \uc774 \ub808\uc2dc\ud53c\ub97c \uc2dc\uc791\ud558\uae30 \uc804\uc5d0 \ub2e4\uc74c\uc774 \uc788\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4:\n* ``torch.compile`` \ubc0f Triton\uc5d0 \ub300\ud55c \uae30\ubcf8\uc801\uc778 \uc774\ud574. \ucc38\uc870:\n\n  * [torch.compiler API \uc124\uba85\uc11c](https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler)_\n  * [torch.compile \uc18c\uac1c](https://tutorials.pytorch.kr/intermediate/torch_compile_tutorial.html)_\n  * [Triton \uc5b8\uc5b4 \ubb38\uc11c](https://triton-lang.org/main/index.html)_\n\n* PyTorch 2.3 \uc774\uc0c1\n* Triton\uc744 \uc9c0\uc6d0\ud558\ub294 GPU\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.utils._triton import has_triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uae30\ubcf8 \uc0ac\uc6a9\ubc95\n\n\uc774 \uc608\uc5d0\uc11c\ub294 Triton \ubb38\uc11c\uc758 \uac04\ub2e8\ud55c \ubca1\ud130 \ub367\uc148 \ucee4\ub110\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n``torch.compile``\uacfc \ud568\uaed8.\n\ucc38\uace0, [Triton \ubb38\uc11c\ub97c \ucc38\uace0\ud558\uc138\uc694](https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html)_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not has_triton():\n    print(\"Skipping because triton is not supported on this device.\")\nelse:\n    import triton\n    from triton import language as tl\n\n    @triton.jit\n    def add_kernel(\n        in_ptr0,\n        in_ptr1,\n        out_ptr,\n        n_elements,\n        BLOCK_SIZE: \"tl.constexpr\",\n    ):\n        pid = tl.program_id(axis=0)\n        block_start = pid * BLOCK_SIZE\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_elements\n        x = tl.load(in_ptr0 + offsets, mask=mask)\n        y = tl.load(in_ptr1 + offsets, mask=mask)\n        output = x + y\n        tl.store(out_ptr + offsets, output, mask=mask)\n\n    @torch.compile(fullgraph=True)\n    def add_fn(x, y):\n        output = torch.zeros_like(x)\n        n_elements = output.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=4)\n        return output\n\n    x = torch.randn(4, device=\"cuda\")\n    y = torch.randn(4, device=\"cuda\")\n    out = add_fn(x, y)\n    print(f\"Vector addition of\\nX:\\t{x}\\nY:\\t{y}\\nis equal to\\n{out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uace0\uae09 \uc0ac\uc6a9\ubc95\n\nTriton\uc758 \uc790\ub3d9 \ud29c\ub2dd \uae30\ub2a5\uc740 Triton \ucee4\ub110\uc758 \uad6c\uc131 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc790\ub3d9\uc73c\ub85c \ucd5c\uc801\ud654\ud574\uc8fc\ub294 \uac15\ub825\ud55c \ub3c4\uad6c\uc785\ub2c8\ub2e4.\n\ub2e4\uc591\ud55c \uc124\uc815\uc744 \uac80\ud1a0\ud558\uc5ec \ud2b9\uc815 \uc0ac\uc6a9 \uc0ac\ub840\uc5d0 \ucd5c\uc801\uc758 \uc131\ub2a5\uc744 \uc81c\uacf5\ud558\ub294 \uad6c\uc131\uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4.\n\n``torch.compile``\uacfc \ud568\uaed8 \uc0ac\uc6a9\ud560 \uacbd\uc6b0 ``triton.autotune``\uc744 \uc0ac\uc6a9\ud558\uba74 PyTorch \ubaa8\ub378\uc744 \ucd5c\ub300\ud55c \ud6a8\uc728\uc801\uc73c\ub85c\n\uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc544\ub798\ub294 ``torch.compile``\uacfc ``triton.autotune``\uc744 \uc0ac\uc6a9\ud558\ub294 \uc608\uc81c\uc785\ub2c8\ub2e4.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.compile``\uc740 ``triton.autotune``\uc5d0 \ub300\ud55c configs\uc640 key \uc778\uc218\ub9cc \uc9c0\uc6d0\ud569\ub2c8\ub2e4.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not has_triton():\n    print(\"Skipping because triton is not supported on this device.\")\nelse:\n    import triton\n    from triton import language as tl\n\n    @triton.autotune(\n        configs=[\n            triton.Config({\"BLOCK_SIZE\": 4}, num_stages=3, num_warps=8),\n            triton.Config({\"BLOCK_SIZE\": 4}, num_stages=4, num_warps=4),\n            triton.Config({\"BLOCK_SIZE\": 2}, num_stages=3, num_warps=8),\n            triton.Config({\"BLOCK_SIZE\": 2}, num_stages=4, num_warps=4),\n        ],\n        key=[],\n    )\n    @triton.jit\n    def add_kernel_autotuned(\n        in_ptr0,\n        in_ptr1,\n        out_ptr,\n        n_elements,\n        BLOCK_SIZE: \"tl.constexpr\",\n    ):\n        pid = tl.program_id(axis=0)\n        block_start = pid * BLOCK_SIZE\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_elements\n        x = tl.load(in_ptr0 + offsets, mask=mask)\n        y = tl.load(in_ptr1 + offsets, mask=mask)\n        output = x + y\n        tl.store(out_ptr + offsets, output, mask=mask)\n\n    @torch.compile(fullgraph=True)\n    def add_fn(x, y):\n        output = torch.zeros_like(x)\n        n_elements = output.numel()\n        grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n        add_kernel_autotuned[grid](x, y, output, n_elements)\n        return output\n\n    x = torch.randn(4, device=\"cuda\")\n    y = torch.randn(4, device=\"cuda\")\n    out = add_fn(x, y)\n    print(f\"Vector addition of\\nX:\\t{x}\\nY:\\t{y}\\nis equal to\\n{out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Composability\n\nUser-defined Triton kernels do not automatically support all PyTorch\nsubsystems. This can be seen in the following use cases:\n\n- Adding a CPU fallback\n- Adding a ``FlopCounter`` formula\n- Composing with Tensor Subclasses\n\nTo compose with additional PyTorch subsystems, use ``torch.library.triton_op``.\n\n``triton_op is`` a structured way of defining a custom operator that is backed by one\nor more Triton kernels: like regular custom operators (``torch.library.custom_op``),\nyou are able to specify the interactions with PyTorch subsystems via ``torch.library``.\nHowever, unlike ``torch.library.custom_op``, which creates opaque callables with respect to\n``torch.compile``, ``torch.compile`` traces into ``triton_op`` to apply optimizations.\n\nHere\u2019s a chart of which API to use when integrating Triton kernels with PyTorch.\n\n.. list-table::\n   :header-rows: 1\n\n   * -\n     - Triton kernel (no explicit ``torch.library`` wrapper)\n     - ``torch.library.triton_op``\n     - ``torch.library.custom_op``\n   * - Supports inference\n     - Yes\n     - Yes\n     - Yes\n   * - Supports training\n     - In the majority of cases\n     - Yes\n     - Yes\n   * - Supports ``torch.compile``\n     - Yes\n     - Yes\n     - Yes\n   * - Supports ``torch.compile(fullgraph=True)``\n     - In the majority of cases\n     - In the majority of cases\n     - In all cases\n   * - Does torch.compile trace into the implementation?\n     - Yes\n     - Yes\n     - No\n   * - Supports AOTInductor\n     - Yes\n     - Yes\n     - No\n   * - Supports PyTorch Subsystems like FlopCounterMode, CPU Fallback, Tensor Subclasses\n     - No\n     - Yes\n     - Yes\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wrapping Triton kernels with ``triton_op``\n\nUse ``torch.library.triton_op`` to wrap a function that may invoke one or more Triton kernels.\nUse ``torch.library.wrap_triton`` to wrap the calls to the Triton kernel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.library import triton_op, wrap_triton\n\n@triton_op(\"mylib::mysin\", mutates_args={})\ndef mysin(x: torch.Tensor) -> torch.Tensor:\n    out = torch.empty_like(x)\n    n_elements = x.numel()\n    wrap_triton(sin_kernel)[(n_elements,)](x, out, n_elements, BLOCK_SIZE=4)\n    return out\n\n@triton.jit\ndef sin_kernel(\n    in_ptr0,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = tl.sin(x)\n    tl.store(out_ptr + offsets, output, mask=mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can invoke the ``triton_op`` in one of the following two ways.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = torch.randn(3, device=\"cuda\")\ny = mysin(x)\nz = torch.ops.mylib.mysin.default(x)\n\nassert torch.allclose(y, x.sin())\nassert torch.allclose(z, x.sin())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting ``triton_op`` works with ``torch.compile`` and ``AOTInductor``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y = torch.compile(mysin)(x)\nassert torch.allclose(y, x.sin())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding training support\n\nUse ``register_autograd`` to add an autograd formula for the ``triton_op``.\nPrefer this to using ``torch.autograd.Function`` (which has various composability footguns\nwith ``torch.compile``).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def backward(ctx, grad):\n    x, = ctx.saved_tensors\n    return grad * x.cos()\n\ndef setup_context(ctx, inputs, output):\n    x, = inputs\n    ctx.save_for_backward(x)\n\nmysin.register_autograd(backward, setup_context=setup_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the backward must be a composition of PyTorch-understood operators.\nIf you want the backward to call Triton kernels, then those must be wrapped in ``triton_op`` as well:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@triton.jit\ndef cos_kernel(\n    in_ptr0,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = tl.cos(x)\n    tl.store(out_ptr + offsets, output, mask=mask)\n\n@triton_op(\"mylib::mycos\", mutates_args={})\ndef mycos(x: torch.Tensor) -> torch.Tensor:\n    out = torch.empty_like(x)\n    n_elements = x.numel()\n    wrap_triton(cos_kernel)[(n_elements,)](x, out, n_elements, BLOCK_SIZE=4)\n    return out\n\ndef backward(ctx, grad):\n    x, = ctx.saved_tensors\n    return grad * mycos(x)\n\ndef setup_context(ctx, inputs, output):\n    x, = inputs\n    ctx.save_for_backward(x)\n\nmysin.register_autograd(backward, setup_context=setup_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding a CPU Fallback\nTriton kernels don\u2019t run on CPU. Use  ``register_kernel`` to add a CPU (or any other device) fallback for the ``triton_op``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@mysin.register_kernel(\"cpu\")\ndef _(x):\n    return torch.sin(x)\n\nx = torch.randn(3)\ny = mysin(x)\nassert torch.allclose(y, x.sin())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fallback must be composed of PyTorch operators.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding a FlopCounter Formula\n\nTo specify how many flops the triton kernel reports under PyTorch's flop counter,\nuse ``register_flop_formula``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.flop_counter import FlopCounterMode, register_flop_formula\n\n@register_flop_formula(torch.ops.mylib.mysin)\ndef _(x_shape):\n    numel = 1\n    for s in x_shape:\n        numel *= s\n    return numel\n\nx = torch.randn(3, device=\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``FlopCounterMode`` requires [tabulate](https://pypi.org/project/tabulate/)_.\nBefore running the code below, make sure you have ``tabulate`` installed or install by\nrunning ``pip install tabulate``.\n\n>>> with FlopCounterMode() as flop_counter:\n>>>     y = mysin(x)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limitations\n\nPyTorch 2.3 \ubc84\uc804 \uae30\uc900\uc73c\ub85c, ``torch.compile``\uc758 \uc0ac\uc6a9\uc790 \uc815\uc758 Triton \ucee4\ub110\uc5d0\ub294 \ub3d9\uc801 \ubaa8\uc591\n``torch.autograd.Function``, JIT inductor, AOT inductor\uac00 \uc9c0\uc6d0\ub429\ub2c8\ub2e4. \uc774 \uae30\ub2a5\ub4e4\uc744\n\uc870\ud569\ud558\uc5ec \ubcf5\uc7a1\ud558\uace0 \uace0\uc131\ub2a5\uc778 \ubaa8\ub378\uc744 \uad6c\ucd95\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\nPyTorch 2.6 \ubc84\uc804\uc5d0\uc11c ``torch.library.triton_op``\uc774 \ucd94\uac00\ub418\uc5b4, Tensor \ud558\uc704 \ud074\ub798\uc2a4 \ubc0f\n\uae30\ud0c0 \uace0\uae09 \uae30\ub2a5\uc5d0 \ub300\ud55c \uc0ac\uc6a9\uc790 \uc815\uc758 Triton \ucee4\ub110 \uc9c0\uc6d0\uc774 \ucd94\uac00\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\n\uadf8\ub7ec\ub098 \uc54c\uc544\ub450\uc5b4\uc57c \ud560 \uba87 \uac00\uc9c0 \uc81c\ud55c \uc0ac\ud56d\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\n* **Triton Features:** ``triton.heuristics`` \ub294 \ub2e8\ub3c5\uc73c\ub85c \uc0ac\uc6a9\ud558\uac70\ub098 ``triton.autotune`` \uc55e\uc5d0\uc11c\n\uc0ac\uc6a9\ud560 \uc218 \uc788\uc9c0\ub9cc, ``triton.autotune`` \ub4a4\uc5d0\uc11c\ub294 \uc0ac\uc6a9\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c ``triton.heuristics`` \uc640\n``triton.autotune``\uc744 \ud568\uaed8 \uc0ac\uc6a9\ud558\ub824\uba74 ``triton.heuristics`` \ub97c \uba3c\uc800 \uc0ac\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n## \uacb0\ub860\n\n\uc774 \ub808\uc2dc\ud53c\uc5d0\uc11c\ub294 \uc0ac\uc6a9\uc790 \uc815\uc758 Triton \ucee4\ub110\uc744 ``torch.compile`` \ub85c \ud65c\uc6a9\ud558\ub294 \ubc29\ubc95\uc744 \uc54c\uc544\ubcf4\uc558\uc2b5\ub2c8\ub2e4. \uac04\ub2e8\ud55c\n\ubca1\ud130 \ub367\uc148 \ucee4\ub110\uc758 \uae30\ubcf8 \uc0ac\uc6a9\ubc95\uacfc Triton\uc758 \uc790\ub3d9 \ud29c\ub2dd \uae30\ub2a5\uc744 \ud3ec\ud568\ud55c \uace0\uae09 \uc0ac\uc6a9\ubc95\uc5d0 \ub300\ud574 \ub2e4\ub918\uc2b5\ub2c8\ub2e4. \ub610\ud55c \uc0ac\uc6a9\uc790\n\uc815\uc758 Triton \ucee4\ub110\uacfc \ub2e4\ub978 Pytorch \uae30\ub2a5\uc758 \uc870\ud569 \uac00\ub2a5\uc131\uc5d0 \ub300\ud574 \ub17c\uc758\ud558\uace0 \ud604\uc7ac\uc758 \uba87 \uac00\uc9c0 \uc81c\ud55c \uc0ac\ud56d\uc744 \uac15\uc870\ud588\uc2b5\ub2c8\ub2e4.\n\n## \uad00\ub828 \ud56d\ubaa9\n\n* [Optimizer \ucef4\ud30c\uc77c\ud558\uae30](https://tutorials.pytorch.kr/recipes/compiling_optimizer.html)_\n* [Scaled Dot Product Attention\uc744 \ud65c\uc6a9\ud55c \uace0\uc131\ub2a5 Transformer \uad6c\ud604\ud558\uae30](https://tutorials.pytorch.kr/intermediate/scaled_dot_product_attention_tutorial.html)_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}