{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uae30 \uc704\ud55c \ud301\uc740 \ub2e4\uc74c\uc744 \ucc38\uc870\ud558\uc138\uc694:\n# https://tutorials.pytorch.kr/beginner/colab \n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Explicit horizontal fusion with foreach_map and torch.compile\n\n**Author:** [Michael Lazos](https://github.com/mlazos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Horizontal fusion is a key optimization in ML compilers. In eager,\n this is typically expressed using the torch._foreach* ops which parallelizes\n operations across a list of tensors. However, supporting all possible permutations\n of arguments is quite difficult (e.g. mixtures of scalars and lists). Foreach_map\n allows conversion of any pointwise op in ``torch`` to a horiztonally fused foreach\n variant. In this tutorial, we will demonstrate how to implement the Adam optimizer\n with ``foreach_map`` to generate a fully fused kernel.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This recipe describes a prototype feature. Prototype features are typically\n   at an early stage for feedback and testing and are subject to change.</p></div>\n\n## Prerequisites\n\n* PyTorch v2.7.0 or later\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Setup\nFor this example, we'll use a simple sequence of linear layers.\nWe instantiate an independent copy to compare the two optimizer implementations.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\n# exit cleanly if we are on a device that doesn't support ``torch.compile``\nif torch.cuda.get_device_capability() < (7, 0):\n    print(\"Exiting because torch.compile is not supported on this device.\")\n    import sys\n    sys.exit(0)\n\n# Create simple model\nmodel = torch.nn.Sequential(\n    *[torch.nn.Linear(1024, 1024, False, device=\"cuda\") for _ in range(10)]\n)\nmodel_copy = torch.nn.Sequential(\n    *[torch.nn.Linear(1024, 1024, False, device=\"cuda\") for _ in range(10)]\n)\ninput = torch.rand(1024, device=\"cuda\")\n\n# run forward pass\noutput = model(input)\noutput_copy = model_copy(input)\n\n# run backward to populate the grads for our optimizer below\noutput.sum().backward()\noutput_copy.sum().backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper functions for foreach_map implementation\n\nIn this section, we'll begin our implementation of the Adam optimizer.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch._higher_order_ops.foreach_map import foreach_map\n\n# Helper function to extract optimizer states from a torch.optim.Adam instance\ndef get_inputs(optim):\n    steps = []\n    params = []\n    grads = []\n    exp_avgs = []\n    exp_avg_sqs = []\n    for group in optim.param_groups:\n        for p in group[\"params\"]:\n            params.append(p)\n            grads.append(p.grad)\n            state = optim.state[p]\n            exp_avgs.append(state[\"exp_avg\"])\n            exp_avg_sqs.append(state[\"exp_avg_sq\"])\n            steps.append(state[\"step\"])\n\n    return steps, params, exp_avgs, exp_avg_sqs\n\n\n# Functions to update the different optimizer states\ndef update_exp_avg_sq(exp_avg_sq, grad, beta2):\n    return exp_avg_sq.mul(beta2).addcmul(grad, grad, value=1 - beta2)\n\ndef update_param(param, step, exp_avg, exp_avg_sq, beta1, beta2, lr, eps):\n    bias_correction1 = 1 - torch.pow(beta1, step)\n    bias_correction2 = (1 - torch.pow(beta2, step)).sqrt()\n    step_size = (lr / bias_correction1).neg()\n    denom = (exp_avg_sq.sqrt() / (bias_correction2 * step_size)).add(eps / step_size)\n    return torch.add(param, torch.div(exp_avg, denom))\n\n# Our full Adam implementation\ndef foreach_map_adam(\n    steps,\n    params,\n    exp_avgs,\n    exp_avg_sqs,\n    weight_decay=0,\n    beta1=0.9,\n    beta2=0.999,\n    lr=1e-3,\n    eps=1e-8,\n):\n    with torch.no_grad():\n        grads = [param.grad for param in params]\n        # update step\n        updated_steps = foreach_map(lambda x: x + 1, steps)\n        torch._foreach_copy_(steps, updated_steps)\n\n        if weight_decay != 0:\n            foreach_map(torch.add, (grads,), alpha=weight_decay)\n\n        # Higher-order operators (HOPs) cannot have multiple outputs at the moment\n        # need to call foreach_map once for each output\n        exp_avgs_updated = foreach_map(torch.lerp, exp_avgs, grads, 1 - beta1)\n        exp_avgs_sq_updated = foreach_map(update_exp_avg_sq, exp_avg_sqs, grads, beta2)\n        params_updated = foreach_map(\n            update_param,\n            params,\n            steps,\n            exp_avgs_updated,\n            exp_avgs_sq_updated,\n            beta1,\n            beta2,\n            lr,\n            eps,\n        )\n        # Higher-order operators (HOPs) don't support input mutation today\n        # so manually  update the states in-place\n        torch._foreach_copy_(exp_avgs, exp_avgs_updated)\n        torch._foreach_copy_(exp_avg_sqs, exp_avgs_sq_updated)\n        torch._foreach_copy_(params, params_updated)\n    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up and running the compiled kernel\n\nIn this section, we'll run our Adam optimizer\nand compare the results\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.compile`` is only supported on CUDA devices that have a compute capability of 7.0 or higher.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt_eager = torch.optim.Adam(model.parameters(), lr=torch.tensor(0.01))\nopt_eager_copy = torch.optim.Adam(model_copy.parameters(), lr=torch.tensor(0.01))\n\n# warm up the optimizer state dict\nopt_eager.step()\nopt_eager_copy.step()\n\ninputs = get_inputs(opt_eager_copy)\ncompiled_adam = torch.compile(foreach_map_adam)\n\n# optionally view the output code\ntorch._logging.set_logs(output_code=True)\n\n# Warmup runs to compile the function\nfor _ in range(5):\n    opt_eager.step()\n    compiled_adam(*inputs)\n\nfor eager_p, compile_p in zip(opt_eager.param_groups[0][\"params\"], opt_eager_copy.param_groups[0][\"params\"]):\n    torch.allclose(eager_p, compile_p)\n\n# Benchmark performance\n\n # Let's define a helpful benchmarking function:\nimport torch.utils.benchmark as benchmark\n\ndef benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n    t0 = benchmark.Timer(\n        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n    )\n    return t0.blocked_autorange().mean * 1e6\n\neager_runtime = benchmark_torch_function_in_microseconds(opt_eager.step)\ncompiled_runtime = benchmark_torch_function_in_microseconds(lambda: compiled_adam(*inputs))\n\nassert eager_runtime > compiled_runtime\n\nprint(f\"eager runtime: {eager_runtime}us\")\nprint(f\"compiled runtime: {compiled_runtime}us\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\nIn this tutorial, we successfully implemented a custom fully-fused Adam optimizer using foreach_map.\nBy leveraging the power of foreach_map and torch.compile, we were able to create an optimized version of the Adam\noptimizer that can be used in various machine learning applications. This tutorial provides a comprehensive guide\non how to use foreach_map and torch.compile to optimize machine learning models, and serves as a\nvaluable resource for developers looking to improve the performance of their models with horizontal fusion.\n\nSee also:\n\n* [Compiled optimizer tutorial](https://tutorials.pytorch.kr/recipes/compiling_optimizer.html)_ - an intro into the compiled optimizer.\n* [Compiling the optimizer with PT2](https://dev-discuss.pytorch.org/t/compiling-the-optimizer-with-pt2/1669)_ - deeper technical details on the compiled optimizer.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}