{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uae30 \uc704\ud55c \ud301\uc740 \ub2e4\uc74c\uc744 \ucc38\uc870\ud558\uc138\uc694:\n# https://tutorials.pytorch.kr/beginner/colab \n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Visualizing Gradients\n\n**Author:** [Justin Silver](https://github.com/j-silv)_\n\nThis tutorial explains how to extract and visualize gradients at any\nlayer in a neural network. By inspecting how information flows from the\nend of the network to the parameters we want to optimize, we can debug\nissues such as [vanishing or exploding\ngradients](https://arxiv.org/abs/1211.5063)_ that occur during\ntraining.\n\nBefore starting, make sure you understand [tensors and how to manipulate\nthem](https://docs.tutorials.pytorch.kr/beginner/basics/tensorqs_tutorial.html)_.\nA basic knowledge of [how autograd\nworks](https://docs.tutorials.pytorch.kr/beginner/basics/autogradqs_tutorial.html)_\nwould also be useful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nFirst, make sure [PyTorch is\ninstalled](https://pytorch.org/get-started/locally/)_ and then import\nthe necessary libraries.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we\u2019ll be creating a network intended for the MNIST dataset,\nsimilar to the architecture described by the [batch normalization\npaper](https://arxiv.org/abs/1502.03167)_.\n\nTo illustrate the importance of gradient visualization, we will\ninstantiate one version of the network with batch normalization\n(BatchNorm), and one without it. Batch normalization is an extremely\neffective technique to resolve [vanishing/exploding\ngradients](https://arxiv.org/abs/1211.5063)_, and we will be verifying\nthat experimentally.\n\nThe model we use has a configurable number of repeating fully-connected\nlayers which alternate between ``nn.Linear``, ``norm_layer``, and\n``nn.Sigmoid``. If batch normalization is enabled, then ``norm_layer``\nwill use\n[BatchNorm1d](https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)_,\notherwise it will use the\n[Identity](https://docs.pytorch.org/docs/stable/generated/torch.nn.Identity.html)_\ntransformation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fc_layer(in_size, out_size, norm_layer):\n    \"\"\"Return a stack of linear->norm->sigmoid layers\"\"\"\n    return nn.Sequential(nn.Linear(in_size, out_size), norm_layer(out_size), nn.Sigmoid())\n\nclass Net(nn.Module):\n    \"\"\"Define a network that has num_layers of linear->norm->sigmoid transformations\"\"\"\n    def __init__(self, in_size=28*28, hidden_size=128,\n                 out_size=10, num_layers=3, batchnorm=False):\n        super().__init__()\n        if batchnorm is False:\n            norm_layer = nn.Identity\n        else:\n            norm_layer = nn.BatchNorm1d\n\n        layers = []\n        layers.append(fc_layer(in_size, hidden_size, norm_layer))\n\n        for i in range(num_layers-1):\n            layers.append(fc_layer(hidden_size, hidden_size, norm_layer))\n\n        layers.append(nn.Linear(hidden_size, out_size))\n\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we set up some dummy data, instantiate two versions of the model,\nand initialize the optimizers.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# set up dummy data\nx = torch.randn(10, 28, 28)\ny = torch.randint(10, (10, ))\n\n# init model\nmodel_bn = Net(batchnorm=True, num_layers=3)\nmodel_nobn = Net(batchnorm=False, num_layers=3)\n\nmodel_bn.train()\nmodel_nobn.train()\n\noptimizer_bn = optim.SGD(model_bn.parameters(), lr=0.01, momentum=0.9)\noptimizer_nobn = optim.SGD(model_nobn.parameters(), lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can verify that batch normalization is only being applied to one of\nthe models by probing one of the internal layers:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(model_bn.layers[0])\nprint(model_nobn.layers[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Registering hooks\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because we wrapped up the logic and state of our model in a\n``nn.Module``, we need another method to access the intermediate\ngradients if we want to avoid modifying the module code directly. This\nis done by [registering a\nhook](https://docs.pytorch.org/docs/stable/notes/autograd.html#backward-hooks-execution)_.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Using backward pass hooks attached to output tensors is preferred over using ``retain_grad()`` on the tensors themselves. An alternative method is to directly attach module hooks (e.g. ``register_full_backward_hook()``) so long as the ``nn.Module`` instance does not do perform any in-place operations. For more information, please refer to [this issue](https://github.com/pytorch/pytorch/issues/61519)_.</p></div>\n\nThe following code defines our hooks and gathers descriptive names for\nthe network\u2019s layers.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# note that wrapper functions are used for Python closure\n# so that we can pass arguments.\n\ndef hook_forward(module_name, grads, hook_backward):\n    def hook(module, args, output):\n        \"\"\"Forward pass hook which attaches backward pass hooks to intermediate tensors\"\"\"\n        output.register_hook(hook_backward(module_name, grads))\n    return hook\n\ndef hook_backward(module_name, grads):\n    def hook(grad):\n        \"\"\"Backward pass hook which appends gradients\"\"\"\n        grads.append((module_name, grad))\n    return hook\n\ndef get_all_layers(model, hook_forward, hook_backward):\n    \"\"\"Register forward pass hook (which registers a backward hook) to model outputs\n\n    Returns:\n        - layers: a dict with keys as layer/module and values as layer/module names\n                  e.g. layers[nn.Conv2d] = layer1.0.conv1\n        - grads: a list of tuples with module name and tensor output gradient\n                 e.g. grads[0] == (layer1.0.conv1, tensor.Torch(...))\n    \"\"\"\n    layers = dict()\n    grads = []\n    for name, layer in model.named_modules():\n        # skip Sequential and/or wrapper modules\n        if any(layer.children()) is False:\n            layers[layer] = name\n            layer.register_forward_hook(hook_forward(name, grads, hook_backward))\n    return layers, grads\n\n# register hooks\nlayers_bn, grads_bn = get_all_layers(model_bn, hook_forward, hook_backward)\nlayers_nobn, grads_nobn = get_all_layers(model_nobn, hook_forward, hook_backward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and visualization\n\nLet\u2019s now train the models for a few epochs:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = 10\n\nfor epoch in range(epochs):\n\n    # important to clear, because we append to\n    # outputs everytime we do a forward pass\n    grads_bn.clear()\n    grads_nobn.clear()\n\n    optimizer_bn.zero_grad()\n    optimizer_nobn.zero_grad()\n\n    y_pred_bn = model_bn(x)\n    y_pred_nobn = model_nobn(x)\n\n    loss_bn = F.cross_entropy(y_pred_bn, y)\n    loss_nobn = F.cross_entropy(y_pred_nobn, y)\n\n    loss_bn.backward()\n    loss_nobn.backward()\n\n    optimizer_bn.step()\n    optimizer_nobn.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running the forward and backward pass, the gradients for all the\nintermediate tensors should be present in ``grads_bn`` and\n``grads_nobn``. We compute the mean absolute value of each gradient\nmatrix so that we can compare the two models.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_grads(grads):\n    layer_idx = []\n    avg_grads = []\n    for idx, (name, grad) in enumerate(grads):\n        if grad is not None:\n            avg_grad = grad.abs().mean()\n            avg_grads.append(avg_grad)\n            # idx is backwards since we appended in backward pass\n            layer_idx.append(len(grads) - 1 - idx)\n    return layer_idx, avg_grads\n\nlayer_idx_bn, avg_grads_bn = get_grads(grads_bn)\nlayer_idx_nobn, avg_grads_nobn = get_grads(grads_nobn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the average gradients computed, we can now plot them and see how\nthe values change as a function of the network depth. Notice that when\nwe don\u2019t apply batch normalization, the gradient values in the\nintermediate layers fall to zero very quickly. The batch normalization\nmodel, however, maintains non-zero gradients in its intermediate layers.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\nax.plot(layer_idx_bn, avg_grads_bn, label=\"With BatchNorm\", marker=\"o\")\nax.plot(layer_idx_nobn, avg_grads_nobn, label=\"Without BatchNorm\", marker=\"x\")\nax.set_xlabel(\"Layer depth\")\nax.set_ylabel(\"Average gradient\")\nax.set_title(\"Gradient flow\")\nax.grid(True)\nax.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this tutorial, we demonstrated how to visualize the gradient flow\nthrough a neural network wrapped in a ``nn.Module`` class. We\nqualitatively showed how batch normalization helps to alleviate the\nvanishing gradient issue which occurs with deep neural networks.\n\nIf you would like to learn more about how PyTorch\u2019s autograd system\nworks, please visit the [references](#references)_ below. If you have\nany feedback for this tutorial (improvements, typo fixes, etc.) then\nplease use the [PyTorch Forums](https://discuss.pytorch.org/)_ and/or\nthe [issue tracker](https://github.com/pytorchkorea/tutorials-kr/issues)_ to\nreach out.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Additional exercises\n\n-  Try increasing the number of layers (``num_layers``) in our model and\n   see what effect this has on the gradient flow graph\n-  How would you adapt the code to visualize average activations instead\n   of average gradients? (*Hint: in the hook_forward() function we have\n   access to the raw tensor output*)\n-  What are some other methods to deal with vanishing and exploding\n   gradients?\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n-  [A Gentle Introduction to\n   torch.autograd](https://docs.tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html)_\n-  [Automatic Differentiation with\n   torch.autograd](https://docs.tutorials.pytorch.kr/beginner/basics/autogradqs_tutorial)_\n-  [Autograd\n   mechanics](https://docs.pytorch.org/docs/stable/notes/autograd.html)_\n-  [Batch Normalization: Accelerating Deep Network Training by Reducing\n   Internal Covariate Shift](https://arxiv.org/abs/1502.03167)_\n-  [On the difficulty of training Recurrent Neural\n   Networks](https://arxiv.org/abs/1211.5063)_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}