{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uae30 \uc704\ud55c \ud301\uc740 \ub2e4\uc74c\uc744 \ucc38\uc870\ud558\uc138\uc694:\n# https://tutorials.pytorch.kr/beginner/colab \n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# \uc790\ub3d9 \ud63c\ud569 \uc815\ubc00\ub3c4(Automatic Mixed Precision) \uac00\uc774\ub4dc\n**\uc800\uc790**: [Michael Carilli](https://github.com/mcarilli)\n**\uc5ed\uc790**: [\uc624\uc655\ud0dd](https://github.com/ohkingtaek)\n\n[torch.cuda.amp](https://pytorch.org/docs/stable/amp.html) \uc5d0\uc11c\ub294 \ud63c\ud569 \uc815\ubc00\ub3c4(mixed precision)\ub97c \uc704\ud574 \ud3b8\ub9ac\ud55c \uba54\uc18c\ub4dc\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\uc774 \ubc29\uc2dd\uc5d0\uc11c\ub294 \uc77c\ubd80 \uc5f0\uc0b0\uc774 ``torch.float32`` (``float``) \ub370\uc774\ud130 \ud0c0\uc785\uc744 \uc0ac\uc6a9\ud558\uace0, \ub2e4\ub978 \uc5f0\uc0b0\uc740 ``torch.float16`` (``half``) \uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \n\uc608\ub97c \ub4e4\uc5b4, \uc120\ud615 \uacc4\uce35\uc774\ub098 \ud569\uc131\uacf1 \uac19\uc740 \uc5f0\uc0b0\uc740 ``float16`` \ub610\ub294 ``bfloat16`` \uc5d0\uc11c \ud6e8\uc52c \ube60\ub974\uac8c \uc2e4\ud589\ub429\ub2c8\ub2e4. \n\ubc18\uba74, \ud569\uacc4(sum), \ud3c9\uade0(mean), \ucd5c\ub300/\ucd5c\uc18c\uac12 \uc5f0\uc0b0\uacfc \uac19\uc740 reduction \uc5f0\uc0b0\uc740 \uac12\uc758 \ubc94\uc704 \ubcc0\ub3d9\uc774 \ud06c\ubbc0\ub85c \ub354 \ub113\uc740 \ub3d9\uc801 \ubc94\uc704\ub97c \uc81c\uacf5\ud558\ub294 ``float32`` \uac00 \ud544\uc694\ud569\ub2c8\ub2e4. \n\ud63c\ud569 \uc815\ubc00\ub3c4\ub294 \uac01 \uc5f0\uc0b0\uc5d0 \uc801\ud569\ud55c \ub370\uc774\ud130 \ud0c0\uc785\uc744 \ub9e4\uce6d\ud558\uc5ec, \ub124\ud2b8\uc6cc\ud06c\uc758 \uc2e4\ud589 \uc2dc\uac04\uacfc \uba54\ubaa8\ub9ac \uc0ac\uc6a9\ub7c9\uc744 \uc904\uc774\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.\n\n\uc77c\ubc18\uc801\uc73c\ub85c \"\uc790\ub3d9 \ud63c\ud569 \uc815\ubc00\ub3c4 \ud559\uc2b5\"\uc740 [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast) \uc640\n[torch.cuda.amp.GradScaler](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler) \ub97c \ud568\uaed8 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\uc774 \uac00\uc774\ub4dc\uc5d0\uc11c\ub294 \uae30\ubcf8 \uc815\ubc00\ub3c4\uc5d0\uc11c \uac04\ub2e8\ud55c \ub124\ud2b8\uc6cc\ud06c\uc758 \uc131\ub2a5\uc744 \uce21\uc815\ud55c \ud6c4, ``autocast`` \uc640 ``GradScaler`` \ub97c \ucd94\uac00\ud558\uc5ec \ub3d9\uc77c\ud55c \uc2e0\uacbd\ub9dd\uc744 \ud63c\ud569 \uc815\ubc00\ub3c4\ub85c \uc2e4\ud589\ud574 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uacfc\uc815\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.\n\n\uc774 \uac00\uc774\ub4dc\ub294 \ud30c\uc774\uc36c \uc2a4\ud06c\ub9bd\ud2b8\ub85c \ub2e4\uc6b4\ub85c\ub4dc\ud558\uc5ec \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud544\uc694\ud55c \uc694\uad6c \uc0ac\ud56d\uc740 PyTorch 1.6 \uc774\uc0c1\uacfc CUDA\ub97c \uc9c0\uc6d0\ud558\ub294 GPU\uc785\ub2c8\ub2e4.\n\n\ud63c\ud569 \uc815\ubc00\ub3c4\ub294 \uc8fc\ub85c Tensor Core\uac00 \uc9c0\uc6d0\ub418\ub294 \uc544\ud0a4\ud14d\ucc98(Volta, Turing, Ampere)\uc5d0\uc11c \uc88b\uc740 \uc131\ub2a5\uc744 \ub0c5\ub2c8\ub2e4. \n\uc774\ub7ec\ud55c \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c\ub294 2~3\ubc30\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \ub098\ud0c0\ub0a0 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n\uc774\uc804 \uc544\ud0a4\ud14d\ucc98(Kepler, Maxwell, Pascal)\uc5d0\uc11c\ub294 \uc57d\uac04\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \nGPU\uc758 \uc544\ud0a4\ud14d\ucc98\ub97c \ud655\uc778\ud558\ub824\uba74 ``nvidia-smi`` \uba85\ub839\uc744 \uc2e4\ud589\ud558\uc138\uc694.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch, time, gc\n\n# \uc2dc\uac04 \ucc98\ub9ac\uc5d0 \uc0ac\uc6a9\ud560 \ud568\uc218\ub4e4\nstart_time = None\n\ndef start_timer():\n    global start_time\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_max_memory_allocated()\n    torch.cuda.synchronize()\n    start_time = time.time()\n\ndef end_timer_and_print(local_msg):\n    torch.cuda.synchronize()\n    end_time = time.time()\n    print(\"\\n\" + local_msg)\n    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uac04\ub2e8\ud55c \uc2e0\uacbd\ub9dd\n\ub2e4\uc74c\uacfc \uac19\uc740 \uc120\ud615 \uacc4\uce35\uacfc ReLU \uc5f0\uc0b0\uc758 \uc5f0\uc18d\uc740 \ud63c\ud569 \uc815\ubc00\ub3c4\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c\n\uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc904 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_model(in_size, out_size, num_layers):\n    layers = []\n    for _ in range(num_layers - 1):\n        layers.append(torch.nn.Linear(in_size, in_size))\n        layers.append(torch.nn.ReLU())\n    layers.append(torch.nn.Linear(in_size, out_size))\n    return torch.nn.Sequential(*tuple(layers)).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``batch_size``, ``in_size``, ``out_size``, \uadf8\ub9ac\uace0 ``num_layers`` \ub294\nGPU\uc5d0 \ucda9\ubd84\ud55c \uc791\uc5c5\ub7c9\uc744 \ubd80\uc5ec\ud558\uae30 \uc704\ud574 \ud06c\uac8c \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c GPU\uac00 \ucda9\ubd84\ud788\n\uc0ac\uc6a9\ub420 \ub54c \ud63c\ud569 \uc815\ubc00\ub3c4\uac00 \uac00\uc7a5 \ud070 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc791\uc740 \uc2e0\uacbd\ub9dd\uc740 CPU\uc5d0 \uc758\ud574\n\uc81c\uc57d\uc744 \ubc1b\uc744 \uc218 \uc788\uc73c\uba70, \uc774 \uacbd\uc6b0 \ud63c\ud569 \uc815\ubc00\ub3c4\ub294 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uc9c0 \ubabb\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ub610\ud55c, \uc120\ud615 \uacc4\uce35\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \ucc28\uc6d0\ub4e4\uc740 8\uc758 \ubc30\uc218\ub85c \uc124\uc815\ub418\uc5b4 Tensor Core\ub97c\n\uc9c0\uc6d0\ud558\ub294 GPU\uc5d0\uc11c Tensor Core\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uac8c \uad6c\uc131\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n(\uc544\ub798 `Troubleshooting \ud574\uacb0 <troubleshooting>` \ucc38\uc870)\n\n\uc5f0\uc2b5 \ubb38\uc81c: \uc0ac\uc6a9\ud560 \uc0ac\uc774\uc988\ub97c \ub2e4\uc591\ud558\uac8c \uc124\uc815\ud558\uc5ec \ud63c\ud569 \uc815\ubc00\ub3c4 \uc0ac\uc6a9 \uc2dc \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc5b4\ub5bb\uac8c \ub2ec\ub77c\uc9c0\ub294\uc9c0 \ud655\uc778\ud574 \ubcf4\uc138\uc694.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 512 # 128, 256, 513\ub3c4 \uc2dc\ub3c4\ud574\ubcf4\uc138\uc694.\nin_size = 4096\nout_size = 4096\nnum_layers = 3\nnum_batches = 50\nepochs = 3\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntorch.set_default_device(device)\n\n# \uae30\ubcf8 \uc815\ubc00\ub3c4\ub85c \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n# \uc544\ub798\uc5d0\uc11c \uae30\ubcf8 \uc815\ubc00\ub3c4\uc640 \ud63c\ud569 \uc815\ubc00\ub3c4 \uc2e4\ud5d8 \ubaa8\ub450\uc5d0\uc11c \ub3d9\uc77c\ud55c \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n# \ud63c\ud569 \uc815\ubc00\ub3c4\ub97c \ud65c\uc131\ud654\ud560 \ub54c \uc785\ub825\uc758 ``dtype`` \uc744 \uc218\ub3d9\uc73c\ub85c \ubcc0\uacbd\ud560 \ud544\uc694\ub294 \uc5c6\uc2b5\ub2c8\ub2e4.\ndata = [torch.randn(batch_size, in_size) for _ in range(num_batches)]\ntargets = [torch.randn(batch_size, out_size) for _ in range(num_batches)]\n\nloss_fn = torch.nn.MSELoss().cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uae30\ubcf8 \uc815\ubc00\ub3c4\n``torch.cuda.amp`` \uc5c6\uc774, \uc544\ub798\uc758 \uac04\ub2e8\ud55c \uc2e0\uacbd\ub9dd\uc740 \ubaa8\ub4e0 \uc5f0\uc0b0\uc744 \uae30\ubcf8\n\uc815\ubc00\ub3c4 (``torch.float32``) \ub85c \uc2e4\ud589\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net = make_model(in_size, out_size, num_layers)\nopt = torch.optim.SGD(net.parameters(), lr=0.001)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        output = net(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # \uc5ec\uae30\uc11c set_to_none=True\ub294 \uc131\ub2a5\uc744 \uc57d\uac04 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nend_timer_and_print(\"Default precision:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``torch.autocast`` \ucd94\uac00\ud558\ub294 \ubc29\ubc95\n[torch.autocast](https://pytorch.org/docs/stable/amp.html#autocasting)\n\uc758 \uc778\uc2a4\ud134\uc2a4\ub294 \uc2a4\ud06c\ub9bd\ud2b8\uc758 \uc77c\ubd80 \uc601\uc5ed\uc744 \ud63c\ud569 \uc815\ubc00\ub3c4\ub85c \uc2e4\ud589\ud560 \uc218 \uc788\ub3c4\ub85d, \ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub85c \uc791\ub3d9\ud569\ub2c8\ub2e4.\n\uc774 \uc601\uc5ed\uc5d0\uc11c CUDA \uc5f0\uc0b0\uc740 \uc131\ub2a5\uc744 \uac1c\uc120\ud558\uba74\uc11c \uc815\ud655\ub3c4\ub97c \uc720\uc9c0\ud558\uae30 \uc704\ud574 ``autocast`` \uac00 \uc120\ud0dd\ud55c ``dtype`` \uc73c\ub85c \uc2e4\ud589\ub429\ub2c8\ub2e4.\n\uac01 \uc5f0\uc0b0\uc5d0 \ub300\ud574 ``autocast`` \uac00 \uc120\ud0dd\ud558\ub294 \uc815\ubc00\ub3c4\uc640 \ud574\ub2f9 \uc815\ubc00\ub3c4\ub97c \uc120\ud0dd\ud558\ub294 \uc0c1\ud669\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740\n[Autocast Op Reference](https://pytorch.org/docs/stable/amp.html#autocast-op-reference) \ub97c \ucc38\uc870\ud558\uc138\uc694.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(0): # 0 epoch\uc73c\ub85c \uc124\uc815\ud55c \uac83\uc740 \uc774 \uc139\uc158\uc744 \uc124\uba85\ud558\uae30 \uc704\ud55c \uac83\uc785\ub2c8\ub2e4.\n    for input, target in zip(data, targets):\n        # ``autocast`` \uc544\ub798\uc5d0\uc11c \uc21c\uc804\ud30c\ub97c \uc2e4\ud589\ud569\ub2c8\ub2e4\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            output = net(input)\n            # output\uc740 float16\uc785\ub2c8\ub2e4. \uc774\ub294 \uc120\ud615 \uacc4\uce35\uc774 ``autocast`` \uc5d0 \uc758\ud574 float16\uc73c\ub85c \ubcc0\ud658\ub418\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n            assert output.dtype is torch.float16\n\n            loss = loss_fn(output, target)\n            # loss\ub294 float32\uc785\ub2c8\ub2e4. \uc774\ub294 ``mse_loss`` \uacc4\uce35\uc774 ``autocast`` \uc5d0 \uc758\ud574 float32\ub85c \ubcc0\ud658\ub418\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n            assert loss.dtype is torch.float32\n\n        # ``autocast`` \uc5d0\uc11c backward() \uc804\uc5d0 \uc885\ub8cc\ud569\ub2c8\ub2e4.\n        # ``autocast`` \uc5d0\uc11c \uc5ed\uc804\ud30c\ub97c \uc2e4\ud589\ud558\ub294 \uac83\uc740 \uad8c\uc7a5\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n        # \uc5ed\uc804\ud30c \uc5f0\uc0b0\uc740 \ud574\ub2f9 \uc21c\uc804\ud30c \uc5f0\uc0b0\uc744 \uc704\ud574 ``autocast`` \uac00 \uc120\ud0dd\ud55c \uac83\uacfc \ub3d9\uc77c\ud55c ``dtype`` \uc73c\ub85c \uc2e4\ud589\ub429\ub2c8\ub2e4.\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # \uc5ec\uae30\uc11c set_to_none=True\ub294 \uc131\ub2a5\uc744 \uc57d\uac04 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ``GradScaler`` \ucd94\uac00\ud558\ub294 \ubc29\ubc95\n[Gradient scaling](https://pytorch.org/docs/stable/amp.html#gradient-scaling)\n\uc740 \ud63c\ud569 \uc815\ubc00\ub3c4\ub85c \ud559\uc2b5\ud560 \ub54c \uc791\uc740 \ud06c\uae30\uc758 \ubcc0\ud654\ub3c4\uac00 0\uc73c\ub85c \uc0ac\ub77c\uc9c0\ub294 (\"underflowing\") \ud558\ub294 \uac83\uc744 \ubc29\uc9c0\ud558\ub294 \ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4.\n[torch.cuda.amp.GradScaler](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler)\n\ub294 \ubcc0\ud654\ub3c4 \uc2a4\ucf00\uc77c\ub9c1 \ub2e8\uacc4\ub97c \ud3b8\ub9ac\ud558\uac8c \uc218\ud589\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \uc218\ub834 \uc2e4\ud589\uc758 \uc2dc\uc791\uc5d0\uc11c \uae30\ubcf8 \uc778\uc790\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud55c \ubc88 ``scaler`` \ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n# \uc2e0\uacbd\ub9dd\uc774 \uae30\ubcf8 ``GradScaler`` \uc778\uc218\ub85c \uc218\ub834\ud558\uc9c0 \uc54a\ub294\ub2e4\uba74, \uc774\uc288\ub97c \uc81c\ucd9c\ud574\uc8fc\uc138\uc694.\n# \uc218\ub834 \uc2e4\ud589 \uc804\uccb4\uc5d0\uc11c \ub3d9\uc77c\ud55c ``GradScaler`` \uc778\uc2a4\ud134\uc2a4\ub97c \uc0ac\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4.\n# \ub3d9\uc77c\ud55c \uc2a4\ud06c\ub9bd\ud2b8\uc5d0\uc11c \uc5ec\ub7ec \ubc88\uc758 \uc218\ub834 \uc2e4\ud589\uc744 \uc218\ud589\ud560 \uacbd\uc6b0, \uac01 \uc2e4\ud589\uc740 \uc804\uc6a9\uc758 \uc0c8\ub85c\uc6b4 ``GradScaler`` \uc778\uc2a4\ud134\uc2a4\ub97c \uc0ac\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4.\n# ``GradScaler`` \uc778\uc2a4\ud134\uc2a4\ub294 \uac00\ubccd\uc2b5\ub2c8\ub2e4.\nscaler = torch.cuda.amp.GradScaler()\n\nfor epoch in range(0): # 0 epoch\uc73c\ub85c \uc124\uc815\ud55c \uac83\uc740 \uc774 \uc139\uc158\uc744 \uc124\uba85\ud558\uae30 \uc704\ud55c \uac83\uc785\ub2c8\ub2e4.\n    for input, target in zip(data, targets):\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n\n        # \uc190\uc2e4\uc744 \uc870\uc815\ud569\ub2c8\ub2e4. \uc870\uc815\ub41c \uc190\uc2e4\uc5d0 \ub300\ud574 ``backward()`` \ub97c \ud638\ucd9c\ud558\uc5ec \uc870\uc815\ub41c \ubcc0\ud654\ub3c4\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n        scaler.scale(loss).backward()\n\n        # ``scaler.step()`` \uc740 \uba3c\uc800 \uc635\ud2f0\ub9c8\uc774\uc800\uc5d0 \ud560\ub2f9\ub41c \ub9e4\uac1c\ubcc0\uc218\uc758 \ubcc0\ud654\ub3c4\ub97c \ubcf5\uc6d0\ud569\ub2c8\ub2e4.\n        # \uc774 \ubcc0\ud654\ub3c4\uc5d0 ``inf`` \ub098 ``NaN`` \uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc9c0 \uc54a\uc73c\uba74 optimizer.step()\uc774 \ud638\ucd9c\ub429\ub2c8\ub2e4.\n        # \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 optimizer.step()\uc744 \uac74\ub108\ub701\ub2c8\ub2e4.\n        scaler.step(opt)\n\n        # \ub2e4\uc74c \ubc18\ubcf5\uc744 \uc704\ud574 \uc870\uc815\ub41c \uac12\uc744 \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4.\n        scaler.update()\n\n        opt.zero_grad() # \uc5ec\uae30\uc11c set_to_none=True\ub294 \uc131\ub2a5\uc744 \uc57d\uac04 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \"Automatic Mixed Precision\" \uc18c\uac1c\ud55c \uac83\ub4e4 \uac19\uc774 \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\n(\ub2e4\uc74c \uc608\uc2dc\ub294 ``autocast`` \uc640 ``GradScaler`` \uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ud3b8\ub9ac\ud55c\n\uc778\uc790\uc778 ``enabled`` \ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ub9cc\uc57d False\ub85c \uc124\uc815\ub418\uba74, ``autocast``\n\uc640 ``GradScaler`` \uc758 \ud638\ucd9c\uc774 \ubb34\ud6a8\ud654\ub429\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 if/else \ubb38 \uc5c6\uc774\n\uae30\ubcf8 \uc815\ubc00\ub3c4\uc640 \ud63c\ud569 \uc815\ubc00\ub3c4 \uac04 \uc804\ud658\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = torch.optim.SGD(net.parameters(), lr=0.001)\nscaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # \uc5ec\uae30\uc11c set_to_none=True\ub294 \uc131\ub2a5\uc744 \uc57d\uac04 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nend_timer_and_print(\"Mixed precision:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ubcc0\ud654\ub3c4 \ud655\uc778/\uc218\uc815\ud558\uae30 (\uc608: \ud074\ub9ac\ud551)\n``scaler.scale(loss).backward()`` \ub85c \uc0dd\uc131\ub41c \ubaa8\ub4e0 \ubcc0\ud654\ub3c4\ub294 \uc870\uc815\ub429\ub2c8\ub2e4.\n\ub9cc\uc57d ``backward()`` \uc640 ``scaler.step(optimizer)`` \uc0ac\uc774\uc5d0\uc11c \ud30c\ub77c\ubbf8\ud130\uc758\n``.grad`` \uc18d\uc131\uc744 \uc218\uc815\ud558\uac70\ub098 \ud655\uc778\ud558\uace0 \uc2f6\ub2e4\uba74, \uba3c\uc800\n[scaler.unscale_(optimizer)](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.unscale_)\n\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubcc0\ud654\ub3c4\ub97c \ubcf5\uc6d0\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(0): # 0 epoch\uc73c\ub85c \uc124\uc815\ud55c \uac83\uc740 \uc774 \uc139\uc158\uc744 \uc124\uba85\ud558\uae30 \uc704\ud55c \uac83\uc785\ub2c8\ub2e4.\n    for input, target in zip(data, targets):\n        with torch.autocast(device_type=device, dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n\n        # \uc635\ud2f0\ub9c8\uc774\uc800\uc5d0 \ud560\ub2f9\ub41c \ud30c\ub77c\ubbf8\ud130\uc758 \ubcc0\ud654\ub3c4\ub97c \uc81c\uc790\ub9ac\uc5d0\uc11c \ubcf5\uc6d0\ud569\ub2c8\ub2e4.\n        scaler.unscale_(opt)\n\n        # \uc635\ud2f0\ub9c8\uc774\uc800\uc5d0 \ud560\ub2f9\ub41c \ud30c\ub77c\ubbf8\ud130\uc758 \ubcc0\ud654\ub3c4\uac00 \uc774\uc81c \ubcf5\uc6d0\ub418\uc5c8\uc73c\ubbc0\ub85c, \ud3c9\uc18c\uc640 \uac19\uc774 \ud074\ub9ac\ud551\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n        # \uc774\ub54c \ud074\ub9ac\ud551\uc5d0 \uc0ac\uc6a9\ud558\ub294 max_norm \uac12\uc740 \ubcc0\ud654\ub3c4 \uc870\uc815\uc774 \uc5c6\uc744 \ub54c\uc640 \ub3d9\uc77c\ud558\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.1)\n\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # \uc5ec\uae30\uc11c set_to_none=True\ub294 \uc131\ub2a5\uc744 \uc57d\uac04 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \uc800\uc7a5/\uc7ac\uac1c\ud558\ub294 \ubc95\nAmp\uac00 \ud65c\uc131\ud654 \uc0c1\ud0dc\uc5d0\uc11c \ube44\ud2b8 \ub2e8\uc704\uc758 \uc815\ud655\ub3c4\ub85c \uc800\uc7a5/\uc7ac\uac1c\ud558\ub824\uba74,\n[scaler.state_dict](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.state_dict) \ub098\n[scaler.load_state_dict](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.load_state_dict)\n\ub97c \uc0ac\uc6a9\ud558\uc138\uc694.\n\n\uc800\uc7a5\ud560 \ub54c\ub294, \uc77c\ubc18\uc801\uc778 \ubaa8\ub378\uacfc \uc635\ud2f0\ub9c8\uc774\uc800\uc758 \uc0c1\ud0dc\uc640 \ud568\uaed8 ``scaler`` \uc758 \uc0c1\ud0dc\ub3c4 \uc800\uc7a5\ud574\uc57c \ud569\ub2c8\ub2e4.\n\uc774\ub97c \uac01 \ubc18\ubcf5\ud558\ub294 \uc2dc\uc791 \uc2dc\uc810, \uc989 \uc5b4\ub5a4 \uc21c\uc804\ud30c \uc804\uc5d0 \ud558\uac70\ub098, \ubc18\ubcf5\uc774 \ub05d\ub09c \ud6c4\uc5d0 ``scaler.update()`` \uc774\ud6c4\uc5d0 \uc218\ud589\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "checkpoint = {\"model\": net.state_dict(),\n              \"optimizer\": opt.state_dict(),\n              \"scaler\": scaler.state_dict()}\n# \uc608\ub97c \ub4e4\uc5b4 \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uc791\uc131\ud558\ub824\uba74,\n# torch.save(checkpoint, \"filename\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc7ac\uac1c\ud560 \ub54c\ub294, \ubaa8\ub378\uacfc \uc635\ud2f0\ub9c8\uc774\uc800\uc758 \uc0c1\ud0dc\uc640 \ud568\uaed8 ``scaler`` \uc758 \uc0c1\ud0dc\ub3c4\n\ub85c\ub4dc\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uc77d\uc73c\ub824\uba74,\n\n```\ndev = torch.cuda.current_device()\ncheckpoint = torch.load(\"filename\",\n                        map_location = lambda storage, loc: storage.cuda(dev))\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net.load_state_dict(checkpoint[\"model\"])\nopt.load_state_dict(checkpoint[\"optimizer\"])\nscaler.load_state_dict(checkpoint[\"scaler\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uccb4\ud06c\ud3ec\uc778\ud2b8\uac00 Amp \uc5c6\uc774 \uc0dd\uc131\ub41c \uacbd\uc6b0, Amp\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\uc744 \uc7ac\uac1c\ud558\uace0 \uc2f6\ub2e4\uba74, \n\ubaa8\ub378\uacfc \uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc\ub97c \ud3c9\uc18c\ucc98\ub7fc \uccb4\ud06c\ud3ec\uc778\ud2b8\uc5d0\uc11c \ub85c\ub4dc\ud569\ub2c8\ub2e4. \uc774 \uccb4\ud06c\ud3ec\uc778\ud2b8\uc5d0\ub294 \n\uc800\uc7a5\ub41c ``scaler`` \uc0c1\ud0dc\uac00 \uc5c6\uc73c\ubbc0\ub85c \uc0c8\ub85c\uc6b4 ``GradScaler`` \uc778\uc2a4\ud134\uc2a4\ub97c \uc0ac\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n\ubc18\ub300\ub85c \uccb4\ud06c\ud3ec\uc778\ud2b8\uac00 Amp\ub85c \uc0dd\uc131\ub41c \uacbd\uc6b0, ``Amp`` \ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 \ud6c8\ub828\uc744 \uc7ac\uac1c\ud558\ub824\uba74,\n\ubaa8\ub378\uacfc \uc635\ud2f0\ub9c8\uc774\uc800 \uc0c1\ud0dc\ub97c \uccb4\ud06c\ud3ec\uc778\ud2b8\uc5d0\uc11c \ud3c9\uc18c\ucc98\ub7fc \ub85c\ub4dc\ud558\uace0, \uc800\uc7a5\ub41c ``scaler`` \uc0c1\ud0dc\ub294 \ubb34\uc2dc\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ucd94\ub860/\ud3c9\uac00\n``autocast`` \ub294 \ub2e8\ub3c5\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec \ucd94\ub860 \ub610\ub294 \ud3c9\uac00\uc758 \uc21c\uc804\ud30c\ub97c \uac10\uc300 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uacbd\uc6b0 ``GradScaler`` \ub294 \ud544\uc694\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## \uace0\uae09 \uc8fc\uc81c\n\uace0\uae09 \uc0ac\uc6a9 \uc0ac\ub840\uc5d0 \ub300\ud55c \ub0b4\uc6a9\uc740 [Automatic Mixed Precision Examples](https://pytorch.org/docs/stable/notes/amp_examples.html)\n\ub97c \ucc38\uc870\ud558\uc138\uc694. \uc608\uc2dc\uc5d0\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \ub0b4\uc6a9\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4:\n\n* \ubcc0\ud654\ub3c4 \ucd95\uc801\n* \ubcc0\ud654\ub3c4 \ud398\ub110\ud2f0/\uc774\uc911 \uc5ed\uc804\ud30c\n* \ub2e4\uc911 \ubaa8\ub378, \uc635\ud2f0\ub9c8\uc774\uc800 \ub610\ub294 \uc190\uc2e4\uc744 \uc0ac\uc6a9\ud558\ub294 \uc2e0\uacbd\ub9dd\n* \ub2e4\uc911 GPU (``torch.nn.DataParallel`` \ub610\ub294 ``torch.nn.parallel.DistributedDataParallel``)\n* \uc0ac\uc6a9\uc790 \uc815\uc758 autograd \ud568\uc218 (``torch.autograd.Function`` \uc758 \uc11c\ube0c\ud074\ub798\uc2a4)\n\n\ub3d9\uc77c\ud55c \uc2a4\ud06c\ub9bd\ud2b8\uc5d0\uc11c \uc5ec\ub7ec \ubc88\uc758 \uc218\ub834 \uc2e4\ud589\uc744 \uc218\ud589\ud558\ub294 \uacbd\uc6b0, \uac01 \uc2e4\ud589\uc740 \uc0c8\ub85c\uc6b4 ``GradScaler``\n\uc778\uc2a4\ud134\uc2a4\ub97c \uc0ac\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4. ``GradScaler`` \uc778\uc2a4\ud134\uc2a4\ub294 \uac00\ubccd\uc2b5\ub2c8\ub2e4.\n\n\ub9cc\uc57d \uc0ac\uc6a9\uc790 \uc815\uc758 C++ \uc5f0\uc0b0\uc744 \ub514\uc2a4\ud328\ucc98\uc5d0 \ub4f1\ub85d\ud558\ub824\uba74, \ub514\uc2a4\ud328\ucc98 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \n[autocast section](https://tutorials.pytorch.kr/advanced/dispatcher.html#autocast)\n\uc744 \ucc38\uc870\ud558\uc138\uc694.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Troubleshooting \ud574\uacb0\nAmp\ub97c \uc0ac\uc6a9\ud55c \uc18d\ub3c4 \ud5a5\uc0c1\uc774 \ubbf8\ubbf8\ud55c \uacbd\uc6b0\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n1. \ub124\ud2b8\uc6cc\ud06c\uac00 GPU(\ub4e4)\uc5d0 \ucda9\ubd84\ud55c \uc791\uc5c5\uc744 \uc81c\uacf5\ud558\uc9c0 \ubabb\ud558\uace0 \uc788\uc5b4, CPU\uc5d0 \uc758\ud55c \ubcd1\ubaa9 \ud604\uc0c1\uc774 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n   \uc774 \uacbd\uc6b0 Amp\uc758 GPU \uc131\ub2a5\uc5d0 \ub300\ud55c \ud6a8\uacfc\ub294 \uc911\uc694\ud558\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n   * GPU\ub97c \ud3ec\ud654 \uc0c1\ud0dc\ub85c \ub9cc\ub4e4\uae30 \uc704\ud55c \ub300\ub7b5\uc801\uc778 \ubc29\ubc95\uc740, \uba54\ubaa8\ub9ac \ubd80\uc871(OOM)\uc774 \ubc1c\uc0dd\ud558\uc9c0 \uc54a\ub294 \uc120\uc5d0\uc11c \uac00\ub2a5\ud55c \ud55c \ubc30\uce58 \ud06c\uae30\ub098 \ub124\ud2b8\uc6cc\ud06c \ud06c\uae30\ub97c \ub298\ub9ac\ub294 \uac83\uc785\ub2c8\ub2e4.\n   * \uacfc\ub3c4\ud55c CPU-GPU \ub3d9\uae30\ud654(\uc608: ``.item()`` \ud638\ucd9c\uc774\ub098 CUDA \ud150\uc11c\uc5d0\uc11c \uac12\uc744 \ucd9c\ub825\ud558\ub294 \uac83)\ub97c \ud53c\ud574\uc57c \ud569\ub2c8\ub2e4.\n   * \uc0ac\uc18c\ud55c\ub370 \ub9ce\uc740 CUDA \uc5f0\uc0b0\ub4e4\uc744 \ud53c\ud558\uace0, \uac00\ub2a5\ud55c \ud55c \uc774\ub7ec\ud55c \uc5f0\uc0b0\ub4e4\uc744 \uba87 \uac1c\uc758 \ud070 CUDA \uc5f0\uc0b0\uc73c\ub85c \ud569\uce58\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4.\n2. \uc2e0\uacbd\ub9dd\uc774 GPU \uacc4\uc0b0 \ubcd1\ubaa9 \ud604\uc0c1\uc744 \uacaa\uace0 \uc788\uc744 \uc218 \uc788\uc9c0\ub9cc (\ub9ce\uc740 ``matmul`` \ub610\ub294 \ud569\uc131\uacf1 \uc5f0\uc0b0), \uc0ac\uc6a9 \uc911\uc778 GPU\uc5d0 \ud150\uc11c \ucf54\uc5b4\uac00 \uc5c6\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n   \uc774 \uacbd\uc6b0 \uc18d\ub3c4 \ud5a5\uc0c1\uc774 \uc801\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n3. ``matmul`` \ucc28\uc6d0\uc774 Tensor Core\uc5d0 \uce5c\ud654\uc801\uc774\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. ``matmul`` \uc5d0 \ucc38\uc5ec\ud558\ub294 \uc0ac\uc774\uc988\uac00 8\uc758 \ubc30\uc218\uc778\uc9c0 \ud655\uc778\ud574\uc57c \ud569\ub2c8\ub2e4.\n   (\uc778\ucf54\ub354/\ub514\ucf54\ub354\uac00 \uc788\ub294 NLP \ubaa8\ub378\uc758 \uacbd\uc6b0, \uc774 \uc810\uc774 \ubbf8\ubb18\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \ud569\uc131\uacf1 \uc5f0\uc0b0\ub3c4 Tensor Core \uc0ac\uc6a9\uc744 \uc704\ud574 \uc720\uc0ac\ud55c \ud06c\uae30 \uc81c\uc57d\uc744 \uac00\uc84c\uc5c8\uc9c0\ub9cc, \n   CuDNN 7.3 \ubc84\uc804 \uc774\ud6c4\ub85c\ub294 \uc774\ub7ec\ud55c \uc81c\uc57d\uc774 \uc5c6\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 [\uc5ec\uae30](https://github.com/NVIDIA/apex/issues/221#issuecomment-478084841)\n   \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.)\n\n### \uc190\uc2e4\uc774 inf/NaN\uc778 \uacbd\uc6b0\n\uba3c\uc800 \uc2e0\uacbd\ub9dd\uc774 `\uace0\uae09 \uc0ac\uc6a9 \uc0ac\ub840 <advanced-topics>` \uc5d0 \ud574\ub2f9\ud558\ub294\uc9c0 \ud655\uc778\ud558\uc138\uc694.\n\ub610\ud55c [Prefer binary_cross_entropy_with_logits over binary_cross_entropy](https://pytorch.org/docs/stable/amp.html#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy)\n\uc744 \ucc38\uc870\ud558\uc138\uc694.\n\nAmp \uc0ac\uc6a9\uc774 \uc62c\ubc14\ub974\ub2e4\uace0 \ud655\uc2e0\ud55c\ub2e4\uba74, \uc774\uc288\ub97c \uc81c\uae30\ud574\uc57c \ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uadf8 \uc804\uc5d0 \ub2e4\uc74c \uc815\ubcf4\ub97c \uc218\uc9d1\ud558\ub294 \uac83\uc774 \ub3c4\uc6c0\uc774 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n1. ``autocast`` \ub610\ub294 ``GradScaler`` \ub97c \uac01\uac01 \ube44\ud65c\uc131\ud654 (``enabled=False`` \ub97c \uc0dd\uc131\uc790\uc5d0 \uc804\ub2ec)\ud558\uace0 ``inf`` \ub098 ``NaN`` \uc774 \uc5ec\uc804\ud788 \ubc1c\uc0dd\ud558\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4.\n2. \uc2e0\uacbd\ub9dd\uc758 \uc77c\ubd80(\uc608: \ubcf5\uc7a1\ud55c \uc190\uc2e4 \ud568\uc218)\uac00 \uc624\ubc84\ud50c\ub85c\uc6b0\ub418\ub294 \uac83\uc774 \uc758\uc2ec\ub41c\ub2e4\uba74, \ud574\ub2f9 \uc21c\uc804\ud30c \uc601\uc5ed\uc744 ``float32`` \ub85c \uc2e4\ud589\ud558\uace0 ``inf`` \ub098 ``NaN`` \uc774 \ubc1c\uc0dd\ud558\ub294\uc9c0 \ud655\uc778\ud558\uc138\uc694. \n   [The autocast docstring](https://pytorch.org/docs/stable/amp.html#torch.autocast) \n   \uc758 \ub9c8\uc9c0\ub9c9 \ucf54\ub4dc \ub2e8\ub77d\uc5d0\uc11c ``autocast`` \ub97c \ub85c\uceec\ub85c \ube44\ud65c\uc131\ud654\ud558\uace0 \ud558\uc704 \uc601\uc5ed\uc758 \uc785\ub825\uc744 \uce90\uc2a4\ud305\ud558\uc5ec \ud558\uc704 \uc601\uc5ed\uc744 ``float32`` \ub85c \uc2e4\ud589\ud558\ub294 \ubc29\ubc95\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n### \ud0c0\uc785 \ubd88\uc77c\uce58 \uc624\ub958 (``CUDNN_STATUS_BAD_PARAM`` \uc73c\ub85c \ub098\ud0c0\ub0a0 \uc218 \uc788\uc74c)\n``Autocast`` \ub294 \uce90\uc2a4\ud305\uc73c\ub85c \uc774\ub4dd\uc744 \uc5bb\uac70\ub098 \ud544\uc694\ud55c \uc5f0\uc0b0\ub4e4\uc744 \ubaa8\ub450 \ucc98\ub9ac\ud558\ub824\uace0 \ud569\ub2c8\ub2e4.\n[\uba85\uc2dc\uc801\uc73c\ub85c \ub2e4\ub8e8\uc5b4\uc9c4 \uc5f0\uc0b0\ub4e4](https://pytorch.org/docs/stable/amp.html#autocast-op-reference)\n\uc740 \uc218\uce58\uc801 \ud2b9\uc131\ubfd0\ub9cc \uc544\ub2c8\ub77c \uacbd\ud5d8\uc5d0 \uae30\ubc18\ud558\uc5ec \uc120\ud0dd\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n``Autocast`` \uac00 \ud65c\uc131\ud654\ub41c \uc21c\uc804\ud30c \uc601\uc5ed\uc774\ub098 \uadf8 \uc601\uc5ed\uc744 \ub530\ub978 \uc5ed\uc804\ud30c\uc5d0\uc11c \ud0c0\uc785 \ubd88\uc77c\uce58 \uc624\ub958\uac00 \ubc1c\uc0dd\ud55c\ub2e4\uba74, ``autocast`` \uac00 \uc5b4\ub5a4 \uc5f0\uc0b0\uc744 \ub193\ucce4\uc744 \uac00\ub2a5\uc131\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\uc624\ub958 \ucd94\uc801\ub0b4\uc6a9\uacfc \ud568\uaed8 \uc774\uc288\ub97c \uc81c\uae30\ud558\uc138\uc694. \uc138\ubd80 \uc815\ubcf4 \uc81c\uacf5\uc744 \uc704\ud574 \uc2a4\ud06c\ub9bd\ud2b8\ub97c \uc2e4\ud589\ud558\uae30 \uc804\uc5d0 ``export TORCH_SHOW_CPP_STACKTRACES=1``\n\uc744 \uc124\uc815\ud558\uc5ec \uc5b4\ub290 \ubc31\uc5d4\ub4dc \uc5f0\uc0b0\uc5d0\uc11c \uc2e4\ud328\ud558\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}