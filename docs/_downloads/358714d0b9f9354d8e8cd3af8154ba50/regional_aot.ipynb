{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Google Colab\uc5d0\uc11c \ub178\ud2b8\ubd81\uc744 \uc2e4\ud589\ud558\uae30 \uc704\ud55c \ud301\uc740 \ub2e4\uc74c\uc744 \ucc38\uc870\ud558\uc138\uc694:\n# https://tutorials.pytorch.kr/beginner/colab \n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Reducing AoT cold start compilation time with regional compilation\n\n**Author:** [Sayak Paul](https://huggingface.co/sayakpaul), [Charles Bensimon](https://huggingface.co/cbensimon), [Angela Yi](https://github.com/angelayi)\n\nIn the [regional compilation recipe](https://docs.tutorials.pytorch.kr/recipes/regional_compilation.html)_, we showed\nhow to reduce cold start compilation times while retaining (almost) full compilation benefits. This was demonstrated for\njust-in-time (JIT) compilation.\n\nThis recipe shows how to apply similar principles when compiling a model ahead-of-time (AoT). If you\nare not familiar with AOTInductor and ``torch.export``, we recommend you to check out [this tutorial](https://docs.tutorials.pytorch.kr/recipes/torch_export_aoti_python.html)_.\n\n## Prerequisites\n\n* Pytorch 2.6 or later\n* Familiarity with regional compilation\n* Familiarity with AOTInductor and ``torch.export``\n\n## Setup\nBefore we begin, we need to install ``torch`` if it is not already\navailable.\n\n```sh\npip install torch\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Steps\n\nIn this recipe, we will follow the same steps as the regional compilation recipe mentioned above:\n\n1. Import all necessary libraries.\n2. Define and initialize a neural network with repeated regions.\n3. Measure the compilation time of the full model and the regional compilation with AoT.\n\nFirst, let's import the necessary libraries for loading our data:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\ntorch.set_grad_enabled(False)\n\nfrom time import perf_counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the Neural Network\n\nWe will use the same neural network structure as the regional compilation recipe.\n\nWe will use a network, composed of repeated layers. This mimics a\nlarge language model, that typically is composed of many Transformer blocks. In this recipe,\nwe will create a ``Layer`` using the ``nn.Module`` class as a proxy for a repeated region.\nWe will then create a ``Model`` which is composed of 64 instances of this\n``Layer`` class.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Layer(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = torch.nn.Linear(10, 10)\n        self.relu1 = torch.nn.ReLU()\n        self.linear2 = torch.nn.Linear(10, 10)\n        self.relu2 = torch.nn.ReLU()\n\n    def forward(self, x):\n        a = self.linear1(x)\n        a = self.relu1(a)\n        a = torch.sigmoid(a)\n        b = self.linear2(a)\n        b = self.relu2(b)\n        return b\n\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 10)\n        self.layers = torch.nn.ModuleList([Layer() for _ in range(64)])\n\n    def forward(self, x):\n        # In regional compilation, the self.linear is outside of the scope of ``torch.compile``.\n        x = self.linear(x)\n        for layer in self.layers:\n            x = layer(x)\n        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compiling the model ahead-of-time\n\nSince we're compiling the model ahead-of-time, we need to prepare representative\ninput examples, that we expect the model to see during actual deployments.\n\nLet's create an instance of ``Model`` and pass it some sample input data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Model().cuda()\ninput = torch.randn(10, 10, device=\"cuda\")\noutput = model(input)\nprint(f\"{output.shape=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's compile our model ahead-of-time. We will use ``input`` created above to pass\nto ``torch.export``. This will yield a ``torch.export.ExportedProgram`` which we can compile.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "path = torch._inductor.aoti_compile_and_package(\n    torch.export.export(model, args=(input,))\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can load from this ``path`` and use it to perform inference.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "compiled_binary = torch._inductor.aoti_load_package(path)\noutput_compiled = compiled_binary(input)\nprint(f\"{output_compiled.shape=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compiling _regions_ of the model ahead-of-time\n\nCompiling model regions ahead-of-time, on the other hand, requires a few key changes.\n\nSince the compute pattern is shared by all the blocks that\nare repeated in a model (``Layer`` instances in this cases), we can just\ncompile a single block and let the inductor reuse it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Model().cuda()\npath = torch._inductor.aoti_compile_and_package(\n    torch.export.export(model.layers[0], args=(input,)),\n    inductor_configs={\n        # compile artifact w/o saving params in the artifact\n        \"aot_inductor.package_constants_in_so\": False,\n    }\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An exported program (``torch.export.ExportedProgram``) contains the Tensor computation,\na ``state_dict`` containing tensor values of all lifted parameters and buffer alongside\nother metadata. We specify the ``aot_inductor.package_constants_in_so`` to be ``False`` to\nnot serialize the model parameters in the generated artifact.\n\nNow, when loading the compiled binary, we can reuse the existing parameters of\neach block. This lets us take advantage of the compiled binary obtained above.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for layer in model.layers:\n    compiled_layer = torch._inductor.aoti_load_package(path)\n    compiled_layer.load_constants(\n        layer.state_dict(), check_full_update=True, user_managed=True\n    )\n    layer.forward = compiled_layer\n\noutput_regional_compiled = model(input)\nprint(f\"{output_regional_compiled.shape=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just like JIT regional compilation, compiling regions within a model ahead-of-time\nleads to significantly reduced cold start times. The actual number will vary from\nmodel to model.\n\nEven though full model compilation offers the fullest scope of optimizations,\nfor practical purposes and depending on the type of model, we have seen regional\ncompilation (both JiT and AoT) providing similar speed benefits, while drastically\nreducing the cold start times.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measuring compilation time\nNext, let's measure the compilation time of the full model and the regional compilation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def measure_compile_time(input, regional=False):\n    start = perf_counter()\n    model = aot_compile_load_model(regional=regional)\n    torch.cuda.synchronize()\n    end = perf_counter()\n    # make sure the model works.\n    _ = model(input)\n    return end - start\n\ndef aot_compile_load_model(regional=False) -> torch.nn.Module:\n    input = torch.randn(10, 10, device=\"cuda\")\n    model = Model().cuda()\n\n    inductor_configs = {}\n    if regional:\n        inductor_configs = {\"aot_inductor.package_constants_in_so\": False}\n\n    # Reset the compiler caches to ensure no reuse between different runs\n    torch.compiler.reset()\n    with torch._inductor.utils.fresh_inductor_cache():\n        path = torch._inductor.aoti_compile_and_package(\n            torch.export.export(\n                model.layers[0] if regional else model,\n                args=(input,)\n            ),\n            inductor_configs=inductor_configs,\n        )\n\n        if regional:\n            for layer in model.layers:\n                compiled_layer = torch._inductor.aoti_load_package(path)\n                compiled_layer.load_constants(\n                    layer.state_dict(), check_full_update=True, user_managed=True\n                )\n                layer.forward = compiled_layer\n        else:\n            model = torch._inductor.aoti_load_package(path)\n    return model\n\ninput = torch.randn(10, 10, device=\"cuda\")\nfull_model_compilation_latency = measure_compile_time(input, regional=False)\nprint(f\"Full model compilation time = {full_model_compilation_latency:.2f} seconds\")\n\nregional_compilation_latency = measure_compile_time(input, regional=True)\nprint(f\"Regional compilation time = {regional_compilation_latency:.2f} seconds\")\n\nassert regional_compilation_latency < full_model_compilation_latency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There may also be layers in a model incompatible with compilation. So,\nfull compilation will result in a fragmented computation graph resulting\nin potential latency degradation. In these case, regional compilation\ncan be beneficial.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nThis recipe shows how to control the cold start time when compiling your\nmodel ahead-of-time. This becomes effective when your model has repeated\nblocks, which is typically seen in large generative models. We used this\nrecipe on various models to speed up real-time performance. Learn more\n[here](https://huggingface.co/blog/zerogpu-aoti)_.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}