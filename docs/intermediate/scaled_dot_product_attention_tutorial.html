
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="(Beta) Scaled Dot Product Attention (SDPA)로 고성능 트랜스포머(Transformers) 구현하기" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/intermediate/scaled_dot_product_attention_tutorial.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Author: Driss Guessous, 번역: 이강희,. 요약: 이 튜토리얼에서, 트랜스포머(Transformer) 아키텍처 구현에 도움이 되는 새로운 torch.nn.functional 모듈의 함수를 소개합니다. 이 함수의 이름은 torch.nn.functional.scaled_dot_product_attention 입니다. 함수에 대한 자세한 설명은 PyTorch 문서 를 참고하세요. 이 함수는 이미 torch.nn.MultiheadAttention 과 torch.nn.TransformerEncoderLayer 에서 사..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="Author: Driss Guessous, 번역: 이강희,. 요약: 이 튜토리얼에서, 트랜스포머(Transformer) 아키텍처 구현에 도움이 되는 새로운 torch.nn.functional 모듈의 함수를 소개합니다. 이 함수의 이름은 torch.nn.functional.scaled_dot_product_attention 입니다. 함수에 대한 자세한 설명은 PyTorch 문서 를 참고하세요. 이 함수는 이미 torch.nn.MultiheadAttention 과 torch.nn.TransformerEncoderLayer 에서 사..." />
<meta property="og:ignore_canonical" content="true" />

    <title>(Beta) Scaled Dot Product Attention (SDPA)로 고성능 트랜스포머(Transformers) 구현하기 &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/scaled_dot_product_attention_tutorial';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/intermediate/scaled_dot_product_attention_tutorial.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="Knowledge Distillation Tutorial" href="../beginner/knowledge_distillation_tutorial.html" />
    <link rel="prev" title="가지치기 기법(Pruning) 튜토리얼" href="pruning_tutorial.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../beginner/profiler.html">PyTorch 모듈 프로파일링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="parametrizations.html">Parametrizations Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="pruning_tutorial.html">가지치기 기법(Pruning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">(Beta) Scaled Dot Product Attention (SDPA)로 고성능 트랜스포머(Transformers) 구현하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/knowledge_distillation_tutorial.html">Knowledge Distillation Tutorial</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frontend APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="memory_format_tutorial.html">(베타) PyTorch를 사용한 Channels Last 메모리 형식</a></li>
<li class="toctree-l1"><a class="reference internal" href="forward_ad_usage.html">Forward-mode Automatic Differentiation (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing function transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="ensembling.html">모델 앙상블</a></li>
<li class="toctree-l1"><a class="reference internal" href="per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_frontend.html">PyTorch C++ 프론트엔드 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_autograd.html">C++ 프론트엔드의 자동 미분 (autograd)</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../deep-dive.html" class="nav-link">Deep Dive</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">(Beta)...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../deep-dive.html">
        <meta itemprop="name" content="Deep Dive">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="(Beta) Scaled Dot Product Attention (SDPA)로 고성능 트랜스포머(Transformers) 구현하기">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">intermediate/scaled_dot_product_attention_tutorial</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-intermediate-scaled-dot-product-attention-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="beta-scaled-dot-product-attention-sdpa-transformers">
<span id="sphx-glr-intermediate-scaled-dot-product-attention-tutorial-py"></span><h1>(Beta) Scaled Dot Product Attention (SDPA)로 고성능 트랜스포머(Transformers) 구현하기<a class="headerlink" href="#beta-scaled-dot-product-attention-sdpa-transformers" title="Link to this heading">#</a></h1>
<dl class="simple">
<dt><strong>Author:</strong> <a class="reference external" href="https://github.com/drisspg">Driss Guessous</a></dt><dd><p><strong>번역:</strong> <a class="reference external" href="https://github.com/khleexv">이강희</a></p>
</dd>
</dl>
<section id="id2">
<h2>요약<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>이 튜토리얼에서, 트랜스포머(Transformer) 아키텍처 구현에 도움이 되는 새로운
<code class="docutils literal notranslate"><span class="pre">torch.nn.functional</span></code> 모듈의 함수를 소개합니다. 이 함수의 이름은 <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.scaled_dot_product_attention</span></code>
입니다. 함수에 대한 자세한 설명은 <a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention">PyTorch 문서</a>
를 참고하세요. 이 함수는 이미 <code class="docutils literal notranslate"><span class="pre">torch.nn.MultiheadAttention</span></code> 과 <code class="docutils literal notranslate"><span class="pre">torch.nn.TransformerEncoderLayer</span></code>
에서 사용되고 있습니다.</p>
</section>
<section id="id3">
<h2>개요<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>고수준에서, 이 PyTorch 함수는 쿼리(query), 키(key), 값(value) 사이의
scaled dot product attention (SDPA)을 계산합니다.
이 함수의 정의는 <a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>
논문에서 찾을 수 있습니다. 이 함수는 기존 함수를 사용하여 PyTorch로 작성할 수 있지만,
퓨즈드(fused) 구현은 단순한 구현보다 큰 성능 이점을 제공할 수 있습니다.</p>
</section>
<section id="id4">
<h2>퓨즈드 구현<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<p>이 함수는 CUDA tensor 입력을 다음 중 하나의 구현을 사용합니다.</p>
<p>구현:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></p></li>
<li><p><a class="reference external" href="https://github.com/facebookresearch/xformers">Memory-Efficient Attention</a></p></li>
<li><p>A PyTorch implementation defined in C++</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>이 튜토리얼은 PyTorch 버전 2.0.0 이상이 필요합니다.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

<span class="c1"># 사용 예시:</span>
<span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[[ 0.8680,  0.2618, -2.2843, -0.5558,  1.1932,  0.5864, -0.3620,
           0.4528],
         [ 1.2298,  0.0192, -2.2668, -0.8033,  1.2979,  0.7643, -0.3926,
           0.4452],
         [-0.4257,  1.1875, -2.3799,  0.4137,  0.9222,  0.0428, -0.1453,
           0.5159]],

        [[ 0.6684,  0.5522,  1.3357, -1.0025,  0.8803, -0.3691, -0.0389,
          -0.2312],
         [ 0.2910,  0.4083,  1.3475, -0.4995,  0.8208, -0.1056, -0.2067,
          -0.3080],
         [ 0.4944,  0.5131,  1.3829, -0.8416,  0.8736, -0.1767, -0.0471,
          -0.2915]]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</section>
<section id="dispatcher">
<h2>명시적 Dispatcher 제어<a class="headerlink" href="#dispatcher" title="Link to this heading">#</a></h2>
<p>이 함수는 암시적으로 세 가지 구현 중 하나를 사용합니다. 하지만 컨텍스트 매니저를
사용하면 명시적으로 어떤 구현을 사용할 지 제어할 수 있습니다. 컨텍스트 매니저를 통해
특정 구현을 명시적으로 비활성화 할 수 있습니다. 특정 입력에 대한 가장 빠른 구현을 찾고자
한다면, 컨텍스트 매니저로 모든 구현의 성능을 측정해볼 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 벤치마크 함수를 정의합니다</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.benchmark</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">benchmark</span>
<span class="k">def</span><span class="w"> </span><span class="nf">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s2">&quot;f(*args, **kwargs)&quot;</span><span class="p">,</span> <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;args&quot;</span><span class="p">:</span> <span class="n">args</span><span class="p">,</span> <span class="s2">&quot;kwargs&quot;</span><span class="p">:</span> <span class="n">kwargs</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">:</span> <span class="n">f</span><span class="p">}</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">t0</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span> <span class="o">*</span> <span class="mf">1e6</span>

<span class="c1"># 입력의 하이퍼파라미터를 정의합니다</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">max_sequence_len</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">embed_dimension</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>

<span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">max_sequence_len</span><span class="p">,</span> <span class="n">embed_dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">max_sequence_len</span><span class="p">,</span> <span class="n">embed_dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">max_sequence_len</span><span class="p">,</span> <span class="n">embed_dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The default implementation runs in </span><span class="si">{</span><span class="n">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">,</span><span class="w"> </span><span class="n">query</span><span class="p">,</span><span class="w"> </span><span class="n">key</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> microseconds&quot;</span><span class="p">)</span>

<span class="c1"># 세 가지 구현의 속도를 측정합니다</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.attention</span><span class="w"> </span><span class="kn">import</span> <span class="n">SDPBackend</span><span class="p">,</span> <span class="n">sdpa_kernel</span>


<span class="k">with</span> <span class="n">sdpa_kernel</span><span class="p">(</span><span class="n">SDPBackend</span><span class="o">.</span><span class="n">MATH</span><span class="p">):</span>
    <span class="n">math_time</span><span class="o">=</span><span class="n">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The math implementation runs in </span><span class="si">{</span><span class="n">math_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> microseconds&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">sdpa_kernel</span><span class="p">(</span><span class="n">SDPBackend</span><span class="o">.</span><span class="n">FLASH_ATTENTION</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">flash_time</span><span class="o">=</span><span class="n">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The flash attention implementation runs in </span><span class="si">{</span><span class="n">flash_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> microseconds&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FlashAttention is not supported. See warnings for reasons.&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">sdpa_kernel</span><span class="p">(</span><span class="n">SDPBackend</span><span class="o">.</span><span class="n">EFFICIENT_ATTENTION</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">efficient_time</span><span class="o">=</span><span class="n">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The memory efficient implementation runs in </span><span class="si">{</span><span class="n">efficient_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> microseconds&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EfficientAttention is not supported. See warnings for reasons.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>The default implementation runs in 680.279 microseconds
The math implementation runs in 13737.935 microseconds
The flash attention implementation runs in 680.598 microseconds
The memory efficient implementation runs in 1890.781 microseconds
</pre></div>
</div>
</section>
<section id="id5">
<h2>하드웨어 의존성<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>위 셀을 어떤 머신에서 실행했는지와 사용 가능한 하드웨어에 따라 결과가 다를 수 있습니다.
- GPU가 없고 FP32 지원 CPU에서 실행 중이라면 컨텍스트 매니저는 효과가 없고 세 가지 실행 모두
유사한 시간을 반환할 것입니다.
- 그래픽 카드가 지원하는 컴퓨팅 능력에 따라 flash attention 또는
memory efficient 구현이 동작하지 않을 수 있습니다.</p>
</section>
<section id="causal-self-attention">
<h2>Causal Self Attention<a class="headerlink" href="#causal-self-attention" title="Link to this heading">#</a></h2>
<p>아래는 multi-head causal self attention 블록의 구현 예시입니다.
<a class="reference external" href="https://github.com/karpathy/nanoGPT">Andrej Karpathy NanoGPT</a> 저장소를 참고했습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CausalSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embed_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">is_causal</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">embed_dimension</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="c1"># key, query, value projections for all heads, but in a batch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dimension</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">embed_dimension</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="c1"># output projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dimension</span><span class="p">,</span> <span class="n">embed_dimension</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="c1"># regularization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resid_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dimension</span> <span class="o">=</span> <span class="n">embed_dimension</span>
        <span class="c1"># Perform causal masking</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_causal</span> <span class="o">=</span> <span class="n">is_causal</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># calculate query, key, values for all heads in batch and move head forward to be the batch dim</span>
        <span class="n">query_projected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">c_attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">query_projected</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">query_projected</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>

        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">query_projected</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">dropout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span>
            <span class="n">is_causal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_causal</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">is_causal</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resid_dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">c_proj</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">y</span>


<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">heads_per_dim</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">embed_dimension</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">heads_per_dim</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CausalSelfAttention</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">embed_dimension</span><span class="o">=</span><span class="n">embed_dimension</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>CausalSelfAttention(
  (c_attn): Linear(in_features=512, out_features=1536, bias=False)
  (c_proj): Linear(in_features=512, out_features=512, bias=False)
  (resid_dropout): Dropout(p=0.1, inplace=False)
)
</pre></div>
</div>
<section id="nestedtensor-dense-tensor">
<h3><code class="docutils literal notranslate"><span class="pre">NestedTensor</span></code> 및 Dense tensor 지원<a class="headerlink" href="#nestedtensor-dense-tensor" title="Link to this heading">#</a></h3>
<p>SDPA는 <code class="docutils literal notranslate"><span class="pre">NestedTensor</span></code> 와 Dense tensor 입력을 모두 지원합니다.
<code class="docutils literal notranslate"><span class="pre">NestedTensors</span></code> 는 입력이 가변 길이 시퀀스로 구성된 배치인 경우에
배치 내 시퀀스의 최대 길이에 맞춰 각 시퀀스를 패딩할 필요가 없습니다. <code class="docutils literal notranslate"><span class="pre">NestedTensors</span></code> 에 대한 자세한 내용은
<a class="reference external" href="https://pytorch.org/docs/stable/nested.html">torch.nested</a> 와 <a class="reference external" href="https://tutorials.pytorch.kr/prototype/nestedtensor.html">NestedTensors 튜토리얼</a> 을 참고하세요.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_rand_batch</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="p">,</span>
    <span class="n">max_sequence_len</span><span class="p">,</span>
    <span class="n">embed_dimension</span><span class="p">,</span>
    <span class="n">pad_percentage</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">pad_percentage</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span>
                <span class="n">max_sequence_len</span><span class="p">,</span>
                <span class="n">embed_dimension</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="c1"># Random sequence lengths</span>
    <span class="n">seq_len_list</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nb">int</span><span class="p">(</span><span class="n">max_sequence_len</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">random</span><span class="o">.</span><span class="n">gauss</span><span class="p">(</span><span class="n">pad_percentage</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="c1"># Make random entry in the batch have max sequence length</span>
    <span class="n">seq_len_list</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">max_sequence_len</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">embed_dimension</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">seq_len</span> <span class="ow">in</span> <span class="n">seq_len_list</span>
            <span class="p">]</span>
        <span class="p">),</span>
        <span class="n">seq_len_list</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">random_nt</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">generate_rand_batch</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">embed_dimension</span><span class="p">,</span> <span class="n">pad_percentage</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">random_dense</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">generate_rand_batch</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">embed_dimension</span><span class="p">,</span> <span class="n">pad_percentage</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 현재 퓨즈드(fused) 구현은 ``NestedTensor`` 로 학습하는 것을 지원하지 않습니다.</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="k">with</span> <span class="n">sdpa_kernel</span><span class="p">(</span><span class="n">SDPBackend</span><span class="o">.</span><span class="n">FLASH_ATTENTION</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random NT runs in </span><span class="si">{</span><span class="n">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">random_nt</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> microseconds&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random Dense runs in </span><span class="si">{</span><span class="n">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">random_dense</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> microseconds&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FlashAttention is not supported. See warnings for reasons.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/opt/conda/lib/python3.11/site-packages/torch/nested/__init__.py:250: UserWarning:

The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)

Random NT runs in 279.788 microseconds
Random Dense runs in 141.282 microseconds
</pre></div>
</div>
</section>
</section>
<section id="torch-compile-sdpa">
<h2><code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> 과 함께 SDPA 사용하기<a class="headerlink" href="#torch-compile-sdpa" title="Link to this heading">#</a></h2>
<p>PyTorch 2.0 릴리즈와 함께 <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code> 라는 새로운 기능이 추가되었는데,
이는 eager mode보다 상당한 성능 향상을 제공할 수 있습니다.
Scaled dot product attention은 <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code> 로 완전히 구성할 수 있습니다.
이를 확인하기 위해 <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code> 을 통해 <code class="docutils literal notranslate"><span class="pre">CausalSelfAttention</span></code> 모듈을 컴파일하고
결과적으로 얻어지는 성능 향상을 알아봅시다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">max_sequence_len</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_sequence_len</span><span class="p">,</span>
               <span class="n">embed_dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;The non compiled module runs in  </span><span class="si">{</span><span class="n">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> microseconds&quot;</span><span class="p">)</span>


<span class="n">compiled_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="c1"># Let&#39;s compile it</span>
<span class="n">compiled_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;The compiled module runs in  </span><span class="si">{</span><span class="n">benchmark_torch_function_in_microseconds</span><span class="p">(</span><span class="n">compiled_model</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> microseconds&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>The non compiled module runs in  95.736 microseconds
The compiled module runs in  146.114 microseconds
</pre></div>
</div>
<p>정확한 실행 시간은 환경에 따라 다르지만, 다음은 저자의 결과입니다.
컴파일 되지 않은 모듈은 실행에 166.616ms 가 소요되었습니다.
컴파일 된 모듈은 실행에 166.726ms 가 소요되었습니다.
이는 우리의 예상과는 다릅니다. 좀 더 자세히 알아봅시다.
PyTorch는 코드의 성능 특성을 점검할 수 있는 놀라운 내장(built-in) 프로파일러를 제공합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.profiler</span><span class="w"> </span><span class="kn">import</span> <span class="n">profile</span><span class="p">,</span> <span class="n">record_function</span><span class="p">,</span> <span class="n">ProfilerActivity</span>
<span class="n">activities</span> <span class="o">=</span> <span class="p">[</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">]</span>
<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
    <span class="n">activities</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">)</span>

<span class="k">with</span> <span class="n">profile</span><span class="p">(</span><span class="n">activities</span><span class="o">=</span><span class="n">activities</span><span class="p">,</span> <span class="n">record_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">&quot; Non-Compilied Causal Attention&quot;</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">25</span><span class="p">):</span>
            <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">()</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;cuda_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>


<span class="k">with</span> <span class="n">profile</span><span class="p">(</span><span class="n">activities</span><span class="o">=</span><span class="n">activities</span><span class="p">,</span> <span class="n">record_shapes</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;Compiled Causal Attention&quot;</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">25</span><span class="p">):</span>
            <span class="n">compiled_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">()</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;cuda_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Non-Compilied Causal Attention         0.00%       0.000us         0.00%       0.000us       0.000us       4.243ms       314.43%       4.243ms       4.243ms             1
                         Non-Compilied Causal Attention        19.50%       1.212ms        99.86%       6.209ms       6.209ms       0.000us         0.00%       1.373ms       1.373ms             1
                                           aten::linear         1.06%      65.900us        54.13%       3.366ms      67.312us       0.000us         0.00%     844.758us      16.895us            50
                                           aten::matmul         2.00%     124.161us        50.59%       3.145ms      62.903us       0.000us         0.00%     844.758us      16.895us            50
                                               aten::mm        13.01%     808.680us        46.30%       2.878ms      57.568us     821.430us        60.87%     844.758us      16.895us            50
              nvjet_hsh_256x128_64x4_1x2_h_bz_coopA_TNT         0.00%       0.000us         0.00%       0.000us       0.000us     802.873us        59.50%     802.873us      16.057us            50
                     aten::scaled_dot_product_attention         1.68%     104.481us        17.33%       1.078ms      43.103us       0.000us         0.00%     528.031us      21.121us            25
              aten::_scaled_dot_product_flash_attention         2.54%     157.700us        15.65%     973.089us      38.924us       0.000us         0.00%     528.031us      21.121us            25
                         aten::_flash_attention_forward         2.98%     185.202us        11.66%     724.993us      29.000us     528.031us        39.13%     528.031us      21.121us            25
void pytorch_flash::flash_fwd_kernel&lt;Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us     528.031us        39.13%     528.031us      21.121us            25
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 6.217ms
Self CUDA time total: 1.349ms

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                              Compiled Causal Attention         0.00%       0.000us         0.00%       0.000us       0.000us       4.677ms       343.47%       4.677ms       4.677ms             1
                              Compiled Causal Attention         8.16%     549.779us        99.85%       6.728ms       6.728ms       0.000us         0.00%       1.362ms       1.362ms             1
                             Torch-Compiled Region: 0/0         8.47%     570.660us        89.28%       6.015ms     240.616us       0.000us         0.00%       1.362ms      54.470us            25
                                       CompiledFunction        23.53%       1.586ms        77.89%       5.248ms     209.929us       0.000us         0.00%       1.362ms      54.470us            25
                                               aten::mm         9.73%     655.325us        14.68%     989.330us      19.787us     820.091us        60.22%     820.091us      16.402us            50
              nvjet_hsh_256x128_64x4_1x2_h_bz_coopA_TNT         0.00%       0.000us         0.00%       0.000us       0.000us     801.693us        58.87%     801.693us      16.034us            50
              aten::_scaled_dot_product_flash_attention         1.74%     117.542us        13.53%     911.568us      36.463us       0.000us         0.00%     541.665us      21.667us            25
                         aten::_flash_attention_forward         2.92%     196.416us        10.30%     693.809us      27.752us     541.665us        39.78%     541.665us      21.667us            25
void pytorch_flash::flash_fwd_kernel&lt;Flash_fwd_kerne...         0.00%       0.000us         0.00%       0.000us       0.000us     541.665us        39.78%     541.665us      21.667us            25
                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us      18.398us         1.35%      18.398us       0.736us            25
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 6.738ms
Self CUDA time total: 1.362ms
</pre></div>
</div>
<p>더 많은 정보를 얻기 위해 추적(trace)를 내보내고 <a href="#id6"><span class="problematic" id="id7">``</span></a>chrome://tracing``을 사용하여 결과를 확인해보세요.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prof</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="p">(</span><span class="s2">&quot;compiled_causal_attention_trace.json&quot;</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
<p>이전 코드 조각(snippet)은 컴파일 된 모듈과 컴파일되지 않은 모듈 모두에 대해
가장 많은 GPU 실행 시간을 차지한 상위 10개의 PyTorch 함수에 대한 보고서를 생성합니다.
분석 결과, 두 모듈 모두 GPU에서 소요된 시간의 대부분이
동일한 함수들에 집중되어 있음을 보여줍니다.
PyTorch가 프레임워크 오버헤드를 제거하는 데 매우 탁월한 <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> 를
제공하기 때문입니다. <code class="docutils literal notranslate"><span class="pre">CausalSelfAttention</span></code> 같은 경우처럼 크고, 효율적인 CUDA 커널을
사용하는 모델에서 PyTorch 오버헤드는 작아질 것입니다.</p>
<p>사실, 모듈은 보통 <code class="docutils literal notranslate"><span class="pre">CausalSelfAttention</span></code> 블럭 하나만으로 구성되지 않습니다.
<a class="reference external" href="https://github.com/karpathy/nanoGPT">Andrej Karpathy NanoGPT</a> 저장소에서 실험한 경우,
모듈을 컴파일 하는 것은 학습의 각 단계별 소요 시간을 <code class="docutils literal notranslate"><span class="pre">6090.49ms</span></code> 에서 <code class="docutils literal notranslate"><span class="pre">3273.17ms</span></code> 로
줄일 수 있었습니다. 이 실험은 NanoGPT 저장소의 <code class="docutils literal notranslate"><span class="pre">ae3a8d5</span></code> 커밋에서 Shakespeare
데이터셋을 사용하여 진행되었습니다.</p>
</section>
<section id="sdpa-atteition-bias">
<h2>SDPA를 <code class="docutils literal notranslate"><span class="pre">atteition.bias</span></code> 하위 클래스와 사용하기<a class="headerlink" href="#sdpa-atteition-bias" title="Link to this heading">#</a></h2>
<p>PyTorch 2.3부터 텐서 하위 클래스를 포함하는 새로운 서브모듈을 추가했습니다.
추가된 모듈의 이름은 <code class="docutils literal notranslate"><span class="pre">torch.nn.attention.bias</span></code> 이며, <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.scaled_dot_product_attention</span></code>
와 함께 사용할 수 있도록 설계되었습니다. 또한, 인과적 어텐션 변형(Causal Attention Variants)을 생성하기
위한 다음 2가지 기능(utilities)을 포함하고 있습니다:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn.attention.bias.causal_upper_left</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn.attention.bias.causal_lower_right</span></code></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>현재 <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.scaled_dot_product_attention</span></code> 의 <code class="docutils literal notranslate"><span class="pre">is_causal</span></code> 인자(argument)는
<code class="docutils literal notranslate"><span class="pre">torch.nn.attention.bias.causal_upper_left</span></code> 를 사용하는 것과 동일합니다.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.attention.bias</span><span class="w"> </span><span class="kn">import</span> <span class="n">causal_lower_right</span><span class="p">,</span> <span class="n">causal_upper_left</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">sequence_length_q</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">sequence_length_kv</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">embed_dimension</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>

<span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">sequence_length_q</span><span class="p">,</span> <span class="n">embed_dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">sequence_length_kv</span><span class="p">,</span> <span class="n">embed_dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">sequence_length_kv</span><span class="p">,</span> <span class="n">embed_dimension</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">upper_left_bias</span> <span class="o">=</span> <span class="n">causal_upper_left</span><span class="p">(</span><span class="n">sequence_length_q</span><span class="p">,</span> <span class="n">sequence_length_kv</span><span class="p">)</span>
<span class="n">lower_right_bias</span> <span class="o">=</span> <span class="n">causal_lower_right</span><span class="p">(</span><span class="n">sequence_length_q</span><span class="p">,</span> <span class="n">sequence_length_kv</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">upper_left_bias</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">lower_right_bias</span><span class="p">))</span>

<span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">upper_left_bias</span><span class="p">)</span> <span class="o">==</span> <span class="nb">type</span><span class="p">(</span><span class="n">lower_right_bias</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">issubclass</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">upper_left_bias</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>

<span class="c1"># 위의 출력에서 볼 수 있듯, 두 객체는 같은 타입인 ``torch.nn.attention.bias.CausalBias`` 이며,</span>
<span class="c1"># ``torch.Tensor`` 의 하위 클래스(subclass)입니다.</span>

<span class="c1"># 각 텐서들이 어떻게 생겼는지 살펴보겠습니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">upper_left_bias</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lower_right_bias</span><span class="p">)</span>

<span class="c1"># Upper Left Bias는 인과적 어텐션 마스크(causal attention mask)를 어텐션 점수 행렬(attention scores matrix)의 왼쪽 상단에 정렬합니다.</span>
<span class="c1"># 이는 어텐션 점수 행렬이 정사각형이 아닌 경우에만 영향을 미치며, 이는 디코딩 상황에서 일반적인 경우입니다.</span>
<span class="c1"># 이 개념을 다른 방식으로 생각하는 방법은, upper left bias를 사용할 때는 쿼리(query)의 0번째 토큰이 키(key)의 0번째 토큰과 정렬된다고</span>
<span class="c1"># 생각하는 것입니다. 즉, 어텐션 점수 행렬(attention score matrix)이 2차원이라고 가정할 때, ``attn_score[0][0]`` 이 쿼리의 0번째 토큰과</span>
<span class="c1"># 키의 0번째 토큰 사이의 어텐션 점수인 것입니다.</span>
<span class="c1"># Lower Right Bias의 경우에는 쿼리(query)의 마지막 토큰이 키(key)의 마지막 토큰과 정렬되도록 쿼리(query)의 시퀀스를 정렬합니다.</span>
<span class="c1"># 예를 들어, ``attn_score[-1][-1]`` 은 쿼리와 키의 길이가 서로 다르더라도 쿼리의 마지막 토큰과 키의 마지막 토큰이 같은 위치에 있기 때문에</span>
<span class="c1"># 모두 True입니다.</span>

<span class="c1"># SDPA와 함께 사용하기 위한 객체들입니다.</span>
<span class="n">out_upper_left</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">upper_left_bias</span><span class="p">)</span>
<span class="n">out_lower_right</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">lower_right_bias</span><span class="p">)</span>
<span class="n">out_is_causal</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">out_upper_left</span><span class="p">,</span> <span class="n">out_is_causal</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">out_upper_left</span><span class="p">,</span> <span class="n">out_lower_right</span><span class="p">)</span>

<span class="c1"># 아래 어텐션 편향(attention bias)들은 torch.compile과 호환됩니다.</span>
<span class="n">compiled_sdpa</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out_upper_left</span> <span class="o">=</span> <span class="n">compiled_sdpa</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">upper_left_bias</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;torch.nn.attention.bias.CausalBias&#39;&gt;
&lt;class &#39;torch.nn.attention.bias.CausalBias&#39;&gt;
tensor([[ True, False, False, False, False, False, False, False, False, False],
        [ True,  True, False, False, False, False, False, False, False, False]])
tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True, False],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])
</pre></div>
</div>
</section>
<section id="id8">
<h2>결론<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<p>이 튜토리얼에서, <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.scaled_dot_product_attention</span></code> 의 기본적인
사용법을 살펴봤습니다. <code class="docutils literal notranslate"><span class="pre">sdpa_kernel</span></code> 컨텍스트 매니저로 GPU가 특정 구현을
사용하도록 할 수 있다는 것을 보았습니다. 또한, 간단한 <code class="docutils literal notranslate"><span class="pre">NestedTensor</span></code> 에서 작동하고
컴파일 가능한 <code class="docutils literal notranslate"><span class="pre">CausalSelfAttention</span></code> 모듈을 만들었습니다.
이 과정에서 프로파일링 도구를 사용하여 유저가 정의한 모듈의 성능 특성을 어떻게
확인할 수 있는지도 살펴봤습니다.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 12.102 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-scaled-dot-product-attention-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/fc133e4ffc6275f9d1c3a74ddd10e0a2/scaled_dot_product_attention_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">scaled_dot_product_attention_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/e40ced94a143a49f0f8745e10c981139/scaled_dot_product_attention_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">scaled_dot_product_attention_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/ac14cbb6e0ecc257ad2661072fa715c2/scaled_dot_product_attention_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">scaled_dot_product_attention_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="pruning_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">가지치기 기법(Pruning) 튜토리얼</p>
      </div>
    </a>
    <a class="right-next"
       href="../beginner/knowledge_distillation_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Knowledge Distillation Tutorial</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="pruning_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">가지치기 기법(Pruning) 튜토리얼</p>
      </div>
    </a>
    <a class="right-next"
       href="../beginner/knowledge_distillation_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Knowledge Distillation Tutorial</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">요약</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">개요</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">퓨즈드 구현</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dispatcher">명시적 Dispatcher 제어</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">하드웨어 의존성</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#causal-self-attention">Causal Self Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nestedtensor-dense-tensor"><code class="docutils literal notranslate"><span class="pre">NestedTensor</span></code> 및 Dense tensor 지원</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-compile-sdpa"><code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> 과 함께 SDPA 사용하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sdpa-atteition-bias">SDPA를 <code class="docutils literal notranslate"><span class="pre">atteition.bias</span></code> 하위 클래스와 사용하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">결론</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "(Beta) Scaled Dot Product Attention (SDPA)\ub85c \uace0\uc131\ub2a5 \ud2b8\ub79c\uc2a4\ud3ec\uba38(Transformers) \uad6c\ud604\ud558\uae30",
       "headline": "(Beta) Scaled Dot Product Attention (SDPA)\ub85c \uace0\uc131\ub2a5 \ud2b8\ub79c\uc2a4\ud3ec\uba38(Transformers) \uad6c\ud604\ud558\uae30",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/intermediate/scaled_dot_product_attention_tutorial.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. (Beta) Scaled Dot Product Attention (SDPA)\ub85c \uace0\uc131\ub2a5 \ud2b8\ub79c\uc2a4\ud3ec\uba38(Transformers) \uad6c\ud604\ud558\uae30# Author: Driss Guessous\ubc88\uc5ed: \uc774\uac15\ud76c \uc694\uc57d# \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c, \ud2b8\ub79c\uc2a4\ud3ec\uba38(Transformer) \uc544\ud0a4\ud14d\ucc98 \uad6c\ud604\uc5d0 \ub3c4\uc6c0\uc774 \ub418\ub294 \uc0c8\ub85c\uc6b4 torch.nn.functional \ubaa8\ub4c8\uc758 \ud568\uc218\ub97c \uc18c\uac1c\ud569\ub2c8\ub2e4. \uc774 \ud568\uc218\uc758 \uc774\ub984\uc740 torch.nn.functional.scaled_dot_product_attention \uc785\ub2c8\ub2e4. \ud568\uc218\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \uc124\uba85\uc740 PyTorch \ubb38\uc11c \ub97c \ucc38\uace0\ud558\uc138\uc694. \uc774 \ud568\uc218\ub294 \uc774\ubbf8 torch.nn.MultiheadAttention \uacfc torch.nn.TransformerEncoderLayer \uc5d0\uc11c \uc0ac\uc6a9\ub418\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uac1c\uc694# \uace0\uc218\uc900\uc5d0\uc11c, \uc774 PyTorch \ud568\uc218\ub294 \ucffc\ub9ac(query), \ud0a4(key), \uac12(value) \uc0ac\uc774\uc758 scaled dot product attention (SDPA)\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4. \uc774 \ud568\uc218\uc758 \uc815\uc758\ub294 Attention is all you need \ub17c\ubb38\uc5d0\uc11c \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud568\uc218\ub294 \uae30\uc874 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec PyTorch\ub85c \uc791\uc131\ud560 \uc218 \uc788\uc9c0\ub9cc, \ud4e8\uc988\ub4dc(fused) \uad6c\ud604\uc740 \ub2e8\uc21c\ud55c \uad6c\ud604\ubcf4\ub2e4 \ud070 \uc131\ub2a5 \uc774\uc810\uc744 \uc81c\uacf5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud4e8\uc988\ub4dc \uad6c\ud604# \uc774 \ud568\uc218\ub294 CUDA tensor \uc785\ub825\uc744 \ub2e4\uc74c \uc911 \ud558\ub098\uc758 \uad6c\ud604\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uad6c\ud604: FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Memory-Efficient Attention A PyTorch implementation defined in C++ \ucc38\uace0 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 PyTorch \ubc84\uc804 2.0.0 \uc774\uc0c1\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. import torch import torch.nn as nn import torch.nn.functional as F device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # \uc0ac\uc6a9 \uc608\uc2dc: query, key, value = torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device) F.scaled_dot_product_attention(query, key, value) tensor([[[ 0.8680, 0.2618, -2.2843, -0.5558, 1.1932, 0.5864, -0.3620, 0.4528], [ 1.2298, 0.0192, -2.2668, -0.8033, 1.2979, 0.7643, -0.3926, 0.4452], [-0.4257, 1.1875, -2.3799, 0.4137, 0.9222, 0.0428, -0.1453, 0.5159]], [[ 0.6684, 0.5522, 1.3357, -1.0025, 0.8803, -0.3691, -0.0389, -0.2312], [ 0.2910, 0.4083, 1.3475, -0.4995, 0.8208, -0.1056, -0.2067, -0.3080], [ 0.4944, 0.5131, 1.3829, -0.8416, 0.8736, -0.1767, -0.0471, -0.2915]]], device=\u0027cuda:0\u0027) \uba85\uc2dc\uc801 Dispatcher \uc81c\uc5b4# \uc774 \ud568\uc218\ub294 \uc554\uc2dc\uc801\uc73c\ub85c \uc138 \uac00\uc9c0 \uad6c\ud604 \uc911 \ud558\ub098\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \ucee8\ud14d\uc2a4\ud2b8 \ub9e4\ub2c8\uc800\ub97c \uc0ac\uc6a9\ud558\uba74 \uba85\uc2dc\uc801\uc73c\ub85c \uc5b4\ub5a4 \uad6c\ud604\uc744 \uc0ac\uc6a9\ud560 \uc9c0 \uc81c\uc5b4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ucee8\ud14d\uc2a4\ud2b8 \ub9e4\ub2c8\uc800\ub97c \ud1b5\ud574 \ud2b9\uc815 \uad6c\ud604\uc744 \uba85\uc2dc\uc801\uc73c\ub85c \ube44\ud65c\uc131\ud654 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\uc815 \uc785\ub825\uc5d0 \ub300\ud55c \uac00\uc7a5 \ube60\ub978 \uad6c\ud604\uc744 \ucc3e\uace0\uc790 \ud55c\ub2e4\uba74, \ucee8\ud14d\uc2a4\ud2b8 \ub9e4\ub2c8\uc800\ub85c \ubaa8\ub4e0 \uad6c\ud604\uc758 \uc131\ub2a5\uc744 \uce21\uc815\ud574\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. # \ubca4\uce58\ub9c8\ud06c \ud568\uc218\ub97c \uc815\uc758\ud569\ub2c8\ub2e4 import torch.utils.benchmark as benchmark def benchmark_torch_function_in_microseconds(f, *args, **kwargs): t0 = benchmark.Timer( stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f} ) return t0.blocked_autorange().mean * 1e6 # \uc785\ub825\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uc815\uc758\ud569\ub2c8\ub2e4 batch_size = 32 max_sequence_len = 1024 num_heads = 32 embed_dimension = 32 dtype = torch.float16 query = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype) key = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype) value = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype) print(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\") # \uc138 \uac00\uc9c0 \uad6c\ud604\uc758 \uc18d\ub3c4\ub97c \uce21\uc815\ud569\ub2c8\ub2e4 from torch.nn.attention import SDPBackend, sdpa_kernel with sdpa_kernel(SDPBackend.MATH): math_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value) print(f\"The math implementation runs in {math_time:.3f} microseconds\") with sdpa_kernel(SDPBackend.FLASH_ATTENTION): try: flash_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value) print(f\"The flash attention implementation runs in {flash_time:.3f} microseconds\") except RuntimeError: print(\"FlashAttention is not supported. See warnings for reasons.\") with sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION): try: efficient_time=benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value) print(f\"The memory efficient implementation runs in {efficient_time:.3f} microseconds\") except RuntimeError: print(\"EfficientAttention is not supported. See warnings for reasons.\") The default implementation runs in 680.279 microseconds The math implementation runs in 13737.935 microseconds The flash attention implementation runs in 680.598 microseconds The memory efficient implementation runs in 1890.781 microseconds \ud558\ub4dc\uc6e8\uc5b4 \uc758\uc874\uc131# \uc704 \uc140\uc744 \uc5b4\ub5a4 \uba38\uc2e0\uc5d0\uc11c \uc2e4\ud589\ud588\ub294\uc9c0\uc640 \uc0ac\uc6a9 \uac00\ub2a5\ud55c \ud558\ub4dc\uc6e8\uc5b4\uc5d0 \ub530\ub77c \uacb0\uacfc\uac00 \ub2e4\ub97c \uc218 \uc788\uc2b5\ub2c8\ub2e4. - GPU\uac00 \uc5c6\uace0 FP32 \uc9c0\uc6d0 CPU\uc5d0\uc11c \uc2e4\ud589 \uc911\uc774\ub77c\uba74 \ucee8\ud14d\uc2a4\ud2b8 \ub9e4\ub2c8\uc800\ub294 \ud6a8\uacfc\uac00 \uc5c6\uace0 \uc138 \uac00\uc9c0 \uc2e4\ud589 \ubaa8\ub450 \uc720\uc0ac\ud55c \uc2dc\uac04\uc744 \ubc18\ud658\ud560 \uac83\uc785\ub2c8\ub2e4. - \uadf8\ub798\ud53d \uce74\ub4dc\uac00 \uc9c0\uc6d0\ud558\ub294 \ucef4\ud4e8\ud305 \ub2a5\ub825\uc5d0 \ub530\ub77c flash attention \ub610\ub294 memory efficient \uad6c\ud604\uc774 \ub3d9\uc791\ud558\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Causal Self Attention# \uc544\ub798\ub294 multi-head causal self attention \ube14\ub85d\uc758 \uad6c\ud604 \uc608\uc2dc\uc785\ub2c8\ub2e4. Andrej Karpathy NanoGPT \uc800\uc7a5\uc18c\ub97c \ucc38\uace0\ud588\uc2b5\ub2c8\ub2e4. class CausalSelfAttention(nn.Module): def __init__(self, num_heads: int, embed_dimension: int, bias: bool=False, is_causal: bool=False, dropout:float=0.0): super().__init__() assert embed_dimension % num_heads == 0 # key, query, value projections for all heads, but in a batch self.c_attn = nn.Linear(embed_dimension, 3 * embed_dimension, bias=bias) # output projection self.c_proj = nn.Linear(embed_dimension, embed_dimension, bias=bias) # regularization self.dropout = dropout self.resid_dropout = nn.Dropout(dropout) self.num_heads = num_heads self.embed_dimension = embed_dimension # Perform causal masking self.is_causal = is_causal def forward(self, x): # calculate query, key, values for all heads in batch and move head forward to be the batch dim query_projected = self.c_attn(x) batch_size = query_projected.size(0) embed_dim = query_projected.size(2) head_dim = embed_dim // (self.num_heads * 3) query, key, value = query_projected.chunk(3, -1) query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2) key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2) value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2) if self.training: dropout = self.dropout is_causal = self.is_causal else: dropout = 0.0 is_causal = False y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=dropout, is_causal=is_causal) y = y.transpose(1, 2).view(batch_size, -1, self.num_heads * head_dim) y = self.resid_dropout(self.c_proj(y)) return y num_heads = 8 heads_per_dim = 64 embed_dimension = num_heads * heads_per_dim dtype = torch.float16 model = CausalSelfAttention(num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_causal=True, dropout=0.1).to(\"cuda\").to(dtype).eval() print(model) CausalSelfAttention( (c_attn): Linear(in_features=512, out_features=1536, bias=False) (c_proj): Linear(in_features=512, out_features=512, bias=False) (resid_dropout): Dropout(p=0.1, inplace=False) ) NestedTensor \ubc0f Dense tensor \uc9c0\uc6d0# SDPA\ub294 NestedTensor \uc640 Dense tensor \uc785\ub825\uc744 \ubaa8\ub450 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. NestedTensors \ub294 \uc785\ub825\uc774 \uac00\ubcc0 \uae38\uc774 \uc2dc\ud000\uc2a4\ub85c \uad6c\uc131\ub41c \ubc30\uce58\uc778 \uacbd\uc6b0\uc5d0 \ubc30\uce58 \ub0b4 \uc2dc\ud000\uc2a4\uc758 \ucd5c\ub300 \uae38\uc774\uc5d0 \ub9de\ucdb0 \uac01 \uc2dc\ud000\uc2a4\ub97c \ud328\ub529\ud560 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4. NestedTensors \uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 torch.nested \uc640 NestedTensors \ud29c\ud1a0\ub9ac\uc5bc \uc744 \ucc38\uace0\ud558\uc138\uc694. import random def generate_rand_batch( batch_size, max_sequence_len, embed_dimension, pad_percentage=None, dtype=torch.float16, device=\"cuda\", ): if not pad_percentage: return ( torch.randn( batch_size, max_sequence_len, embed_dimension, dtype=dtype, device=device, ), None, ) # Random sequence lengths seq_len_list = [ int(max_sequence_len * (1 - random.gauss(pad_percentage, 0.01))) for _ in range(batch_size) ] # Make random entry in the batch have max sequence length seq_len_list[random.randint(0, batch_size - 1)] = max_sequence_len return ( torch.nested.nested_tensor( [ torch.randn(seq_len, embed_dimension, dtype=dtype, device=device) for seq_len in seq_len_list ] ), seq_len_list, ) random_nt, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=0.5, dtype=dtype, device=device) random_dense, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=None, dtype=dtype, device=device) # \ud604\uc7ac \ud4e8\uc988\ub4dc(fused) \uad6c\ud604\uc740 ``NestedTensor`` \ub85c \ud559\uc2b5\ud558\ub294 \uac83\uc744 \uc9c0\uc6d0\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. model.eval() with sdpa_kernel(SDPBackend.FLASH_ATTENTION): try: print(f\"Random NT runs in {benchmark_torch_function_in_microseconds(model, random_nt):.3f} microseconds\") print(f\"Random Dense runs in {benchmark_torch_function_in_microseconds(model, random_dense):.3f} microseconds\") except RuntimeError: print(\"FlashAttention is not supported. See warnings for reasons.\") /opt/conda/lib/python3.11/site-packages/torch/nested/__init__.py:250: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.) Random NT runs in 279.788 microseconds Random Dense runs in 141.282 microseconds torch.compile \uacfc \ud568\uaed8 SDPA \uc0ac\uc6a9\ud558\uae30# PyTorch 2.0 \ub9b4\ub9ac\uc988\uc640 \ud568\uaed8 torch.compile() \ub77c\ub294 \uc0c8\ub85c\uc6b4 \uae30\ub2a5\uc774 \ucd94\uac00\ub418\uc5c8\ub294\ub370, \uc774\ub294 eager mode\ubcf4\ub2e4 \uc0c1\ub2f9\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc81c\uacf5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Scaled dot product attention\uc740 torch.compile() \ub85c \uc644\uc804\ud788 \uad6c\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud655\uc778\ud558\uae30 \uc704\ud574 torch.compile() \uc744 \ud1b5\ud574 CausalSelfAttention \ubaa8\ub4c8\uc744 \ucef4\ud30c\uc77c\ud558\uace0 \uacb0\uacfc\uc801\uc73c\ub85c \uc5bb\uc5b4\uc9c0\ub294 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc54c\uc544\ubd05\uc2dc\ub2e4. batch_size = 32 max_sequence_len = 256 x = torch.rand(batch_size, max_sequence_len, embed_dimension, device=device, dtype=dtype) print( f\"The non compiled module runs in {benchmark_torch_function_in_microseconds(model, x):.3f} microseconds\") compiled_model = torch.compile(model) # Let\u0027s compile it compiled_model(x) print( f\"The compiled module runs in {benchmark_torch_function_in_microseconds(compiled_model, x):.3f} microseconds\") The non compiled module runs in 95.736 microseconds The compiled module runs in 146.114 microseconds \uc815\ud655\ud55c \uc2e4\ud589 \uc2dc\uac04\uc740 \ud658\uacbd\uc5d0 \ub530\ub77c \ub2e4\ub974\uc9c0\ub9cc, \ub2e4\uc74c\uc740 \uc800\uc790\uc758 \uacb0\uacfc\uc785\ub2c8\ub2e4. \ucef4\ud30c\uc77c \ub418\uc9c0 \uc54a\uc740 \ubaa8\ub4c8\uc740 \uc2e4\ud589\uc5d0 166.616ms \uac00 \uc18c\uc694\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ucef4\ud30c\uc77c \ub41c \ubaa8\ub4c8\uc740 \uc2e4\ud589\uc5d0 166.726ms \uac00 \uc18c\uc694\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc6b0\ub9ac\uc758 \uc608\uc0c1\uacfc\ub294 \ub2e4\ub985\ub2c8\ub2e4. \uc880 \ub354 \uc790\uc138\ud788 \uc54c\uc544\ubd05\uc2dc\ub2e4. PyTorch\ub294 \ucf54\ub4dc\uc758 \uc131\ub2a5 \ud2b9\uc131\uc744 \uc810\uac80\ud560 \uc218 \uc788\ub294 \ub180\ub77c\uc6b4 \ub0b4\uc7a5(built-in) \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. from torch.profiler import profile, record_function, ProfilerActivity activities = [ProfilerActivity.CPU] if device == \u0027cuda\u0027: activities.append(ProfilerActivity.CUDA) with profile(activities=activities, record_shapes=False) as prof: with record_function(\" Non-Compilied Causal Attention\"): for _ in range(25): model(x) print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10)) with profile(activities=activities, record_shapes=False) as prof: with record_function(\"Compiled Causal Attention\"): for _ in range(25): compiled_model(x) print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10)) ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg # of Calls ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Non-Compilied Causal Attention 0.00% 0.000us 0.00% 0.000us 0.000us 4.243ms 314.43% 4.243ms 4.243ms 1 Non-Compilied Causal Attention 19.50% 1.212ms 99.86% 6.209ms 6.209ms 0.000us 0.00% 1.373ms 1.373ms 1 aten::linear 1.06% 65.900us 54.13% 3.366ms 67.312us 0.000us 0.00% 844.758us 16.895us 50 aten::matmul 2.00% 124.161us 50.59% 3.145ms 62.903us 0.000us 0.00% 844.758us 16.895us 50 aten::mm 13.01% 808.680us 46.30% 2.878ms 57.568us 821.430us 60.87% 844.758us 16.895us 50 nvjet_hsh_256x128_64x4_1x2_h_bz_coopA_TNT 0.00% 0.000us 0.00% 0.000us 0.000us 802.873us 59.50% 802.873us 16.057us 50 aten::scaled_dot_product_attention 1.68% 104.481us 17.33% 1.078ms 43.103us 0.000us 0.00% 528.031us 21.121us 25 aten::_scaled_dot_product_flash_attention 2.54% 157.700us 15.65% 973.089us 38.924us 0.000us 0.00% 528.031us 21.121us 25 aten::_flash_attention_forward 2.98% 185.202us 11.66% 724.993us 29.000us 528.031us 39.13% 528.031us 21.121us 25 void pytorch_flash::flash_fwd_kernel\u003cFlash_fwd_kerne... 0.00% 0.000us 0.00% 0.000us 0.000us 528.031us 39.13% 528.031us 21.121us 25 ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 6.217ms Self CUDA time total: 1.349ms ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg # of Calls ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Compiled Causal Attention 0.00% 0.000us 0.00% 0.000us 0.000us 4.677ms 343.47% 4.677ms 4.677ms 1 Compiled Causal Attention 8.16% 549.779us 99.85% 6.728ms 6.728ms 0.000us 0.00% 1.362ms 1.362ms 1 Torch-Compiled Region: 0/0 8.47% 570.660us 89.28% 6.015ms 240.616us 0.000us 0.00% 1.362ms 54.470us 25 CompiledFunction 23.53% 1.586ms 77.89% 5.248ms 209.929us 0.000us 0.00% 1.362ms 54.470us 25 aten::mm 9.73% 655.325us 14.68% 989.330us 19.787us 820.091us 60.22% 820.091us 16.402us 50 nvjet_hsh_256x128_64x4_1x2_h_bz_coopA_TNT 0.00% 0.000us 0.00% 0.000us 0.000us 801.693us 58.87% 801.693us 16.034us 50 aten::_scaled_dot_product_flash_attention 1.74% 117.542us 13.53% 911.568us 36.463us 0.000us 0.00% 541.665us 21.667us 25 aten::_flash_attention_forward 2.92% 196.416us 10.30% 693.809us 27.752us 541.665us 39.78% 541.665us 21.667us 25 void pytorch_flash::flash_fwd_kernel\u003cFlash_fwd_kerne... 0.00% 0.000us 0.00% 0.000us 0.000us 541.665us 39.78% 541.665us 21.667us 25 Memset (Device) 0.00% 0.000us 0.00% 0.000us 0.000us 18.398us 1.35% 18.398us 0.736us 25 ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 6.738ms Self CUDA time total: 1.362ms \ub354 \ub9ce\uc740 \uc815\ubcf4\ub97c \uc5bb\uae30 \uc704\ud574 \ucd94\uc801(trace)\ub97c \ub0b4\ubcf4\ub0b4\uace0 ``chrome://tracing``\uc744 \uc0ac\uc6a9\ud558\uc5ec \uacb0\uacfc\ub97c \ud655\uc778\ud574\ubcf4\uc138\uc694. prof.export_chrome_trace(\"compiled_causal_attention_trace.json\"). \uc774\uc804 \ucf54\ub4dc \uc870\uac01(snippet)\uc740 \ucef4\ud30c\uc77c \ub41c \ubaa8\ub4c8\uacfc \ucef4\ud30c\uc77c\ub418\uc9c0 \uc54a\uc740 \ubaa8\ub4c8 \ubaa8\ub450\uc5d0 \ub300\ud574 \uac00\uc7a5 \ub9ce\uc740 GPU \uc2e4\ud589 \uc2dc\uac04\uc744 \ucc28\uc9c0\ud55c \uc0c1\uc704 10\uac1c\uc758 PyTorch \ud568\uc218\uc5d0 \ub300\ud55c \ubcf4\uace0\uc11c\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ubd84\uc11d \uacb0\uacfc, \ub450 \ubaa8\ub4c8 \ubaa8\ub450 GPU\uc5d0\uc11c \uc18c\uc694\ub41c \uc2dc\uac04\uc758 \ub300\ubd80\ubd84\uc774 \ub3d9\uc77c\ud55c \ud568\uc218\ub4e4\uc5d0 \uc9d1\uc911\ub418\uc5b4 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. PyTorch\uac00 \ud504\ub808\uc784\uc6cc\ud06c \uc624\ubc84\ud5e4\ub4dc\ub97c \uc81c\uac70\ud558\ub294 \ub370 \ub9e4\uc6b0 \ud0c1\uc6d4\ud55c torch.compile \ub97c \uc81c\uacf5\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. CausalSelfAttention \uac19\uc740 \uacbd\uc6b0\ucc98\ub7fc \ud06c\uace0, \ud6a8\uc728\uc801\uc778 CUDA \ucee4\ub110\uc744 \uc0ac\uc6a9\ud558\ub294 \ubaa8\ub378\uc5d0\uc11c PyTorch \uc624\ubc84\ud5e4\ub4dc\ub294 \uc791\uc544\uc9c8 \uac83\uc785\ub2c8\ub2e4. \uc0ac\uc2e4, \ubaa8\ub4c8\uc740 \ubcf4\ud1b5 CausalSelfAttention \ube14\ub7ed \ud558\ub098\ub9cc\uc73c\ub85c \uad6c\uc131\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. Andrej Karpathy NanoGPT \uc800\uc7a5\uc18c\uc5d0\uc11c \uc2e4\ud5d8\ud55c \uacbd\uc6b0, \ubaa8\ub4c8\uc744 \ucef4\ud30c\uc77c \ud558\ub294 \uac83\uc740 \ud559\uc2b5\uc758 \uac01 \ub2e8\uacc4\ubcc4 \uc18c\uc694 \uc2dc\uac04\uc744 6090.49ms \uc5d0\uc11c 3273.17ms \ub85c \uc904\uc77c \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \uc2e4\ud5d8\uc740 NanoGPT \uc800\uc7a5\uc18c\uc758 ae3a8d5 \ucee4\ubc0b\uc5d0\uc11c Shakespeare \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc9c4\ud589\ub418\uc5c8\uc2b5\ub2c8\ub2e4. SDPA\ub97c atteition.bias \ud558\uc704 \ud074\ub798\uc2a4\uc640 \uc0ac\uc6a9\ud558\uae30# PyTorch 2.3\ubd80\ud130 \ud150\uc11c \ud558\uc704 \ud074\ub798\uc2a4\ub97c \ud3ec\ud568\ud558\ub294 \uc0c8\ub85c\uc6b4 \uc11c\ube0c\ubaa8\ub4c8\uc744 \ucd94\uac00\ud588\uc2b5\ub2c8\ub2e4. \ucd94\uac00\ub41c \ubaa8\ub4c8\uc758 \uc774\ub984\uc740 torch.nn.attention.bias \uc774\uba70, torch.nn.functional.scaled_dot_product_attention \uc640 \ud568\uaed8 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uc778\uacfc\uc801 \uc5b4\ud150\uc158 \ubcc0\ud615(Causal Attention Variants)\uc744 \uc0dd\uc131\ud558\uae30 \uc704\ud55c \ub2e4\uc74c 2\uac00\uc9c0 \uae30\ub2a5(utilities)\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4: torch.nn.attention.bias.causal_upper_left torch.nn.attention.bias.causal_lower_right \ucc38\uace0 \ud604\uc7ac torch.nn.functional.scaled_dot_product_attention \uc758 is_causal \uc778\uc790(argument)\ub294 torch.nn.attention.bias.causal_upper_left \ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uacfc \ub3d9\uc77c\ud569\ub2c8\ub2e4. from torch.nn.attention.bias import causal_lower_right, causal_upper_left batch_size = 32 sequence_length_q = 2 sequence_length_kv = 10 num_heads = 16 embed_dimension = 32 dtype = torch.float16 query = torch.rand(batch_size, num_heads, sequence_length_q, embed_dimension, device=device, dtype=dtype) key = torch.rand(batch_size, num_heads, sequence_length_kv, embed_dimension, device=device, dtype=dtype) value = torch.rand(batch_size, num_heads, sequence_length_kv, embed_dimension, device=device, dtype=dtype) upper_left_bias = causal_upper_left(sequence_length_q, sequence_length_kv) lower_right_bias = causal_lower_right(sequence_length_q, sequence_length_kv) print(type(upper_left_bias)) print(type(lower_right_bias)) assert type(upper_left_bias) == type(lower_right_bias) assert issubclass(type(upper_left_bias), torch.Tensor) # \uc704\uc758 \ucd9c\ub825\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef, \ub450 \uac1d\uccb4\ub294 \uac19\uc740 \ud0c0\uc785\uc778 ``torch.nn.attention.bias.CausalBias`` \uc774\uba70, # ``torch.Tensor`` \uc758 \ud558\uc704 \ud074\ub798\uc2a4(subclass)\uc785\ub2c8\ub2e4. # \uac01 \ud150\uc11c\ub4e4\uc774 \uc5b4\ub5bb\uac8c \uc0dd\uacbc\ub294\uc9c0 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. print(upper_left_bias) print(lower_right_bias) # Upper Left Bias\ub294 \uc778\uacfc\uc801 \uc5b4\ud150\uc158 \ub9c8\uc2a4\ud06c(causal attention mask)\ub97c \uc5b4\ud150\uc158 \uc810\uc218 \ud589\ub82c(attention scores matrix)\uc758 \uc67c\ucabd \uc0c1\ub2e8\uc5d0 \uc815\ub82c\ud569\ub2c8\ub2e4. # \uc774\ub294 \uc5b4\ud150\uc158 \uc810\uc218 \ud589\ub82c\uc774 \uc815\uc0ac\uac01\ud615\uc774 \uc544\ub2cc \uacbd\uc6b0\uc5d0\ub9cc \uc601\ud5a5\uc744 \ubbf8\uce58\uba70, \uc774\ub294 \ub514\ucf54\ub529 \uc0c1\ud669\uc5d0\uc11c \uc77c\ubc18\uc801\uc778 \uacbd\uc6b0\uc785\ub2c8\ub2e4. # \uc774 \uac1c\ub150\uc744 \ub2e4\ub978 \ubc29\uc2dd\uc73c\ub85c \uc0dd\uac01\ud558\ub294 \ubc29\ubc95\uc740, upper left bias\ub97c \uc0ac\uc6a9\ud560 \ub54c\ub294 \ucffc\ub9ac(query)\uc758 0\ubc88\uc9f8 \ud1a0\ud070\uc774 \ud0a4(key)\uc758 0\ubc88\uc9f8 \ud1a0\ud070\uacfc \uc815\ub82c\ub41c\ub2e4\uace0 # \uc0dd\uac01\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc989, \uc5b4\ud150\uc158 \uc810\uc218 \ud589\ub82c(attention score matrix)\uc774 2\ucc28\uc6d0\uc774\ub77c\uace0 \uac00\uc815\ud560 \ub54c, ``attn_score[0][0]`` \uc774 \ucffc\ub9ac\uc758 0\ubc88\uc9f8 \ud1a0\ud070\uacfc # \ud0a4\uc758 0\ubc88\uc9f8 \ud1a0\ud070 \uc0ac\uc774\uc758 \uc5b4\ud150\uc158 \uc810\uc218\uc778 \uac83\uc785\ub2c8\ub2e4. # Lower Right Bias\uc758 \uacbd\uc6b0\uc5d0\ub294 \ucffc\ub9ac(query)\uc758 \ub9c8\uc9c0\ub9c9 \ud1a0\ud070\uc774 \ud0a4(key)\uc758 \ub9c8\uc9c0\ub9c9 \ud1a0\ud070\uacfc \uc815\ub82c\ub418\ub3c4\ub85d \ucffc\ub9ac(query)\uc758 \uc2dc\ud000\uc2a4\ub97c \uc815\ub82c\ud569\ub2c8\ub2e4. # \uc608\ub97c \ub4e4\uc5b4, ``attn_score[-1][-1]`` \uc740 \ucffc\ub9ac\uc640 \ud0a4\uc758 \uae38\uc774\uac00 \uc11c\ub85c \ub2e4\ub974\ub354\ub77c\ub3c4 \ucffc\ub9ac\uc758 \ub9c8\uc9c0\ub9c9 \ud1a0\ud070\uacfc \ud0a4\uc758 \ub9c8\uc9c0\ub9c9 \ud1a0\ud070\uc774 \uac19\uc740 \uc704\uce58\uc5d0 \uc788\uae30 \ub54c\ubb38\uc5d0 # \ubaa8\ub450 True\uc785\ub2c8\ub2e4. # SDPA\uc640 \ud568\uaed8 \uc0ac\uc6a9\ud558\uae30 \uc704\ud55c \uac1d\uccb4\ub4e4\uc785\ub2c8\ub2e4. out_upper_left = F.scaled_dot_product_attention(query, key, value, upper_left_bias) out_lower_right = F.scaled_dot_product_attention(query, key, value, lower_right_bias) out_is_causal = F.scaled_dot_product_attention(query, key, value, is_causal=True) assert torch.allclose(out_upper_left, out_is_causal) assert not torch.allclose(out_upper_left, out_lower_right) # \uc544\ub798 \uc5b4\ud150\uc158 \ud3b8\ud5a5(attention bias)\ub4e4\uc740 torch.compile\uacfc \ud638\ud658\ub429\ub2c8\ub2e4. compiled_sdpa = torch.compile(F.scaled_dot_product_attention, fullgraph=True) out_upper_left = compiled_sdpa(query, key, value, upper_left_bias) \u003cclass \u0027torch.nn.attention.bias.CausalBias\u0027\u003e \u003cclass \u0027torch.nn.attention.bias.CausalBias\u0027\u003e tensor([[ True, False, False, False, False, False, False, False, False, False], [ True, True, False, False, False, False, False, False, False, False]]) tensor([[ True, True, True, True, True, True, True, True, True, False], [ True, True, True, True, True, True, True, True, True, True]]) \uacb0\ub860# \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c, torch.nn.functional.scaled_dot_product_attention \uc758 \uae30\ubcf8\uc801\uc778 \uc0ac\uc6a9\ubc95\uc744 \uc0b4\ud3b4\ubd24\uc2b5\ub2c8\ub2e4. sdpa_kernel \ucee8\ud14d\uc2a4\ud2b8 \ub9e4\ub2c8\uc800\ub85c GPU\uac00 \ud2b9\uc815 \uad6c\ud604\uc744 \uc0ac\uc6a9\ud558\ub3c4\ub85d \ud560 \uc218 \uc788\ub2e4\ub294 \uac83\uc744 \ubcf4\uc558\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uac04\ub2e8\ud55c NestedTensor \uc5d0\uc11c \uc791\ub3d9\ud558\uace0 \ucef4\ud30c\uc77c \uac00\ub2a5\ud55c CausalSelfAttention \ubaa8\ub4c8\uc744 \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \uacfc\uc815\uc5d0\uc11c \ud504\ub85c\ud30c\uc77c\ub9c1 \ub3c4\uad6c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc720\uc800\uac00 \uc815\uc758\ud55c \ubaa8\ub4c8\uc758 \uc131\ub2a5 \ud2b9\uc131\uc744 \uc5b4\ub5bb\uac8c \ud655\uc778\ud560 \uc218 \uc788\ub294\uc9c0\ub3c4 \uc0b4\ud3b4\ubd24\uc2b5\ub2c8\ub2e4. Total running time of the script: (0 minutes 12.102 seconds) Download Jupyter notebook: scaled_dot_product_attention_tutorial.ipynb Download Python source code: scaled_dot_product_attention_tutorial.py Download zipped: scaled_dot_product_attention_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/intermediate/scaled_dot_product_attention_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>