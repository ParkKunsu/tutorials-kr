
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="torch.export Tutorial" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/intermediate/torch_export_tutorial.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Author: William Wen, Zhengxu Chen, Angela Yi, Pian Pawakapan torch.export() is the PyTorch 2.X way to export PyTorch models into standardized model representations, intended to be run on different (i.e. Python-less) environments. The official documentation can be found here. In this tutorial, you..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="Author: William Wen, Zhengxu Chen, Angela Yi, Pian Pawakapan torch.export() is the PyTorch 2.X way to export PyTorch models into standardized model representations, intended to be run on different (i.e. Python-less) environments. The official documentation can be found here. In this tutorial, you..." />
<meta property="og:ignore_canonical" content="true" />

    <title>torch.export Tutorial &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/torch_export_tutorial';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/intermediate/torch_export_tutorial.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="torch.export AOTInductor Tutorial for Python runtime (Beta)" href="../recipes/torch_export_aoti_python.html" />
    <link rel="prev" title="Reducing torch.compile cold start compilation time with regional compilation" href="../recipes/regional_compilation.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">torch.compile</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="torch_compile_tutorial.html">Introduction to <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="compiled_autograd_tutorial.html">Compiled Autograd: Capturing a larger backward graph for <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_compiler_set_stance_tutorial.html">Dynamic Compilation Control with <code class="docutils literal notranslate"><span class="pre">torch.compiler.set_stance</span></code></a></li>


<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_export_challenges_solutions.html">Demonstration of torch.export flow, common challenges and the solutions to address them</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/compiling_optimizer.html">(beta) Compiling the optimizer with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/compiling_optimizer_lr_scheduler.html">(beta) Running the compiled optimizer with an LR Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_compile_user_defined_triton_kernel_tutorial.html">사용자 정의 Triton 커널을 ``torch.compile``과 함께 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_compile_caching_tutorial.html">Compile Time Caching in <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/regional_compilation.html">Reducing torch.compile cold start compilation time with regional compilation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">torch.export</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">torch.export Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_export_aoti_python.html"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/torch_export_challenges_solutions.html">Demonstration of torch.export flow, common challenges and the solutions to address them</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../beginner/onnx/intro_onnx.html">Introduction to ONNX</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../beginner/onnx/export_simple_model_to_onnx_tutorial.html">Export a PyTorch model to ONNX</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../beginner/onnx/onnx_registry_tutorial.html">Extending the ONNX Registry</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../beginner/onnx/export_control_flow_model_to_onnx_tutorial.html">Export a model with control flow to ONNX</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Code Transforms with FX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="torch_compile_conv_bn_fuser.html">Building a Convolution/Batch Norm fuser with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx_profiling_tutorial.html">(beta) Building a Simple CPU Performance Profiler with FX</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../compilers_index.html" class="nav-link">Compilers</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">torch.export...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../compilers_index.html">
        <meta itemprop="name" content="Compilers">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="torch.export Tutorial">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">intermediate/torch_export_tutorial</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-intermediate-torch-export-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="torch-export-tutorial">
<span id="sphx-glr-intermediate-torch-export-tutorial-py"></span><h1>torch.export Tutorial<a class="headerlink" href="#torch-export-tutorial" title="Link to this heading">#</a></h1>
<p><strong>Author:</strong> William Wen, Zhengxu Chen, Angela Yi, Pian Pawakapan</p>
<div class="admonition warning">
<p class="admonition-title">경고</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> and its related features are in prototype status and are subject to backwards compatibility
breaking changes. This tutorial provides a snapshot of <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> usage as of PyTorch 2.5.</p>
</div>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export()</span></code> is the PyTorch 2.X way to export PyTorch models into
standardized model representations, intended
to be run on different (i.e. Python-less) environments. The official
documentation can be found <a class="reference external" href="https://pytorch.org/docs/main/export.html">here</a>.</p>
<p>In this tutorial, you will learn how to use <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.export()</span></code> to extract
<code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>’s (i.e. single-graph representations) from PyTorch programs.
We also detail some considerations/modifications that you may need
to make in order to make your model compatible with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
<p><strong>Contents</strong></p>
<nav class="contents local" id="id1">
<ul class="simple">
<li><p><a class="reference internal" href="#basic-usage" id="id3">Basic Usage</a></p></li>
<li><p><a class="reference internal" href="#graph-breaks" id="id4">Graph Breaks</a></p></li>
<li><p><a class="reference internal" href="#non-strict-export" id="id5">Non-Strict Export</a></p></li>
<li><p><a class="reference internal" href="#control-flow-ops" id="id6">Control Flow Ops</a></p></li>
<li><p><a class="reference internal" href="#constraints-dynamic-shapes" id="id7">Constraints/Dynamic Shapes</a></p>
<ul>
<li><p><a class="reference internal" href="#basic-concepts-symbols-and-guards" id="id8">Basic concepts: symbols and guards</a></p></li>
<li><p><a class="reference internal" href="#specialization" id="id9">0/1 specialization</a></p></li>
<li><p><a class="reference internal" href="#named-dims" id="id10">Named Dims</a></p></li>
<li><p><a class="reference internal" href="#constraint-violations-suggested-fixes" id="id11">Constraint violations, suggested fixes</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#data-dependent-errors" id="id12">Data-dependent errors</a></p>
<ul>
<li><p><a class="reference internal" href="#guards-torch-check" id="id13">Guards, torch._check()</a></p></li>
<li><p><a class="reference internal" href="#specialized-values" id="id14">Specialized values</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#custom-ops" id="id15">Custom Ops</a></p></li>
<li><p><a class="reference internal" href="#ir-decompositions" id="id16">IR/Decompositions</a></p></li>
<li><p><a class="reference internal" href="#exportdb" id="id17">ExportDB</a></p></li>
<li><p><a class="reference internal" href="#running-the-exported-program" id="id18">Running the Exported Program</a></p></li>
<li><p><a class="reference internal" href="#conclusion" id="id19">Conclusion</a></p></li>
</ul>
</nav>
<section id="basic-usage">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Basic Usage</a><a class="headerlink" href="#basic-usage" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> extracts single-graph representations from PyTorch programs
by tracing the target function, given example inputs.
<code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> is the main entry point for <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
<p>In this tutorial, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> are practically synonymous,
though <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> generally refers to the PyTorch 2.X export process, and <code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code>
generally refers to the actual function call.</p>
<p>The signature of <code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">export</span><span class="p">(</span>
    <span class="n">mod</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">dynamic_shapes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Dim</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ExportedProgram</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch.export.export()</span></code> traces the tensor computation graph from calling <code class="docutils literal notranslate"><span class="pre">mod(*args,</span> <span class="pre">**kwargs)</span></code>
and wraps it in an <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, which can be serialized or executed later with
different inputs. To execute the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> we can call <code class="docutils literal notranslate"><span class="pre">.module()</span></code>
on it to return a <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> which is callable, just like the
original program.
We will detail the <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code> argument later in the tutorial.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">export</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">),</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">()</span>
<span class="n">exported_mod</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">exported_mod</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_mod</span><span class="o">.</span><span class="n">module</span><span class="p">()(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">100</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;torch.export.exported_program.ExportedProgram&#39;&gt;
tensor([[0.5697, 0.0000, 0.5520, 0.0000, 0.0000, 0.0000, 0.0000, 0.3805, 1.2986,
         0.6166],
        [0.0000, 0.0000, 0.0000, 0.7790, 0.0000, 1.6360, 0.0000, 0.0000, 0.0000,
         0.0000],
        [0.0000, 0.0000, 0.4730, 0.0000, 1.4367, 0.0000, 0.0000, 0.7210, 0.0000,
         0.6131],
        [0.0000, 0.0000, 0.0000, 0.3966, 0.6764, 0.0000, 0.4882, 0.0000, 0.7645,
         0.6003],
        [0.0000, 0.4958, 0.8105, 0.0000, 0.0000, 0.2007, 0.3920, 0.0000, 1.0997,
         0.5876],
        [0.4842, 1.1476, 0.6031, 0.0118, 1.2018, 0.1046, 0.0000, 0.0000, 0.2046,
         0.2781],
        [0.6158, 0.3322, 1.2286, 0.0853, 0.1356, 0.9100, 0.4171, 0.0000, 0.0000,
         0.4035],
        [0.0000, 0.1702, 1.0550, 0.0000, 1.3246, 0.1930, 0.0000, 0.0000, 0.0000,
         0.0000]], grad_fn=&lt;ReluBackward0&gt;)
</pre></div>
</div>
<p>Let’s review some attributes of <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> that are of interest.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">graph</span></code> attribute is an <a class="reference external" href="https://pytorch.org/docs/stable/fx.html#torch.fx.Graph">FX graph</a>
traced from the function we exported, that is, the computation graph of all PyTorch operations.
The FX graph is in “ATen IR” meaning that it contains only “ATen-level” operations.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">graph_signature</span></code> attribute gives a more detailed description of the
input and output nodes in the exported graph, describing which ones are
parameters, buffers, user inputs, or user outputs.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">range_constraints</span></code> attributes will be covered later.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">exported_mod</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_lin_weight: &quot;f32[10, 100]&quot;, p_lin_bias: &quot;f32[10]&quot;, x: &quot;f32[8, 100]&quot;, y: &quot;f32[8, 100]&quot;):
             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:71 in forward, code: return torch.nn.functional.relu(self.lin(x + y), inplace=True)
            add: &quot;f32[8, 100]&quot; = torch.ops.aten.add.Tensor(x, y);  x = y = None

             # File: /opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear: &quot;f32[8, 10]&quot; = torch.ops.aten.linear.default(add, p_lin_weight, p_lin_bias);  add = p_lin_weight = p_lin_bias = None

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:71 in forward, code: return torch.nn.functional.relu(self.lin(x + y), inplace=True)
            relu_: &quot;f32[8, 10]&quot; = torch.ops.aten.relu_.default(linear);  linear = None
            return (relu_,)

Graph signature:
    # inputs
    p_lin_weight: PARAMETER target=&#39;lin.weight&#39;
    p_lin_bias: PARAMETER target=&#39;lin.bias&#39;
    x: USER_INPUT
    y: USER_INPUT

    # outputs
    relu_: USER_OUTPUT

Range constraints: {}
</pre></div>
</div>
<p>See the <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> <a class="reference external" href="https://pytorch.org/docs/main/export.html#torch.export.export">documentation</a>
for more details.</p>
</section>
<section id="graph-breaks">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Graph Breaks</a><a class="headerlink" href="#graph-breaks" title="Link to this heading">#</a></h2>
<p>Although <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> shares components with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>,
the key limitation of <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, especially when compared to
<code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>, is that it does not support graph breaks. This is because
handling graph breaks involves interpreting the unsupported operation with
default Python evaluation, which is incompatible with the export use case.
Therefore, in order to make your model code compatible with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>,
you will need to modify your code to remove graph breaks.</p>
<p>A graph break is necessary in cases such as:</p>
<ul class="simple">
<li><p>data-dependent control flow</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad1</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">traceback</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tb</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">export</span><span class="p">(</span><span class="n">Bad1</span><span class="p">(),</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>def forward(self, arg0_1: &quot;f32[3, 3]&quot;):
     # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:116 in forward, code: if x.sum() &gt; 0:
    sum_1: &quot;f32[]&quot; = torch.ops.aten.sum.default(arg0_1);  arg0_1 = None
    gt: &quot;b8[]&quot; = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None
    ne: &quot;b8[]&quot; = torch.ops.aten.ne.Scalar(gt, 0);  gt = None
    item: &quot;Sym(Eq(u0, 1))&quot; = torch.ops.aten.item.default(ne);  ne = item = None




def forward(self, arg0_1: &quot;f32[3, 3]&quot;):
     # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:116 in forward, code: if x.sum() &gt; 0:
    sum_1: &quot;f32[]&quot; = torch.ops.aten.sum.default(arg0_1);  arg0_1 = None
    gt: &quot;b8[]&quot; = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None
    ne: &quot;b8[]&quot; = torch.ops.aten.ne.Scalar(gt, 0);  gt = None
    item: &quot;Sym(Eq(u0, 1))&quot; = torch.ops.aten.item.default(ne);  ne = item = None

Traceback (most recent call last):
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 122, in &lt;module&gt;
    export(Bad1(), (torch.randn(3, 3),))
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 319, in export
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
           ^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1164, in wrapper
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
         ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1164, in wrapper
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
                      ^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1770, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1900, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1685, in _make_fx_helper
    gm = make_fx(
         ^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2318, in wrapped
    return make_fx_tracer.trace(f, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2250, in trace
    return self._trace_inner(f, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2221, in _trace_inner
    t = dispatch_trace(
        ^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_compile.py&quot;, line 53, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1254, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1835, in trace
    res = super().trace(root, concrete_args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace
    (self.create_arg(fn(*args)),),
                     ^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1312, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
          ^^^^^^^^^^^
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1589, in wrapped_fn
    return tuple(flat_fn(*args))
                 ^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn
    tree_out = fn(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call
    out = mod(*args[params_len:], **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1884, in forward
    tree_out = mod(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 116, in forward
    if x.sum() &gt; 0:
       ^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1360, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1407, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 1051, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 538, in guard_bool
    r = self.evaluate()
        ^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 512, in evaluate
    return self.shape_env.evaluate_sym_node(self, size_oblivious)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7223, in evaluate_sym_node
    return self.evaluate_expr(
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7323, in evaluate_expr
    return self._inner_evaluate_expr(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
    return retlog(fn(*args, **kwargs))
                  ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7346, in _inner_evaluate_expr
    return self._evaluate_expr(
           ^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7570, in _evaluate_expr
    raise self._make_data_dependent_error(
torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).  (Size-like symbols: none)

Caused by: (_export/non_strict_utils.py:1051 in __torch_function__)
For more information, run with TORCH_LOGS=&quot;dynamic&quot;
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;u0&quot;
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1

The following call raised this error:
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 116, in forward
    if x.sum() &gt; 0:


The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
<ul class="simple">
<li><p>accessing tensor data with <code class="docutils literal notranslate"><span class="pre">.data</span></code></p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad2</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">export</span><span class="p">(</span><span class="n">Bad2</span><span class="p">(),</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>calling unsupported functions (such as many built-in functions)</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad3</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="nb">id</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">export</span><span class="p">(</span><span class="n">Bad3</span><span class="p">(),</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="non-strict-export">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Non-Strict Export</a><a class="headerlink" href="#non-strict-export" title="Link to this heading">#</a></h2>
<p>To trace the program, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> uses TorchDynamo by default, a byte
code analysis engine, to symbolically analyze the Python code and build a
graph based on the results. This analysis allows <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> to provide
stronger guarantees about safety, but not all Python code is supported,
causing these graph breaks.</p>
<p>To address this issue, in PyTorch 2.3, we introduced a new mode of
exporting called non-strict mode, where we trace through the program using the
Python interpreter executing it exactly as it would in eager mode, allowing us
to skip over unsupported Python features. This is done through adding a
<code class="docutils literal notranslate"><span class="pre">strict=False</span></code> flag.</p>
<p>Looking at some of the previous examples which resulted in graph breaks:</p>
<ul class="simple">
<li><p>Calling unsupported functions (such as many built-in functions) traces</p></li>
</ul>
<p>through, but in this case, <code class="docutils literal notranslate"><span class="pre">id(x)</span></code> gets specialized as a constant integer in
the graph. This is because <code class="docutils literal notranslate"><span class="pre">id(x)</span></code> is not a tensor operation, so the
operation is not recorded in the graph.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad3</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="nb">id</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">bad3_nonstrict</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">Bad3</span><span class="p">(),</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bad3_nonstrict</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bad3_nonstrict</span><span class="o">.</span><span class="n">module</span><span class="p">()(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: &quot;f32[3, 3]&quot;):
             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:179 in forward, code: x = x + 1
            add: &quot;f32[3, 3]&quot; = torch.ops.aten.add.Tensor(x, 1);  x = None

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:180 in forward, code: return x + id(x)
            add_1: &quot;f32[3, 3]&quot; = torch.ops.aten.add.Tensor(add, 139699698668784);  add = None
            return (add_1,)

Graph signature:
    # inputs
    x: USER_INPUT

    # outputs
    add_1: USER_OUTPUT

Range constraints: {}

tensor([[1.3970e+14, 1.3970e+14, 1.3970e+14],
        [1.3970e+14, 1.3970e+14, 1.3970e+14],
        [1.3970e+14, 1.3970e+14, 1.3970e+14]])
</pre></div>
</div>
<p>However, there are still some features that require rewrites to the original
module:</p>
</section>
<section id="control-flow-ops">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Control Flow Ops</a><a class="headerlink" href="#control-flow-ops" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> actually does support data-dependent control flow.
But these need to be expressed using control flow ops. For example,
we can fix the control flow example above using the <code class="docutils literal notranslate"><span class="pre">cond</span></code> op, like so:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Bad1Fixed</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">true_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">false_fn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">true_fn</span><span class="p">,</span> <span class="n">false_fn</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>

<span class="n">exported_bad1_fixed</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">Bad1Fixed</span><span class="p">(),</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_bad1_fixed</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_bad1_fixed</span><span class="o">.</span><span class="n">module</span><span class="p">()(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_bad1_fixed</span><span class="o">.</span><span class="n">module</span><span class="p">()(</span><span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: &quot;f32[3, 3]&quot;):
             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:205 in forward, code: return torch.cond(x.sum() &gt; 0, true_fn, false_fn, [x])
            sum_1: &quot;f32[]&quot; = torch.ops.aten.sum.default(x)
            gt: &quot;b8[]&quot; = torch.ops.aten.gt.Scalar(sum_1, 0);  sum_1 = None

             # File: &lt;eval_with_key&gt;.33:9 in forward, code: cond = torch.ops.higher_order.cond(l_args_0_, cond_true_0, cond_false_0, (l_args_3_0_,));  l_args_0_ = cond_true_0 = cond_false_0 = l_args_3_0_ = None
            true_graph_0 = self.true_graph_0
            false_graph_0 = self.false_graph_0
            cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, (x,));  gt = true_graph_0 = false_graph_0 = x = None
            getitem: &quot;f32[3, 3]&quot; = cond[0];  cond = None
            return (getitem,)

        class true_graph_0(torch.nn.Module):
            def forward(self, x: &quot;f32[3, 3]&quot;):
                 # File: &lt;eval_with_key&gt;.30:6 in forward, code: sin = torch.sin(l_args_3_0__1);  l_args_3_0__1 = None
                sin: &quot;f32[3, 3]&quot; = torch.ops.aten.sin.default(x);  x = None
                return (sin,)

        class false_graph_0(torch.nn.Module):
            def forward(self, x: &quot;f32[3, 3]&quot;):
                 # File: &lt;eval_with_key&gt;.31:6 in forward, code: cos = torch.cos(l_args_3_0__1);  l_args_3_0__1 = None
                cos: &quot;f32[3, 3]&quot; = torch.ops.aten.cos.default(x);  x = None
                return (cos,)

Graph signature:
    # inputs
    x: USER_INPUT

    # outputs
    getitem: USER_OUTPUT

Range constraints: {}

tensor([[0.8415, 0.8415, 0.8415],
        [0.8415, 0.8415, 0.8415],
        [0.8415, 0.8415, 0.8415]])
tensor([[0.5403, 0.5403, 0.5403],
        [0.5403, 0.5403, 0.5403],
        [0.5403, 0.5403, 0.5403]])
</pre></div>
</div>
<p>There are limitations to <code class="docutils literal notranslate"><span class="pre">cond</span></code> that one should be aware of:</p>
<ul class="simple">
<li><p>The predicate (i.e. <code class="docutils literal notranslate"><span class="pre">x.sum()</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>) must result in a boolean or a single-element tensor.</p></li>
<li><p>The operands (i.e. <code class="docutils literal notranslate"><span class="pre">[x]</span></code>) must be tensors.</p></li>
<li><p>The branch function (i.e. <code class="docutils literal notranslate"><span class="pre">true_fn</span></code> and <code class="docutils literal notranslate"><span class="pre">false_fn</span></code>) signature must match with the
operands and they must both return a single tensor with the same metadata (for example, <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, <code class="docutils literal notranslate"><span class="pre">shape</span></code>, etc.).</p></li>
<li><p>Branch functions cannot mutate input or global variables.</p></li>
<li><p>Branch functions cannot access closure variables, except for <code class="docutils literal notranslate"><span class="pre">self</span></code> if the function is
defined in the scope of a method.</p></li>
</ul>
<p>For more details about <code class="docutils literal notranslate"><span class="pre">cond</span></code>, check out the <a class="reference external" href="https://pytorch.org/docs/main/cond.html">cond documentation</a>.</p>
<p>We can also use <code class="docutils literal notranslate"><span class="pre">map</span></code>, which applies a function across the first dimension
of the first tensor argument.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch._higher_order_ops.map</span><span class="w"> </span><span class="kn">import</span> <span class="nb">map</span> <span class="k">as</span> <span class="n">torch_map</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MapModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">body</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span>

        <span class="k">return</span> <span class="n">torch_map</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="n">exported_map_example</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">MapModule</span><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_map_example</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_map_example</span><span class="o">.</span><span class="n">module</span><span class="p">()(</span><span class="o">*</span><span class="n">inps</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, xs: &quot;f32[6, 4]&quot;, y: &quot;i64[]&quot;, z: &quot;i64[]&quot;):
             # File: &lt;eval_with_key&gt;.58:9 in forward, code: map_impl = torch.ops.higher_order.map_impl(map_body_0, [l_flat_xs_0_], [l_flat_args_0_, l_flat_args_1_]);  map_body_0 = l_flat_xs_0_ = l_flat_args_0_ = l_flat_args_1_ = None
            body_graph_0 = self.body_graph_0
            map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y, z]);  body_graph_0 = xs = y = z = None
            getitem: &quot;f32[6, 4]&quot; = map_impl[0];  map_impl = None
            return (getitem,)

        class body_graph_0(torch.nn.Module):
            def forward(self, xs: &quot;f32[4]&quot;, y: &quot;i64[]&quot;, z: &quot;i64[]&quot;):
                 # File: &lt;eval_with_key&gt;.56:5 in forward, code: add = child.add(l_flat_args_0_);  child = l_flat_args_0_ = None
                add: &quot;f32[4]&quot; = torch.ops.aten.add.Tensor(xs, y);  xs = y = None

                 # File: &lt;eval_with_key&gt;.56:6 in forward, code: add_1 = add.add(l_flat_args_1_);  add = l_flat_args_1_ = None
                add_1: &quot;f32[4]&quot; = torch.ops.aten.add.Tensor(add, z);  add = z = None
                return (add_1,)

Graph signature:
    # inputs
    xs: USER_INPUT
    y: USER_INPUT
    z: USER_INPUT

    # outputs
    getitem: USER_OUTPUT

Range constraints: {}

tensor([[10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.],
        [10., 10., 10., 10.]])
</pre></div>
</div>
<p>Other control flow ops include <code class="docutils literal notranslate"><span class="pre">while_loop</span></code>, <code class="docutils literal notranslate"><span class="pre">associative_scan</span></code>, and
<code class="docutils literal notranslate"><span class="pre">scan</span></code>. For more documentation on each operator, please refer to
<a class="reference external" href="https://github.com/pytorch/pytorch/tree/main/torch/_higher_order_ops">this page</a>.</p>
</section>
<section id="constraints-dynamic-shapes">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Constraints/Dynamic Shapes</a><a class="headerlink" href="#constraints-dynamic-shapes" title="Link to this heading">#</a></h2>
<p>This section covers dynamic behavior and representation of exported programs. Dynamic behavior is
subjective to the particular model being exported, so for the most part of this tutorial, we’ll focus
on this particular toy model (with the resulting tensor shapes annotated):</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DynamicModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">w</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [6, 5]</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [4]</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [8, 4]</span>
        <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [32]</span>
    <span class="p">):</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>  <span class="c1"># [8, 4]</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>  <span class="c1"># [6, 3]</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># [32]</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">z</span>  <span class="c1"># [32]</span>
        <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span>
</pre></div>
</div>
<p>By default, <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> produces a static program. One consequence of this is that at runtime,
the program won’t work on inputs with different shapes, even if they’re valid in eager mode.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DynamicModel</span><span class="p">()</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">))</span>
<span class="n">model</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">12</span><span class="p">))</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">ep</span><span class="o">.</span><span class="n">module</span><span class="p">()(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">12</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Traceback (most recent call last):
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 286, in &lt;module&gt;
    ep.module()(w, x, torch.randn(3, 4), torch.randn(12))
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/graph_module.py&quot;, line 848, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/graph_module.py&quot;, line 424, in __call__
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/graph_module.py&quot;, line 411, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1879, in _call_impl
    return inner()
           ^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1806, in inner
    args_kwargs_result = hook(self, args, kwargs)  # type: ignore[misc]
                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_unlift.py&quot;, line 83, in _check_input_constraints_pre_hook
    _check_input_constraints_for_graph(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/utils.py&quot;, line 426, in _check_input_constraints_for_graph
    _check_symint(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/utils.py&quot;, line 390, in _check_symint
    raise RuntimeError(
RuntimeError: Expected input at *args[2].shape[0] to be equal to 8, but got 3. If you meant for this dimension to be dynamic, please re-export and specify dynamic_shapes (e.g. with Dim.DYNAMIC)
</pre></div>
</div>
<section id="basic-concepts-symbols-and-guards">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Basic concepts: symbols and guards</a><a class="headerlink" href="#basic-concepts-symbols-and-guards" title="Link to this heading">#</a></h3>
<p>To enable dynamism, <code class="docutils literal notranslate"><span class="pre">export()</span></code> provides a <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code> argument. The easiest way to work with
dynamic shapes is using <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> and looking at the program that’s returned. Dynamic behavior is specified
at a input dimension-level; for each input we can specify a tuple of values:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.export.dynamic_shapes</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dim</span>

<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;w&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
    <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">&quot;z&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
<span class="p">}</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
</pre></div>
</div>
<p>Before we look at the program that’s produced, let’s understand what specifying <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code> entails,
and how that interacts with export. For every input dimension where a <code class="docutils literal notranslate"><span class="pre">Dim</span></code> object is specified, a symbol is
<a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html#basics-of-symbolic-shapes">allocated</a>,
taking on a range of <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">inf]</span></code> (why not <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">inf]</span></code> or <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">inf]</span></code>? we’ll explain later in the
0/1 specialization section).</p>
<p>Export then runs model tracing, looking at each operation that’s performed by the model. Each individual operation can emit
what’s called “guards”; basically boolean condition that are required to be true for the program to be valid.
When guards involve symbols allocated for input dimensions, the program contains restrictions on what input shapes are valid;
i.e. the program’s dynamic behavior. The symbolic shapes subsystem is the part responsible for taking in all the emitted guards
and producing a final program representation that adheres to all of these guards. Before we see this “final representation” in
an <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, let’s look at the guards emitted by the toy model we’re tracing.</p>
<p>Here, each forward input tensor is annotated with the symbol allocated at the start of tracing:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DynamicModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">w</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [s0, s1]</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [s2]</span>
        <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [s3, s4]</span>
        <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [s5]</span>
    <span class="p">):</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>  <span class="c1"># guard: s2 == s4</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>  <span class="c1"># guard: s1 == 5</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>  <span class="c1"># no guard added here</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">z</span>  <span class="c1"># guard: s3 * s4 == s5</span>
        <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span>
</pre></div>
</div>
<p>Let’s understand each of the operations and the emitted guards:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x0</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">+</span> <span class="pre">y</span></code>: This is an element-wise add with broadcasting, since <code class="docutils literal notranslate"><span class="pre">x</span></code> is a 1-d tensor and <code class="docutils literal notranslate"><span class="pre">y</span></code> a 2-d tensor. <code class="docutils literal notranslate"><span class="pre">x</span></code> is broadcasted along the last dimension of <code class="docutils literal notranslate"><span class="pre">y</span></code>, emitting the guard <code class="docutils literal notranslate"><span class="pre">s2</span> <span class="pre">==</span> <span class="pre">s4</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x1</span> <span class="pre">=</span> <span class="pre">self.l(w)</span></code>: Calling <code class="docutils literal notranslate"><span class="pre">nn.Linear()</span></code> performs a matrix multiplication with model parameters. In export, parameters, buffers, and constants are considered program state, which is considered static, and so this is a matmul between a dynamic input (<code class="docutils literal notranslate"><span class="pre">w:</span> <span class="pre">[s0,</span> <span class="pre">s1]</span></code>), and a statically-shaped tensor. This emits the guard <code class="docutils literal notranslate"><span class="pre">s1</span> <span class="pre">==</span> <span class="pre">5</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x2</span> <span class="pre">=</span> <span class="pre">x0.flatten()</span></code>: This call actually doesn’t emit any guards! (at least none relevant to input shapes)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x3</span> <span class="pre">=</span> <span class="pre">x2</span> <span class="pre">+</span> <span class="pre">z</span></code>: <code class="docutils literal notranslate"><span class="pre">x2</span></code> has shape <code class="docutils literal notranslate"><span class="pre">[s3*s4]</span></code> after flattening, and this element-wise add emits <code class="docutils literal notranslate"><span class="pre">s3</span> <span class="pre">*</span> <span class="pre">s4</span> <span class="pre">==</span> <span class="pre">s5</span></code>.</p></li>
</ul>
<p>Writing all of these guards down and summarizing is almost like a mathematical proof, which is what the symbolic shapes
subsystem tries to do! In summary, we can conclude that the program must have the following input shapes to be valid:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">w:</span> <span class="pre">[s0,</span> <span class="pre">5]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x:</span> <span class="pre">[s2]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y:</span> <span class="pre">[s3,</span> <span class="pre">s2]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">z:</span> <span class="pre">[s2*s3]</span></code></p></li>
</ul>
<p>And when we do finally print out the exported program to see our result, those shapes are what we see annotated on the
corresponding inputs:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">ep</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_l_weight: &quot;f32[3, 5]&quot;, p_l_bias: &quot;f32[3]&quot;, w: &quot;f32[s15, 5]&quot;, x: &quot;f32[s77]&quot;, y: &quot;f32[s17, s77]&quot;, z: &quot;f32[s17*s77]&quot;):
             #
            sym_size_int_1 = torch.ops.aten.sym_size.int(w, 1)
            sym_size_int_2: &quot;Sym(s77)&quot; = torch.ops.aten.sym_size.int(x, 0)
            sym_size_int_3: &quot;Sym(s17)&quot; = torch.ops.aten.sym_size.int(y, 0)
            sym_size_int_4: &quot;Sym(s77)&quot; = torch.ops.aten.sym_size.int(y, 1)
            sym_size_int_5: &quot;Sym(s17*s77)&quot; = torch.ops.aten.sym_size.int(z, 0)

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:268 in forward, code: x0 = x + y  # [8, 4]
            add: &quot;f32[s17, s77]&quot; = torch.ops.aten.add.Tensor(x, y);  x = y = None

             #
            eq: &quot;Sym(True)&quot; = sym_size_int_2 == sym_size_int_4;  sym_size_int_4 = None
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(eq, &quot;Runtime assertion failed for expression Eq(s77, s94) on node &#39;eq&#39;&quot;);  eq = _assert_scalar_default = None
            eq_1 = sym_size_int_1 == 5;  sym_size_int_1 = None
            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(eq_1, &quot;Runtime assertion failed for expression Eq(s21, 5) on node &#39;eq_1&#39;&quot;);  eq_1 = _assert_scalar_default_1 = None
            mul: &quot;Sym(s17*s77)&quot; = sym_size_int_3 * sym_size_int_2;  sym_size_int_3 = sym_size_int_2 = None
            eq_2: &quot;Sym(True)&quot; = mul == sym_size_int_5;  mul = sym_size_int_5 = None
            _assert_scalar_default_2 = torch.ops.aten._assert_scalar.default(eq_2, &quot;Runtime assertion failed for expression Eq(s17*s77, s68) on node &#39;eq_2&#39;&quot;);  eq_2 = _assert_scalar_default_2 = None

             # File: /opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            linear: &quot;f32[s15, 3]&quot; = torch.ops.aten.linear.default(w, p_l_weight, p_l_bias);  w = p_l_weight = p_l_bias = None

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:270 in forward, code: x2 = x0.flatten()  # [32]
            flatten: &quot;f32[s17*s77]&quot; = torch.ops.aten.flatten.using_ints(add);  add = None

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:271 in forward, code: x3 = x2 + z  # [32]
            add_1: &quot;f32[s17*s77]&quot; = torch.ops.aten.add.Tensor(flatten, z);  flatten = z = None
            return (linear, add_1)

Graph signature:
    # inputs
    p_l_weight: PARAMETER target=&#39;l.weight&#39;
    p_l_bias: PARAMETER target=&#39;l.bias&#39;
    w: USER_INPUT
    x: USER_INPUT
    y: USER_INPUT
    z: USER_INPUT

    # outputs
    linear: USER_OUTPUT
    add_1: USER_OUTPUT

Range constraints: {s15: VR[2, int_oo], s77: VR[2, int_oo], s17: VR[2, int_oo], s17*s77: VR[4, int_oo]}
</pre></div>
</div>
<p>Another feature to notice is the range_constraints field above, which contains a valid range for each symbol. This isn’t
so interesting currently, since this export call doesn’t emit any guards related to symbol bounds and each base symbol has
a generic bound, but this will come up later.</p>
<p>So far, because we’ve been exporting this toy model, this experience has not been representative of how hard
it typically is to debug dynamic shapes guards &amp; issues. In most cases it isn’t obvious what guards are being emitted,
and which operations and parts of user code are responsible. For this toy model we pinpoint the exact lines, and the guards
are rather intuitive.</p>
<p>In more complicated cases, a helpful first step is always to enable verbose logging. This can be done either with the environment
variable <code class="docutils literal notranslate"><span class="pre">TORCH_LOGS=&quot;+dynamic&quot;</span></code>, or interactively with <code class="docutils literal notranslate"><span class="pre">torch._logging.set_logs(dynamic=10)</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">_logging</span><span class="o">.</span><span class="n">set_logs</span><span class="p">(</span><span class="n">dynamic</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:19:47.195000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:19:47.196000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s15 = 6 for L[&#39;w&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s15&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:19:47.197000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s21 = 5 for L[&#39;w&#39;].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s21&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
V1004 00:19:47.197000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
I1004 00:19:47.199000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s77 = 4 for L[&#39;x&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s77&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:19:47.200000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s17 = 8 for L[&#39;y&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s17&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:19:47.200000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s94 = 4 for L[&#39;y&#39;].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s94&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:19:47.202000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s68 = 32 for L[&#39;z&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s68&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
V1004 00:19:47.205000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.206000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.206000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.207000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.208000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.208000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.209000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.209000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.210000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.210000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.212000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
I1004 00:19:47.213000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s77, s94) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s77, s94)&quot;
I1004 00:19:47.214000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s94 = s77 (solve) VR[2, int_oo]
V1004 00:19:47.215000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
I1004 00:19:47.220000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s21, 5) [guard added] (_meta_registrations.py:2417 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s21, 5)&quot;
V1004 00:19:47.220000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s21 = VR[5, 5] (update)
I1004 00:19:47.221000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5]
V1004 00:19:47.230000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval size_oblivious(Eq(s17*s77, 1)) == False [statically known]
V1004 00:19:47.231000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
I1004 00:19:47.233000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s17*s77, s68) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s17*s77, s68)&quot;
V1004 00:19:47.234000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s68 = VR[4, int_oo] (update)
I1004 00:19:47.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s68 = s17*s77 (solve) VR[4, int_oo]
I1004 00:19:47.239000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:19:47.239000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].size()[0] s15 None
V1004 00:19:47.239000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].size()[1] 5 None
V1004 00:19:47.239000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].stride()[0] 5 None
V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].stride()[1] 1 None
V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].storage_offset() 0 None
V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[0] s77 None
V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[0] 1 None
V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].storage_offset() 0 None
V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[0] s17 None
V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[1] s77 None
V1004 00:19:47.241000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[0] s77 None
V1004 00:19:47.241000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[1] 1 None
V1004 00:19:47.241000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].storage_offset() 0 None
V1004 00:19:47.241000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;z&#39;].size()[0] s17*s77 None
V1004 00:19:47.241000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;z&#39;].stride()[0] 1 None
V1004 00:19:47.241000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;z&#39;].storage_offset() 0 None
V1004 00:19:47.249000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
V1004 00:19:47.254000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7461] eval 5 [trivial]
</pre></div>
</div>
<p>This spits out quite a handful, even with this simple toy model. The log lines here have been cut short at front and end
to ignore unnecessary info, but looking through the logs we can see the lines relevant to what we described above;
e.g. the allocation of symbols:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">create_symbol s0 = 6 for L[&#39;w&#39;].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s1 = 5 for L[&#39;w&#39;].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">runtime_assert True == True [statically known]</span>
<span class="sd">create_symbol s2 = 4 for L[&#39;x&#39;].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s3 = 8 for L[&#39;y&#39;].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s4 = 4 for L[&#39;y&#39;].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">create_symbol s5 = 32 for L[&#39;z&#39;].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&quot;\ncreate_symbol s0 = 6 for L[&#39;w&#39;].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s1 = 5 for L[&#39;w&#39;].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\nruntime_assert True == True [statically known]\ncreate_symbol s2 = 4 for L[&#39;x&#39;].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s3 = 8 for L[&#39;y&#39;].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s4 = 4 for L[&#39;y&#39;].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\ncreate_symbol s5 = 32 for L[&#39;z&#39;].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in &lt;lambda&gt;)\n&quot;
</pre></div>
</div>
<p>The lines with <cite>create_symbol</cite> show when a new symbol has been allocated, and the logs also identify the tensor variable names
and dimensions they’ve been allocated for. In other lines we can also see the guards emitted:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">runtime_assert Eq(s2, s4) [guard added] x0 = x + y  # output shape: [8, 4]  # dynamic_shapes_tutorial.py:16 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s2, s4)&quot;</span>
<span class="sd">runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # dynamic_shapes_tutorial.py:17 in forward (_meta_registrations.py:2127 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s1, 5)&quot;</span>
<span class="sd">runtime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # [32]  # dynamic_shapes_tutorial.py:19 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s2*s3, s5)&quot;</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&#39;\nruntime_assert Eq(s2, s4) [guard added] x0 = x + y  # output shape: [8, 4]  # dynamic_shapes_tutorial.py:16 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s2, s4)&quot;\nruntime_assert Eq(s1, 5) [guard added] x1 = self.l(w)  # [6, 3]  # dynamic_shapes_tutorial.py:17 in forward (_meta_registrations.py:2127 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s1, 5)&quot;\nruntime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z  # [32]  # dynamic_shapes_tutorial.py:19 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s2*s3, s5)&quot;\n&#39;
</pre></div>
</div>
<p>Next to the <code class="docutils literal notranslate"><span class="pre">[guard</span> <span class="pre">added]</span></code> messages, we also see the responsible user lines of code - luckily here the model is simple enough.
In many real-world cases it’s not so straightforward: high-level torch operations can have complicated fake-kernel implementations
or operator decompositions that complicate where and what guards are emitted. In such cases the best way to dig deeper and investigate
is to follow the logs’ suggestion, and re-run with environment variable <code class="docutils literal notranslate"><span class="pre">TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;...&quot;</span></code>, to further
attribute the guard of interest.</p>
<p><code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> is just one of the available options for interacting with <code class="docutils literal notranslate"><span class="pre">dynamic_shapes</span></code>; as of writing this 2 other options are available:
<code class="docutils literal notranslate"><span class="pre">Dim.DYNAMIC</span></code>, and <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code>. <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code> simply marks a dimension static, while <code class="docutils literal notranslate"><span class="pre">Dim.DYNAMIC</span></code> is similar to <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> in all
ways except one: it raises an error when specializing to a constant; this is designed to maintain dynamism. See for example what happens when a
static guard is emitted on a dynamically-marked dimension:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="n">Dim</span><span class="o">.</span><span class="n">DYNAMIC</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:19:47.258000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:19:47.259000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s15 = 6 for L[&#39;w&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s15&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:19:47.259000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s21 = 5 for L[&#39;w&#39;].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s21&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
V1004 00:19:47.260000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
I1004 00:19:47.261000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s77 = 4 for L[&#39;x&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s77&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:19:47.262000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s17 = 8 for L[&#39;y&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s17&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:19:47.262000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s94 = 4 for L[&#39;y&#39;].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s94&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:19:47.264000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s68 = 32 for L[&#39;z&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s68&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
V1004 00:19:47.267000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.268000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.268000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.269000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.269000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.270000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.271000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.271000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.272000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.272000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:19:47.274000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
I1004 00:19:47.275000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s77, s94) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s77, s94)&quot;
I1004 00:19:47.276000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s94 = s77 (solve) VR[2, int_oo]
V1004 00:19:47.277000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
I1004 00:19:47.281000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s21, 5) [guard added] (_meta_registrations.py:2417 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s21, 5)&quot;
V1004 00:19:47.282000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s21 = VR[5, 5] (update)
I1004 00:19:47.282000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5]
V1004 00:19:47.292000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval size_oblivious(Eq(s17*s77, 1)) == False [statically known]
V1004 00:19:47.293000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
I1004 00:19:47.295000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s17*s77, s68) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s17*s77, s68)&quot;
V1004 00:19:47.296000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s68 = VR[4, int_oo] (update)
I1004 00:19:47.297000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s68 = s17*s77 (solve) VR[4, int_oo]
I1004 00:19:47.300000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:19:47.301000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].size()[0] s15 None
V1004 00:19:47.301000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].size()[1] 5 RelaxedUnspecConstraint(warn_only=False)
V1004 00:20:20.110000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].stride()[0] 5 None
V1004 00:20:20.111000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].stride()[1] 1 None
V1004 00:20:20.112000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].storage_offset() 0 None
V1004 00:20:20.113000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[0] s77 None
V1004 00:20:20.114000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[0] 1 None
V1004 00:20:20.114000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].storage_offset() 0 None
V1004 00:20:20.115000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[0] s17 None
V1004 00:20:20.115000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[1] s77 None
V1004 00:20:20.116000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[0] s77 None
V1004 00:20:20.117000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[1] 1 None
V1004 00:20:20.117000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].storage_offset() 0 None
V1004 00:20:20.118000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;z&#39;].size()[0] s17*s77 None
V1004 00:20:20.119000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;z&#39;].stride()[0] 1 None
V1004 00:20:20.120000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;z&#39;].storage_offset() 0 None
Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1798, in _export_to_aten_ir_make_fx
    produce_guards_callback(gm)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1944, in _produce_guards_callback
    return produce_guards_and_solve_constraints(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 549, in produce_guards_and_solve_constraints
    raise constraint_violation_error
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 514, in produce_guards_and_solve_constraints
    shape_env.produce_guards(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 5200, in produce_guards
    return self.produce_guards_verbose(*args, **kwargs, langs=(&quot;python&quot;,))[0].exprs
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 5932, in produce_guards_verbose
    raise ConstraintViolationError(
torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L[&#39;w&#39;].size()[1])! For more information, run with TORCH_LOGS=&quot;+dynamic&quot;.
  - You marked L[&#39;w&#39;].size()[1] as dynamic but your code specialized it to be a constant (5). If you&#39;re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you&#39;re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.

Framework stack:
  File &quot;/opt/conda/bin/sphinx-build&quot;, line 7, in &lt;module&gt;
    sys.exit(main())
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 339, in main
    return make_main(argv)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 213, in make_main
    return make_mode.run_make_mode(argv[1:])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 181, in run_make_mode
    return make.run_generic_build(args[0])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 169, in run_generic_build
    return build_main(args + opts)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 293, in build_main
    app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 272, in __init__
    self._init_builder()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 343, in _init_builder
    self.events.emit(&#39;builder-inited&#39;)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/events.py&quot;, line 97, in emit
    results.append(listener.handler(self.app, *args))
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py&quot;, line 757, in generate_gallery_rst
    ) = generate_dir_rst(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 606, in generate_dir_rst
    results = parallel(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 607, in &lt;genexpr&gt;
    p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 86, in wrapper
    p.start()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 121, in start
    self._popen = self._Popen(self)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 281, in _Popen
    return Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 19, in __init__
    self._launch(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 314, in _bootstrap
    self.run()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 108, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 74, in call_fn
    result = func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1374, in generate_file_rst
    output_blocks, time_elapsed = execute_script(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1192, in execute_script
    execute_code_block(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1048, in execute_code_block
    is_last_expr, mem_max = _exec_and_get_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 876, in _exec_and_get_memory
    mem_max, _ = call_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1725, in _sg_call_memory_noop
    return 0.0, func()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 794, in __call__
    exec(self.code, self.fake_main.__dict__)
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 418, in &lt;module&gt;
    export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1770, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1900, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1685, in _make_fx_helper
    gm = make_fx(
  File &quot;??&quot;, line 0, in _start
  File &quot;??&quot;, line 0, in __libc_start_main
  File &quot;??&quot;, line 0, in __libc_init_first
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 738, in Py_BytesMain
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 360, in pymain_run_file_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 79, in _PyRun_AnyFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 440, in _PyRun_SimpleFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1657, in pyrun_file
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1762, in run_mod
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1741, in run_eval_code_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2318, in wrapped
    return make_fx_tracer.trace(f, *args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2250, in trace
    return self._trace_inner(f, *args)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2221, in _trace_inner
    t = dispatch_trace(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_compile.py&quot;, line 53, in inner
    return disable_fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4984, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 2790, in list___init___impl
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 966, in list_extend
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1254, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1835, in trace
    res = super().trace(root, concrete_args)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace
    (self.create_arg(fn(*args)),),
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1312, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1589, in wrapped_fn
    return tuple(flat_fn(*args))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 5091, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn
    tree_out = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c&quot;, line 1077, in builtin_exec_impl
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call
    out = mod(*args[params_len:], **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1884, in forward
    tree_out = mod(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 269, in forward
    x1 = self.l(w)  # [6, 3]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py&quot;, line 125, in forward
    return F.linear(input, self.weight, self.bias)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1360, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1407, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 1051, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 950, in handler
    return torch._library.utils.handle_dispatch_mode(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py&quot;, line 296, in handle_dispatch_mode
    return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1462, in __torch_dispatch__
    return proxy_call(self, func, self.pre_dispatch, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 914, in proxy_call
    out = func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
    return self._op(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1352, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;python_nn_functions.cpp&quot;, line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2058, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;python_nn_functions.cpp&quot;, line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1487, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;python_nn_functions.cpp&quot;, line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2601, in _dispatch_impl
    decomposition_table[func](*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;python_nn_functions.cpp&quot;, line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*)
  File &quot;??&quot;, line 0, in at::_ops::linear::call(at::Tensor const&amp;, at::Tensor const&amp;, std::optional&lt;at::Tensor&gt; const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_prims_common/wrappers.py&quot;, line 309, in _fn
    result = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_decomp/decompositions.py&quot;, line 90, in inner
    r = f(*tree_map(increase_prec, args), **tree_map(increase_prec, kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_decomp/decompositions.py&quot;, line 1462, in addmm
    out = alpha * torch.mm(mat1, mat2)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;init.cpp&quot;, line 0, in pybind11::cpp_function::initialize&lt;torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}, pybind11::object, pybind11::args const&amp;, pybind11::kwargs const&amp;&gt;(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}&amp;&amp;, pybind11::object (*)(pybind11::args const&amp;, pybind11::kwargs const&amp;))::{lambda(pybind11::detail::function_call&amp;)#1}::_FUN(pybind11::detail::function_call&amp;)
  File &quot;init.cpp&quot;, line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}::operator()(pybind11::args const&amp;, pybind11::kwargs const&amp;) const
  File &quot;??&quot;, line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, c10::Symbol, pybind11::args const&amp;, pybind11::kwargs const&amp;, bool, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;??&quot;, line 0, in torch::jit::invokeOperatorFromPython(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, pybind11::args const&amp;, pybind11::kwargs const&amp;, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;register_c10_ops.cpp&quot;, line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const [clone .isra.0]
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in void c10::BoxedKernel::make_boxed_function&lt;&amp;(anonymous namespace)::pythonTLSSnapshotFallback&gt;(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;RegisterCompositeImplicitAutograd_0.cpp&quot;, line 0, in c10::impl::make_boxed_from_unboxed_functor&lt;c10::impl::detail::WrapFunctionIntoFunctor_&lt;c10::CompileTimeFunctionPointer&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;, std::optional&lt;at::Tensor&gt; const&amp;), &amp;at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__linear&gt;, at::Tensor, c10::guts::typelist::typelist&lt;at::Tensor const&amp;, at::Tensor const&amp;, std::optional&lt;at::Tensor&gt; const&amp;&gt; &gt;, false&gt;::call(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;??&quot;, line 0, in at::native::linear(at::Tensor const&amp;, at::Tensor const&amp;, std::optional&lt;at::Tensor&gt; const&amp;)
  File &quot;??&quot;, line 0, in at::_ops::addmm::call(at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;)
  File &quot;&quot;, line 0, in c10::impl::BoxedKernelWrapper&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;), void&gt;::call(c10::BoxedKernel const&amp;, c10::OperatorHandle const&amp;, c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in void c10::BoxedKernel::make_boxed_function&lt;&amp;(anonymous namespace)::pythonTLSSnapshotFallback&gt;(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;VariableType_0.cpp&quot;, line 0, in c10::impl::make_boxed_from_unboxed_functor&lt;c10::impl::detail::WrapFunctionIntoFunctor_&lt;c10::CompileTimeFunctionPointer&lt;at::Tensor (c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;), &amp;torch::autograd::VariableType::(anonymous namespace)::addmm&gt;, at::Tensor, c10::guts::typelist::typelist&lt;c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;&gt; &gt;, false&gt;::call(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;VariableType_0.cpp&quot;, line 0, in torch::autograd::VariableType::(anonymous namespace)::addmm(c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;)
  File &quot;??&quot;, line 0, in at::_ops::addmm::redispatch(c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1352, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2058, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1487, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2717, in _dispatch_impl
    r = func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
    return self._op(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_prims_common/wrappers.py&quot;, line 309, in _fn
    result = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;python_torch_functions_1.cpp&quot;, line 0, in torch::autograd::THPVariable_mm(_object*, _object*, _object*)
  File &quot;??&quot;, line 0, in at::_ops::mm::call(at::Tensor const&amp;, at::Tensor const&amp;)
  File &quot;&quot;, line 0, in c10::impl::BoxedKernelWrapper&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;), void&gt;::call(c10::BoxedKernel const&amp;, c10::OperatorHandle const&amp;, c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_meta_registrations.py&quot;, line 2417, in meta_mm
    torch._check(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 1684, in _check
    _check_with(RuntimeError, cond, message)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 1647, in _check_with
    if expect_true(cond):
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;init.cpp&quot;, line 0, in pybind11::cpp_function::initialize&lt;torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}, pybind11::object, pybind11::args const&amp;, pybind11::kwargs const&amp;&gt;(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}&amp;&amp;, pybind11::object (*)(pybind11::args const&amp;, pybind11::kwargs const&amp;))::{lambda(pybind11::detail::function_call&amp;)#1}::_FUN(pybind11::detail::function_call&amp;)
  File &quot;init.cpp&quot;, line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}::operator()(pybind11::args const&amp;, pybind11::kwargs const&amp;) const
  File &quot;??&quot;, line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, c10::Symbol, pybind11::args const&amp;, pybind11::kwargs const&amp;, bool, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;??&quot;, line 0, in torch::jit::invokeOperatorFromPython(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, pybind11::args const&amp;, pybind11::kwargs const&amp;, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;register_c10_ops.cpp&quot;, line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const [clone .isra.0]
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 1702, in expect_true
    return a.node.expect_true(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 559, in expect_true
    return self.shape_env.guard_or_defer_runtime_assert(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
    return retlog(fn(*args, **kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 1021, in bounded_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7730, in guard_or_defer_runtime_assert
    self._maybe_guard_rel(expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6868, in _maybe_guard_rel
    self._refine_ranges(expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7819, in _refine_ranges
    self._set_replacement(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 1021, in bounded_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6768, in _set_replacement
    CapturedTraceback.extract(cpp=True)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py&quot;, line 212, in extract
    torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp),
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in pybind11::cpp_function::initialize&lt;std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt;, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v&gt;(std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt; (*)(bool, bool, bool), pybind11::name const&amp;, pybind11::scope const&amp;, pybind11::sibling const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::operator()(pybind11::detail::function_call&amp;) const
  File &quot;??&quot;, line 0, in torch::CapturedTraceback::gather(bool, bool, bool)
  File &quot;??&quot;, line 0, in torch::unwind::unwind()


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 418, in &lt;module&gt;
    export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 319, in export
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
           ^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1164, in wrapper
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
         ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1164, in wrapper
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
                      ^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1800, in _export_to_aten_ir_make_fx
    raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch._dynamo.exc.UserError: Constraints violated (L[&#39;w&#39;].size()[1])! For more information, run with TORCH_LOGS=&quot;+dynamic&quot;.
  - You marked L[&#39;w&#39;].size()[1] as dynamic but your code specialized it to be a constant (5). If you&#39;re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you&#39;re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.

Framework stack:
  File &quot;/opt/conda/bin/sphinx-build&quot;, line 7, in &lt;module&gt;
    sys.exit(main())
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 339, in main
    return make_main(argv)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 213, in make_main
    return make_mode.run_make_mode(argv[1:])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 181, in run_make_mode
    return make.run_generic_build(args[0])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 169, in run_generic_build
    return build_main(args + opts)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 293, in build_main
    app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 272, in __init__
    self._init_builder()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 343, in _init_builder
    self.events.emit(&#39;builder-inited&#39;)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/events.py&quot;, line 97, in emit
    results.append(listener.handler(self.app, *args))
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py&quot;, line 757, in generate_gallery_rst
    ) = generate_dir_rst(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 606, in generate_dir_rst
    results = parallel(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 607, in &lt;genexpr&gt;
    p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 86, in wrapper
    p.start()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 121, in start
    self._popen = self._Popen(self)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 281, in _Popen
    return Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 19, in __init__
    self._launch(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 314, in _bootstrap
    self.run()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 108, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 74, in call_fn
    result = func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1374, in generate_file_rst
    output_blocks, time_elapsed = execute_script(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1192, in execute_script
    execute_code_block(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1048, in execute_code_block
    is_last_expr, mem_max = _exec_and_get_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 876, in _exec_and_get_memory
    mem_max, _ = call_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1725, in _sg_call_memory_noop
    return 0.0, func()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 794, in __call__
    exec(self.code, self.fake_main.__dict__)
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 418, in &lt;module&gt;
    export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1770, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1900, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1685, in _make_fx_helper
    gm = make_fx(
  File &quot;??&quot;, line 0, in _start
  File &quot;??&quot;, line 0, in __libc_start_main
  File &quot;??&quot;, line 0, in __libc_init_first
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 738, in Py_BytesMain
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 360, in pymain_run_file_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 79, in _PyRun_AnyFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 440, in _PyRun_SimpleFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1657, in pyrun_file
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1762, in run_mod
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1741, in run_eval_code_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2318, in wrapped
    return make_fx_tracer.trace(f, *args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2250, in trace
    return self._trace_inner(f, *args)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2221, in _trace_inner
    t = dispatch_trace(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_compile.py&quot;, line 53, in inner
    return disable_fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4984, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 2790, in list___init___impl
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 966, in list_extend
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1254, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1835, in trace
    res = super().trace(root, concrete_args)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace
    (self.create_arg(fn(*args)),),
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1312, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1589, in wrapped_fn
    return tuple(flat_fn(*args))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 5091, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn
    tree_out = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c&quot;, line 1077, in builtin_exec_impl
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call
    out = mod(*args[params_len:], **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1884, in forward
    tree_out = mod(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 269, in forward
    x1 = self.l(w)  # [6, 3]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py&quot;, line 125, in forward
    return F.linear(input, self.weight, self.bias)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1360, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1407, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 1051, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 950, in handler
    return torch._library.utils.handle_dispatch_mode(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py&quot;, line 296, in handle_dispatch_mode
    return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1462, in __torch_dispatch__
    return proxy_call(self, func, self.pre_dispatch, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 914, in proxy_call
    out = func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
    return self._op(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1352, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;python_nn_functions.cpp&quot;, line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2058, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;python_nn_functions.cpp&quot;, line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1487, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;python_nn_functions.cpp&quot;, line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2601, in _dispatch_impl
    decomposition_table[func](*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;python_nn_functions.cpp&quot;, line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*)
  File &quot;??&quot;, line 0, in at::_ops::linear::call(at::Tensor const&amp;, at::Tensor const&amp;, std::optional&lt;at::Tensor&gt; const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_prims_common/wrappers.py&quot;, line 309, in _fn
    result = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_decomp/decompositions.py&quot;, line 90, in inner
    r = f(*tree_map(increase_prec, args), **tree_map(increase_prec, kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_decomp/decompositions.py&quot;, line 1462, in addmm
    out = alpha * torch.mm(mat1, mat2)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;init.cpp&quot;, line 0, in pybind11::cpp_function::initialize&lt;torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}, pybind11::object, pybind11::args const&amp;, pybind11::kwargs const&amp;&gt;(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}&amp;&amp;, pybind11::object (*)(pybind11::args const&amp;, pybind11::kwargs const&amp;))::{lambda(pybind11::detail::function_call&amp;)#1}::_FUN(pybind11::detail::function_call&amp;)
  File &quot;init.cpp&quot;, line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}::operator()(pybind11::args const&amp;, pybind11::kwargs const&amp;) const
  File &quot;??&quot;, line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, c10::Symbol, pybind11::args const&amp;, pybind11::kwargs const&amp;, bool, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;??&quot;, line 0, in torch::jit::invokeOperatorFromPython(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, pybind11::args const&amp;, pybind11::kwargs const&amp;, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;register_c10_ops.cpp&quot;, line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const [clone .isra.0]
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in void c10::BoxedKernel::make_boxed_function&lt;&amp;(anonymous namespace)::pythonTLSSnapshotFallback&gt;(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;RegisterCompositeImplicitAutograd_0.cpp&quot;, line 0, in c10::impl::make_boxed_from_unboxed_functor&lt;c10::impl::detail::WrapFunctionIntoFunctor_&lt;c10::CompileTimeFunctionPointer&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;, std::optional&lt;at::Tensor&gt; const&amp;), &amp;at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__linear&gt;, at::Tensor, c10::guts::typelist::typelist&lt;at::Tensor const&amp;, at::Tensor const&amp;, std::optional&lt;at::Tensor&gt; const&amp;&gt; &gt;, false&gt;::call(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;??&quot;, line 0, in at::native::linear(at::Tensor const&amp;, at::Tensor const&amp;, std::optional&lt;at::Tensor&gt; const&amp;)
  File &quot;??&quot;, line 0, in at::_ops::addmm::call(at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;)
  File &quot;&quot;, line 0, in c10::impl::BoxedKernelWrapper&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;), void&gt;::call(c10::BoxedKernel const&amp;, c10::OperatorHandle const&amp;, c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in void c10::BoxedKernel::make_boxed_function&lt;&amp;(anonymous namespace)::pythonTLSSnapshotFallback&gt;(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;VariableType_0.cpp&quot;, line 0, in c10::impl::make_boxed_from_unboxed_functor&lt;c10::impl::detail::WrapFunctionIntoFunctor_&lt;c10::CompileTimeFunctionPointer&lt;at::Tensor (c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;), &amp;torch::autograd::VariableType::(anonymous namespace)::addmm&gt;, at::Tensor, c10::guts::typelist::typelist&lt;c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;&gt; &gt;, false&gt;::call(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;VariableType_0.cpp&quot;, line 0, in torch::autograd::VariableType::(anonymous namespace)::addmm(c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;)
  File &quot;??&quot;, line 0, in at::_ops::addmm::redispatch(c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;, c10::Scalar const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1352, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2058, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1487, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2717, in _dispatch_impl
    r = func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
    return self._op(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_prims_common/wrappers.py&quot;, line 309, in _fn
    result = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;python_torch_functions_1.cpp&quot;, line 0, in torch::autograd::THPVariable_mm(_object*, _object*, _object*)
  File &quot;??&quot;, line 0, in at::_ops::mm::call(at::Tensor const&amp;, at::Tensor const&amp;)
  File &quot;&quot;, line 0, in c10::impl::BoxedKernelWrapper&lt;at::Tensor (at::Tensor const&amp;, at::Tensor const&amp;), void&gt;::call(c10::BoxedKernel const&amp;, c10::OperatorHandle const&amp;, c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_meta_registrations.py&quot;, line 2417, in meta_mm
    torch._check(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 1684, in _check
    _check_with(RuntimeError, cond, message)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 1647, in _check_with
    if expect_true(cond):
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;init.cpp&quot;, line 0, in pybind11::cpp_function::initialize&lt;torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}, pybind11::object, pybind11::args const&amp;, pybind11::kwargs const&amp;&gt;(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}&amp;&amp;, pybind11::object (*)(pybind11::args const&amp;, pybind11::kwargs const&amp;))::{lambda(pybind11::detail::function_call&amp;)#1}::_FUN(pybind11::detail::function_call&amp;)
  File &quot;init.cpp&quot;, line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}::operator()(pybind11::args const&amp;, pybind11::kwargs const&amp;) const
  File &quot;??&quot;, line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, c10::Symbol, pybind11::args const&amp;, pybind11::kwargs const&amp;, bool, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;??&quot;, line 0, in torch::jit::invokeOperatorFromPython(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, pybind11::args const&amp;, pybind11::kwargs const&amp;, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;register_c10_ops.cpp&quot;, line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const [clone .isra.0]
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 1702, in expect_true
    return a.node.expect_true(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 559, in expect_true
    return self.shape_env.guard_or_defer_runtime_assert(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
    return retlog(fn(*args, **kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 1021, in bounded_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7730, in guard_or_defer_runtime_assert
    self._maybe_guard_rel(expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6868, in _maybe_guard_rel
    self._refine_ranges(expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7819, in _refine_ranges
    self._set_replacement(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 1021, in bounded_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6768, in _set_replacement
    CapturedTraceback.extract(cpp=True)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py&quot;, line 212, in extract
    torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp),
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in pybind11::cpp_function::initialize&lt;std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt;, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v&gt;(std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt; (*)(bool, bool, bool), pybind11::name const&amp;, pybind11::scope const&amp;, pybind11::sibling const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::operator()(pybind11::detail::function_call&amp;) const
  File &quot;??&quot;, line 0, in torch::CapturedTraceback::gather(bool, bool, bool)
  File &quot;??&quot;, line 0, in torch::unwind::unwind()


The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
<p>Static guards also aren’t always inherent to the model; they can also come from user specifications. In fact, a common pitfall leading to shape
specializations is when the user specifies conflicting markers for equivalent dimensions; one dynamic and another static. The same error type is
raised when this is the case for <code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code> and <code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">)</span>
<span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">STATIC</span><span class="p">,)</span>
<span class="n">dynamic_shapes</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="n">Dim</span><span class="o">.</span><span class="n">DYNAMIC</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:20.182000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:20.183000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s15 = 6 for L[&#39;w&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s15&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:20:20.184000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s21 = 5 for L[&#39;w&#39;].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s21&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
V1004 00:20:20.184000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
I1004 00:20:20.186000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s17 = 8 for L[&#39;y&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s17&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:20:20.187000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s94 = 4 for L[&#39;y&#39;].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s94&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:20:20.189000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s68 = 32 for L[&#39;z&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s68&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
V1004 00:20:20.194000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.194000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.195000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.196000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.197000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.197000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.198000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.198000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
I1004 00:20:20.203000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s94, 4) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s94, 4)&quot;
V1004 00:20:20.204000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s94 = VR[4, 4] (update)
I1004 00:20:20.204000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s94 = 4 (range_refined_to_singleton) VR[4, 4]
I1004 00:20:20.211000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s21, 5) [guard added] (_meta_registrations.py:2417 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s21, 5)&quot;
V1004 00:20:20.211000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s21 = VR[5, 5] (update)
I1004 00:20:20.212000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5]
V1004 00:20:20.213000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
I1004 00:20:20.227000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(4*s17, s68) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(4*s17, s68)&quot;
V1004 00:20:20.228000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s68 = VR[8, int_oo] (update)
I1004 00:20:20.230000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s68 = 4*s17 (solve) VR[8, int_oo]
I1004 00:20:20.234000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:20:20.234000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].size()[0] s15 None
V1004 00:20:20.234000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].size()[1] 5 None
V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].stride()[0] 5 None
V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].stride()[1] 1 None
V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].storage_offset() 0 None
V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[0] 4 None
V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[0] 1 None
V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].storage_offset() 0 None
V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[0] s17 None
V1004 00:20:20.236000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[1] 4 RelaxedUnspecConstraint(warn_only=False)
V1004 00:20:20.251000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[0] 4 None
V1004 00:20:20.252000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[1] 1 None
V1004 00:20:20.252000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].storage_offset() 0 None
V1004 00:20:20.252000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;z&#39;].size()[0] 4*s17 None
V1004 00:20:20.253000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;z&#39;].stride()[0] 1 None
V1004 00:20:20.253000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;z&#39;].storage_offset() 0 None
Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1798, in _export_to_aten_ir_make_fx
    produce_guards_callback(gm)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1944, in _produce_guards_callback
    return produce_guards_and_solve_constraints(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 549, in produce_guards_and_solve_constraints
    raise constraint_violation_error
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 514, in produce_guards_and_solve_constraints
    shape_env.produce_guards(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 5200, in produce_guards
    return self.produce_guards_verbose(*args, **kwargs, langs=(&quot;python&quot;,))[0].exprs
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 5932, in produce_guards_verbose
    raise ConstraintViolationError(
torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L[&#39;y&#39;].size()[1])! For more information, run with TORCH_LOGS=&quot;+dynamic&quot;.
  - You marked L[&#39;y&#39;].size()[1] as dynamic but your code specialized it to be a constant (4). If you&#39;re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you&#39;re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.

Framework stack:
  File &quot;/opt/conda/bin/sphinx-build&quot;, line 7, in &lt;module&gt;
    sys.exit(main())
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 339, in main
    return make_main(argv)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 213, in make_main
    return make_mode.run_make_mode(argv[1:])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 181, in run_make_mode
    return make.run_generic_build(args[0])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 169, in run_generic_build
    return build_main(args + opts)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 293, in build_main
    app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 272, in __init__
    self._init_builder()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 343, in _init_builder
    self.events.emit(&#39;builder-inited&#39;)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/events.py&quot;, line 97, in emit
    results.append(listener.handler(self.app, *args))
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py&quot;, line 757, in generate_gallery_rst
    ) = generate_dir_rst(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 606, in generate_dir_rst
    results = parallel(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 607, in &lt;genexpr&gt;
    p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 86, in wrapper
    p.start()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 121, in start
    self._popen = self._Popen(self)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 281, in _Popen
    return Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 19, in __init__
    self._launch(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 314, in _bootstrap
    self.run()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 108, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 74, in call_fn
    result = func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1374, in generate_file_rst
    output_blocks, time_elapsed = execute_script(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1192, in execute_script
    execute_code_block(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1048, in execute_code_block
    is_last_expr, mem_max = _exec_and_get_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 876, in _exec_and_get_memory
    mem_max, _ = call_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1725, in _sg_call_memory_noop
    return 0.0, func()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 794, in __call__
    exec(self.code, self.fake_main.__dict__)
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 431, in &lt;module&gt;
    export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1770, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
  File &quot;??&quot;, line 0, in _start
  File &quot;??&quot;, line 0, in __libc_start_main
  File &quot;??&quot;, line 0, in __libc_init_first
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 738, in Py_BytesMain
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 360, in pymain_run_file_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 79, in _PyRun_AnyFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 440, in _PyRun_SimpleFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1657, in pyrun_file
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1762, in run_mod
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1741, in run_eval_code_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1900, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1685, in _make_fx_helper
    gm = make_fx(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2318, in wrapped
    return make_fx_tracer.trace(f, *args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2250, in trace
    return self._trace_inner(f, *args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4984, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2221, in _trace_inner
    t = dispatch_trace(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 2790, in list___init___impl
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 966, in list_extend
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_compile.py&quot;, line 53, in inner
    return disable_fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1254, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1835, in trace
    res = super().trace(root, concrete_args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace
    (self.create_arg(fn(*args)),),
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1312, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 5091, in _PyEval_EvalFrameDefault
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;
  File &quot;/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c&quot;, line 1077, in builtin_exec_impl
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1589, in wrapped_fn
    return tuple(flat_fn(*args))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn
    tree_out = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call
    out = mod(*args[params_len:], **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1884, in forward
    tree_out = mod(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 268, in forward
    x0 = x + y  # [8, 4]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1360, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1407, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 1051, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 950, in handler
    return torch._library.utils.handle_dispatch_mode(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py&quot;, line 296, in handle_dispatch_mode
    return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1462, in __torch_dispatch__
    return proxy_call(self, func, self.pre_dispatch, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 914, in proxy_call
    out = func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
    return self._op(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 5548, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/abstract.c&quot;, line 893, in binary_op1
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7420, in slot_nb_add
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in _object* torch::autograd::TypeError_to_NotImplemented_&lt;&amp;torch::autograd::THPVariable_add&gt;(_object*, _object*, _object*)
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1352, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2058, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1487, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;??&quot;, line 0, in at::_ops::add_Tensor::call(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2581, in _dispatch_impl
    return maybe_propagate_real_tensors(fast_impl(self, *args, **kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py&quot;, line 962, in fast_binary_impl
    final_shape = infer_size(final_shape, shape)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py&quot;, line 922, in infer_size
    torch._check(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 1684, in _check
    _check_with(RuntimeError, cond, message)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;init.cpp&quot;, line 0, in pybind11::cpp_function::initialize&lt;torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}, pybind11::object, pybind11::args const&amp;, pybind11::kwargs const&amp;&gt;(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}&amp;&amp;, pybind11::object (*)(pybind11::args const&amp;, pybind11::kwargs const&amp;))::{lambda(pybind11::detail::function_call&amp;)#1}::_FUN(pybind11::detail::function_call&amp;)
  File &quot;init.cpp&quot;, line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}::operator()(pybind11::args const&amp;, pybind11::kwargs const&amp;) const
  File &quot;??&quot;, line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, c10::Symbol, pybind11::args const&amp;, pybind11::kwargs const&amp;, bool, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;??&quot;, line 0, in torch::jit::invokeOperatorFromPython(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, pybind11::args const&amp;, pybind11::kwargs const&amp;, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;register_c10_ops.cpp&quot;, line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const [clone .isra.0]
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in void c10::BoxedKernel::make_boxed_function&lt;&amp;(anonymous namespace)::pythonTLSSnapshotFallback&gt;(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;VariableType_2.cpp&quot;, line 0, in c10::impl::make_boxed_from_unboxed_functor&lt;c10::impl::detail::WrapFunctionIntoFunctor_&lt;c10::CompileTimeFunctionPointer&lt;at::Tensor (c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;), &amp;torch::autograd::VariableType::(anonymous namespace)::add_Tensor&gt;, at::Tensor, c10::guts::typelist::typelist&lt;c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;&gt; &gt;, false&gt;::call(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;VariableType_2.cpp&quot;, line 0, in torch::autograd::VariableType::(anonymous namespace)::add_Tensor(c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;)
  File &quot;??&quot;, line 0, in at::_ops::add_Tensor::redispatch(c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 1647, in _check_with
    if expect_true(cond):
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 1702, in expect_true
    return a.node.expect_true(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 559, in expect_true
    return self.shape_env.guard_or_defer_runtime_assert(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
    return retlog(fn(*args, **kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 1021, in bounded_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7730, in guard_or_defer_runtime_assert
    self._maybe_guard_rel(expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6868, in _maybe_guard_rel
    self._refine_ranges(expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7819, in _refine_ranges
    self._set_replacement(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 1021, in bounded_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6768, in _set_replacement
    CapturedTraceback.extract(cpp=True)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py&quot;, line 212, in extract
    torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp),
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in pybind11::cpp_function::initialize&lt;std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt;, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v&gt;(std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt; (*)(bool, bool, bool), pybind11::name const&amp;, pybind11::scope const&amp;, pybind11::sibling const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::operator()(pybind11::detail::function_call&amp;) const
  File &quot;??&quot;, line 0, in torch::CapturedTraceback::gather(bool, bool, bool)
  File &quot;??&quot;, line 0, in torch::unwind::unwind()


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 431, in &lt;module&gt;
    export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 319, in export
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
           ^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1164, in wrapper
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
         ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1164, in wrapper
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
                      ^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1800, in _export_to_aten_ir_make_fx
    raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch._dynamo.exc.UserError: Constraints violated (L[&#39;y&#39;].size()[1])! For more information, run with TORCH_LOGS=&quot;+dynamic&quot;.
  - You marked L[&#39;y&#39;].size()[1] as dynamic but your code specialized it to be a constant (4). If you&#39;re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you&#39;re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.

Framework stack:
  File &quot;/opt/conda/bin/sphinx-build&quot;, line 7, in &lt;module&gt;
    sys.exit(main())
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 339, in main
    return make_main(argv)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 213, in make_main
    return make_mode.run_make_mode(argv[1:])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 181, in run_make_mode
    return make.run_generic_build(args[0])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 169, in run_generic_build
    return build_main(args + opts)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 293, in build_main
    app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 272, in __init__
    self._init_builder()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 343, in _init_builder
    self.events.emit(&#39;builder-inited&#39;)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/events.py&quot;, line 97, in emit
    results.append(listener.handler(self.app, *args))
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py&quot;, line 757, in generate_gallery_rst
    ) = generate_dir_rst(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 606, in generate_dir_rst
    results = parallel(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 607, in &lt;genexpr&gt;
    p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 86, in wrapper
    p.start()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 121, in start
    self._popen = self._Popen(self)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 281, in _Popen
    return Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 19, in __init__
    self._launch(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 314, in _bootstrap
    self.run()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 108, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 74, in call_fn
    result = func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1374, in generate_file_rst
    output_blocks, time_elapsed = execute_script(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1192, in execute_script
    execute_code_block(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1048, in execute_code_block
    is_last_expr, mem_max = _exec_and_get_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 876, in _exec_and_get_memory
    mem_max, _ = call_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1725, in _sg_call_memory_noop
    return 0.0, func()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 794, in __call__
    exec(self.code, self.fake_main.__dict__)
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 431, in &lt;module&gt;
    export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1770, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
  File &quot;??&quot;, line 0, in _start
  File &quot;??&quot;, line 0, in __libc_start_main
  File &quot;??&quot;, line 0, in __libc_init_first
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 738, in Py_BytesMain
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 360, in pymain_run_file_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 79, in _PyRun_AnyFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 440, in _PyRun_SimpleFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1657, in pyrun_file
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1762, in run_mod
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1741, in run_eval_code_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1900, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1685, in _make_fx_helper
    gm = make_fx(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2318, in wrapped
    return make_fx_tracer.trace(f, *args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2250, in trace
    return self._trace_inner(f, *args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4984, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2221, in _trace_inner
    t = dispatch_trace(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 2790, in list___init___impl
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 966, in list_extend
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_compile.py&quot;, line 53, in inner
    return disable_fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1254, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1835, in trace
    res = super().trace(root, concrete_args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace
    (self.create_arg(fn(*args)),),
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1312, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 5091, in _PyEval_EvalFrameDefault
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;
  File &quot;/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c&quot;, line 1077, in builtin_exec_impl
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1589, in wrapped_fn
    return tuple(flat_fn(*args))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn
    tree_out = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call
    out = mod(*args[params_len:], **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1884, in forward
    tree_out = mod(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 268, in forward
    x0 = x + y  # [8, 4]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1360, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1407, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 1051, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 950, in handler
    return torch._library.utils.handle_dispatch_mode(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py&quot;, line 296, in handle_dispatch_mode
    return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1462, in __torch_dispatch__
    return proxy_call(self, func, self.pre_dispatch, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 914, in proxy_call
    out = func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
    return self._op(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 5548, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/abstract.c&quot;, line 893, in binary_op1
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7420, in slot_nb_add
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in _object* torch::autograd::TypeError_to_NotImplemented_&lt;&amp;torch::autograd::THPVariable_add&gt;(_object*, _object*, _object*)
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1352, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2058, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1487, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;??&quot;, line 0, in at::_ops::add_Tensor::call(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2581, in _dispatch_impl
    return maybe_propagate_real_tensors(fast_impl(self, *args, **kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py&quot;, line 962, in fast_binary_impl
    final_shape = infer_size(final_shape, shape)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py&quot;, line 922, in infer_size
    torch._check(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 1684, in _check
    _check_with(RuntimeError, cond, message)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;init.cpp&quot;, line 0, in pybind11::cpp_function::initialize&lt;torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}, pybind11::object, pybind11::args const&amp;, pybind11::kwargs const&amp;&gt;(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}&amp;&amp;, pybind11::object (*)(pybind11::args const&amp;, pybind11::kwargs const&amp;))::{lambda(pybind11::detail::function_call&amp;)#1}::_FUN(pybind11::detail::function_call&amp;)
  File &quot;init.cpp&quot;, line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}::operator()(pybind11::args const&amp;, pybind11::kwargs const&amp;) const
  File &quot;??&quot;, line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, c10::Symbol, pybind11::args const&amp;, pybind11::kwargs const&amp;, bool, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;??&quot;, line 0, in torch::jit::invokeOperatorFromPython(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, pybind11::args const&amp;, pybind11::kwargs const&amp;, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;register_c10_ops.cpp&quot;, line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const [clone .isra.0]
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in void c10::BoxedKernel::make_boxed_function&lt;&amp;(anonymous namespace)::pythonTLSSnapshotFallback&gt;(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;VariableType_2.cpp&quot;, line 0, in c10::impl::make_boxed_from_unboxed_functor&lt;c10::impl::detail::WrapFunctionIntoFunctor_&lt;c10::CompileTimeFunctionPointer&lt;at::Tensor (c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;), &amp;torch::autograd::VariableType::(anonymous namespace)::add_Tensor&gt;, at::Tensor, c10::guts::typelist::typelist&lt;c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;&gt; &gt;, false&gt;::call(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;VariableType_2.cpp&quot;, line 0, in torch::autograd::VariableType::(anonymous namespace)::add_Tensor(c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;)
  File &quot;??&quot;, line 0, in at::_ops::add_Tensor::redispatch(c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 1647, in _check_with
    if expect_true(cond):
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 1702, in expect_true
    return a.node.expect_true(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 559, in expect_true
    return self.shape_env.guard_or_defer_runtime_assert(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
    return retlog(fn(*args, **kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 1021, in bounded_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7730, in guard_or_defer_runtime_assert
    self._maybe_guard_rel(expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6868, in _maybe_guard_rel
    self._refine_ranges(expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7819, in _refine_ranges
    self._set_replacement(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 1021, in bounded_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6768, in _set_replacement
    CapturedTraceback.extract(cpp=True)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py&quot;, line 212, in extract
    torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp),
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in pybind11::cpp_function::initialize&lt;std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt;, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v&gt;(std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt; (*)(bool, bool, bool), pybind11::name const&amp;, pybind11::scope const&amp;, pybind11::sibling const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::operator()(pybind11::detail::function_call&amp;) const
  File &quot;??&quot;, line 0, in torch::CapturedTraceback::gather(bool, bool, bool)
  File &quot;??&quot;, line 0, in torch::unwind::unwind()


The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
<p>Here you might ask why export “specializes”, i.e. why we resolve this static/dynamic conflict by going with the static route. The answer is because
of the symbolic shapes system described above, of symbols and guards. When <code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code> is marked static, we don’t allocate a symbol, and compile
treating this shape as a concrete integer 4. A symbol is allocated for <code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code>, and so we finally emit the guard <code class="docutils literal notranslate"><span class="pre">s3</span> <span class="pre">==</span> <span class="pre">4</span></code>, leading to
specialization.</p>
<p>One feature of export is that during tracing, statements like asserts, <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code>, and <code class="docutils literal notranslate"><span class="pre">if/else</span></code> conditions will also emit guards.
See what happens when we augment the existing model with such statements:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DynamicModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">512</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">4</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">x0</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
            <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
            <span class="n">x2</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">z</span>
            <span class="k">return</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">w</span>

<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;w&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
    <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">),</span>
    <span class="s2">&quot;z&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,),</span>
<span class="p">}</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">DynamicModel</span><span class="p">(),</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span> <span class="n">dynamic_shapes</span><span class="o">=</span><span class="n">dynamic_shapes</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:20.287000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:20.290000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s15 = 6 for L[&#39;w&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s15&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:20:20.290000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s21 = 5 for L[&#39;w&#39;].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s21&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
V1004 00:20:20.291000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
I1004 00:20:20.292000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s77 = 4 for L[&#39;x&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s77&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:20:20.293000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s17 = 8 for L[&#39;y&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s17&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:20:20.294000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s94 = 4 for L[&#39;y&#39;].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s94&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:20:20.295000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s68 = 32 for L[&#39;z&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s68&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
V1004 00:20:20.299000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.299000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.300000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.301000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.301000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.302000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.302000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.303000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.304000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.304000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
I1004 00:20:20.314000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] eval s15 &lt;= 512 [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:450 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;s15 &lt;= 512&quot;
V1004 00:20:20.315000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s15 = VR[2, 512] (update)
I1004 00:20:20.320000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert s77 &gt;= 4 [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:451 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;s77 &gt;= 4&quot;
V1004 00:20:20.321000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s77 = VR[4, int_oo] (update)
I1004 00:20:20.326000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] eval Eq(s15, s77 + 2) [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:452 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s15, s77 + 2)&quot;
V1004 00:20:20.328000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s15 = VR[6, 512] (update)
V1004 00:20:20.330000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s77 = VR[4, 510] (update)
I1004 00:20:20.331000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s15 = s77 + 2 (solve) VR[6, 512]
V1004 00:20:20.333000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
I1004 00:20:20.335000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s77, s94) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s77, s94)&quot;
V1004 00:20:20.336000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s94 = VR[4, 510] (update)
I1004 00:20:20.337000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s94 = s77 (solve) VR[4, 510]
V1004 00:20:20.340000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
I1004 00:20:20.346000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s21, 5) [guard added] (_meta_registrations.py:2417 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s21, 5)&quot;
V1004 00:20:20.347000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s21 = VR[5, 5] (update)
I1004 00:20:20.347000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5]
V1004 00:20:20.359000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval size_oblivious(Eq(s17*s77, 1)) == False [statically known]
V1004 00:20:20.361000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
I1004 00:20:20.369000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s17*s77, s68) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s17*s77, s68)&quot;
V1004 00:20:20.369000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s68 = VR[8, int_oo] (update)
I1004 00:20:20.370000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s68 = s17*s77 (solve) VR[8, int_oo]
I1004 00:20:20.375000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:20:20.375000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].size()[0] s77 + 2 None
V1004 00:20:20.375000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].size()[1] 5 None
V1004 00:20:20.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].stride()[0] 5 None
V1004 00:20:20.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].stride()[1] 1 None
V1004 00:20:20.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;w&#39;].storage_offset() 0 None
V1004 00:20:20.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[0] s77 None
V1004 00:20:20.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[0] 1 None
V1004 00:20:20.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].storage_offset() 0 None
V1004 00:20:20.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[0] s17 None
V1004 00:20:20.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[1] s77 None
V1004 00:20:20.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[0] s77 None
V1004 00:20:20.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[1] 1 None
V1004 00:20:20.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].storage_offset() 0 None
V1004 00:20:20.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;z&#39;].size()[0] s17*s77 None
V1004 00:20:20.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;z&#39;].stride()[0] 1 None
V1004 00:20:20.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;z&#39;].storage_offset() 0 None
V1004 00:20:20.388000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert s77 &gt;= 4 == True [statically known]
V1004 00:20:20.389000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
V1004 00:20:20.394000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7461] eval 5 [trivial]
</pre></div>
</div>
<p>Each of these statements emits an additional guard, and the exported program shows the changes; <code class="docutils literal notranslate"><span class="pre">s0</span></code> is eliminated in favor of <code class="docutils literal notranslate"><span class="pre">s2</span> <span class="pre">+</span> <span class="pre">2</span></code>,
and <code class="docutils literal notranslate"><span class="pre">s2</span></code> now contains lower and upper bounds, reflected in <code class="docutils literal notranslate"><span class="pre">range_constraints</span></code>.</p>
<p>For the if/else condition, you might ask why the True branch was taken, and why it wasn’t the <code class="docutils literal notranslate"><span class="pre">w.shape[0]</span> <span class="pre">!=</span> <span class="pre">x.shape[0]</span> <span class="pre">+</span> <span class="pre">2</span></code> guard that
got emitted from tracing. The answer is that export is guided by the sample inputs provided by tracing, and specializes on the branches taken.
If different sample input shapes were provided that fail the <code class="docutils literal notranslate"><span class="pre">if</span></code> condition, export would trace and emit guards corresponding to the <code class="docutils literal notranslate"><span class="pre">else</span></code> branch.
Additionally, you might ask why we traced only the <code class="docutils literal notranslate"><span class="pre">if</span></code> branch, and if it’s possible to maintain control-flow in your program and keep both branches
alive. For that, refer to rewriting your model code following the <code class="docutils literal notranslate"><span class="pre">Control</span> <span class="pre">Flow</span> <span class="pre">Ops</span></code> section above.</p>
</section>
<section id="specialization">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">0/1 specialization</a><a class="headerlink" href="#specialization" title="Link to this heading">#</a></h3>
<p>Since we’re talking about guards and specializations, it’s a good time to talk about the 0/1 specialization issue we brought up earlier.
The bottom line is that export will specialize on sample input dimensions with value 0 or 1, because these shapes have trace-time properties that
don’t generalize to other shapes. For example, size 1 tensors can broadcast while other sizes fail; and size 0 … . This just means that you should
specify 0/1 sample inputs when you’d like your program to hardcode them, and non-0/1 sample inputs when dynamic behavior is desirable. See what happens
at runtime when we export this linear layer:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),),</span>
    <span class="n">dynamic_shapes</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="n">Dim</span><span class="o">.</span><span class="n">STATIC</span><span class="p">),</span>
    <span class="p">},</span>
<span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">ep</span><span class="o">.</span><span class="n">module</span><span class="p">()(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:20.399000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:20.408000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:20:20.409000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;input&#39;].size()[0] 1 None
V1004 00:20:20.409000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;input&#39;].size()[1] 4 None
V1004 00:20:20.409000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;input&#39;].stride()[0] 4 None
V1004 00:20:20.409000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;input&#39;].stride()[1] 1 None
V1004 00:20:20.409000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;input&#39;].storage_offset() 0 None
W1004 00:20:20.411000 3670787 site-packages/torch/_export/non_strict_utils.py:580] dimension inputs[&#39;input&#39;].shape[0] 0/1 specialized; Dim.AUTO was specified along with a sample input with hint = 1.
Traceback (most recent call last):
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 500, in &lt;module&gt;
    ep.module()(torch.randn(2, 4))
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/graph_module.py&quot;, line 848, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/graph_module.py&quot;, line 424, in __call__
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/graph_module.py&quot;, line 411, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1879, in _call_impl
    return inner()
           ^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1806, in inner
    args_kwargs_result = hook(self, args, kwargs)  # type: ignore[misc]
                         ^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_unlift.py&quot;, line 83, in _check_input_constraints_pre_hook
    _check_input_constraints_for_graph(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/utils.py&quot;, line 426, in _check_input_constraints_for_graph
    _check_symint(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/utils.py&quot;, line 390, in _check_symint
    raise RuntimeError(
RuntimeError: Expected input at *args[0].shape[0] to be equal to 1, but got 2. If you meant for this dimension to be dynamic, please re-export and specify dynamic_shapes (e.g. with Dim.DYNAMIC)
</pre></div>
</div>
</section>
<section id="named-dims">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">Named Dims</a><a class="headerlink" href="#named-dims" title="Link to this heading">#</a></h3>
<p>So far we’ve only been talking about 3 ways to specify dynamic shapes: <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code>, <code class="docutils literal notranslate"><span class="pre">Dim.DYNAMIC</span></code>, and <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code>. The attraction of these is the
low-friction user experience; all the guards emitted during model tracing are adhered to, and dynamic behavior like min/max ranges, relations, and static/dynamic
dimensions are automatically figured out underneath export. The dynamic shapes subsystem essentially acts as a “discovery” process, summarizing these guards
and presenting what export believes is the overall dynamic behavior of the program. The drawback of this design appears once the user has stronger expectations or
beliefs about the dynamic behavior of these models - maybe there is a strong desire on dynamism and specializations on particular dimensions are to be avoided at
all costs, or maybe we just want to catch changes in dynamic behavior with changes to the original model code, or possibly underlying decompositions or meta-kernels.
These changes won’t be detected and the <code class="docutils literal notranslate"><span class="pre">export()</span></code> call will most likely succeed, unless tests are in place that check the resulting <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> representation.</p>
<p>For such cases, our stance is to recommend the “traditional” way of specifying dynamic shapes, which longer-term users of export might be familiar with: named <code class="docutils literal notranslate"><span class="pre">Dims</span></code>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dx</span> <span class="o">=</span> <span class="n">Dim</span><span class="p">(</span><span class="s2">&quot;dx&quot;</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="n">dh</span> <span class="o">=</span> <span class="n">Dim</span><span class="p">(</span><span class="s2">&quot;dh&quot;</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dh</span><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This style of dynamic shapes allows the user to specify what symbols are allocated for input dimensions, min/max bounds on those symbols, and places restrictions on the
dynamic behavior of the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code> produced; <code class="docutils literal notranslate"><span class="pre">ConstraintViolation</span></code> errors will be raised if model tracing emits guards that conflict with the relations or static/dynamic
specifications given. For example, in the above specification, the following is asserted:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x.shape[0]</span></code> is to have range <code class="docutils literal notranslate"><span class="pre">[4,</span> <span class="pre">256]</span></code>, and related to <code class="docutils literal notranslate"><span class="pre">y.shape[0]</span></code> by <code class="docutils literal notranslate"><span class="pre">y.shape[0]</span> <span class="pre">==</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">x.shape[0]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x.shape[1]</span></code> is static.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y.shape[1]</span></code> has range <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">512]</span></code>, and is unrelated to any other dimension.</p></li>
</ul>
<p>In this design, we allow relations between dimensions to be specified with univariate linear expressions: <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">*</span> <span class="pre">dim</span> <span class="pre">+</span> <span class="pre">B</span></code> can be specified for any dimension. This allows users
to specify more complex constraints like integer divisibility for dynamic dimensions:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dx</span> <span class="o">=</span> <span class="n">Dim</span><span class="p">(</span><span class="s2">&quot;dx&quot;</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">dx</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>  <span class="c1"># x.shape[0] has range [16, 2048], and is divisible by 4.</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="constraint-violations-suggested-fixes">
<h3><a class="toc-backref" href="#id11" role="doc-backlink">Constraint violations, suggested fixes</a><a class="headerlink" href="#constraint-violations-suggested-fixes" title="Link to this heading">#</a></h3>
<p>One common issue with this specification style (before <code class="docutils literal notranslate"><span class="pre">Dim.AUTO</span></code> was introduced), is that the specification would often be mismatched with what was produced by model tracing.
That would lead to <code class="docutils literal notranslate"><span class="pre">ConstraintViolation</span></code> errors and export suggested fixes - see for example with this model &amp; specification, where the model inherently requires equality between
dimensions 0 of <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>, and requires dimension 1 to be static.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">w</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">d1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">dims</span><span class="p">(</span><span class="s2">&quot;dx&quot;</span><span class="p">,</span> <span class="s2">&quot;dy&quot;</span><span class="p">,</span> <span class="s2">&quot;d1&quot;</span><span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span>
        <span class="n">Foo</span><span class="p">(),</span>
        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span>
        <span class="n">dynamic_shapes</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">d1</span><span class="p">),</span>
            <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">d1</span><span class="p">),</span>
        <span class="p">},</span>
    <span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:20.418000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:20.419000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s77 = 6 for L[&#39;x&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s77&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:20:20.421000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s27 = 4 for L[&#39;x&#39;].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s27&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
V1004 00:20:20.422000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
I1004 00:20:20.424000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s17 = 6 for L[&#39;y&#39;].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s17&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
I1004 00:20:20.424000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s94 = 4 for L[&#39;y&#39;].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;s94&quot; or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=&quot;0&quot;
V1004 00:20:20.429000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.430000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.431000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.432000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.433000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.433000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
V1004 00:20:20.435000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
I1004 00:20:20.437000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s27, s94) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s27, s94)&quot;
I1004 00:20:20.438000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s94 = s27 (solve) VR[2, int_oo]
I1004 00:20:20.439000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s77, s17) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s77, s17)&quot;
I1004 00:20:20.440000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s77 = s17 (solve) VR[2, int_oo]
V1004 00:20:20.442000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known]
I1004 00:20:20.451000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s27, 4) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;Eq(s27, 4)&quot;
V1004 00:20:20.451000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s27 = VR[4, 4] (update)
I1004 00:20:20.452000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s27 = 4 (range_refined_to_singleton) VR[4, 4]
I1004 00:20:20.456000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:20:20.456000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s94 = VR[4, 4] (update)
I1004 00:20:20.457000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s94 = 4 (find) VR[4, 4]
V1004 00:20:20.457000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[0] s17 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V1004 00:20:20.457000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[1] 4 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V1004 00:20:20.459000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[0] 4 None
V1004 00:20:20.459000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[1] 1 None
V1004 00:20:20.460000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].storage_offset() 0 None
V1004 00:20:20.460000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[0] s17 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V1004 00:20:20.460000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[1] 4 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo])
V1004 00:20:20.464000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[0] 4 None
V1004 00:20:20.464000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[1] 1 None
V1004 00:20:20.464000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].storage_offset() 0 None
Traceback (most recent call last):
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1798, in _export_to_aten_ir_make_fx
    produce_guards_callback(gm)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1944, in _produce_guards_callback
    return produce_guards_and_solve_constraints(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 549, in produce_guards_and_solve_constraints
    raise constraint_violation_error
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 514, in produce_guards_and_solve_constraints
    shape_env.produce_guards(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 5200, in produce_guards
    return self.produce_guards_verbose(*args, **kwargs, langs=(&quot;python&quot;,))[0].exprs
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 5932, in produce_guards_verbose
    raise ConstraintViolationError(
torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS=&quot;+dynamic&quot;.
  - You marked d1 as dynamic but your code specialized it to be a constant (4). If you&#39;re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you&#39;re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.

Framework stack:
  File &quot;/opt/conda/bin/sphinx-build&quot;, line 7, in &lt;module&gt;
    sys.exit(main())
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 339, in main
    return make_main(argv)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 213, in make_main
    return make_mode.run_make_mode(argv[1:])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 181, in run_make_mode
    return make.run_generic_build(args[0])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 169, in run_generic_build
    return build_main(args + opts)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 293, in build_main
    app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 272, in __init__
    self._init_builder()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 343, in _init_builder
    self.events.emit(&#39;builder-inited&#39;)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/events.py&quot;, line 97, in emit
    results.append(listener.handler(self.app, *args))
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py&quot;, line 757, in generate_gallery_rst
    ) = generate_dir_rst(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 606, in generate_dir_rst
    results = parallel(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 607, in &lt;genexpr&gt;
    p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 86, in wrapper
    p.start()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 121, in start
    self._popen = self._Popen(self)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 281, in _Popen
    return Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 19, in __init__
    self._launch(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 314, in _bootstrap
    self.run()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 108, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 74, in call_fn
    result = func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1374, in generate_file_rst
    output_blocks, time_elapsed = execute_script(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1192, in execute_script
    execute_code_block(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1048, in execute_code_block
    is_last_expr, mem_max = _exec_and_get_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 876, in _exec_and_get_memory
    mem_max, _ = call_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1725, in _sg_call_memory_noop
    return 0.0, func()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 794, in __call__
    exec(self.code, self.fake_main.__dict__)
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 557, in &lt;module&gt;
    ep = export(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1770, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
  File &quot;??&quot;, line 0, in _start
  File &quot;??&quot;, line 0, in __libc_start_main
  File &quot;??&quot;, line 0, in __libc_init_first
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 738, in Py_BytesMain
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 360, in pymain_run_file_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 79, in _PyRun_AnyFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 440, in _PyRun_SimpleFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1657, in pyrun_file
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1762, in run_mod
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1741, in run_eval_code_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1900, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1685, in _make_fx_helper
    gm = make_fx(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2318, in wrapped
    return make_fx_tracer.trace(f, *args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2250, in trace
    return self._trace_inner(f, *args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4984, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2221, in _trace_inner
    t = dispatch_trace(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 2790, in list___init___impl
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 966, in list_extend
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_compile.py&quot;, line 53, in inner
    return disable_fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1254, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1835, in trace
    res = super().trace(root, concrete_args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace
    (self.create_arg(fn(*args)),),
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1312, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 5091, in _PyEval_EvalFrameDefault
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;
  File &quot;/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c&quot;, line 1077, in builtin_exec_impl
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1589, in wrapped_fn
    return tuple(flat_fn(*args))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn
    tree_out = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call
    out = mod(*args[params_len:], **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1884, in forward
    tree_out = mod(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 553, in forward
    return w + torch.ones(4)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1360, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1407, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 1051, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 950, in handler
    return torch._library.utils.handle_dispatch_mode(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py&quot;, line 296, in handle_dispatch_mode
    return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1462, in __torch_dispatch__
    return proxy_call(self, func, self.pre_dispatch, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 914, in proxy_call
    out = func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
    return self._op(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 5548, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/abstract.c&quot;, line 893, in binary_op1
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7420, in slot_nb_add
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in _object* torch::autograd::TypeError_to_NotImplemented_&lt;&amp;torch::autograd::THPVariable_add&gt;(_object*, _object*, _object*)
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1352, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2058, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1487, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;??&quot;, line 0, in at::_ops::add_Tensor::call(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2581, in _dispatch_impl
    return maybe_propagate_real_tensors(fast_impl(self, *args, **kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py&quot;, line 962, in fast_binary_impl
    final_shape = infer_size(final_shape, shape)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py&quot;, line 922, in infer_size
    torch._check(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 1684, in _check
    _check_with(RuntimeError, cond, message)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;init.cpp&quot;, line 0, in pybind11::cpp_function::initialize&lt;torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}, pybind11::object, pybind11::args const&amp;, pybind11::kwargs const&amp;&gt;(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}&amp;&amp;, pybind11::object (*)(pybind11::args const&amp;, pybind11::kwargs const&amp;))::{lambda(pybind11::detail::function_call&amp;)#1}::_FUN(pybind11::detail::function_call&amp;)
  File &quot;init.cpp&quot;, line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}::operator()(pybind11::args const&amp;, pybind11::kwargs const&amp;) const
  File &quot;??&quot;, line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, c10::Symbol, pybind11::args const&amp;, pybind11::kwargs const&amp;, bool, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;??&quot;, line 0, in torch::jit::invokeOperatorFromPython(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, pybind11::args const&amp;, pybind11::kwargs const&amp;, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;register_c10_ops.cpp&quot;, line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const [clone .isra.0]
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in void c10::BoxedKernel::make_boxed_function&lt;&amp;(anonymous namespace)::pythonTLSSnapshotFallback&gt;(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;VariableType_2.cpp&quot;, line 0, in c10::impl::make_boxed_from_unboxed_functor&lt;c10::impl::detail::WrapFunctionIntoFunctor_&lt;c10::CompileTimeFunctionPointer&lt;at::Tensor (c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;), &amp;torch::autograd::VariableType::(anonymous namespace)::add_Tensor&gt;, at::Tensor, c10::guts::typelist::typelist&lt;c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;&gt; &gt;, false&gt;::call(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;VariableType_2.cpp&quot;, line 0, in torch::autograd::VariableType::(anonymous namespace)::add_Tensor(c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;)
  File &quot;??&quot;, line 0, in at::_ops::add_Tensor::redispatch(c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 1647, in _check_with
    if expect_true(cond):
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 1702, in expect_true
    return a.node.expect_true(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 559, in expect_true
    return self.shape_env.guard_or_defer_runtime_assert(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
    return retlog(fn(*args, **kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 1021, in bounded_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7730, in guard_or_defer_runtime_assert
    self._maybe_guard_rel(expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6868, in _maybe_guard_rel
    self._refine_ranges(expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7819, in _refine_ranges
    self._set_replacement(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 1021, in bounded_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6768, in _set_replacement
    CapturedTraceback.extract(cpp=True)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py&quot;, line 212, in extract
    torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp),
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in pybind11::cpp_function::initialize&lt;std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt;, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v&gt;(std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt; (*)(bool, bool, bool), pybind11::name const&amp;, pybind11::scope const&amp;, pybind11::sibling const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::operator()(pybind11::detail::function_call&amp;) const
  File &quot;??&quot;, line 0, in torch::CapturedTraceback::gather(bool, bool, bool)
  File &quot;??&quot;, line 0, in torch::unwind::unwind()

  - You marked d1 as dynamic but your code specialized it to be a constant (4). If you&#39;re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you&#39;re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.

Framework stack:
  File &quot;/opt/conda/bin/sphinx-build&quot;, line 7, in &lt;module&gt;
    sys.exit(main())
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 339, in main
    return make_main(argv)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 213, in make_main
    return make_mode.run_make_mode(argv[1:])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 181, in run_make_mode
    return make.run_generic_build(args[0])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 169, in run_generic_build
    return build_main(args + opts)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 293, in build_main
    app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 272, in __init__
    self._init_builder()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 343, in _init_builder
    self.events.emit(&#39;builder-inited&#39;)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/events.py&quot;, line 97, in emit
    results.append(listener.handler(self.app, *args))
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py&quot;, line 757, in generate_gallery_rst
    ) = generate_dir_rst(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 606, in generate_dir_rst
    results = parallel(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 607, in &lt;genexpr&gt;
    p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 86, in wrapper
    p.start()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 121, in start
    self._popen = self._Popen(self)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 281, in _Popen
    return Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 19, in __init__
    self._launch(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 314, in _bootstrap
    self.run()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 108, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 74, in call_fn
    result = func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1374, in generate_file_rst
    output_blocks, time_elapsed = execute_script(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1192, in execute_script
    execute_code_block(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1048, in execute_code_block
    is_last_expr, mem_max = _exec_and_get_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 876, in _exec_and_get_memory
    mem_max, _ = call_memory(
  File &quot;??&quot;, line 0, in _start
  File &quot;??&quot;, line 0, in __libc_start_main
  File &quot;??&quot;, line 0, in __libc_init_first
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 738, in Py_BytesMain
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 360, in pymain_run_file_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 79, in _PyRun_AnyFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 440, in _PyRun_SimpleFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1657, in pyrun_file
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1762, in run_mod
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1741, in run_eval_code_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1725, in _sg_call_memory_noop
    return 0.0, func()
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 794, in __call__
    exec(self.code, self.fake_main.__dict__)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 557, in &lt;module&gt;
    ep = export(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4984, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 2790, in list___init___impl
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 966, in list_extend
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1798, in _export_to_aten_ir_make_fx
    produce_guards_callback(gm)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 5091, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1944, in _produce_guards_callback
    return produce_guards_and_solve_constraints(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c&quot;, line 1077, in builtin_exec_impl
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 514, in produce_guards_and_solve_constraints
    shape_env.produce_guards(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 5200, in produce_guards
    return self.produce_guards_verbose(*args, **kwargs, langs=(&quot;python&quot;,))[0].exprs
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 5405, in produce_guards_verbose
    expr1, expr2 = get_expression(src1), get_expression(src2)  # type: ignore[]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 5399, in get_expression
    return symint.node.expr
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 189, in expr
    return self.shape_env.replace(self._expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/classobject.c&quot;, line 59, in method_vectorcall
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 2539, in wrapper
    return fn_cache(self, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 3461, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6324, in replace
    r = self._find(s)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/object.c&quot;, line 1368, in PyObject_GenericGetAttr
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/object.c&quot;, line 1278, in _PyObject_GenericGetAttrWithDict
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 2539, in wrapper
    return fn_cache(self, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 917, in infinite_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
    return retlog(fn(*args, **kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 917, in infinite_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6809, in _find
    self._set_replacement(a, replaced, &quot;find&quot;)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6768, in _set_replacement
    CapturedTraceback.extract(cpp=True)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py&quot;, line 212, in extract
    torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp),
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in pybind11::cpp_function::initialize&lt;std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt;, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v&gt;(std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt; (*)(bool, bool, bool), pybind11::name const&amp;, pybind11::scope const&amp;, pybind11::sibling const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::operator()(pybind11::detail::function_call&amp;) const
  File &quot;??&quot;, line 0, in torch::CapturedTraceback::gather(bool, bool, bool)
  File &quot;??&quot;, line 0, in torch::unwind::unwind()

  - The values of dy = L[&#39;y&#39;].size()[0] and dx = L[&#39;x&#39;].size()[0] must always be equal.
Suggested fixes:
  d1 = 4
  dy = dx

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 557, in &lt;module&gt;
    ep = export(
         ^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 319, in export
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
           ^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1164, in wrapper
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
         ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1164, in wrapper
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
                      ^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1800, in _export_to_aten_ir_make_fx
    raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e))  # noqa: B904
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch._dynamo.exc.UserError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS=&quot;+dynamic&quot;.
  - You marked d1 as dynamic but your code specialized it to be a constant (4). If you&#39;re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you&#39;re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.

Framework stack:
  File &quot;/opt/conda/bin/sphinx-build&quot;, line 7, in &lt;module&gt;
    sys.exit(main())
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 339, in main
    return make_main(argv)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 213, in make_main
    return make_mode.run_make_mode(argv[1:])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 181, in run_make_mode
    return make.run_generic_build(args[0])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 169, in run_generic_build
    return build_main(args + opts)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 293, in build_main
    app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 272, in __init__
    self._init_builder()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 343, in _init_builder
    self.events.emit(&#39;builder-inited&#39;)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/events.py&quot;, line 97, in emit
    results.append(listener.handler(self.app, *args))
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py&quot;, line 757, in generate_gallery_rst
    ) = generate_dir_rst(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 606, in generate_dir_rst
    results = parallel(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 607, in &lt;genexpr&gt;
    p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 86, in wrapper
    p.start()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 121, in start
    self._popen = self._Popen(self)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 281, in _Popen
    return Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 19, in __init__
    self._launch(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 314, in _bootstrap
    self.run()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 108, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 74, in call_fn
    result = func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1374, in generate_file_rst
    output_blocks, time_elapsed = execute_script(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1192, in execute_script
    execute_code_block(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1048, in execute_code_block
    is_last_expr, mem_max = _exec_and_get_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 876, in _exec_and_get_memory
    mem_max, _ = call_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1725, in _sg_call_memory_noop
    return 0.0, func()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 794, in __call__
    exec(self.code, self.fake_main.__dict__)
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 557, in &lt;module&gt;
    ep = export(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1770, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
  File &quot;??&quot;, line 0, in _start
  File &quot;??&quot;, line 0, in __libc_start_main
  File &quot;??&quot;, line 0, in __libc_init_first
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 738, in Py_BytesMain
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 360, in pymain_run_file_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 79, in _PyRun_AnyFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 440, in _PyRun_SimpleFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1657, in pyrun_file
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1762, in run_mod
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1741, in run_eval_code_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1900, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1685, in _make_fx_helper
    gm = make_fx(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2318, in wrapped
    return make_fx_tracer.trace(f, *args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2250, in trace
    return self._trace_inner(f, *args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4984, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2221, in _trace_inner
    t = dispatch_trace(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 2790, in list___init___impl
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 966, in list_extend
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_compile.py&quot;, line 53, in inner
    return disable_fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1254, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1835, in trace
    res = super().trace(root, concrete_args)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace
    (self.create_arg(fn(*args)),),
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1312, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 5091, in _PyEval_EvalFrameDefault
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;
  File &quot;/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c&quot;, line 1077, in builtin_exec_impl
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1589, in wrapped_fn
    return tuple(flat_fn(*args))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn
    tree_out = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call
    out = mod(*args[params_len:], **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1884, in forward
    tree_out = mod(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 553, in forward
    return w + torch.ones(4)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1360, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1407, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 1051, in __torch_function__
    return func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 950, in handler
    return torch._library.utils.handle_dispatch_mode(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py&quot;, line 296, in handle_dispatch_mode
    return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1462, in __torch_dispatch__
    return proxy_call(self, func, self.pre_dispatch, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 914, in proxy_call
    out = func(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
    return self._op(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 5548, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/abstract.c&quot;, line 893, in binary_op1
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7420, in slot_nb_add
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in _object* torch::autograd::TypeError_to_NotImplemented_&lt;&amp;torch::autograd::THPVariable_add&gt;(_object*, _object*, _object*)
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1352, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2058, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in torch::handle_torch_function(torch::PythonArgs&amp;, _object*, _object*, _object*, _object*, char const*, char const*)
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1487, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/descrobject.c&quot;, line 364, in method_vectorcall_VARARGS_KEYWORDS
  File &quot;python_variable_methods.cpp&quot;, line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*)
  File &quot;??&quot;, line 0, in at::_ops::add_Tensor::call(at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2581, in _dispatch_impl
    return maybe_propagate_real_tensors(fast_impl(self, *args, **kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py&quot;, line 962, in fast_binary_impl
    final_shape = infer_size(final_shape, shape)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py&quot;, line 922, in infer_size
    torch._check(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 1684, in _check
    _check_with(RuntimeError, cond, message)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7321, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;init.cpp&quot;, line 0, in pybind11::cpp_function::initialize&lt;torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}, pybind11::object, pybind11::args const&amp;, pybind11::kwargs const&amp;&gt;(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}&amp;&amp;, pybind11::object (*)(pybind11::args const&amp;, pybind11::kwargs const&amp;))::{lambda(pybind11::detail::function_call&amp;)#1}::_FUN(pybind11::detail::function_call&amp;)
  File &quot;init.cpp&quot;, line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;)#2}::operator()(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) const::{lambda(pybind11::args const&amp;, pybind11::kwargs const&amp;)#1}::operator()(pybind11::args const&amp;, pybind11::kwargs const&amp;) const
  File &quot;??&quot;, line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, c10::Symbol, pybind11::args const&amp;, pybind11::kwargs const&amp;, bool, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;??&quot;, line 0, in torch::jit::invokeOperatorFromPython(std::vector&lt;std::shared_ptr&lt;torch::jit::Operator&gt;, std::allocator&lt;std::shared_ptr&lt;torch::jit::Operator&gt; &gt; &gt; const&amp;, pybind11::args const&amp;, pybind11::kwargs const&amp;, std::optional&lt;c10::DispatchKey&gt;)
  File &quot;register_c10_ops.cpp&quot;, line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const [clone .isra.0]
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in void c10::BoxedKernel::make_boxed_function&lt;&amp;(anonymous namespace)::pythonTLSSnapshotFallback&gt;(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;VariableType_2.cpp&quot;, line 0, in c10::impl::make_boxed_from_unboxed_functor&lt;c10::impl::detail::WrapFunctionIntoFunctor_&lt;c10::CompileTimeFunctionPointer&lt;at::Tensor (c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;), &amp;torch::autograd::VariableType::(anonymous namespace)::add_Tensor&gt;, at::Tensor, c10::guts::typelist::typelist&lt;c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;&gt; &gt;, false&gt;::call(c10::OperatorKernel*, c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;VariableType_2.cpp&quot;, line 0, in torch::autograd::VariableType::(anonymous namespace)::add_Tensor(c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;)
  File &quot;??&quot;, line 0, in at::_ops::add_Tensor::redispatch(c10::DispatchKeySet, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar const&amp;)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;&quot;, line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;&amp;) const
  File &quot;PythonFallbackKernel.cpp&quot;, line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const&amp;, c10::DispatchKeySet, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*)
  File &quot;PyInterpreter.cpp&quot;, line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const&amp;, std::vector&lt;c10::IValue, std::allocator&lt;c10::IValue&gt; &gt;*) const
  File &quot;??&quot;, line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef&lt;_object*&gt;, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 627, in PyObject_CallMethod
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 1647, in _check_with
    if expect_true(cond):
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 1702, in expect_true
    return a.node.expect_true(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 559, in expect_true
    return self.shape_env.guard_or_defer_runtime_assert(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
    return retlog(fn(*args, **kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 1021, in bounded_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7730, in guard_or_defer_runtime_assert
    self._maybe_guard_rel(expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6868, in _maybe_guard_rel
    self._refine_ranges(expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7819, in _refine_ranges
    self._set_replacement(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 1021, in bounded_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6768, in _set_replacement
    CapturedTraceback.extract(cpp=True)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py&quot;, line 212, in extract
    torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp),
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in pybind11::cpp_function::initialize&lt;std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt;, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v&gt;(std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt; (*)(bool, bool, bool), pybind11::name const&amp;, pybind11::scope const&amp;, pybind11::sibling const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::operator()(pybind11::detail::function_call&amp;) const
  File &quot;??&quot;, line 0, in torch::CapturedTraceback::gather(bool, bool, bool)
  File &quot;??&quot;, line 0, in torch::unwind::unwind()

  - You marked d1 as dynamic but your code specialized it to be a constant (4). If you&#39;re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you&#39;re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.

Framework stack:
  File &quot;/opt/conda/bin/sphinx-build&quot;, line 7, in &lt;module&gt;
    sys.exit(main())
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 339, in main
    return make_main(argv)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 213, in make_main
    return make_mode.run_make_mode(argv[1:])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 181, in run_make_mode
    return make.run_generic_build(args[0])
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 169, in run_generic_build
    return build_main(args + opts)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 293, in build_main
    app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 272, in __init__
    self._init_builder()
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 343, in _init_builder
    self.events.emit(&#39;builder-inited&#39;)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/events.py&quot;, line 97, in emit
    results.append(listener.handler(self.app, *args))
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py&quot;, line 757, in generate_gallery_rst
    ) = generate_dir_rst(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 606, in generate_dir_rst
    results = parallel(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 607, in &lt;genexpr&gt;
    p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 86, in wrapper
    p.start()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 121, in start
    self._popen = self._Popen(self)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 281, in _Popen
    return Popen(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 19, in __init__
    self._launch(process_obj)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 314, in _bootstrap
    self.run()
  File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 108, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/workspace/tutorials-kr/conf.py&quot;, line 74, in call_fn
    result = func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1374, in generate_file_rst
    output_blocks, time_elapsed = execute_script(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1192, in execute_script
    execute_code_block(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1048, in execute_code_block
    is_last_expr, mem_max = _exec_and_get_memory(
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 876, in _exec_and_get_memory
    mem_max, _ = call_memory(
  File &quot;??&quot;, line 0, in _start
  File &quot;??&quot;, line 0, in __libc_start_main
  File &quot;??&quot;, line 0, in __libc_init_first
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 738, in Py_BytesMain
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/main.c&quot;, line 360, in pymain_run_file_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 79, in _PyRun_AnyFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 440, in _PyRun_SimpleFileObject
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1657, in pyrun_file
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1762, in run_mod
  File &quot;/usr/local/src/conda/python-3.11.13/Python/pythonrun.c&quot;, line 1741, in run_eval_code_obj
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1725, in _sg_call_memory_noop
    return 0.0, func()
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 794, in __call__
    exec(self.code, self.fake_main.__dict__)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 557, in &lt;module&gt;
    ep = export(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4984, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 2790, in list___init___impl
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/listobject.c&quot;, line 966, in list_extend
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 1103, in type_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/typeobject.c&quot;, line 7624, in slot_tp_call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 482, in _PyObject_Call_Prepend
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 141, in _PyObject_FastCallDictTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1798, in _export_to_aten_ir_make_fx
    produce_guards_callback(gm)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 5091, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1944, in _produce_guards_callback
    return produce_guards_and_solve_constraints(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c&quot;, line 1077, in builtin_exec_impl
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 1148, in PyEval_EvalCode
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 514, in produce_guards_and_solve_constraints
    shape_env.produce_guards(
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 5200, in produce_guards
    return self.produce_guards_verbose(*args, **kwargs, langs=(&quot;python&quot;,))[0].exprs
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 5405, in produce_guards_verbose
    expr1, expr2 = get_expression(src1), get_expression(src2)  # type: ignore[]
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 5399, in get_expression
    return symint.node.expr
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 189, in expr
    return self.shape_env.replace(self._expr)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 257, in _PyVectorcall_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/classobject.c&quot;, line 59, in method_vectorcall
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 2539, in wrapper
    return fn_cache(self, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 3461, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6324, in replace
    r = self._find(s)
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/object.c&quot;, line 1368, in PyObject_GenericGetAttr
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/object.c&quot;, line 1278, in _PyObject_GenericGetAttrWithDict
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h&quot;, line 92, in _PyObject_VectorcallTstate
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 2539, in wrapper
    return fn_cache(self, *args, **kwargs)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 917, in infinite_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
    return retlog(fn(*args, **kwargs))
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 343, in _PyObject_Call
  File &quot;/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c&quot;, line 917, in infinite_lru_cache_wrapper
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6809, in _find
    self._set_replacement(a, replaced, &quot;find&quot;)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 7349, in do_call_core
  File &quot;/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h&quot;, line 73, in _PyEval_EvalFrame
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 6768, in _set_replacement
    CapturedTraceback.extract(cpp=True)
  File &quot;/usr/local/src/conda/python-3.11.13/Python/ceval.c&quot;, line 4769, in _PyEval_EvalFrameDefault
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py&quot;, line 212, in extract
    torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp),
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/call.c&quot;, line 214, in _PyObject_MakeTpCall
  File &quot;/usr/local/src/conda/python-3.11.13/Objects/methodobject.c&quot;, line 542, in cfunction_call
  File &quot;&quot;, line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*)
  File &quot;&quot;, line 0, in pybind11::cpp_function::initialize&lt;std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt;, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v&gt;(std::shared_ptr&lt;torch::CapturedTraceback&gt; (*&amp;)(bool, bool, bool), std::shared_ptr&lt;torch::CapturedTraceback&gt; (*)(bool, bool, bool), pybind11::name const&amp;, pybind11::scope const&amp;, pybind11::sibling const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;, pybind11::arg_v const&amp;)::{lambda(pybind11::detail::function_call&amp;)#3}::operator()(pybind11::detail::function_call&amp;) const
  File &quot;??&quot;, line 0, in torch::CapturedTraceback::gather(bool, bool, bool)
  File &quot;??&quot;, line 0, in torch::unwind::unwind()

  - The values of dy = L[&#39;y&#39;].size()[0] and dx = L[&#39;x&#39;].size()[0] must always be equal.
Suggested fixes:
  d1 = 4
  dy = dx

The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
<p>The expectation with suggested fixes is that the user can interactively copy-paste the changes into their dynamic shapes specification, and successfully export afterwards.</p>
<p>Lastly, there’s couple nice-to-knows about the options for specification:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> is a good option for static behavior:
- <code class="docutils literal notranslate"><span class="pre">dynamic_shapes=None</span></code> (default) exports with the entire model being static.
- specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> at an input-level exports with all tensor dimensions static, and is also required for non-tensor inputs.
- specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> at a dimension-level specializes that dimension, though this is deprecated in favor of <code class="docutils literal notranslate"><span class="pre">Dim.STATIC</span></code>.</p></li>
<li><p>specifying per-dimension integer values also produces static behavior, and will additionally check that the provided sample input matches the specification.</p></li>
</ul>
<p>These options are combined in the inputs &amp; dynamic shapes spec below:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="mi">16</span><span class="p">,</span>
    <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">dynamic_shapes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;tensor_0&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Dim</span><span class="o">.</span><span class="n">AUTO</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="s2">&quot;tensor_1&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;int_val&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;bool_val&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="data-dependent-errors">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">Data-dependent errors</a><a class="headerlink" href="#data-dependent-errors" title="Link to this heading">#</a></h2>
<p>While trying to export models, you have may have encountered errors like “Could not guard on data-dependent expression”, or Could not extract specialized integer from data-dependent expression”.
These errors exist because <code class="docutils literal notranslate"><span class="pre">torch.export()</span></code> compiles programs using FakeTensors, which symbolically represent their real tensor counterparts. While these have equivalent symbolic properties
(e.g. sizes, strides, dtypes), they diverge in that FakeTensors do not contain any data values. While this avoids unnecessary memory usage and expensive computation, it does mean that export may be
unable to out-of-the-box compile parts of user code where compilation relies on data values. In short, if the compiler requires a concrete, data-dependent value in order to proceed, it will error out,
complaining that the value is not available.</p>
<p>Data-dependent values appear in many places, and common sources are calls like <code class="docutils literal notranslate"><span class="pre">item()</span></code>, <code class="docutils literal notranslate"><span class="pre">tolist()</span></code>, or <code class="docutils literal notranslate"><span class="pre">torch.unbind()</span></code> that extract scalar values from tensors.
How are these values represented in the exported program? In the <a class="reference external" href="https://tutorials.pytorch.kr/intermediate/torch_export_tutorial.html#constraints-dynamic-shapes">Constraints/Dynamic Shapes</a>
section, we talked about allocating symbols to represent dynamic input dimensions.
The same happens here: we allocate symbols for every data-dependent value that appears in the program. The important distinction is that these are “unbacked” symbols,
in contrast to the “backed” symbols allocated for input dimensions. The <a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html#basics-of-symbolic-shapes">“backed/unbacked”</a>
nomenclature refers to the presence/absence of a “hint” for the symbol: a concrete value backing the symbol, that can inform the compiler on how to proceed.</p>
<p>In the input shape symbol case (backed symbols), these hints are simply the sample input shapes provided, which explains why control-flow branching is determined by the sample input properties.
For data-dependent values, the symbols are taken from FakeTensor “data” during tracing, and so the compiler doesn’t know the actual values (hints) that these symbols would take on.</p>
<p>Let’s see how these show up in exported programs:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
<span class="p">)</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">Foo</span><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ep</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:20.495000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:20.501000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense)
I1004 00:20:20.501000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u0]
I1004 00:20:20.506000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u1 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense)
I1004 00:20:20.506000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u1]
I1004 00:20:20.507000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u2 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense)
I1004 00:20:20.508000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u2]
I1004 00:20:20.509000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:20:20.510000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].storage_offset() 0 None
V1004 00:20:20.510000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[0] 2 None
V1004 00:20:20.510000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[0] 1 None
V1004 00:20:20.510000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].storage_offset() 0 None
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: &quot;i64[]&quot;, y: &quot;i64[2]&quot;):
             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:618 in forward, code: a = x.item()
            item: &quot;Sym(u0)&quot; = torch.ops.aten.item.default(x);  x = None

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:619 in forward, code: b = y.tolist()
            unbind = torch.ops.aten.unbind.int(y);  y = None
            getitem: &quot;i64[]&quot; = unbind[0]
            getitem_1: &quot;i64[]&quot; = unbind[1];  unbind = None
            item_1: &quot;Sym(u1)&quot; = torch.ops.aten.item.default(getitem);  getitem = None
            item_2: &quot;Sym(u2)&quot; = torch.ops.aten.item.default(getitem_1);  getitem_1 = None
            return (item_1, item_2, item)

Graph signature:
    # inputs
    x: USER_INPUT
    y: USER_INPUT

    # outputs
    item_1: USER_OUTPUT
    item_2: USER_OUTPUT
    item: USER_OUTPUT

Range constraints: {u0: VR[-int_oo, int_oo], u1: VR[-int_oo, int_oo], u2: VR[-int_oo, int_oo]}
</pre></div>
</div>
<p>The result is that 3 unbacked symbols (notice they’re prefixed with “u”, instead of the usual “s” for input shape/backed symbols) are allocated and returned:
1 for the <code class="docutils literal notranslate"><span class="pre">item()</span></code> call, and 1 for each of the elements of <code class="docutils literal notranslate"><span class="pre">y</span></code> with the <code class="docutils literal notranslate"><span class="pre">tolist()</span></code> call.
Note from the range constraints field that these take on ranges of <code class="docutils literal notranslate"><span class="pre">[-int_oo,</span> <span class="pre">int_oo]</span></code>, not the default <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">int_oo]</span></code> range allocated to input shape symbols,
since we have no information on what these values are - they don’t represent sizes, so don’t necessarily have positive values.</p>
<section id="guards-torch-check">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">Guards, torch._check()</a><a class="headerlink" href="#guards-torch-check" title="Link to this heading">#</a></h3>
<p>But the case above is easy to export, because the concrete values of these symbols aren’t used in any compiler decision-making; all that’s relevant is that the return values are unbacked symbols.
The data-dependent errors highlighted in this section are cases like the following, where <a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html#control-flow-static-vs-dynamic">data-dependent guards</a> are encountered:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">5</span>
</pre></div>
</div>
<p>Here we actually need the “hint”, or the concrete value of <code class="docutils literal notranslate"><span class="pre">a</span></code> for the compiler to decide whether to trace <code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">y</span> <span class="pre">+</span> <span class="pre">2</span></code> or <code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">y</span> <span class="pre">*</span> <span class="pre">5</span></code> as the output.
Because we trace with FakeTensors, we don’t know what <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">//</span> <span class="pre">2</span> <span class="pre">&gt;=</span> <span class="pre">5</span></code> actually evaluates to, and export errors out with “Could not guard on data-dependent expression <code class="docutils literal notranslate"><span class="pre">u0</span> <span class="pre">//</span> <span class="pre">2</span> <span class="pre">&gt;=</span> <span class="pre">5</span> <span class="pre">(unhinted)</span></code>”.</p>
<p>So how do we export this toy model? Unlike <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code>, export requires full graph compilation, and we can’t just graph break on this. Here are some basic options:</p>
<ol class="arabic simple">
<li><p>Manual specialization: we could intervene by selecting the branch to trace, either by removing the control-flow code to contain only the specialized branch, or using <code class="docutils literal notranslate"><span class="pre">torch.compiler.is_compiling()</span></code> to guard what’s traced at compile-time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code>: we could rewrite the control-flow code to use <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> so we don’t specialize on a branch.</p></li>
</ol>
<p>While these options are valid, they have their pitfalls. Option 1 sometimes requires drastic, invasive rewrites of the model code to specialize, and <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> is not a comprehensive system for handling data-dependent errors.
As we will see, there are data-dependent errors that do not involve control-flow.</p>
<p>The generally recommended approach is to start with <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls. While these give the impression of purely being assert statements, they are in fact a system of informing the compiler on properties of symbols.
While a <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> call does act as an assertion at runtime, when traced at compile-time, the checked expression is sent to the symbolic shapes subsystem for reasoning, and any symbol properties that follow from the expression being true,
are stored as symbol properties (provided it’s smart enough to infer those properties). So even if unbacked symbols don’t have hints, if we’re able to communicate properties that are generally true for these symbols via
<code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls, we can potentially bypass data-dependent guards without rewriting the offending model code.</p>
<p>For example in the model above, inserting <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">&gt;=</span> <span class="pre">10)</span></code> would tell the compiler that <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">+</span> <span class="pre">2</span></code> can always be returned, and <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">==</span> <span class="pre">4)</span></code> tells it to return <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">*</span> <span class="pre">5</span></code>.
See what happens when we re-export this model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&lt;=</span> <span class="mi">60</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">5</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">Foo</span><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ep</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:20.515000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:20.519000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense)
I1004 00:20:20.519000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u0]
I1004 00:20:20.521000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert u0 &gt;= 10 [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:673 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;u0 &gt;= 10&quot;
V1004 00:20:20.522000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range u0 = VR[10, int_oo] (update)
I1004 00:20:20.527000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert u0 &lt;= 60 [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:674 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;u0 &lt;= 60&quot;
V1004 00:20:20.528000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range u0 = VR[10, 60] (update)
V1004 00:20:20.534000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == True [statically known]
I1004 00:20:20.537000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:20:20.538000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].storage_offset() 0 None
V1004 00:20:20.538000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[0] 4 None
V1004 00:20:20.538000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[0] 1 None
V1004 00:20:20.538000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].storage_offset() 0 None
V1004 00:20:20.539000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 &gt;= 10 == True [statically known]
V1004 00:20:20.540000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 &lt;= 60 == True [statically known]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: &quot;i64[]&quot;, y: &quot;f32[4]&quot;):
             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:672 in forward, code: a = x.item()
            item: &quot;Sym(u0)&quot; = torch.ops.aten.item.default(x);  x = None
            ge_2: &quot;Sym(u0 &gt;= 10)&quot; = item &gt;= 10
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_2, &quot;Runtime assertion failed for expression u0 &gt;= 10 on node &#39;ge_2&#39;&quot;);  ge_2 = _assert_scalar_default = None
            le_1: &quot;Sym(u0 &lt;= 60)&quot; = item &lt;= 60;  item = None
            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, &quot;Runtime assertion failed for expression u0 &lt;= 60 on node &#39;le_1&#39;&quot;);  le_1 = _assert_scalar_default_1 = None

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:676 in forward, code: return y + 2
            add: &quot;f32[4]&quot; = torch.ops.aten.add.Tensor(y, 2);  y = None
            return (add,)

Graph signature:
    # inputs
    x: USER_INPUT
    y: USER_INPUT

    # outputs
    add: USER_OUTPUT

Range constraints: {u0: VR[10, 60]}
</pre></div>
</div>
<p>Export succeeds, and note from the range constraints field that <code class="docutils literal notranslate"><span class="pre">u0</span></code> takes on a range of <code class="docutils literal notranslate"><span class="pre">[10,</span> <span class="pre">60]</span></code>.</p>
<p>So what information do <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls actually communicate? This varies as the symbolic shapes subsystem gets smarter, but at a fundamental level, these are generally true:</p>
<ol class="arabic simple">
<li><p>Equality with non-data-dependent expressions: <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls that communicate equalities like <code class="docutils literal notranslate"><span class="pre">u0</span> <span class="pre">==</span> <span class="pre">s0</span> <span class="pre">+</span> <span class="pre">4</span></code> or <code class="docutils literal notranslate"><span class="pre">u0</span> <span class="pre">==</span> <span class="pre">5</span></code>.</p></li>
<li><p>Range refinement: calls that provide lower or upper bounds for symbols, like the above.</p></li>
<li><p>Some basic reasoning around more complicated expressions: inserting <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">&lt;</span> <span class="pre">4)</span></code> will typically tell the compiler that <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">&gt;=</span> <span class="pre">4</span></code> is false. Checks on complex expressions like <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">**</span> <span class="pre">2</span> <span class="pre">-</span> <span class="pre">3</span> <span class="pre">*</span> <span class="pre">a</span> <span class="pre">&lt;=</span> <span class="pre">10)</span></code> will typically get you past identical guards.</p></li>
</ol>
<p>As mentioned previously, <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls have applicability outside of data-dependent control flow. For example, here’s a model where <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> insertion
prevails while manual specialization &amp; <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> do not:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">y</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">export</span><span class="p">(</span><span class="n">Foo</span><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:20.544000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:20.549000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense)
I1004 00:20:20.549000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u0]
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] Data dependent variable &#39;u0&#39; allocated at:
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/bin/sphinx-build&quot;, line 7, in &lt;module&gt;
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     sys.exit(main())
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 339, in main
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return make_main(argv)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 213, in make_main
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return make_mode.run_make_mode(argv[1:])
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 181, in run_make_mode
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return make.run_generic_build(args[0])
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 169, in run_generic_build
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return build_main(args + opts)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 293, in build_main
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 272, in __init__
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     self._init_builder()
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 343, in _init_builder
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     self.events.emit(&#39;builder-inited&#39;)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/events.py&quot;, line 97, in emit
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     results.append(listener.handler(self.app, *args))
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py&quot;, line 757, in generate_gallery_rst
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     ) = generate_dir_rst(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 606, in generate_dir_rst
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     results = parallel(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 607, in &lt;genexpr&gt;
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/workspace/tutorials-kr/conf.py&quot;, line 86, in wrapper
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     p.start()
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 121, in start
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     self._popen = self._Popen(self)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 224, in _Popen
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return _default_context.get_context().Process._Popen(process_obj)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 281, in _Popen
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return Popen(process_obj)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 19, in __init__
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     self._launch(process_obj)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 71, in _launch
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     code = process_obj._bootstrap(parent_sentinel=child_r)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 314, in _bootstrap
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     self.run()
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 108, in run
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     self._target(*self._args, **self._kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/workspace/tutorials-kr/conf.py&quot;, line 74, in call_fn
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     result = func(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1374, in generate_file_rst
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     output_blocks, time_elapsed = execute_script(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1192, in execute_script
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     execute_code_block(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1048, in execute_code_block
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     is_last_expr, mem_max = _exec_and_get_memory(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 876, in _exec_and_get_memory
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     mem_max, _ = call_memory(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1725, in _sg_call_memory_noop
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return 0.0, func()
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 794, in __call__
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     exec(self.code, self.fake_main.__dict__)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 709, in &lt;module&gt;
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     export(Foo(), inps)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return _export(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     ep = fn(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return fn(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     ep = _export_for_training(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     ep = fn(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return fn(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     export_artifact = export_func(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     aten_export_artifact = _to_aten_func(  # type: ignore[operator]
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1770, in _export_to_aten_ir_make_fx
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     gm, graph_signature = transform(_make_fx_helper)(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1900, in _aot_export_non_strict
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1685, in _make_fx_helper
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     gm = make_fx(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2318, in wrapped
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return make_fx_tracer.trace(f, *args)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2250, in trace
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self._trace_inner(f, *args)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2221, in _trace_inner
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     t = dispatch_trace(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_compile.py&quot;, line 53, in inner
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return disable_fn(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return fn(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1254, in dispatch_trace
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1835, in trace
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     res = super().trace(root, concrete_args)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return fn(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     (self.create_arg(fn(*args)),),
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1312, in wrapped
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     out = f(*tensors)  # type:ignore[call-arg]
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1589, in wrapped_fn
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return tuple(flat_fn(*args))
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     tree_out = fn(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     out = mod(*args[params_len:], **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self.call_module(mod, forward, args, kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return Tracer.call_module(self, m, forward, args, kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     ret_val = forward(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return _orig_module_call(mod, *args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self._call_impl(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return forward_call(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1884, in forward
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     tree_out = mod(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self.call_module(mod, forward, args, kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return Tracer.call_module(self, m, forward, args, kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     ret_val = forward(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return _orig_module_call(mod, *args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self._call_impl(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return forward_call(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 701, in forward
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     a = x.item()
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1360, in __torch_function__
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return func(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1407, in __torch_function__
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return func(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 1051, in __torch_function__
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return func(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 950, in handler
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return torch._library.utils.handle_dispatch_mode(
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py&quot;, line 296, in handle_dispatch_mode
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return fn(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1462, in __torch_dispatch__
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return proxy_call(self, func, self.pre_dispatch, args, kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 914, in proxy_call
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     out = func(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self._op(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return fn(*args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1352, in __torch_dispatch__
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self.dispatch(func, types, args, kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2058, in dispatch
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self._cached_dispatch_impl(func, types, args, kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1474, in _cached_dispatch_impl
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self._dispatch_impl(func, types, args, kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2687, in _dispatch_impl
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     op_impl_out = op_impl(self, func, *args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py&quot;, line 163, in dispatch_to_op_implementations_dict
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py&quot;, line 425, in local_scalar_dense
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     r = fake_mode.shape_env.create_unbacked_symint()
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return retlog(fn(*args, **kwargs))
V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] failed while attempting to run meta for aten.select.int
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] Traceback (most recent call last):
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2717, in _dispatch_impl
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]     r = func(*args, **kwargs)
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]         ^^^^^^^^^^^^^^^^^^^^^
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]     return self._op(*args, **kwargs)
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]            ^^^^^^^^^^^^^^^^^^^^^^^^^
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_meta_registrations.py&quot;, line 5545, in meta_select
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]     guard_size_oblivious(-index &gt; size) or guard_size_oblivious(index &gt;= size)
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 473, in guard_size_oblivious
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]     return expr.node.guard_size_oblivious(&quot;&quot;, 0)
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 596, in guard_size_oblivious
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]     r = self.evaluate(size_oblivious=True)
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 512, in evaluate
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]     return self.shape_env.evaluate_sym_node(self, size_oblivious)
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7223, in evaluate_sym_node
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]     return self.evaluate_expr(
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]            ^^^^^^^^^^^^^^^^^^^
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7323, in evaluate_expr
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]     return self._inner_evaluate_expr(
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]     return retlog(fn(*args, **kwargs))
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]                   ^^^^^^^^^^^^^^^^^^^
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7346, in _inner_evaluate_expr
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]     return self._evaluate_expr(
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]            ^^^^^^^^^^^^^^^^^^^^
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7570, in _evaluate_expr
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]     raise self._make_data_dependent_error(
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression -u0 &gt; 60 (unhinted: -u0 &gt; 60).  (Size-like symbols: none)
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] Caused by: (_meta_registrations.py:5545 in meta_select)
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] For more information, run with TORCH_LOGS=&quot;dynamic&quot;
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;u0&quot;
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721]
E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1



def forward(self, arg0_1: &quot;i64[]&quot;, arg1_1: &quot;f32[60]&quot;):
     # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:701 in forward, code: a = x.item()
    item: &quot;Sym(u0)&quot; = torch.ops.aten.item.default(arg0_1);  arg0_1 = None

     # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:702 in forward, code: return y[a]
    select = torch.ops.aten.select.int(arg1_1, 0, item);  arg1_1 = item = select = None




def forward(self, arg0_1: &quot;i64[]&quot;, arg1_1: &quot;f32[60]&quot;):
     # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:701 in forward, code: a = x.item()
    item: &quot;Sym(u0)&quot; = torch.ops.aten.item.default(arg0_1);  arg0_1 = None

     # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:702 in forward, code: return y[a]
    select = torch.ops.aten.select.int(arg1_1, 0, item);  arg1_1 = item = select = None

Traceback (most recent call last):
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 709, in &lt;module&gt;
    export(Foo(), inps)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 319, in export
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
           ^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1164, in wrapper
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
         ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1164, in wrapper
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
                      ^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1770, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1900, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1685, in _make_fx_helper
    gm = make_fx(
         ^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2318, in wrapped
    return make_fx_tracer.trace(f, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2250, in trace
    return self._trace_inner(f, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2221, in _trace_inner
    t = dispatch_trace(
        ^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_compile.py&quot;, line 53, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1254, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1835, in trace
    res = super().trace(root, concrete_args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace
    (self.create_arg(fn(*args)),),
                     ^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1312, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
          ^^^^^^^^^^^
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1589, in wrapped_fn
    return tuple(flat_fn(*args))
                 ^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn
    tree_out = fn(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call
    out = mod(*args[params_len:], **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1884, in forward
    tree_out = mod(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 702, in forward
    return y[a]
           ~^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1360, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1407, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 1051, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 1026, in run
    t = _method(t, *_args)
        ^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 950, in handler
    return torch._library.utils.handle_dispatch_mode(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py&quot;, line 296, in handle_dispatch_mode
    return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1462, in __torch_dispatch__
    return proxy_call(self, func, self.pre_dispatch, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 914, in proxy_call
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
    return self._op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1352, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2058, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1487, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2717, in _dispatch_impl
    r = func(*args, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
    return self._op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_meta_registrations.py&quot;, line 5545, in meta_select
    guard_size_oblivious(-index &gt; size) or guard_size_oblivious(index &gt;= size)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 473, in guard_size_oblivious
    return expr.node.guard_size_oblivious(&quot;&quot;, 0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 596, in guard_size_oblivious
    r = self.evaluate(size_oblivious=True)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 512, in evaluate
    return self.shape_env.evaluate_sym_node(self, size_oblivious)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7223, in evaluate_sym_node
    return self.evaluate_expr(
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7323, in evaluate_expr
    return self._inner_evaluate_expr(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
    return retlog(fn(*args, **kwargs))
                  ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7346, in _inner_evaluate_expr
    return self._evaluate_expr(
           ^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7570, in _evaluate_expr
    raise self._make_data_dependent_error(
torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression -u0 &gt; 60 (unhinted: -u0 &gt; 60).  (Size-like symbols: none)

Caused by: (_meta_registrations.py:5545 in meta_select)
For more information, run with TORCH_LOGS=&quot;dynamic&quot;
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;u0&quot;
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1

The following call raised this error:
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 702, in forward
    return y[a]

To fix the error, insert one of the following checks before this call:
  1. torch._check((-1)*a &gt; 60)
  2. torch._check((-1)*a &lt;= 60)

(These suggested fixes were derived by replacing `u0` with a in -u0 &gt; 60 and its negation.)

The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
<p>Here is a scenario where <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> insertion is required simply to prevent an operation from failing. The export call will fail with
“Could not guard on data-dependent expression <code class="docutils literal notranslate"><span class="pre">-u0</span> <span class="pre">&gt;</span> <span class="pre">60</span></code>”, implying that the compiler doesn’t know if this is a valid indexing operation -
if the value of <code class="docutils literal notranslate"><span class="pre">x</span></code> is out-of-bounds for <code class="docutils literal notranslate"><span class="pre">y</span></code> or not. Here, manual specialization is too prohibitive, and <code class="docutils literal notranslate"><span class="pre">torch.cond()</span></code> has no place.
Instead, informing the compiler of <code class="docutils literal notranslate"><span class="pre">u0</span></code>’s range is sufficient:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_check</span><span class="p">(</span><span class="n">a</span> <span class="o">&lt;</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">y</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">Foo</span><span class="p">(),</span> <span class="n">inps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ep</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:20.569000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:20.573000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense)
I1004 00:20:20.574000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u0]
I1004 00:20:20.575000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert u0 &gt;= 0 [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:722 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;u0 &gt;= 0&quot;
V1004 00:20:20.575000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range u0 = VR[0, int_oo] (update)
I1004 00:20:20.578000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert u0 &lt; 60 [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:723 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;u0 &lt; 60&quot;
V1004 00:20:20.579000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range u0 = VR[0, 59] (update)
V1004 00:20:20.582000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval size_oblivious(-u0 &gt; 60) == False [statically known]
V1004 00:20:20.582000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval size_oblivious(u0 &gt;= 60) == False [statically known]
V1004 00:20:20.583000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == True [statically known]
V1004 00:20:20.583000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == True [statically known]
I1004 00:20:20.585000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:20:20.586000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].storage_offset() 0 None
V1004 00:20:20.586000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[0] 60 None
V1004 00:20:20.586000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[0] 1 None
V1004 00:20:20.586000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].storage_offset() 0 None
V1004 00:20:20.587000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 &gt;= 0 == True [statically known]
V1004 00:20:20.589000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 &lt;= 59 == True [statically known]
V1004 00:20:20.589000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 &lt; 60 == True [statically known]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: &quot;i64[]&quot;, y: &quot;f32[60]&quot;):
             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:721 in forward, code: a = x.item()
            item: &quot;Sym(u0)&quot; = torch.ops.aten.item.default(x);  x = None
            ge_1: &quot;Sym(u0 &gt;= 0)&quot; = item &gt;= 0
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, &quot;Runtime assertion failed for expression u0 &gt;= 0 on node &#39;ge_1&#39;&quot;);  ge_1 = _assert_scalar_default = None
            le: &quot;Sym(u0 &lt;= 59)&quot; = item &lt;= 59
            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le, &quot;Runtime assertion failed for expression u0 &lt;= 59 on node &#39;le&#39;&quot;);  le = _assert_scalar_default_1 = None

             #
            lt_1: &quot;Sym(u0 &lt; 60)&quot; = item &lt; 60
            _assert_scalar_default_2 = torch.ops.aten._assert_scalar.default(lt_1, &quot;Runtime assertion failed for expression u0 &lt; 60 on node &#39;lt_1&#39;&quot;);  lt_1 = _assert_scalar_default_2 = None

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:724 in forward, code: return y[a]
            select: &quot;f32[]&quot; = torch.ops.aten.select.int(y, 0, item);  y = item = None
            return (select,)

Graph signature:
    # inputs
    x: USER_INPUT
    y: USER_INPUT

    # outputs
    select: USER_OUTPUT

Range constraints: {u0: VR[0, 59]}
</pre></div>
</div>
</section>
<section id="specialized-values">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">Specialized values</a><a class="headerlink" href="#specialized-values" title="Link to this heading">#</a></h3>
<p>Another category of data-dependent error happens when the program attempts to extract a concrete data-dependent integer/float value
while tracing. This looks something like “Could not extract specialized integer from data-dependent expression”, and is analogous to
the previous class of errors - if these occur when attempting to evaluate concrete integer/float values, data-dependent guard errors arise
with evaluating concrete boolean values.</p>
<p>This error typically occurs when there is an explicit or implicit <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast on a data-dependent expression. For example, this list comprehension
has a <cite>range()</cite> call that implicitly does an <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast on the size of the list:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">y</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">a</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">export</span><span class="p">(</span><span class="n">Foo</span><span class="p">(),</span> <span class="n">inps</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="n">tb</span><span class="o">.</span><span class="n">print_exc</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:20.594000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:20.599000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense)
I1004 00:20:20.599000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u0]
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] Data dependent variable &#39;u0&#39; allocated at:
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/bin/sphinx-build&quot;, line 7, in &lt;module&gt;
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     sys.exit(main())
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 339, in main
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return make_main(argv)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 213, in make_main
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return make_mode.run_make_mode(argv[1:])
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 181, in run_make_mode
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return make.run_generic_build(args[0])
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py&quot;, line 169, in run_generic_build
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return build_main(args + opts)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py&quot;, line 293, in build_main
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 272, in __init__
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     self._init_builder()
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/application.py&quot;, line 343, in _init_builder
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     self.events.emit(&#39;builder-inited&#39;)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx/events.py&quot;, line 97, in emit
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     results.append(listener.handler(self.app, *args))
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py&quot;, line 757, in generate_gallery_rst
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     ) = generate_dir_rst(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 606, in generate_dir_rst
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     results = parallel(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 607, in &lt;genexpr&gt;
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/workspace/tutorials-kr/conf.py&quot;, line 86, in wrapper
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     p.start()
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 121, in start
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     self._popen = self._Popen(self)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 224, in _Popen
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return _default_context.get_context().Process._Popen(process_obj)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/context.py&quot;, line 281, in _Popen
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return Popen(process_obj)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 19, in __init__
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     self._launch(process_obj)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/popen_fork.py&quot;, line 71, in _launch
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     code = process_obj._bootstrap(parent_sentinel=child_r)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 314, in _bootstrap
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     self.run()
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/multiprocessing/process.py&quot;, line 108, in run
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     self._target(*self._args, **self._kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/workspace/tutorials-kr/conf.py&quot;, line 74, in call_fn
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     result = func(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1374, in generate_file_rst
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     output_blocks, time_elapsed = execute_script(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1192, in execute_script
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     execute_code_block(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1048, in execute_code_block
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     is_last_expr, mem_max = _exec_and_get_memory(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 876, in _exec_and_get_memory
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     mem_max, _ = call_memory(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 1725, in _sg_call_memory_noop
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return 0.0, func()
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py&quot;, line 794, in __call__
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     exec(self.code, self.fake_main.__dict__)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 756, in &lt;module&gt;
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     export(Foo(), inps, strict=False)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return _export(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     ep = fn(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return fn(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     ep = _export_for_training(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     ep = fn(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return fn(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     export_artifact = export_func(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     aten_export_artifact = _to_aten_func(  # type: ignore[operator]
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1770, in _export_to_aten_ir_make_fx
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     gm, graph_signature = transform(_make_fx_helper)(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1900, in _aot_export_non_strict
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1685, in _make_fx_helper
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     gm = make_fx(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2318, in wrapped
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return make_fx_tracer.trace(f, *args)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2250, in trace
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self._trace_inner(f, *args)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2221, in _trace_inner
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     t = dispatch_trace(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_compile.py&quot;, line 53, in inner
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return disable_fn(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return fn(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1254, in dispatch_trace
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1835, in trace
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     res = super().trace(root, concrete_args)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return fn(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     (self.create_arg(fn(*args)),),
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1312, in wrapped
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     out = f(*tensors)  # type:ignore[call-arg]
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1589, in wrapped_fn
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return tuple(flat_fn(*args))
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     tree_out = fn(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     out = mod(*args[params_len:], **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self.call_module(mod, forward, args, kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return Tracer.call_module(self, m, forward, args, kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     ret_val = forward(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return _orig_module_call(mod, *args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self._call_impl(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return forward_call(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1884, in forward
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     tree_out = mod(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self.call_module(mod, forward, args, kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return Tracer.call_module(self, m, forward, args, kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     ret_val = forward(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return _orig_module_call(mod, *args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self._call_impl(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return forward_call(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 747, in forward
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     a = x.item()
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1360, in __torch_function__
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return func(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1407, in __torch_function__
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return func(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py&quot;, line 1051, in __torch_function__
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return func(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 950, in handler
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return torch._library.utils.handle_dispatch_mode(
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py&quot;, line 296, in handle_dispatch_mode
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return fn(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1462, in __torch_dispatch__
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return proxy_call(self, func, self.pre_dispatch, args, kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 914, in proxy_call
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     out = func(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_ops.py&quot;, line 829, in __call__
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self._op(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py&quot;, line 28, in wrapper
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return fn(*args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1352, in __torch_dispatch__
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self.dispatch(func, types, args, kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2058, in dispatch
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self._cached_dispatch_impl(func, types, args, kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 1474, in _cached_dispatch_impl
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return self._dispatch_impl(func, types, args, kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py&quot;, line 2687, in _dispatch_impl
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     op_impl_out = op_impl(self, func, *args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py&quot;, line 163, in dispatch_to_op_implementations_dict
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py&quot;, line 425, in local_scalar_dense
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     r = fake_mode.shape_env.create_unbacked_symint()
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]   File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]     return retlog(fn(*args, **kwargs))
V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519]



def forward(self, arg0_1: &quot;i64[]&quot;, arg1_1: &quot;f32[60]&quot;):
     # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:747 in forward, code: a = x.item()
    item: &quot;Sym(u0)&quot; = torch.ops.aten.item.default(arg0_1);  arg0_1 = item = None




def forward(self, arg0_1: &quot;i64[]&quot;, arg1_1: &quot;f32[60]&quot;):
     # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:747 in forward, code: a = x.item()
    item: &quot;Sym(u0)&quot; = torch.ops.aten.item.default(arg0_1);  arg0_1 = item = None

Traceback (most recent call last):
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 756, in &lt;module&gt;
    export(Foo(), inps, strict=False)
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 319, in export
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py&quot;, line 286, in export
    return _export(
           ^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1164, in wrapper
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2176, in _export
    ep = _export_for_training(
         ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1164, in wrapper
    raise e
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1130, in wrapper
    ep = fn(*args, **kwargs)
         ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py&quot;, line 123, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 2037, in _export_for_training
    export_artifact = export_func(
                      ^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1979, in _non_strict_export
    aten_export_artifact = _to_aten_func(  # type: ignore[operator]
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1770, in _export_to_aten_ir_make_fx
    gm, graph_signature = transform(_make_fx_helper)(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1900, in _aot_export_non_strict
    gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1685, in _make_fx_helper
    gm = make_fx(
         ^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2318, in wrapped
    return make_fx_tracer.trace(f, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2250, in trace
    return self._trace_inner(f, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 2221, in _trace_inner
    t = dispatch_trace(
        ^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_compile.py&quot;, line 53, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1254, in dispatch_trace
    graph = tracer.trace(root, concrete_args)  # type: ignore[arg-type]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1835, in trace
    res = super().trace(root, concrete_args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py&quot;, line 929, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 850, in trace
    (self.create_arg(fn(*args)),),
                     ^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1312, in wrapped
    out = f(*tensors)  # type:ignore[call-arg]
          ^^^^^^^^^^^
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1589, in wrapped_fn
    return tuple(flat_fn(*args))
                 ^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py&quot;, line 184, in flat_fn
    tree_out = fn(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py&quot;, line 906, in functional_call
    out = mod(*args[params_len:], **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py&quot;, line 1884, in forward
    tree_out = mod(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 825, in module_call_wrapper
    return self.call_module(mod, forward, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py&quot;, line 1905, in call_module
    return Tracer.call_module(self, m, forward, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 542, in call_module
    ret_val = forward(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py&quot;, line 818, in forward
    return _orig_module_call(mod, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py&quot;, line 748, in forward
    b = torch.cat([y for y in range(a)], dim=0)
                              ^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/__init__.py&quot;, line 438, in __index__
    return self.node.int_()
           ^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 468, in int_
    return self.guard_int(&quot;&quot;, 0)  # NB: uses Python backtrace
           ^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 518, in guard_int
    r = self.evaluate()
        ^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py&quot;, line 512, in evaluate
    return self.shape_env.evaluate_sym_node(self, size_oblivious)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7223, in evaluate_sym_node
    return self.evaluate_expr(
           ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7323, in evaluate_expr
    return self._inner_evaluate_expr(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py&quot;, line 272, in wrapper
    return retlog(fn(*args, **kwargs))
                  ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7346, in _inner_evaluate_expr
    return self._evaluate_expr(
           ^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py&quot;, line 7570, in _evaluate_expr
    raise self._make_data_dependent_error(
torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0).  (Size-like symbols: none)

Caused by: (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:748 in forward)
For more information, run with TORCH_LOGS=&quot;dynamic&quot;
For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=&quot;u0&quot;
If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing

For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1

The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.
</pre></div>
</div>
<p>For these errors, some basic options you have are:</p>
<ol class="arabic simple">
<li><p>Avoid unnecessary <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast calls, in this case the <code class="docutils literal notranslate"><span class="pre">int(a)</span></code> in the return statement.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">torch._check()</span></code> calls; unfortunately all you may be able to do in this case is specialize (with <code class="docutils literal notranslate"><span class="pre">torch._check(a</span> <span class="pre">==</span> <span class="pre">60)</span></code>).</p></li>
<li><p>Rewrite the offending code at a higher level. For example, the list comprehension is semantically a <code class="docutils literal notranslate"><span class="pre">repeat()</span></code> op, which doesn’t involve an <code class="docutils literal notranslate"><span class="pre">int()</span></code> cast. The following rewrite avoids data-dependent errors:</p></li>
</ol>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Foo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">b</span> <span class="o">+</span> <span class="n">a</span>

<span class="n">inps</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">Foo</span><span class="p">(),</span> <span class="n">inps</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ep</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:20.611000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:20.616000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense)
I1004 00:20:20.616000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u0]
I1004 00:20:20.620000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert u0 &gt;= 0 [guard added] (_meta_registrations.py:4247 in meta_repeat), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=&quot;u0 &gt;= 0&quot;
V1004 00:20:20.620000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range u0 = VR[0, int_oo] (update)
V1004 00:20:20.621000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 &gt;= 0 == True [statically known]
I1004 00:20:20.623000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7369] could not evaluate Eq(u0, 0) due to data dependency, it was assumed to be False with no runtime assertions (utils/_stats.py:28 in wrapper)
I1004 00:20:20.623000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7369] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
I1004 00:20:20.628000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7369] could not evaluate 60*u0 &lt; 2 due to data dependency, it was assumed to be False with no runtime assertions (_prims_common/__init__.py:279 in is_contiguous)
I1004 00:20:20.628000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7369] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
I1004 00:20:20.629000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7369] could not evaluate Eq(u0, 1) due to data dependency, it was assumed to be False with no runtime assertions (_prims_common/__init__.py:285 in is_contiguous)
I1004 00:20:20.629000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7369] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1
V1004 00:20:20.631000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known]
I1004 00:20:20.635000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:20:20.636000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].storage_offset() 0 None
V1004 00:20:20.636000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].size()[0] 60 None
V1004 00:20:20.636000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].stride()[0] 1 None
V1004 00:20:20.636000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;y&#39;].storage_offset() 0 None
V1004 00:20:20.637000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 &gt;= 0 == True [statically known]
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: &quot;i64[]&quot;, y: &quot;f32[60]&quot;):
             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:769 in forward, code: a = x.item()
            item: &quot;Sym(u0)&quot; = torch.ops.aten.item.default(x);  x = None

             #
            sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item);  sym_constrain_range_for_size_default = None

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:769 in forward, code: a = x.item()
            ge: &quot;Sym(u0 &gt;= 0)&quot; = item &gt;= 0
            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge, &quot;Runtime assertion failed for expression u0 &gt;= 0 on node &#39;ge&#39;&quot;);  ge = _assert_scalar_default = None

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:770 in forward, code: b = y.unsqueeze(0).repeat(a, 1)
            unsqueeze: &quot;f32[1, 60]&quot; = torch.ops.aten.unsqueeze.default(y, 0);  y = None
            repeat: &quot;f32[u0, 60]&quot; = torch.ops.aten.repeat.default(unsqueeze, [item, 1]);  unsqueeze = None

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:771 in forward, code: return b + a
            add: &quot;f32[u0, 60]&quot; = torch.ops.aten.add.Tensor(repeat, item);  repeat = item = None
            return (add,)

Graph signature:
    # inputs
    x: USER_INPUT
    y: USER_INPUT

    # outputs
    add: USER_OUTPUT

Range constraints: {u0: VR[0, int_oo]}
</pre></div>
</div>
<p>Data-dependent errors can be much more involved, and there are many more options in your toolkit to deal with them: <code class="docutils literal notranslate"><span class="pre">torch._check_is_size()</span></code>, <code class="docutils literal notranslate"><span class="pre">guard_size_oblivious()</span></code>, or real-tensor tracing, as starters.
For more in-depth guides, please refer to the <a class="reference external" href="https://pytorch.org/docs/main/export.programming_model.html">Export Programming Model</a>,
or <a class="reference external" href="https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs">Dealing with GuardOnDataDependentSymNode errors</a>.</p>
</section>
</section>
<section id="custom-ops">
<h2><a class="toc-backref" href="#id15" role="doc-backlink">Custom Ops</a><a class="headerlink" href="#custom-ops" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> can export PyTorch programs with custom operators. Please
refer to <a class="reference external" href="https://tutorials.pytorch.kr/advanced/custom_ops_landing_page.html">this page</a>
on how to author a custom operator in either C++ or Python.</p>
<p>The following is an example of registering a custom operator in python to be
used by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>. The important thing to note is that the custom op
must have a <a class="reference external" href="https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit?tab=t.0#heading=h.xvrg7clz290">FakeTensor kernel</a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">custom_op</span><span class="p">(</span><span class="s2">&quot;my_custom_library::custom_op&quot;</span><span class="p">,</span> <span class="n">mutates_args</span><span class="o">=</span><span class="p">{})</span>
<span class="k">def</span><span class="w"> </span><span class="nf">custom_op</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;custom_op called!&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nd">@custom_op</span><span class="o">.</span><span class="n">register_fake</span>
<span class="k">def</span><span class="w"> </span><span class="nf">custom_op_meta</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Returns an empty tensor with the same shape as the expected output</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is an example of exporting a program with the custom op.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CustomOpExample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">my_custom_library</span><span class="o">.</span><span class="n">custom_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">exported_custom_op_example</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">CustomOpExample</span><span class="p">(),</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_custom_op_example</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">exported_custom_op_example</span><span class="o">.</span><span class="n">module</span><span class="p">()(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:20.685000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:20.695000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:20:20.695000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[0] 3 None
V1004 00:20:20.695000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[1] 3 None
V1004 00:20:20.695000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[0] 3 None
V1004 00:20:20.695000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[1] 1 None
V1004 00:20:20.695000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].storage_offset() 0 None
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: &quot;f32[3, 3]&quot;):
             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:812 in forward, code: x = torch.sin(x)
            sin: &quot;f32[3, 3]&quot; = torch.ops.aten.sin.default(x);  x = None

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:813 in forward, code: x = torch.ops.my_custom_library.custom_op(x)
            custom_op: &quot;f32[3, 3]&quot; = torch.ops.my_custom_library.custom_op.default(sin);  sin = None

             # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:814 in forward, code: x = torch.cos(x)
            cos: &quot;f32[3, 3]&quot; = torch.ops.aten.cos.default(custom_op);  custom_op = None
            return (cos,)

Graph signature:
    # inputs
    x: USER_INPUT

    # outputs
    cos: USER_OUTPUT

Range constraints: {}

custom_op called!
tensor([[0.7485, 0.6790, 0.8114],
        [1.0000, 0.9969, 1.0000],
        [0.9950, 1.0000, 0.6667]])
</pre></div>
</div>
<p>Note that in the <code class="docutils literal notranslate"><span class="pre">ExportedProgram</span></code>, the custom operator is included in the graph.</p>
</section>
<section id="ir-decompositions">
<h2><a class="toc-backref" href="#id16" role="doc-backlink">IR/Decompositions</a><a class="headerlink" href="#ir-decompositions" title="Link to this heading">#</a></h2>
<p>The graph produced by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> returns a graph containing only
<a class="reference external" href="https://pytorch.org/cppdocs/#aten">ATen operators</a>, which are the
basic unit of computation in PyTorch. As there are over 3000 ATen operators,
export provides a way to narrow down the operator set used in the graph based
on certain characteristics, creating different IRs.</p>
<p>By default, export produces the most generic IR which contains all ATen
operators, including both functional and non-functional operators. A functional
operator is one that does not contain any mutations or aliasing of the inputs.
You can find a list of all ATen operators
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml">here</a>
and you can inspect if an operator is functional by checking
<code class="docutils literal notranslate"><span class="pre">op._schema.is_mutable</span></code>, for example:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">add_</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_schema</span><span class="o">.</span><span class="n">is_mutable</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>False
True
</pre></div>
</div>
<p>This generic IR can be used to train in eager PyTorch Autograd. This IR can be
more explicitly reached through the API <code class="docutils literal notranslate"><span class="pre">torch.export.export_for_training</span></code>,
which was introduced in PyTorch 2.5, but calling <code class="docutils literal notranslate"><span class="pre">torch.export.export</span></code>
should produce the same graph as of PyTorch 2.6.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DecompExample</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">,)</span>

<span class="n">ep_for_training</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export_for_training</span><span class="p">(</span><span class="n">DecompExample</span><span class="p">(),</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ep_for_training</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:20.702000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:20.727000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:20:20.727000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[0] 1 None
V1004 00:20:20.727000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[1] 1 None
V1004 00:20:20.727000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[2] 3 None
V1004 00:20:20.727000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[3] 3 None
V1004 00:20:20.728000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[0] 9 None
V1004 00:20:20.728000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[1] 9 None
V1004 00:20:20.728000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[2] 3 None
V1004 00:20:20.728000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[3] 1 None
V1004 00:20:20.728000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].storage_offset() 0 None
graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv_weight, %p_conv_bias), kwargs = {})
    %add_ : [num_users=0] = call_function[target=torch.ops.aten.add_.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %batch_norm : [num_users=1] = call_function[target=torch.ops.aten.batch_norm.default](args = (%conv2d, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05, True), kwargs = {})
    return (batch_norm,)
</pre></div>
</div>
<p>We can then lower this exported program to an operator set which only contains
functional ATen operators through the API <code class="docutils literal notranslate"><span class="pre">run_decompositions</span></code>, which
decomposes the ATen operators into the ones specified in the decomposition
table, and functionalizes the graph. By specifying an empty set, we’re only
performing functionalization, and does not do any additional decompositions.
This results in an IR which contains ~2000 operators (instead of the 3000
operators above), and is ideal for inference cases.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">ep_for_inference</span> <span class="o">=</span> <span class="n">ep_for_training</span><span class="o">.</span><span class="n">run_decompositions</span><span class="p">(</span><span class="n">decomp_table</span><span class="o">=</span><span class="p">{})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ep_for_inference</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv_weight, %p_conv_bias), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%conv2d, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {})
    return (getitem_3, getitem_4, add, getitem)
</pre></div>
</div>
<p>As we can see, the previously mutable operator,
<code class="docutils literal notranslate"><span class="pre">torch.ops.aten.add_.default</span></code> has now been replaced with
<code class="docutils literal notranslate"><span class="pre">torch.ops.aten.add.default</span></code>, a l operator.</p>
<p>We can also further lower this exported program to an operator set which only
contains the
<a class="reference external" href="https://pytorch.org/docs/main/torch.compiler_ir.html#core-aten-ir">Core ATen Operator Set</a>,
which is a collection of only ~180 operators. This IR is optimal for backends
who do not want to reimplement all ATen operators.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">default_decompositions</span>

<span class="n">core_aten_decomp_table</span> <span class="o">=</span> <span class="n">default_decompositions</span><span class="p">()</span>
<span class="n">core_aten_ep</span> <span class="o">=</span> <span class="n">ep_for_training</span><span class="o">.</span><span class="n">run_decompositions</span><span class="p">(</span><span class="n">decomp_table</span><span class="o">=</span><span class="n">core_aten_decomp_table</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">core_aten_ep</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%x, %p_conv_weight, %p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%convolution, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {})
    return (getitem_3, getitem_4, add, getitem)
</pre></div>
</div>
<p>We now see that <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.conv2d.default</span></code> has been decomposed
into <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.convolution.default</span></code>. This is because <code class="docutils literal notranslate"><span class="pre">convolution</span></code>
is a more “core” operator, as operations like <code class="docutils literal notranslate"><span class="pre">conv1d</span></code> and <code class="docutils literal notranslate"><span class="pre">conv2d</span></code> can be
implemented using the same op.</p>
<p>We can also specify our own decomposition behaviors:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">my_decomp_table</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">default_decompositions</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">my_awesome_custom_conv2d_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dilation</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">groups</span><span class="p">)</span>

<span class="n">my_decomp_table</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">conv2d</span><span class="o">.</span><span class="n">default</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_awesome_custom_conv2d_function</span>
<span class="n">my_ep</span> <span class="o">=</span> <span class="n">ep_for_training</span><span class="o">.</span><span class="n">run_decompositions</span><span class="p">(</span><span class="n">my_decomp_table</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">my_ep</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>graph():
    %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight]
    %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias]
    %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight]
    %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias]
    %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean]
    %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var]
    %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked]
    %x : [num_users=1] = placeholder[target=x]
    %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%x, %p_conv_weight, %p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution, 2), kwargs = {})
    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {})
    %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%mul, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {})
    %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {})
    %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {})
    %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {})
    return (getitem_3, getitem_4, add, getitem)
</pre></div>
</div>
<p>Notice that instead of <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.conv2d.default</span></code> being decomposed
into <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.convolution.default</span></code>, it is now decomposed into
<code class="docutils literal notranslate"><span class="pre">torch.ops.aten.convolution.default</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.ops.aten.mul.Tensor</span></code>,
which matches our custom decomposition rule.</p>
</section>
<section id="exportdb">
<h2><a class="toc-backref" href="#id17" role="doc-backlink">ExportDB</a><a class="headerlink" href="#exportdb" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> will only ever export a single computation graph from a PyTorch program. Because of this requirement,
there will be Python or PyTorch features that are not compatible with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, which will require users to
rewrite parts of their model code. We have seen examples of this earlier in the tutorial – for example, rewriting
if-statements using <code class="docutils literal notranslate"><span class="pre">cond</span></code>.</p>
<p><a class="reference external" href="https://pytorch.org/docs/main/generated/exportdb/index.html">ExportDB</a> is the standard reference that documents
supported and unsupported Python/PyTorch features for <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>. It is essentially a list a program samples, each
of which represents the usage of one particular Python/PyTorch feature and its interaction with <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.
Examples are also tagged by category so that they can be more easily searched.</p>
<p>For example, let’s use ExportDB to get a better understanding of how the predicate works in the <code class="docutils literal notranslate"><span class="pre">cond</span></code> operator.
We can look at the example called <code class="docutils literal notranslate"><span class="pre">cond_predicate</span></code>, which has a <code class="docutils literal notranslate"><span class="pre">torch.cond</span></code> tag. The example code looks like:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cond_predicate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The conditional statement (aka predicate) passed to ``cond()`` must be one of the following:</span>
<span class="sd">    - ``torch.Tensor`` with a single element</span>
<span class="sd">    - boolean expression</span>
<span class="sd">    NOTE: If the `pred` is test on a dim with batch size &lt; 2, it will be specialized.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">10</span>
    <span class="k">return</span> <span class="n">cond</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">cos</span><span class="p">(),</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">y</span><span class="o">.</span><span class="n">sin</span><span class="p">(),</span> <span class="p">[</span><span class="n">x</span><span class="p">])</span>
</pre></div>
</div>
<p>More generally, ExportDB can be used as a reference when one of the following occurs:</p>
<ol class="arabic simple">
<li><p>Before attempting <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, you know ahead of time that your model uses some tricky Python/PyTorch features
and you want to know if <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> covers that feature.</p></li>
<li><p>When attempting <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, there is a failure and it’s unclear how to work around it.</p></li>
</ol>
<p>ExportDB is not exhaustive, but is intended to cover all use cases found in typical PyTorch code. Feel free to reach
out if there is an important Python/PyTorch feature that should be added to ExportDB or supported by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>.</p>
</section>
<section id="running-the-exported-program">
<h2><a class="toc-backref" href="#id18" role="doc-backlink">Running the Exported Program</a><a class="headerlink" href="#running-the-exported-program" title="Link to this heading">#</a></h2>
<p>As <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> is only a graph capturing mechanism, calling the artifact
produced by <code class="docutils literal notranslate"><span class="pre">torch.export</span></code> eagerly will be equivalent to running the eager
module. To optimize the execution of the Exported Program, we can pass this
exported artifact to backends such as Inductor through <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>,
<a class="reference external" href="https://pytorch.org/docs/main/torch.compiler_aot_inductor.html">AOTInductor</a>,
or <a class="reference external" href="https://pytorch.org/TensorRT/dynamo/dynamo_export.html">TensorRT</a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">M</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">M</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">ep</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">(</span><span class="n">inp</span><span class="p">,))</span>

<span class="c1"># Run it eagerly</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">ep</span><span class="o">.</span><span class="n">module</span><span class="p">()(</span><span class="n">inp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="c1"># Run it with torch.compile</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">ep</span><span class="o">.</span><span class="n">module</span><span class="p">(),</span> <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;inductor&quot;</span><span class="p">)(</span><span class="n">inp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>I1004 00:20:21.493000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env
I1004 00:20:21.503000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards
V1004 00:20:21.503000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[0] 2 None
V1004 00:20:21.504000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].size()[1] 3 None
V1004 00:20:21.504000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[0] 3 None
V1004 00:20:21.504000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].stride()[1] 1 None
V1004 00:20:21.504000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[&#39;x&#39;].storage_offset() 0 None
tensor([[-0.8696,  0.6491,  1.1789],
        [ 1.0454, -1.0737, -0.5275]], device=&#39;cuda:0&#39;,
       grad_fn=&lt;AddmmBackward0&gt;)
I1004 00:20:22.488000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] [2/0] create_env
/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:282: UserWarning:

TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision(&#39;high&#39;)` for better performance.

V1004 00:20:24.047000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7461] [2/0] eval 0 [trivial]
V1004 00:20:24.052000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7461] [2/0] eval 1 [trivial]
I1004 00:20:24.368000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] [2/0] produce_guards
I1004 00:20:24.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] [2/0] produce_guards
V1004 00:20:24.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;x&#39;].size()[0] 2 None
V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;x&#39;].size()[1] 3 None
V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;x&#39;].stride()[0] 3 None
V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;x&#39;].stride()[1] 1 None
V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;x&#39;].storage_offset() 0 None
V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;weight&#39;].size()[0] 3 None
V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;weight&#39;].size()[1] 3 None
V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;weight&#39;].stride()[0] 3 None
V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;weight&#39;].stride()[1] 1 None
V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;weight&#39;].storage_offset() 0 None
V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;bias&#39;].size()[0] 3 None
V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;bias&#39;].stride()[0] 1 None
V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;bias&#39;].storage_offset() 0 None
V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;x&#39;].size()[0] == 2
V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;x&#39;].size()[1] == 3
V1004 00:20:24.379000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;x&#39;].stride()[0] == 3
V1004 00:20:24.379000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;x&#39;].stride()[1] == 1
V1004 00:20:24.379000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;x&#39;].storage_offset() == 0
V1004 00:20:24.379000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;weight&#39;].size()[0] == 3
V1004 00:20:24.379000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;weight&#39;].size()[1] == 3
V1004 00:20:24.379000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;weight&#39;].stride()[0] == 3
V1004 00:20:24.380000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;weight&#39;].stride()[1] == 1
V1004 00:20:24.380000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;weight&#39;].storage_offset() == 0
V1004 00:20:24.380000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;bias&#39;].size()[0] == 3
V1004 00:20:24.380000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;bias&#39;].stride()[0] == 1
V1004 00:20:24.380000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[&#39;self&#39;]._modules[&#39;linear&#39;]._parameters[&#39;bias&#39;].storage_offset() == 0
tensor([[-0.8696,  0.6491,  1.1789],
        [ 1.0454, -1.0737, -0.5275]], device=&#39;cuda:0&#39;,
       grad_fn=&lt;CompiledFunctionBackward&gt;)
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch._inductor</span>

<span class="c1"># Note: these APIs are subject to change</span>
<span class="c1"># Compile the exported program to a PT2 archive using ``AOTInductor``</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">pt2_path</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_inductor</span><span class="o">.</span><span class="n">aoti_compile_and_package</span><span class="p">(</span><span class="n">ep</span><span class="p">)</span>

<span class="c1"># Load and run the .so file in Python.</span>
<span class="c1"># To load and run it in a C++ environment, see:</span>
<span class="c1"># https://pytorch.org/docs/main/torch.compiler_aot_inductor.html</span>
<span class="n">aoti_compiled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_inductor</span><span class="o">.</span><span class="n">aoti_load_package</span><span class="p">(</span><span class="n">pt2_path</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">aoti_compiled</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id19" role="doc-backlink">Conclusion</a><a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>We introduced <code class="docutils literal notranslate"><span class="pre">torch.export</span></code>, the new PyTorch 2.X way to export single computation
graphs from PyTorch programs. In particular, we demonstrate several code modifications
and considerations (control flow ops, constraints, etc.) that need to be made in order to export a graph.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 42.143 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-torch-export-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/fb8083290582c4f473d970913a4186c4/torch_export_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">torch_export_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/b865be3c401c1b4fbdb03f49916ac8e8/torch_export_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">torch_export_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/eb8d1a44ecbb6f9b3f0732396942ffcd/torch_export_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">torch_export_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="../recipes/regional_compilation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Reducing torch.compile cold start compilation time with regional compilation</p>
      </div>
    </a>
    <a class="right-next"
       href="../recipes/torch_export_aoti_python.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../recipes/regional_compilation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Reducing torch.compile cold start compilation time with regional compilation</p>
      </div>
    </a>
    <a class="right-next"
       href="../recipes/torch_export_aoti_python.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-usage">Basic Usage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-breaks">Graph Breaks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-strict-export">Non-Strict Export</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#control-flow-ops">Control Flow Ops</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constraints-dynamic-shapes">Constraints/Dynamic Shapes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-concepts-symbols-and-guards">Basic concepts: symbols and guards</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specialization">0/1 specialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#named-dims">Named Dims</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#constraint-violations-suggested-fixes">Constraint violations, suggested fixes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-dependent-errors">Data-dependent errors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#guards-torch-check">Guards, torch._check()</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specialized-values">Specialized values</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-ops">Custom Ops</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ir-decompositions">IR/Decompositions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exportdb">ExportDB</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-the-exported-program">Running the Exported Program</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "torch.export Tutorial",
       "headline": "torch.export Tutorial",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/intermediate/torch_export_tutorial.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. torch.export Tutorial# Author: William Wen, Zhengxu Chen, Angela Yi, Pian Pawakapan \uacbd\uace0 torch.export and its related features are in prototype status and are subject to backwards compatibility breaking changes. This tutorial provides a snapshot of torch.export usage as of PyTorch 2.5. torch.export() is the PyTorch 2.X way to export PyTorch models into standardized model representations, intended to be run on different (i.e. Python-less) environments. The official documentation can be found here. In this tutorial, you will learn how to use torch.export() to extract ExportedProgram\u2019s (i.e. single-graph representations) from PyTorch programs. We also detail some considerations/modifications that you may need to make in order to make your model compatible with torch.export. Contents Basic Usage Graph Breaks Non-Strict Export Control Flow Ops Constraints/Dynamic Shapes Basic concepts: symbols and guards 0/1 specialization Named Dims Constraint violations, suggested fixes Data-dependent errors Guards, torch._check() Specialized values Custom Ops IR/Decompositions ExportDB Running the Exported Program Conclusion Basic Usage# torch.export extracts single-graph representations from PyTorch programs by tracing the target function, given example inputs. torch.export.export() is the main entry point for torch.export. In this tutorial, torch.export and torch.export.export() are practically synonymous, though torch.export generally refers to the PyTorch 2.X export process, and torch.export.export() generally refers to the actual function call. The signature of torch.export.export() is: export( mod: torch.nn.Module, args: Tuple[Any, ...], kwargs: Optional[Dict[str, Any]] = None, *, dynamic_shapes: Optional[Dict[str, Dict[int, Dim]]] = None ) -\u003e ExportedProgram torch.export.export() traces the tensor computation graph from calling mod(*args, **kwargs) and wraps it in an ExportedProgram, which can be serialized or executed later with different inputs. To execute the ExportedProgram we can call .module() on it to return a torch.nn.Module which is callable, just like the original program. We will detail the dynamic_shapes argument later in the tutorial. import torch from torch.export import export class MyModule(torch.nn.Module): def __init__(self): super().__init__() self.lin = torch.nn.Linear(100, 10) def forward(self, x, y): return torch.nn.functional.relu(self.lin(x + y), inplace=True) mod = MyModule() exported_mod = export(mod, (torch.randn(8, 100), torch.randn(8, 100))) print(type(exported_mod)) print(exported_mod.module()(torch.randn(8, 100), torch.randn(8, 100))) \u003cclass \u0027torch.export.exported_program.ExportedProgram\u0027\u003e tensor([[0.5697, 0.0000, 0.5520, 0.0000, 0.0000, 0.0000, 0.0000, 0.3805, 1.2986, 0.6166], [0.0000, 0.0000, 0.0000, 0.7790, 0.0000, 1.6360, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.4730, 0.0000, 1.4367, 0.0000, 0.0000, 0.7210, 0.0000, 0.6131], [0.0000, 0.0000, 0.0000, 0.3966, 0.6764, 0.0000, 0.4882, 0.0000, 0.7645, 0.6003], [0.0000, 0.4958, 0.8105, 0.0000, 0.0000, 0.2007, 0.3920, 0.0000, 1.0997, 0.5876], [0.4842, 1.1476, 0.6031, 0.0118, 1.2018, 0.1046, 0.0000, 0.0000, 0.2046, 0.2781], [0.6158, 0.3322, 1.2286, 0.0853, 0.1356, 0.9100, 0.4171, 0.0000, 0.0000, 0.4035], [0.0000, 0.1702, 1.0550, 0.0000, 1.3246, 0.1930, 0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=\u003cReluBackward0\u003e) Let\u2019s review some attributes of ExportedProgram that are of interest. The graph attribute is an FX graph traced from the function we exported, that is, the computation graph of all PyTorch operations. The FX graph is in \u201cATen IR\u201d meaning that it contains only \u201cATen-level\u201d operations. The graph_signature attribute gives a more detailed description of the input and output nodes in the exported graph, describing which ones are parameters, buffers, user inputs, or user outputs. The range_constraints attributes will be covered later. print(exported_mod) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, p_lin_weight: \"f32[10, 100]\", p_lin_bias: \"f32[10]\", x: \"f32[8, 100]\", y: \"f32[8, 100]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:71 in forward, code: return torch.nn.functional.relu(self.lin(x + y), inplace=True) add: \"f32[8, 100]\" = torch.ops.aten.add.Tensor(x, y); x = y = None # File: /opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias) linear: \"f32[8, 10]\" = torch.ops.aten.linear.default(add, p_lin_weight, p_lin_bias); add = p_lin_weight = p_lin_bias = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:71 in forward, code: return torch.nn.functional.relu(self.lin(x + y), inplace=True) relu_: \"f32[8, 10]\" = torch.ops.aten.relu_.default(linear); linear = None return (relu_,) Graph signature: # inputs p_lin_weight: PARAMETER target=\u0027lin.weight\u0027 p_lin_bias: PARAMETER target=\u0027lin.bias\u0027 x: USER_INPUT y: USER_INPUT # outputs relu_: USER_OUTPUT Range constraints: {} See the torch.export documentation for more details. Graph Breaks# Although torch.export shares components with torch.compile, the key limitation of torch.export, especially when compared to torch.compile, is that it does not support graph breaks. This is because handling graph breaks involves interpreting the unsupported operation with default Python evaluation, which is incompatible with the export use case. Therefore, in order to make your model code compatible with torch.export, you will need to modify your code to remove graph breaks. A graph break is necessary in cases such as: data-dependent control flow class Bad1(torch.nn.Module): def forward(self, x): if x.sum() \u003e 0: return torch.sin(x) return torch.cos(x) import traceback as tb try: export(Bad1(), (torch.randn(3, 3),)) except Exception: tb.print_exc() def forward(self, arg0_1: \"f32[3, 3]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:116 in forward, code: if x.sum() \u003e 0: sum_1: \"f32[]\" = torch.ops.aten.sum.default(arg0_1); arg0_1 = None gt: \"b8[]\" = torch.ops.aten.gt.Scalar(sum_1, 0); sum_1 = None ne: \"b8[]\" = torch.ops.aten.ne.Scalar(gt, 0); gt = None item: \"Sym(Eq(u0, 1))\" = torch.ops.aten.item.default(ne); ne = item = None def forward(self, arg0_1: \"f32[3, 3]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:116 in forward, code: if x.sum() \u003e 0: sum_1: \"f32[]\" = torch.ops.aten.sum.default(arg0_1); arg0_1 = None gt: \"b8[]\" = torch.ops.aten.gt.Scalar(sum_1, 0); sum_1 = None ne: \"b8[]\" = torch.ops.aten.ne.Scalar(gt, 0); gt = None item: \"Sym(Eq(u0, 1))\" = torch.ops.aten.item.default(ne); ne = item = None Traceback (most recent call last): File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 122, in \u003cmodule\u003e export(Bad1(), (torch.randn(3, 3),)) File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 319, in export raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( ^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1164, in wrapper raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1164, in wrapper raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( ^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1770, in _export_to_aten_ir_make_fx gm, graph_signature = transform(_make_fx_helper)( ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1900, in _aot_export_non_strict gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1685, in _make_fx_helper gm = make_fx( ^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in wrapped return make_fx_tracer.trace(f, *args) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2250, in trace return self._trace_inner(f, *args) ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2221, in _trace_inner t = dispatch_trace( ^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_compile.py\", line 53, in inner return disable_fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1254, in dispatch_trace graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1835, in trace res = super().trace(root, concrete_args) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 850, in trace (self.create_arg(fn(*args)),), ^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1312, in wrapped out = f(*tensors) # type:ignore[call-arg] ^^^^^^^^^^^ File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1589, in wrapped_fn return tuple(flat_fn(*args)) ^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn tree_out = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 906, in functional_call out = mod(*args[params_len:], **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1884, in forward tree_out = mod(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 116, in forward if x.sum() \u003e 0: ^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1360, in __torch_function__ return func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1407, in __torch_function__ return func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 1051, in __torch_function__ return func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 538, in guard_bool r = self.evaluate() ^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 512, in evaluate return self.shape_env.evaluate_sym_node(self, size_oblivious) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7223, in evaluate_sym_node return self.evaluate_expr( ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7323, in evaluate_expr return self._inner_evaluate_expr( ^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7346, in _inner_evaluate_expr return self._evaluate_expr( ^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7570, in _evaluate_expr raise self._make_data_dependent_error( torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)). (Size-like symbols: none) Caused by: (_export/non_strict_utils.py:1051 in __torch_function__) For more information, run with TORCH_LOGS=\"dynamic\" For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\" If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 The following call raised this error: File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 116, in forward if x.sum() \u003e 0: The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`. accessing tensor data with .data class Bad2(torch.nn.Module): def forward(self, x): x.data[0, 0] = 3 return x try: export(Bad2(), (torch.randn(3, 3),)) except Exception: tb.print_exc() calling unsupported functions (such as many built-in functions) class Bad3(torch.nn.Module): def forward(self, x): x = x + 1 return x + id(x) try: export(Bad3(), (torch.randn(3, 3),)) except Exception: tb.print_exc() Non-Strict Export# To trace the program, torch.export uses TorchDynamo by default, a byte code analysis engine, to symbolically analyze the Python code and build a graph based on the results. This analysis allows torch.export to provide stronger guarantees about safety, but not all Python code is supported, causing these graph breaks. To address this issue, in PyTorch 2.3, we introduced a new mode of exporting called non-strict mode, where we trace through the program using the Python interpreter executing it exactly as it would in eager mode, allowing us to skip over unsupported Python features. This is done through adding a strict=False flag. Looking at some of the previous examples which resulted in graph breaks: Calling unsupported functions (such as many built-in functions) traces through, but in this case, id(x) gets specialized as a constant integer in the graph. This is because id(x) is not a tensor operation, so the operation is not recorded in the graph. class Bad3(torch.nn.Module): def forward(self, x): x = x + 1 return x + id(x) bad3_nonstrict = export(Bad3(), (torch.randn(3, 3),), strict=False) print(bad3_nonstrict) print(bad3_nonstrict.module()(torch.ones(3, 3))) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 3]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:179 in forward, code: x = x + 1 add: \"f32[3, 3]\" = torch.ops.aten.add.Tensor(x, 1); x = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:180 in forward, code: return x + id(x) add_1: \"f32[3, 3]\" = torch.ops.aten.add.Tensor(add, 139699698668784); add = None return (add_1,) Graph signature: # inputs x: USER_INPUT # outputs add_1: USER_OUTPUT Range constraints: {} tensor([[1.3970e+14, 1.3970e+14, 1.3970e+14], [1.3970e+14, 1.3970e+14, 1.3970e+14], [1.3970e+14, 1.3970e+14, 1.3970e+14]]) However, there are still some features that require rewrites to the original module: Control Flow Ops# torch.export actually does support data-dependent control flow. But these need to be expressed using control flow ops. For example, we can fix the control flow example above using the cond op, like so: class Bad1Fixed(torch.nn.Module): def forward(self, x): def true_fn(x): return torch.sin(x) def false_fn(x): return torch.cos(x) return torch.cond(x.sum() \u003e 0, true_fn, false_fn, [x]) exported_bad1_fixed = export(Bad1Fixed(), (torch.randn(3, 3),)) print(exported_bad1_fixed) print(exported_bad1_fixed.module()(torch.ones(3, 3))) print(exported_bad1_fixed.module()(-torch.ones(3, 3))) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 3]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:205 in forward, code: return torch.cond(x.sum() \u003e 0, true_fn, false_fn, [x]) sum_1: \"f32[]\" = torch.ops.aten.sum.default(x) gt: \"b8[]\" = torch.ops.aten.gt.Scalar(sum_1, 0); sum_1 = None # File: \u003ceval_with_key\u003e.33:9 in forward, code: cond = torch.ops.higher_order.cond(l_args_0_, cond_true_0, cond_false_0, (l_args_3_0_,)); l_args_0_ = cond_true_0 = cond_false_0 = l_args_3_0_ = None true_graph_0 = self.true_graph_0 false_graph_0 = self.false_graph_0 cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, (x,)); gt = true_graph_0 = false_graph_0 = x = None getitem: \"f32[3, 3]\" = cond[0]; cond = None return (getitem,) class true_graph_0(torch.nn.Module): def forward(self, x: \"f32[3, 3]\"): # File: \u003ceval_with_key\u003e.30:6 in forward, code: sin = torch.sin(l_args_3_0__1); l_args_3_0__1 = None sin: \"f32[3, 3]\" = torch.ops.aten.sin.default(x); x = None return (sin,) class false_graph_0(torch.nn.Module): def forward(self, x: \"f32[3, 3]\"): # File: \u003ceval_with_key\u003e.31:6 in forward, code: cos = torch.cos(l_args_3_0__1); l_args_3_0__1 = None cos: \"f32[3, 3]\" = torch.ops.aten.cos.default(x); x = None return (cos,) Graph signature: # inputs x: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {} tensor([[0.8415, 0.8415, 0.8415], [0.8415, 0.8415, 0.8415], [0.8415, 0.8415, 0.8415]]) tensor([[0.5403, 0.5403, 0.5403], [0.5403, 0.5403, 0.5403], [0.5403, 0.5403, 0.5403]]) There are limitations to cond that one should be aware of: The predicate (i.e. x.sum() \u003e 0) must result in a boolean or a single-element tensor. The operands (i.e. [x]) must be tensors. The branch function (i.e. true_fn and false_fn) signature must match with the operands and they must both return a single tensor with the same metadata (for example, dtype, shape, etc.). Branch functions cannot mutate input or global variables. Branch functions cannot access closure variables, except for self if the function is defined in the scope of a method. For more details about cond, check out the cond documentation. We can also use map, which applies a function across the first dimension of the first tensor argument. from torch._higher_order_ops.map import map as torch_map class MapModule(torch.nn.Module): def forward(self, xs, y, z): def body(x, y, z): return x + y + z return torch_map(body, xs, y, z) inps = (torch.ones(6, 4), torch.tensor(5), torch.tensor(4)) exported_map_example = export(MapModule(), inps) print(exported_map_example) print(exported_map_example.module()(*inps)) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, xs: \"f32[6, 4]\", y: \"i64[]\", z: \"i64[]\"): # File: \u003ceval_with_key\u003e.58:9 in forward, code: map_impl = torch.ops.higher_order.map_impl(map_body_0, [l_flat_xs_0_], [l_flat_args_0_, l_flat_args_1_]); map_body_0 = l_flat_xs_0_ = l_flat_args_0_ = l_flat_args_1_ = None body_graph_0 = self.body_graph_0 map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y, z]); body_graph_0 = xs = y = z = None getitem: \"f32[6, 4]\" = map_impl[0]; map_impl = None return (getitem,) class body_graph_0(torch.nn.Module): def forward(self, xs: \"f32[4]\", y: \"i64[]\", z: \"i64[]\"): # File: \u003ceval_with_key\u003e.56:5 in forward, code: add = child.add(l_flat_args_0_); child = l_flat_args_0_ = None add: \"f32[4]\" = torch.ops.aten.add.Tensor(xs, y); xs = y = None # File: \u003ceval_with_key\u003e.56:6 in forward, code: add_1 = add.add(l_flat_args_1_); add = l_flat_args_1_ = None add_1: \"f32[4]\" = torch.ops.aten.add.Tensor(add, z); add = z = None return (add_1,) Graph signature: # inputs xs: USER_INPUT y: USER_INPUT z: USER_INPUT # outputs getitem: USER_OUTPUT Range constraints: {} tensor([[10., 10., 10., 10.], [10., 10., 10., 10.], [10., 10., 10., 10.], [10., 10., 10., 10.], [10., 10., 10., 10.], [10., 10., 10., 10.]]) Other control flow ops include while_loop, associative_scan, and scan. For more documentation on each operator, please refer to this page. Constraints/Dynamic Shapes# This section covers dynamic behavior and representation of exported programs. Dynamic behavior is subjective to the particular model being exported, so for the most part of this tutorial, we\u2019ll focus on this particular toy model (with the resulting tensor shapes annotated): class DynamicModel(torch.nn.Module): def __init__(self): super().__init__() self.l = torch.nn.Linear(5, 3) def forward( self, w: torch.Tensor, # [6, 5] x: torch.Tensor, # [4] y: torch.Tensor, # [8, 4] z: torch.Tensor, # [32] ): x0 = x + y # [8, 4] x1 = self.l(w) # [6, 3] x2 = x0.flatten() # [32] x3 = x2 + z # [32] return x1, x3 By default, torch.export produces a static program. One consequence of this is that at runtime, the program won\u2019t work on inputs with different shapes, even if they\u2019re valid in eager mode. w = torch.randn(6, 5) x = torch.randn(4) y = torch.randn(8, 4) z = torch.randn(32) model = DynamicModel() ep = export(model, (w, x, y, z)) model(w, x, torch.randn(3, 4), torch.randn(12)) try: ep.module()(w, x, torch.randn(3, 4), torch.randn(12)) except Exception: tb.print_exc() Traceback (most recent call last): File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 286, in \u003cmodule\u003e ep.module()(w, x, torch.randn(3, 4), torch.randn(12)) File \"/opt/conda/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 848, in call_wrapped return self._wrapped_call(self, *args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 424, in __call__ raise e File \"/opt/conda/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 411, in __call__ return super(self.cls, obj).__call__(*args, **kwargs) # type: ignore[misc] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1879, in _call_impl return inner() ^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1806, in inner args_kwargs_result = hook(self, args, kwargs) # type: ignore[misc] ^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_unlift.py\", line 83, in _check_input_constraints_pre_hook _check_input_constraints_for_graph( File \"/opt/conda/lib/python3.11/site-packages/torch/_export/utils.py\", line 426, in _check_input_constraints_for_graph _check_symint( File \"/opt/conda/lib/python3.11/site-packages/torch/_export/utils.py\", line 390, in _check_symint raise RuntimeError( RuntimeError: Expected input at *args[2].shape[0] to be equal to 8, but got 3. If you meant for this dimension to be dynamic, please re-export and specify dynamic_shapes (e.g. with Dim.DYNAMIC) Basic concepts: symbols and guards# To enable dynamism, export() provides a dynamic_shapes argument. The easiest way to work with dynamic shapes is using Dim.AUTO and looking at the program that\u2019s returned. Dynamic behavior is specified at a input dimension-level; for each input we can specify a tuple of values: from torch.export.dynamic_shapes import Dim dynamic_shapes = { \"w\": (Dim.AUTO, Dim.AUTO), \"x\": (Dim.AUTO,), \"y\": (Dim.AUTO, Dim.AUTO), \"z\": (Dim.AUTO,), } ep = export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) Before we look at the program that\u2019s produced, let\u2019s understand what specifying dynamic_shapes entails, and how that interacts with export. For every input dimension where a Dim object is specified, a symbol is allocated, taking on a range of [2, inf] (why not [0, inf] or [1, inf]? we\u2019ll explain later in the 0/1 specialization section). Export then runs model tracing, looking at each operation that\u2019s performed by the model. Each individual operation can emit what\u2019s called \u201cguards\u201d; basically boolean condition that are required to be true for the program to be valid. When guards involve symbols allocated for input dimensions, the program contains restrictions on what input shapes are valid; i.e. the program\u2019s dynamic behavior. The symbolic shapes subsystem is the part responsible for taking in all the emitted guards and producing a final program representation that adheres to all of these guards. Before we see this \u201cfinal representation\u201d in an ExportedProgram, let\u2019s look at the guards emitted by the toy model we\u2019re tracing. Here, each forward input tensor is annotated with the symbol allocated at the start of tracing: class DynamicModel(torch.nn.Module): def __init__(self): super().__init__() self.l = torch.nn.Linear(5, 3) def forward( self, w: torch.Tensor, # [s0, s1] x: torch.Tensor, # [s2] y: torch.Tensor, # [s3, s4] z: torch.Tensor, # [s5] ): x0 = x + y # guard: s2 == s4 x1 = self.l(w) # guard: s1 == 5 x2 = x0.flatten() # no guard added here x3 = x2 + z # guard: s3 * s4 == s5 return x1, x3 Let\u2019s understand each of the operations and the emitted guards: x0 = x + y: This is an element-wise add with broadcasting, since x is a 1-d tensor and y a 2-d tensor. x is broadcasted along the last dimension of y, emitting the guard s2 == s4. x1 = self.l(w): Calling nn.Linear() performs a matrix multiplication with model parameters. In export, parameters, buffers, and constants are considered program state, which is considered static, and so this is a matmul between a dynamic input (w: [s0, s1]), and a statically-shaped tensor. This emits the guard s1 == 5. x2 = x0.flatten(): This call actually doesn\u2019t emit any guards! (at least none relevant to input shapes) x3 = x2 + z: x2 has shape [s3*s4] after flattening, and this element-wise add emits s3 * s4 == s5. Writing all of these guards down and summarizing is almost like a mathematical proof, which is what the symbolic shapes subsystem tries to do! In summary, we can conclude that the program must have the following input shapes to be valid: w: [s0, 5] x: [s2] y: [s3, s2] z: [s2*s3] And when we do finally print out the exported program to see our result, those shapes are what we see annotated on the corresponding inputs: print(ep) ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, p_l_weight: \"f32[3, 5]\", p_l_bias: \"f32[3]\", w: \"f32[s15, 5]\", x: \"f32[s77]\", y: \"f32[s17, s77]\", z: \"f32[s17*s77]\"): # sym_size_int_1 = torch.ops.aten.sym_size.int(w, 1) sym_size_int_2: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0) sym_size_int_3: \"Sym(s17)\" = torch.ops.aten.sym_size.int(y, 0) sym_size_int_4: \"Sym(s77)\" = torch.ops.aten.sym_size.int(y, 1) sym_size_int_5: \"Sym(s17*s77)\" = torch.ops.aten.sym_size.int(z, 0) # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:268 in forward, code: x0 = x + y # [8, 4] add: \"f32[s17, s77]\" = torch.ops.aten.add.Tensor(x, y); x = y = None # eq: \"Sym(True)\" = sym_size_int_2 == sym_size_int_4; sym_size_int_4 = None _assert_scalar_default = torch.ops.aten._assert_scalar.default(eq, \"Runtime assertion failed for expression Eq(s77, s94) on node \u0027eq\u0027\"); eq = _assert_scalar_default = None eq_1 = sym_size_int_1 == 5; sym_size_int_1 = None _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(eq_1, \"Runtime assertion failed for expression Eq(s21, 5) on node \u0027eq_1\u0027\"); eq_1 = _assert_scalar_default_1 = None mul: \"Sym(s17*s77)\" = sym_size_int_3 * sym_size_int_2; sym_size_int_3 = sym_size_int_2 = None eq_2: \"Sym(True)\" = mul == sym_size_int_5; mul = sym_size_int_5 = None _assert_scalar_default_2 = torch.ops.aten._assert_scalar.default(eq_2, \"Runtime assertion failed for expression Eq(s17*s77, s68) on node \u0027eq_2\u0027\"); eq_2 = _assert_scalar_default_2 = None # File: /opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias) linear: \"f32[s15, 3]\" = torch.ops.aten.linear.default(w, p_l_weight, p_l_bias); w = p_l_weight = p_l_bias = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:270 in forward, code: x2 = x0.flatten() # [32] flatten: \"f32[s17*s77]\" = torch.ops.aten.flatten.using_ints(add); add = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:271 in forward, code: x3 = x2 + z # [32] add_1: \"f32[s17*s77]\" = torch.ops.aten.add.Tensor(flatten, z); flatten = z = None return (linear, add_1) Graph signature: # inputs p_l_weight: PARAMETER target=\u0027l.weight\u0027 p_l_bias: PARAMETER target=\u0027l.bias\u0027 w: USER_INPUT x: USER_INPUT y: USER_INPUT z: USER_INPUT # outputs linear: USER_OUTPUT add_1: USER_OUTPUT Range constraints: {s15: VR[2, int_oo], s77: VR[2, int_oo], s17: VR[2, int_oo], s17*s77: VR[4, int_oo]} Another feature to notice is the range_constraints field above, which contains a valid range for each symbol. This isn\u2019t so interesting currently, since this export call doesn\u2019t emit any guards related to symbol bounds and each base symbol has a generic bound, but this will come up later. So far, because we\u2019ve been exporting this toy model, this experience has not been representative of how hard it typically is to debug dynamic shapes guards \u0026 issues. In most cases it isn\u2019t obvious what guards are being emitted, and which operations and parts of user code are responsible. For this toy model we pinpoint the exact lines, and the guards are rather intuitive. In more complicated cases, a helpful first step is always to enable verbose logging. This can be done either with the environment variable TORCH_LOGS=\"+dynamic\", or interactively with torch._logging.set_logs(dynamic=10): torch._logging.set_logs(dynamic=10) ep = export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) I1004 00:19:47.195000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:19:47.196000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s15 = 6 for L[\u0027w\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s15\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:19:47.197000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s21 = 5 for L[\u0027w\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s21\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V1004 00:19:47.197000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] I1004 00:19:47.199000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s77 = 4 for L[\u0027x\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s77\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:19:47.200000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s17 = 8 for L[\u0027y\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s17\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:19:47.200000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s94 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s94\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:19:47.202000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s68 = 32 for L[\u0027z\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s68\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V1004 00:19:47.205000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.206000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.206000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.207000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.208000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.208000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.209000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.209000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.210000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.210000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.212000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] I1004 00:19:47.213000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s77, s94) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s77, s94)\" I1004 00:19:47.214000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s94 = s77 (solve) VR[2, int_oo] V1004 00:19:47.215000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] I1004 00:19:47.220000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s21, 5) [guard added] (_meta_registrations.py:2417 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s21, 5)\" V1004 00:19:47.220000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s21 = VR[5, 5] (update) I1004 00:19:47.221000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5] V1004 00:19:47.230000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval size_oblivious(Eq(s17*s77, 1)) == False [statically known] V1004 00:19:47.231000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] I1004 00:19:47.233000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s17*s77, s68) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s17*s77, s68)\" V1004 00:19:47.234000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s68 = VR[4, int_oo] (update) I1004 00:19:47.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s68 = s17*s77 (solve) VR[4, int_oo] I1004 00:19:47.239000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:19:47.239000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].size()[0] s15 None V1004 00:19:47.239000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].size()[1] 5 None V1004 00:19:47.239000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].stride()[0] 5 None V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].stride()[1] 1 None V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].storage_offset() 0 None V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[0] s77 None V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[0] 1 None V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].storage_offset() 0 None V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[0] s17 None V1004 00:19:47.240000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[1] s77 None V1004 00:19:47.241000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[0] s77 None V1004 00:19:47.241000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[1] 1 None V1004 00:19:47.241000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].storage_offset() 0 None V1004 00:19:47.241000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027z\u0027].size()[0] s17*s77 None V1004 00:19:47.241000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027z\u0027].stride()[0] 1 None V1004 00:19:47.241000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027z\u0027].storage_offset() 0 None V1004 00:19:47.249000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] V1004 00:19:47.254000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7461] eval 5 [trivial] This spits out quite a handful, even with this simple toy model. The log lines here have been cut short at front and end to ignore unnecessary info, but looking through the logs we can see the lines relevant to what we described above; e.g. the allocation of symbols: \"\"\" create_symbol s0 = 6 for L[\u0027w\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e) create_symbol s1 = 5 for L[\u0027w\u0027].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e) runtime_assert True == True [statically known] create_symbol s2 = 4 for L[\u0027x\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e) create_symbol s3 = 8 for L[\u0027y\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e) create_symbol s4 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e) create_symbol s5 = 32 for L[\u0027z\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e) \"\"\" \"\\ncreate_symbol s0 = 6 for L[\u0027w\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e)\\ncreate_symbol s1 = 5 for L[\u0027w\u0027].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e)\\nruntime_assert True == True [statically known]\\ncreate_symbol s2 = 4 for L[\u0027x\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e)\\ncreate_symbol s3 = 8 for L[\u0027y\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e)\\ncreate_symbol s4 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e)\\ncreate_symbol s5 = 32 for L[\u0027z\u0027].size()[0] [2, int_oo] (_dynamo/variables/builder.py:2841 in \u003clambda\u003e)\\n\" The lines with create_symbol show when a new symbol has been allocated, and the logs also identify the tensor variable names and dimensions they\u2019ve been allocated for. In other lines we can also see the guards emitted: \"\"\" runtime_assert Eq(s2, s4) [guard added] x0 = x + y # output shape: [8, 4] # dynamic_shapes_tutorial.py:16 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s2, s4)\" runtime_assert Eq(s1, 5) [guard added] x1 = self.l(w) # [6, 3] # dynamic_shapes_tutorial.py:17 in forward (_meta_registrations.py:2127 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s1, 5)\" runtime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z # [32] # dynamic_shapes_tutorial.py:19 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s2*s3, s5)\" \"\"\" \u0027\\nruntime_assert Eq(s2, s4) [guard added] x0 = x + y # output shape: [8, 4] # dynamic_shapes_tutorial.py:16 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s2, s4)\"\\nruntime_assert Eq(s1, 5) [guard added] x1 = self.l(w) # [6, 3] # dynamic_shapes_tutorial.py:17 in forward (_meta_registrations.py:2127 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s1, 5)\"\\nruntime_assert Eq(s2*s3, s5) [guard added] x3 = x2 + z # [32] # dynamic_shapes_tutorial.py:19 in forward (_subclasses/fake_impls.py:845 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s2*s3, s5)\"\\n\u0027 Next to the [guard added] messages, we also see the responsible user lines of code - luckily here the model is simple enough. In many real-world cases it\u2019s not so straightforward: high-level torch operations can have complicated fake-kernel implementations or operator decompositions that complicate where and what guards are emitted. In such cases the best way to dig deeper and investigate is to follow the logs\u2019 suggestion, and re-run with environment variable TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"...\", to further attribute the guard of interest. Dim.AUTO is just one of the available options for interacting with dynamic_shapes; as of writing this 2 other options are available: Dim.DYNAMIC, and Dim.STATIC. Dim.STATIC simply marks a dimension static, while Dim.DYNAMIC is similar to Dim.AUTO in all ways except one: it raises an error when specializing to a constant; this is designed to maintain dynamism. See for example what happens when a static guard is emitted on a dynamically-marked dimension: dynamic_shapes[\"w\"] = (Dim.AUTO, Dim.DYNAMIC) try: export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) except Exception: tb.print_exc() I1004 00:19:47.258000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:19:47.259000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s15 = 6 for L[\u0027w\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s15\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:19:47.259000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s21 = 5 for L[\u0027w\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s21\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V1004 00:19:47.260000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] I1004 00:19:47.261000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s77 = 4 for L[\u0027x\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s77\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:19:47.262000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s17 = 8 for L[\u0027y\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s17\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:19:47.262000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s94 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s94\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:19:47.264000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s68 = 32 for L[\u0027z\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s68\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V1004 00:19:47.267000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.268000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.268000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.269000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.269000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.270000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.271000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.271000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.272000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.272000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:19:47.274000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] I1004 00:19:47.275000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s77, s94) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s77, s94)\" I1004 00:19:47.276000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s94 = s77 (solve) VR[2, int_oo] V1004 00:19:47.277000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] I1004 00:19:47.281000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s21, 5) [guard added] (_meta_registrations.py:2417 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s21, 5)\" V1004 00:19:47.282000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s21 = VR[5, 5] (update) I1004 00:19:47.282000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5] V1004 00:19:47.292000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval size_oblivious(Eq(s17*s77, 1)) == False [statically known] V1004 00:19:47.293000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] I1004 00:19:47.295000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s17*s77, s68) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s17*s77, s68)\" V1004 00:19:47.296000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s68 = VR[4, int_oo] (update) I1004 00:19:47.297000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s68 = s17*s77 (solve) VR[4, int_oo] I1004 00:19:47.300000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:19:47.301000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].size()[0] s15 None V1004 00:19:47.301000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].size()[1] 5 RelaxedUnspecConstraint(warn_only=False) V1004 00:20:20.110000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].stride()[0] 5 None V1004 00:20:20.111000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].stride()[1] 1 None V1004 00:20:20.112000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].storage_offset() 0 None V1004 00:20:20.113000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[0] s77 None V1004 00:20:20.114000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[0] 1 None V1004 00:20:20.114000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].storage_offset() 0 None V1004 00:20:20.115000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[0] s17 None V1004 00:20:20.115000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[1] s77 None V1004 00:20:20.116000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[0] s77 None V1004 00:20:20.117000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[1] 1 None V1004 00:20:20.117000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].storage_offset() 0 None V1004 00:20:20.118000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027z\u0027].size()[0] s17*s77 None V1004 00:20:20.119000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027z\u0027].stride()[0] 1 None V1004 00:20:20.120000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027z\u0027].storage_offset() 0 None Traceback (most recent call last): File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1798, in _export_to_aten_ir_make_fx produce_guards_callback(gm) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1944, in _produce_guards_callback return produce_guards_and_solve_constraints( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 549, in produce_guards_and_solve_constraints raise constraint_violation_error File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 514, in produce_guards_and_solve_constraints shape_env.produce_guards( File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5200, in produce_guards return self.produce_guards_verbose(*args, **kwargs, langs=(\"python\",))[0].exprs ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5932, in produce_guards_verbose raise ConstraintViolationError( torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L[\u0027w\u0027].size()[1])! For more information, run with TORCH_LOGS=\"+dynamic\". - You marked L[\u0027w\u0027].size()[1] as dynamic but your code specialized it to be a constant (5). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. Framework stack: File \"/opt/conda/bin/sphinx-build\", line 7, in \u003cmodule\u003e sys.exit(main()) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 339, in main return make_main(argv) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 213, in make_main return make_mode.run_make_mode(argv[1:]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 181, in run_make_mode return make.run_generic_build(args[0]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 169, in run_generic_build return build_main(args + opts) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 293, in build_main app = Sphinx(args.sourcedir, args.confdir, args.outputdir, File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 272, in __init__ self._init_builder() File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 343, in _init_builder self.events.emit(\u0027builder-inited\u0027) File \"/opt/conda/lib/python3.11/site-packages/sphinx/events.py\", line 97, in emit results.append(listener.handler(self.app, *args)) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py\", line 757, in generate_gallery_rst ) = generate_dir_rst( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 606, in generate_dir_rst results = parallel( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 607, in \u003cgenexpr\u003e p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator File \"/workspace/tutorials-kr/conf.py\", line 86, in wrapper p.start() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 121, in start self._popen = self._Popen(self) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen return _default_context.get_context().Process._Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 281, in _Popen return Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__ self._launch(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 71, in _launch code = process_obj._bootstrap(parent_sentinel=child_r) File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap self.run() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run self._target(*self._args, **self._kwargs) File \"/workspace/tutorials-kr/conf.py\", line 74, in call_fn result = func(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1374, in generate_file_rst output_blocks, time_elapsed = execute_script( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1192, in execute_script execute_code_block( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1048, in execute_code_block is_last_expr, mem_max = _exec_and_get_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 876, in _exec_and_get_memory mem_max, _ = call_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1725, in _sg_call_memory_noop return 0.0, func() File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 794, in __call__ exec(self.code, self.fake_main.__dict__) File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 418, in \u003cmodule\u003e export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1770, in _export_to_aten_ir_make_fx gm, graph_signature = transform(_make_fx_helper)( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1900, in _aot_export_non_strict gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1685, in _make_fx_helper gm = make_fx( File \"??\", line 0, in _start File \"??\", line 0, in __libc_start_main File \"??\", line 0, in __libc_init_first File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 738, in Py_BytesMain File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 360, in pymain_run_file_obj File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 79, in _PyRun_AnyFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 440, in _PyRun_SimpleFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1657, in pyrun_file File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1762, in run_mod File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1741, in run_eval_code_obj File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in wrapped return make_fx_tracer.trace(f, *args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2250, in trace return self._trace_inner(f, *args) File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2221, in _trace_inner t = dispatch_trace( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_compile.py\", line 53, in inner return disable_fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4984, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 2790, in list___init___impl File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 966, in list_extend File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1254, in dispatch_trace graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1835, in trace res = super().trace(root, concrete_args) File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 850, in trace (self.create_arg(fn(*args)),), File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1312, in wrapped out = f(*tensors) # type:ignore[call-arg] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1589, in wrapped_fn return tuple(flat_fn(*args)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 5091, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn tree_out = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c\", line 1077, in builtin_exec_impl File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 906, in functional_call out = mod(*args[params_len:], **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1884, in forward tree_out = mod(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 269, in forward x1 = self.l(w) # [6, 3] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 125, in forward return F.linear(input, self.weight, self.bias) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1360, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1407, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 1051, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 950, in handler return torch._library.utils.handle_dispatch_mode( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py\", line 296, in handle_dispatch_mode return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1462, in __torch_dispatch__ return proxy_call(self, func, self.pre_dispatch, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 914, in proxy_call out = func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ return self._op(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__ return self.dispatch(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"python_nn_functions.cpp\", line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch return self._cached_dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"python_nn_functions.cpp\", line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1487, in _cached_dispatch_impl output = self._dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"python_nn_functions.cpp\", line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2601, in _dispatch_impl decomposition_table[func](*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"python_nn_functions.cpp\", line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*) File \"??\", line 0, in at::_ops::linear::call(at::Tensor const\u0026, at::Tensor const\u0026, std::optional\u003cat::Tensor\u003e const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_prims_common/wrappers.py\", line 309, in _fn result = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_decomp/decompositions.py\", line 90, in inner r = f(*tree_map(increase_prec, args), **tree_map(increase_prec, kwargs)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_decomp/decompositions.py\", line 1462, in addmm out = alpha * torch.mm(mat1, mat2) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"init.cpp\", line 0, in pybind11::cpp_function::initialize\u003ctorch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}, pybind11::object, pybind11::args const\u0026, pybind11::kwargs const\u0026\u003e(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}\u0026\u0026, pybind11::object (*)(pybind11::args const\u0026, pybind11::kwargs const\u0026))::{lambda(pybind11::detail::function_call\u0026)#1}::_FUN(pybind11::detail::function_call\u0026) File \"init.cpp\", line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}::operator()(pybind11::args const\u0026, pybind11::kwargs const\u0026) const File \"??\", line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, c10::Symbol, pybind11::args const\u0026, pybind11::kwargs const\u0026, bool, std::optional\u003cc10::DispatchKey\u003e) File \"??\", line 0, in torch::jit::invokeOperatorFromPython(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, pybind11::args const\u0026, pybind11::kwargs const\u0026, std::optional\u003cc10::DispatchKey\u003e) File \"register_c10_ops.cpp\", line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const [clone .isra.0] File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in void c10::BoxedKernel::make_boxed_function\u003c\u0026(anonymous namespace)::pythonTLSSnapshotFallback\u003e(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"RegisterCompositeImplicitAutograd_0.cpp\", line 0, in c10::impl::make_boxed_from_unboxed_functor\u003cc10::impl::detail::WrapFunctionIntoFunctor_\u003cc10::CompileTimeFunctionPointer\u003cat::Tensor (at::Tensor const\u0026, at::Tensor const\u0026, std::optional\u003cat::Tensor\u003e const\u0026), \u0026at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__linear\u003e, at::Tensor, c10::guts::typelist::typelist\u003cat::Tensor const\u0026, at::Tensor const\u0026, std::optional\u003cat::Tensor\u003e const\u0026\u003e \u003e, false\u003e::call(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"??\", line 0, in at::native::linear(at::Tensor const\u0026, at::Tensor const\u0026, std::optional\u003cat::Tensor\u003e const\u0026) File \"??\", line 0, in at::_ops::addmm::call(at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026) File \"\", line 0, in c10::impl::BoxedKernelWrapper\u003cat::Tensor (at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026), void\u003e::call(c10::BoxedKernel const\u0026, c10::OperatorHandle const\u0026, c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in void c10::BoxedKernel::make_boxed_function\u003c\u0026(anonymous namespace)::pythonTLSSnapshotFallback\u003e(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"VariableType_0.cpp\", line 0, in c10::impl::make_boxed_from_unboxed_functor\u003cc10::impl::detail::WrapFunctionIntoFunctor_\u003cc10::CompileTimeFunctionPointer\u003cat::Tensor (c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026), \u0026torch::autograd::VariableType::(anonymous namespace)::addmm\u003e, at::Tensor, c10::guts::typelist::typelist\u003cc10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026\u003e \u003e, false\u003e::call(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"VariableType_0.cpp\", line 0, in torch::autograd::VariableType::(anonymous namespace)::addmm(c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026) File \"??\", line 0, in at::_ops::addmm::redispatch(c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__ return self.dispatch(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch return self._cached_dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1487, in _cached_dispatch_impl output = self._dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2717, in _dispatch_impl r = func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ return self._op(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_prims_common/wrappers.py\", line 309, in _fn result = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"python_torch_functions_1.cpp\", line 0, in torch::autograd::THPVariable_mm(_object*, _object*, _object*) File \"??\", line 0, in at::_ops::mm::call(at::Tensor const\u0026, at::Tensor const\u0026) File \"\", line 0, in c10::impl::BoxedKernelWrapper\u003cat::Tensor (at::Tensor const\u0026, at::Tensor const\u0026), void\u003e::call(c10::BoxedKernel const\u0026, c10::OperatorHandle const\u0026, c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_meta_registrations.py\", line 2417, in meta_mm torch._check( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 1684, in _check _check_with(RuntimeError, cond, message) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 1647, in _check_with if expect_true(cond): File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"init.cpp\", line 0, in pybind11::cpp_function::initialize\u003ctorch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}, pybind11::object, pybind11::args const\u0026, pybind11::kwargs const\u0026\u003e(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}\u0026\u0026, pybind11::object (*)(pybind11::args const\u0026, pybind11::kwargs const\u0026))::{lambda(pybind11::detail::function_call\u0026)#1}::_FUN(pybind11::detail::function_call\u0026) File \"init.cpp\", line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}::operator()(pybind11::args const\u0026, pybind11::kwargs const\u0026) const File \"??\", line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, c10::Symbol, pybind11::args const\u0026, pybind11::kwargs const\u0026, bool, std::optional\u003cc10::DispatchKey\u003e) File \"??\", line 0, in torch::jit::invokeOperatorFromPython(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, pybind11::args const\u0026, pybind11::kwargs const\u0026, std::optional\u003cc10::DispatchKey\u003e) File \"register_c10_ops.cpp\", line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const [clone .isra.0] File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 1702, in expect_true return a.node.expect_true( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 559, in expect_true return self.shape_env.guard_or_defer_runtime_assert( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 1021, in bounded_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7730, in guard_or_defer_runtime_assert self._maybe_guard_rel(expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6868, in _maybe_guard_rel self._refine_ranges(expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7819, in _refine_ranges self._set_replacement( File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 1021, in bounded_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6768, in _set_replacement CapturedTraceback.extract(cpp=True) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py\", line 212, in extract torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp), File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"\", line 0, in pybind11::cpp_function::initialize\u003cstd::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v\u003e(std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*)(bool, bool, bool), pybind11::name const\u0026, pybind11::scope const\u0026, pybind11::sibling const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026)::{lambda(pybind11::detail::function_call\u0026)#3}::operator()(pybind11::detail::function_call\u0026) const File \"??\", line 0, in torch::CapturedTraceback::gather(bool, bool, bool) File \"??\", line 0, in torch::unwind::unwind() During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 418, in \u003cmodule\u003e export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 319, in export raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( ^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1164, in wrapper raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1164, in wrapper raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( ^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1800, in _export_to_aten_ir_make_fx raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e)) # noqa: B904 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ torch._dynamo.exc.UserError: Constraints violated (L[\u0027w\u0027].size()[1])! For more information, run with TORCH_LOGS=\"+dynamic\". - You marked L[\u0027w\u0027].size()[1] as dynamic but your code specialized it to be a constant (5). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. Framework stack: File \"/opt/conda/bin/sphinx-build\", line 7, in \u003cmodule\u003e sys.exit(main()) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 339, in main return make_main(argv) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 213, in make_main return make_mode.run_make_mode(argv[1:]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 181, in run_make_mode return make.run_generic_build(args[0]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 169, in run_generic_build return build_main(args + opts) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 293, in build_main app = Sphinx(args.sourcedir, args.confdir, args.outputdir, File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 272, in __init__ self._init_builder() File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 343, in _init_builder self.events.emit(\u0027builder-inited\u0027) File \"/opt/conda/lib/python3.11/site-packages/sphinx/events.py\", line 97, in emit results.append(listener.handler(self.app, *args)) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py\", line 757, in generate_gallery_rst ) = generate_dir_rst( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 606, in generate_dir_rst results = parallel( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 607, in \u003cgenexpr\u003e p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator File \"/workspace/tutorials-kr/conf.py\", line 86, in wrapper p.start() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 121, in start self._popen = self._Popen(self) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen return _default_context.get_context().Process._Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 281, in _Popen return Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__ self._launch(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 71, in _launch code = process_obj._bootstrap(parent_sentinel=child_r) File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap self.run() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run self._target(*self._args, **self._kwargs) File \"/workspace/tutorials-kr/conf.py\", line 74, in call_fn result = func(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1374, in generate_file_rst output_blocks, time_elapsed = execute_script( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1192, in execute_script execute_code_block( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1048, in execute_code_block is_last_expr, mem_max = _exec_and_get_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 876, in _exec_and_get_memory mem_max, _ = call_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1725, in _sg_call_memory_noop return 0.0, func() File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 794, in __call__ exec(self.code, self.fake_main.__dict__) File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 418, in \u003cmodule\u003e export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1770, in _export_to_aten_ir_make_fx gm, graph_signature = transform(_make_fx_helper)( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1900, in _aot_export_non_strict gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1685, in _make_fx_helper gm = make_fx( File \"??\", line 0, in _start File \"??\", line 0, in __libc_start_main File \"??\", line 0, in __libc_init_first File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 738, in Py_BytesMain File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 360, in pymain_run_file_obj File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 79, in _PyRun_AnyFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 440, in _PyRun_SimpleFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1657, in pyrun_file File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1762, in run_mod File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1741, in run_eval_code_obj File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in wrapped return make_fx_tracer.trace(f, *args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2250, in trace return self._trace_inner(f, *args) File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2221, in _trace_inner t = dispatch_trace( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_compile.py\", line 53, in inner return disable_fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4984, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 2790, in list___init___impl File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 966, in list_extend File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1254, in dispatch_trace graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1835, in trace res = super().trace(root, concrete_args) File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 850, in trace (self.create_arg(fn(*args)),), File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1312, in wrapped out = f(*tensors) # type:ignore[call-arg] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1589, in wrapped_fn return tuple(flat_fn(*args)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 5091, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn tree_out = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c\", line 1077, in builtin_exec_impl File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 906, in functional_call out = mod(*args[params_len:], **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1884, in forward tree_out = mod(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 269, in forward x1 = self.l(w) # [6, 3] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 125, in forward return F.linear(input, self.weight, self.bias) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1360, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1407, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 1051, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 950, in handler return torch._library.utils.handle_dispatch_mode( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py\", line 296, in handle_dispatch_mode return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1462, in __torch_dispatch__ return proxy_call(self, func, self.pre_dispatch, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 914, in proxy_call out = func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ return self._op(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__ return self.dispatch(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"python_nn_functions.cpp\", line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch return self._cached_dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"python_nn_functions.cpp\", line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1487, in _cached_dispatch_impl output = self._dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"python_nn_functions.cpp\", line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2601, in _dispatch_impl decomposition_table[func](*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"python_nn_functions.cpp\", line 0, in torch::autograd::THPVariable_linear(_object*, _object*, _object*) File \"??\", line 0, in at::_ops::linear::call(at::Tensor const\u0026, at::Tensor const\u0026, std::optional\u003cat::Tensor\u003e const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_prims_common/wrappers.py\", line 309, in _fn result = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_decomp/decompositions.py\", line 90, in inner r = f(*tree_map(increase_prec, args), **tree_map(increase_prec, kwargs)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_decomp/decompositions.py\", line 1462, in addmm out = alpha * torch.mm(mat1, mat2) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"init.cpp\", line 0, in pybind11::cpp_function::initialize\u003ctorch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}, pybind11::object, pybind11::args const\u0026, pybind11::kwargs const\u0026\u003e(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}\u0026\u0026, pybind11::object (*)(pybind11::args const\u0026, pybind11::kwargs const\u0026))::{lambda(pybind11::detail::function_call\u0026)#1}::_FUN(pybind11::detail::function_call\u0026) File \"init.cpp\", line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}::operator()(pybind11::args const\u0026, pybind11::kwargs const\u0026) const File \"??\", line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, c10::Symbol, pybind11::args const\u0026, pybind11::kwargs const\u0026, bool, std::optional\u003cc10::DispatchKey\u003e) File \"??\", line 0, in torch::jit::invokeOperatorFromPython(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, pybind11::args const\u0026, pybind11::kwargs const\u0026, std::optional\u003cc10::DispatchKey\u003e) File \"register_c10_ops.cpp\", line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const [clone .isra.0] File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in void c10::BoxedKernel::make_boxed_function\u003c\u0026(anonymous namespace)::pythonTLSSnapshotFallback\u003e(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"RegisterCompositeImplicitAutograd_0.cpp\", line 0, in c10::impl::make_boxed_from_unboxed_functor\u003cc10::impl::detail::WrapFunctionIntoFunctor_\u003cc10::CompileTimeFunctionPointer\u003cat::Tensor (at::Tensor const\u0026, at::Tensor const\u0026, std::optional\u003cat::Tensor\u003e const\u0026), \u0026at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeImplicitAutograd__linear\u003e, at::Tensor, c10::guts::typelist::typelist\u003cat::Tensor const\u0026, at::Tensor const\u0026, std::optional\u003cat::Tensor\u003e const\u0026\u003e \u003e, false\u003e::call(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"??\", line 0, in at::native::linear(at::Tensor const\u0026, at::Tensor const\u0026, std::optional\u003cat::Tensor\u003e const\u0026) File \"??\", line 0, in at::_ops::addmm::call(at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026) File \"\", line 0, in c10::impl::BoxedKernelWrapper\u003cat::Tensor (at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026), void\u003e::call(c10::BoxedKernel const\u0026, c10::OperatorHandle const\u0026, c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in void c10::BoxedKernel::make_boxed_function\u003c\u0026(anonymous namespace)::pythonTLSSnapshotFallback\u003e(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"VariableType_0.cpp\", line 0, in c10::impl::make_boxed_from_unboxed_functor\u003cc10::impl::detail::WrapFunctionIntoFunctor_\u003cc10::CompileTimeFunctionPointer\u003cat::Tensor (c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026), \u0026torch::autograd::VariableType::(anonymous namespace)::addmm\u003e, at::Tensor, c10::guts::typelist::typelist\u003cc10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026\u003e \u003e, false\u003e::call(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"VariableType_0.cpp\", line 0, in torch::autograd::VariableType::(anonymous namespace)::addmm(c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026) File \"??\", line 0, in at::_ops::addmm::redispatch(c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026, c10::Scalar const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__ return self.dispatch(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch return self._cached_dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1487, in _cached_dispatch_impl output = self._dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2717, in _dispatch_impl r = func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ return self._op(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_prims_common/wrappers.py\", line 309, in _fn result = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"python_torch_functions_1.cpp\", line 0, in torch::autograd::THPVariable_mm(_object*, _object*, _object*) File \"??\", line 0, in at::_ops::mm::call(at::Tensor const\u0026, at::Tensor const\u0026) File \"\", line 0, in c10::impl::BoxedKernelWrapper\u003cat::Tensor (at::Tensor const\u0026, at::Tensor const\u0026), void\u003e::call(c10::BoxedKernel const\u0026, c10::OperatorHandle const\u0026, c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_meta_registrations.py\", line 2417, in meta_mm torch._check( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 1684, in _check _check_with(RuntimeError, cond, message) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 1647, in _check_with if expect_true(cond): File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"init.cpp\", line 0, in pybind11::cpp_function::initialize\u003ctorch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}, pybind11::object, pybind11::args const\u0026, pybind11::kwargs const\u0026\u003e(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}\u0026\u0026, pybind11::object (*)(pybind11::args const\u0026, pybind11::kwargs const\u0026))::{lambda(pybind11::detail::function_call\u0026)#1}::_FUN(pybind11::detail::function_call\u0026) File \"init.cpp\", line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}::operator()(pybind11::args const\u0026, pybind11::kwargs const\u0026) const File \"??\", line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, c10::Symbol, pybind11::args const\u0026, pybind11::kwargs const\u0026, bool, std::optional\u003cc10::DispatchKey\u003e) File \"??\", line 0, in torch::jit::invokeOperatorFromPython(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, pybind11::args const\u0026, pybind11::kwargs const\u0026, std::optional\u003cc10::DispatchKey\u003e) File \"register_c10_ops.cpp\", line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const [clone .isra.0] File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 1702, in expect_true return a.node.expect_true( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 559, in expect_true return self.shape_env.guard_or_defer_runtime_assert( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 1021, in bounded_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7730, in guard_or_defer_runtime_assert self._maybe_guard_rel(expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6868, in _maybe_guard_rel self._refine_ranges(expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7819, in _refine_ranges self._set_replacement( File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 1021, in bounded_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6768, in _set_replacement CapturedTraceback.extract(cpp=True) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py\", line 212, in extract torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp), File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"\", line 0, in pybind11::cpp_function::initialize\u003cstd::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v\u003e(std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*)(bool, bool, bool), pybind11::name const\u0026, pybind11::scope const\u0026, pybind11::sibling const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026)::{lambda(pybind11::detail::function_call\u0026)#3}::operator()(pybind11::detail::function_call\u0026) const File \"??\", line 0, in torch::CapturedTraceback::gather(bool, bool, bool) File \"??\", line 0, in torch::unwind::unwind() The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`. Static guards also aren\u2019t always inherent to the model; they can also come from user specifications. In fact, a common pitfall leading to shape specializations is when the user specifies conflicting markers for equivalent dimensions; one dynamic and another static. The same error type is raised when this is the case for x.shape[0] and y.shape[1]: dynamic_shapes[\"w\"] = (Dim.AUTO, Dim.AUTO) dynamic_shapes[\"x\"] = (Dim.STATIC,) dynamic_shapes[\"y\"] = (Dim.AUTO, Dim.DYNAMIC) try: export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) except Exception: tb.print_exc() I1004 00:20:20.182000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:20.183000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s15 = 6 for L[\u0027w\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s15\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:20:20.184000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s21 = 5 for L[\u0027w\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s21\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V1004 00:20:20.184000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] I1004 00:20:20.186000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s17 = 8 for L[\u0027y\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s17\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:20:20.187000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s94 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s94\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:20:20.189000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s68 = 32 for L[\u0027z\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s68\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V1004 00:20:20.194000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.194000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.195000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.196000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.197000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.197000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.198000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.198000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] I1004 00:20:20.203000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s94, 4) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s94, 4)\" V1004 00:20:20.204000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s94 = VR[4, 4] (update) I1004 00:20:20.204000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s94 = 4 (range_refined_to_singleton) VR[4, 4] I1004 00:20:20.211000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s21, 5) [guard added] (_meta_registrations.py:2417 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s21, 5)\" V1004 00:20:20.211000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s21 = VR[5, 5] (update) I1004 00:20:20.212000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5] V1004 00:20:20.213000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] I1004 00:20:20.227000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(4*s17, s68) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(4*s17, s68)\" V1004 00:20:20.228000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s68 = VR[8, int_oo] (update) I1004 00:20:20.230000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s68 = 4*s17 (solve) VR[8, int_oo] I1004 00:20:20.234000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:20:20.234000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].size()[0] s15 None V1004 00:20:20.234000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].size()[1] 5 None V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].stride()[0] 5 None V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].stride()[1] 1 None V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].storage_offset() 0 None V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[0] 4 None V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[0] 1 None V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].storage_offset() 0 None V1004 00:20:20.235000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[0] s17 None V1004 00:20:20.236000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[1] 4 RelaxedUnspecConstraint(warn_only=False) V1004 00:20:20.251000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[0] 4 None V1004 00:20:20.252000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[1] 1 None V1004 00:20:20.252000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].storage_offset() 0 None V1004 00:20:20.252000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027z\u0027].size()[0] 4*s17 None V1004 00:20:20.253000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027z\u0027].stride()[0] 1 None V1004 00:20:20.253000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027z\u0027].storage_offset() 0 None Traceback (most recent call last): File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1798, in _export_to_aten_ir_make_fx produce_guards_callback(gm) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1944, in _produce_guards_callback return produce_guards_and_solve_constraints( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 549, in produce_guards_and_solve_constraints raise constraint_violation_error File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 514, in produce_guards_and_solve_constraints shape_env.produce_guards( File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5200, in produce_guards return self.produce_guards_verbose(*args, **kwargs, langs=(\"python\",))[0].exprs ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5932, in produce_guards_verbose raise ConstraintViolationError( torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L[\u0027y\u0027].size()[1])! For more information, run with TORCH_LOGS=\"+dynamic\". - You marked L[\u0027y\u0027].size()[1] as dynamic but your code specialized it to be a constant (4). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. Framework stack: File \"/opt/conda/bin/sphinx-build\", line 7, in \u003cmodule\u003e sys.exit(main()) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 339, in main return make_main(argv) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 213, in make_main return make_mode.run_make_mode(argv[1:]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 181, in run_make_mode return make.run_generic_build(args[0]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 169, in run_generic_build return build_main(args + opts) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 293, in build_main app = Sphinx(args.sourcedir, args.confdir, args.outputdir, File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 272, in __init__ self._init_builder() File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 343, in _init_builder self.events.emit(\u0027builder-inited\u0027) File \"/opt/conda/lib/python3.11/site-packages/sphinx/events.py\", line 97, in emit results.append(listener.handler(self.app, *args)) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py\", line 757, in generate_gallery_rst ) = generate_dir_rst( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 606, in generate_dir_rst results = parallel( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 607, in \u003cgenexpr\u003e p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator File \"/workspace/tutorials-kr/conf.py\", line 86, in wrapper p.start() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 121, in start self._popen = self._Popen(self) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen return _default_context.get_context().Process._Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 281, in _Popen return Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__ self._launch(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 71, in _launch code = process_obj._bootstrap(parent_sentinel=child_r) File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap self.run() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run self._target(*self._args, **self._kwargs) File \"/workspace/tutorials-kr/conf.py\", line 74, in call_fn result = func(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1374, in generate_file_rst output_blocks, time_elapsed = execute_script( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1192, in execute_script execute_code_block( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1048, in execute_code_block is_last_expr, mem_max = _exec_and_get_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 876, in _exec_and_get_memory mem_max, _ = call_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1725, in _sg_call_memory_noop return 0.0, func() File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 794, in __call__ exec(self.code, self.fake_main.__dict__) File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 431, in \u003cmodule\u003e export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1770, in _export_to_aten_ir_make_fx gm, graph_signature = transform(_make_fx_helper)( File \"??\", line 0, in _start File \"??\", line 0, in __libc_start_main File \"??\", line 0, in __libc_init_first File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 738, in Py_BytesMain File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 360, in pymain_run_file_obj File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 79, in _PyRun_AnyFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 440, in _PyRun_SimpleFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1657, in pyrun_file File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1762, in run_mod File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1741, in run_eval_code_obj File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1900, in _aot_export_non_strict gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1685, in _make_fx_helper gm = make_fx( File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in wrapped return make_fx_tracer.trace(f, *args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2250, in trace return self._trace_inner(f, *args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4984, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2221, in _trace_inner t = dispatch_trace( File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 2790, in list___init___impl File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 966, in list_extend File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_compile.py\", line 53, in inner return disable_fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1254, in dispatch_trace graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1835, in trace res = super().trace(root, concrete_args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 850, in trace (self.create_arg(fn(*args)),), File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1312, in wrapped out = f(*tensors) # type:ignore[call-arg] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 5091, in _PyEval_EvalFrameDefault File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e File \"/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c\", line 1077, in builtin_exec_impl File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1589, in wrapped_fn return tuple(flat_fn(*args)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn tree_out = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 906, in functional_call out = mod(*args[params_len:], **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1884, in forward tree_out = mod(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 268, in forward x0 = x + y # [8, 4] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1360, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1407, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 1051, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 950, in handler return torch._library.utils.handle_dispatch_mode( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py\", line 296, in handle_dispatch_mode return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1462, in __torch_dispatch__ return proxy_call(self, func, self.pre_dispatch, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 914, in proxy_call out = func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ return self._op(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 5548, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/abstract.c\", line 893, in binary_op1 File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7420, in slot_nb_add File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in _object* torch::autograd::TypeError_to_NotImplemented_\u003c\u0026torch::autograd::THPVariable_add\u003e(_object*, _object*, _object*) File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__ return self.dispatch(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch return self._cached_dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1487, in _cached_dispatch_impl output = self._dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"??\", line 0, in at::_ops::add_Tensor::call(at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2581, in _dispatch_impl return maybe_propagate_real_tensors(fast_impl(self, *args, **kwargs)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py\", line 962, in fast_binary_impl final_shape = infer_size(final_shape, shape) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py\", line 922, in infer_size torch._check( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 1684, in _check _check_with(RuntimeError, cond, message) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"init.cpp\", line 0, in pybind11::cpp_function::initialize\u003ctorch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}, pybind11::object, pybind11::args const\u0026, pybind11::kwargs const\u0026\u003e(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}\u0026\u0026, pybind11::object (*)(pybind11::args const\u0026, pybind11::kwargs const\u0026))::{lambda(pybind11::detail::function_call\u0026)#1}::_FUN(pybind11::detail::function_call\u0026) File \"init.cpp\", line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}::operator()(pybind11::args const\u0026, pybind11::kwargs const\u0026) const File \"??\", line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, c10::Symbol, pybind11::args const\u0026, pybind11::kwargs const\u0026, bool, std::optional\u003cc10::DispatchKey\u003e) File \"??\", line 0, in torch::jit::invokeOperatorFromPython(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, pybind11::args const\u0026, pybind11::kwargs const\u0026, std::optional\u003cc10::DispatchKey\u003e) File \"register_c10_ops.cpp\", line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const [clone .isra.0] File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in void c10::BoxedKernel::make_boxed_function\u003c\u0026(anonymous namespace)::pythonTLSSnapshotFallback\u003e(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"VariableType_2.cpp\", line 0, in c10::impl::make_boxed_from_unboxed_functor\u003cc10::impl::detail::WrapFunctionIntoFunctor_\u003cc10::CompileTimeFunctionPointer\u003cat::Tensor (c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026), \u0026torch::autograd::VariableType::(anonymous namespace)::add_Tensor\u003e, at::Tensor, c10::guts::typelist::typelist\u003cc10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026\u003e \u003e, false\u003e::call(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"VariableType_2.cpp\", line 0, in torch::autograd::VariableType::(anonymous namespace)::add_Tensor(c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026) File \"??\", line 0, in at::_ops::add_Tensor::redispatch(c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 1647, in _check_with if expect_true(cond): File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 1702, in expect_true return a.node.expect_true( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 559, in expect_true return self.shape_env.guard_or_defer_runtime_assert( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 1021, in bounded_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7730, in guard_or_defer_runtime_assert self._maybe_guard_rel(expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6868, in _maybe_guard_rel self._refine_ranges(expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7819, in _refine_ranges self._set_replacement( File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 1021, in bounded_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6768, in _set_replacement CapturedTraceback.extract(cpp=True) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py\", line 212, in extract torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp), File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"\", line 0, in pybind11::cpp_function::initialize\u003cstd::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v\u003e(std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*)(bool, bool, bool), pybind11::name const\u0026, pybind11::scope const\u0026, pybind11::sibling const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026)::{lambda(pybind11::detail::function_call\u0026)#3}::operator()(pybind11::detail::function_call\u0026) const File \"??\", line 0, in torch::CapturedTraceback::gather(bool, bool, bool) File \"??\", line 0, in torch::unwind::unwind() During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 431, in \u003cmodule\u003e export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 319, in export raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( ^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1164, in wrapper raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1164, in wrapper raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( ^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1800, in _export_to_aten_ir_make_fx raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e)) # noqa: B904 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ torch._dynamo.exc.UserError: Constraints violated (L[\u0027y\u0027].size()[1])! For more information, run with TORCH_LOGS=\"+dynamic\". - You marked L[\u0027y\u0027].size()[1] as dynamic but your code specialized it to be a constant (4). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. Framework stack: File \"/opt/conda/bin/sphinx-build\", line 7, in \u003cmodule\u003e sys.exit(main()) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 339, in main return make_main(argv) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 213, in make_main return make_mode.run_make_mode(argv[1:]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 181, in run_make_mode return make.run_generic_build(args[0]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 169, in run_generic_build return build_main(args + opts) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 293, in build_main app = Sphinx(args.sourcedir, args.confdir, args.outputdir, File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 272, in __init__ self._init_builder() File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 343, in _init_builder self.events.emit(\u0027builder-inited\u0027) File \"/opt/conda/lib/python3.11/site-packages/sphinx/events.py\", line 97, in emit results.append(listener.handler(self.app, *args)) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py\", line 757, in generate_gallery_rst ) = generate_dir_rst( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 606, in generate_dir_rst results = parallel( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 607, in \u003cgenexpr\u003e p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator File \"/workspace/tutorials-kr/conf.py\", line 86, in wrapper p.start() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 121, in start self._popen = self._Popen(self) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen return _default_context.get_context().Process._Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 281, in _Popen return Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__ self._launch(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 71, in _launch code = process_obj._bootstrap(parent_sentinel=child_r) File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap self.run() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run self._target(*self._args, **self._kwargs) File \"/workspace/tutorials-kr/conf.py\", line 74, in call_fn result = func(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1374, in generate_file_rst output_blocks, time_elapsed = execute_script( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1192, in execute_script execute_code_block( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1048, in execute_code_block is_last_expr, mem_max = _exec_and_get_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 876, in _exec_and_get_memory mem_max, _ = call_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1725, in _sg_call_memory_noop return 0.0, func() File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 794, in __call__ exec(self.code, self.fake_main.__dict__) File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 431, in \u003cmodule\u003e export(model, (w, x, y, z), dynamic_shapes=dynamic_shapes) File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1770, in _export_to_aten_ir_make_fx gm, graph_signature = transform(_make_fx_helper)( File \"??\", line 0, in _start File \"??\", line 0, in __libc_start_main File \"??\", line 0, in __libc_init_first File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 738, in Py_BytesMain File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 360, in pymain_run_file_obj File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 79, in _PyRun_AnyFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 440, in _PyRun_SimpleFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1657, in pyrun_file File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1762, in run_mod File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1741, in run_eval_code_obj File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1900, in _aot_export_non_strict gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1685, in _make_fx_helper gm = make_fx( File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in wrapped return make_fx_tracer.trace(f, *args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2250, in trace return self._trace_inner(f, *args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4984, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2221, in _trace_inner t = dispatch_trace( File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 2790, in list___init___impl File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 966, in list_extend File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_compile.py\", line 53, in inner return disable_fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1254, in dispatch_trace graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1835, in trace res = super().trace(root, concrete_args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 850, in trace (self.create_arg(fn(*args)),), File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1312, in wrapped out = f(*tensors) # type:ignore[call-arg] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 5091, in _PyEval_EvalFrameDefault File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e File \"/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c\", line 1077, in builtin_exec_impl File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1589, in wrapped_fn return tuple(flat_fn(*args)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn tree_out = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 906, in functional_call out = mod(*args[params_len:], **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1884, in forward tree_out = mod(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 268, in forward x0 = x + y # [8, 4] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1360, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1407, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 1051, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 950, in handler return torch._library.utils.handle_dispatch_mode( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py\", line 296, in handle_dispatch_mode return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1462, in __torch_dispatch__ return proxy_call(self, func, self.pre_dispatch, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 914, in proxy_call out = func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ return self._op(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 5548, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/abstract.c\", line 893, in binary_op1 File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7420, in slot_nb_add File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in _object* torch::autograd::TypeError_to_NotImplemented_\u003c\u0026torch::autograd::THPVariable_add\u003e(_object*, _object*, _object*) File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__ return self.dispatch(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch return self._cached_dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1487, in _cached_dispatch_impl output = self._dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"??\", line 0, in at::_ops::add_Tensor::call(at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2581, in _dispatch_impl return maybe_propagate_real_tensors(fast_impl(self, *args, **kwargs)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py\", line 962, in fast_binary_impl final_shape = infer_size(final_shape, shape) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py\", line 922, in infer_size torch._check( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 1684, in _check _check_with(RuntimeError, cond, message) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"init.cpp\", line 0, in pybind11::cpp_function::initialize\u003ctorch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}, pybind11::object, pybind11::args const\u0026, pybind11::kwargs const\u0026\u003e(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}\u0026\u0026, pybind11::object (*)(pybind11::args const\u0026, pybind11::kwargs const\u0026))::{lambda(pybind11::detail::function_call\u0026)#1}::_FUN(pybind11::detail::function_call\u0026) File \"init.cpp\", line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}::operator()(pybind11::args const\u0026, pybind11::kwargs const\u0026) const File \"??\", line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, c10::Symbol, pybind11::args const\u0026, pybind11::kwargs const\u0026, bool, std::optional\u003cc10::DispatchKey\u003e) File \"??\", line 0, in torch::jit::invokeOperatorFromPython(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, pybind11::args const\u0026, pybind11::kwargs const\u0026, std::optional\u003cc10::DispatchKey\u003e) File \"register_c10_ops.cpp\", line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const [clone .isra.0] File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in void c10::BoxedKernel::make_boxed_function\u003c\u0026(anonymous namespace)::pythonTLSSnapshotFallback\u003e(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"VariableType_2.cpp\", line 0, in c10::impl::make_boxed_from_unboxed_functor\u003cc10::impl::detail::WrapFunctionIntoFunctor_\u003cc10::CompileTimeFunctionPointer\u003cat::Tensor (c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026), \u0026torch::autograd::VariableType::(anonymous namespace)::add_Tensor\u003e, at::Tensor, c10::guts::typelist::typelist\u003cc10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026\u003e \u003e, false\u003e::call(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"VariableType_2.cpp\", line 0, in torch::autograd::VariableType::(anonymous namespace)::add_Tensor(c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026) File \"??\", line 0, in at::_ops::add_Tensor::redispatch(c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 1647, in _check_with if expect_true(cond): File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 1702, in expect_true return a.node.expect_true( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 559, in expect_true return self.shape_env.guard_or_defer_runtime_assert( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 1021, in bounded_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7730, in guard_or_defer_runtime_assert self._maybe_guard_rel(expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6868, in _maybe_guard_rel self._refine_ranges(expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7819, in _refine_ranges self._set_replacement( File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 1021, in bounded_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6768, in _set_replacement CapturedTraceback.extract(cpp=True) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py\", line 212, in extract torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp), File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"\", line 0, in pybind11::cpp_function::initialize\u003cstd::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v\u003e(std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*)(bool, bool, bool), pybind11::name const\u0026, pybind11::scope const\u0026, pybind11::sibling const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026)::{lambda(pybind11::detail::function_call\u0026)#3}::operator()(pybind11::detail::function_call\u0026) const File \"??\", line 0, in torch::CapturedTraceback::gather(bool, bool, bool) File \"??\", line 0, in torch::unwind::unwind() The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`. Here you might ask why export \u201cspecializes\u201d, i.e. why we resolve this static/dynamic conflict by going with the static route. The answer is because of the symbolic shapes system described above, of symbols and guards. When x.shape[0] is marked static, we don\u2019t allocate a symbol, and compile treating this shape as a concrete integer 4. A symbol is allocated for y.shape[1], and so we finally emit the guard s3 == 4, leading to specialization. One feature of export is that during tracing, statements like asserts, torch._check(), and if/else conditions will also emit guards. See what happens when we augment the existing model with such statements: class DynamicModel(torch.nn.Module): def __init__(self): super().__init__() self.l = torch.nn.Linear(5, 3) def forward(self, w, x, y, z): assert w.shape[0] \u003c= 512 torch._check(x.shape[0] \u003e= 4) if w.shape[0] == x.shape[0] + 2: x0 = x + y x1 = self.l(w) x2 = x0.flatten() x3 = x2 + z return x1, x3 else: return w dynamic_shapes = { \"w\": (Dim.AUTO, Dim.AUTO), \"x\": (Dim.AUTO,), \"y\": (Dim.AUTO, Dim.AUTO), \"z\": (Dim.AUTO,), } try: ep = export(DynamicModel(), (w, x, y, z), dynamic_shapes=dynamic_shapes) except Exception: tb.print_exc() I1004 00:20:20.287000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:20.290000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s15 = 6 for L[\u0027w\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s15\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:20:20.290000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s21 = 5 for L[\u0027w\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s21\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V1004 00:20:20.291000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] I1004 00:20:20.292000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s77 = 4 for L[\u0027x\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s77\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:20:20.293000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s17 = 8 for L[\u0027y\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s17\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:20:20.294000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s94 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s94\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:20:20.295000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s68 = 32 for L[\u0027z\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s68\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V1004 00:20:20.299000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.299000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.300000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.301000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.301000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.302000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.302000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.303000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.304000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.304000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] I1004 00:20:20.314000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] eval s15 \u003c= 512 [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:450 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"s15 \u003c= 512\" V1004 00:20:20.315000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s15 = VR[2, 512] (update) I1004 00:20:20.320000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert s77 \u003e= 4 [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:451 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"s77 \u003e= 4\" V1004 00:20:20.321000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s77 = VR[4, int_oo] (update) I1004 00:20:20.326000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] eval Eq(s15, s77 + 2) [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:452 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s15, s77 + 2)\" V1004 00:20:20.328000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s15 = VR[6, 512] (update) V1004 00:20:20.330000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s77 = VR[4, 510] (update) I1004 00:20:20.331000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s15 = s77 + 2 (solve) VR[6, 512] V1004 00:20:20.333000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] I1004 00:20:20.335000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s77, s94) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s77, s94)\" V1004 00:20:20.336000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s94 = VR[4, 510] (update) I1004 00:20:20.337000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s94 = s77 (solve) VR[4, 510] V1004 00:20:20.340000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] I1004 00:20:20.346000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s21, 5) [guard added] (_meta_registrations.py:2417 in meta_mm), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s21, 5)\" V1004 00:20:20.347000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s21 = VR[5, 5] (update) I1004 00:20:20.347000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s21 = 5 (range_refined_to_singleton) VR[5, 5] V1004 00:20:20.359000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval size_oblivious(Eq(s17*s77, 1)) == False [statically known] V1004 00:20:20.361000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] I1004 00:20:20.369000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s17*s77, s68) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s17*s77, s68)\" V1004 00:20:20.369000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s68 = VR[8, int_oo] (update) I1004 00:20:20.370000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s68 = s17*s77 (solve) VR[8, int_oo] I1004 00:20:20.375000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:20:20.375000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].size()[0] s77 + 2 None V1004 00:20:20.375000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].size()[1] 5 None V1004 00:20:20.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].stride()[0] 5 None V1004 00:20:20.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].stride()[1] 1 None V1004 00:20:20.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027w\u0027].storage_offset() 0 None V1004 00:20:20.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[0] s77 None V1004 00:20:20.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[0] 1 None V1004 00:20:20.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].storage_offset() 0 None V1004 00:20:20.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[0] s17 None V1004 00:20:20.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[1] s77 None V1004 00:20:20.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[0] s77 None V1004 00:20:20.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[1] 1 None V1004 00:20:20.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].storage_offset() 0 None V1004 00:20:20.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027z\u0027].size()[0] s17*s77 None V1004 00:20:20.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027z\u0027].stride()[0] 1 None V1004 00:20:20.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027z\u0027].storage_offset() 0 None V1004 00:20:20.388000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert s77 \u003e= 4 == True [statically known] V1004 00:20:20.389000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] V1004 00:20:20.394000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7461] eval 5 [trivial] Each of these statements emits an additional guard, and the exported program shows the changes; s0 is eliminated in favor of s2 + 2, and s2 now contains lower and upper bounds, reflected in range_constraints. For the if/else condition, you might ask why the True branch was taken, and why it wasn\u2019t the w.shape[0] != x.shape[0] + 2 guard that got emitted from tracing. The answer is that export is guided by the sample inputs provided by tracing, and specializes on the branches taken. If different sample input shapes were provided that fail the if condition, export would trace and emit guards corresponding to the else branch. Additionally, you might ask why we traced only the if branch, and if it\u2019s possible to maintain control-flow in your program and keep both branches alive. For that, refer to rewriting your model code following the Control Flow Ops section above. 0/1 specialization# Since we\u2019re talking about guards and specializations, it\u2019s a good time to talk about the 0/1 specialization issue we brought up earlier. The bottom line is that export will specialize on sample input dimensions with value 0 or 1, because these shapes have trace-time properties that don\u2019t generalize to other shapes. For example, size 1 tensors can broadcast while other sizes fail; and size 0 \u2026 . This just means that you should specify 0/1 sample inputs when you\u2019d like your program to hardcode them, and non-0/1 sample inputs when dynamic behavior is desirable. See what happens at runtime when we export this linear layer: ep = export( torch.nn.Linear(4, 3), (torch.randn(1, 4),), dynamic_shapes={ \"input\": (Dim.AUTO, Dim.STATIC), }, ) try: ep.module()(torch.randn(2, 4)) except Exception: tb.print_exc() I1004 00:20:20.399000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:20.408000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:20:20.409000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027input\u0027].size()[0] 1 None V1004 00:20:20.409000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027input\u0027].size()[1] 4 None V1004 00:20:20.409000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027input\u0027].stride()[0] 4 None V1004 00:20:20.409000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027input\u0027].stride()[1] 1 None V1004 00:20:20.409000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027input\u0027].storage_offset() 0 None W1004 00:20:20.411000 3670787 site-packages/torch/_export/non_strict_utils.py:580] dimension inputs[\u0027input\u0027].shape[0] 0/1 specialized; Dim.AUTO was specified along with a sample input with hint = 1. Traceback (most recent call last): File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 500, in \u003cmodule\u003e ep.module()(torch.randn(2, 4)) File \"/opt/conda/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 848, in call_wrapped return self._wrapped_call(self, *args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 424, in __call__ raise e File \"/opt/conda/lib/python3.11/site-packages/torch/fx/graph_module.py\", line 411, in __call__ return super(self.cls, obj).__call__(*args, **kwargs) # type: ignore[misc] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1879, in _call_impl return inner() ^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1806, in inner args_kwargs_result = hook(self, args, kwargs) # type: ignore[misc] ^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_unlift.py\", line 83, in _check_input_constraints_pre_hook _check_input_constraints_for_graph( File \"/opt/conda/lib/python3.11/site-packages/torch/_export/utils.py\", line 426, in _check_input_constraints_for_graph _check_symint( File \"/opt/conda/lib/python3.11/site-packages/torch/_export/utils.py\", line 390, in _check_symint raise RuntimeError( RuntimeError: Expected input at *args[0].shape[0] to be equal to 1, but got 2. If you meant for this dimension to be dynamic, please re-export and specify dynamic_shapes (e.g. with Dim.DYNAMIC) Named Dims# So far we\u2019ve only been talking about 3 ways to specify dynamic shapes: Dim.AUTO, Dim.DYNAMIC, and Dim.STATIC. The attraction of these is the low-friction user experience; all the guards emitted during model tracing are adhered to, and dynamic behavior like min/max ranges, relations, and static/dynamic dimensions are automatically figured out underneath export. The dynamic shapes subsystem essentially acts as a \u201cdiscovery\u201d process, summarizing these guards and presenting what export believes is the overall dynamic behavior of the program. The drawback of this design appears once the user has stronger expectations or beliefs about the dynamic behavior of these models - maybe there is a strong desire on dynamism and specializations on particular dimensions are to be avoided at all costs, or maybe we just want to catch changes in dynamic behavior with changes to the original model code, or possibly underlying decompositions or meta-kernels. These changes won\u2019t be detected and the export() call will most likely succeed, unless tests are in place that check the resulting ExportedProgram representation. For such cases, our stance is to recommend the \u201ctraditional\u201d way of specifying dynamic shapes, which longer-term users of export might be familiar with: named Dims: dx = Dim(\"dx\", min=4, max=256) dh = Dim(\"dh\", max=512) dynamic_shapes = { \"x\": (dx, None), \"y\": (2 * dx, dh), } This style of dynamic shapes allows the user to specify what symbols are allocated for input dimensions, min/max bounds on those symbols, and places restrictions on the dynamic behavior of the ExportedProgram produced; ConstraintViolation errors will be raised if model tracing emits guards that conflict with the relations or static/dynamic specifications given. For example, in the above specification, the following is asserted: x.shape[0] is to have range [4, 256], and related to y.shape[0] by y.shape[0] == 2 * x.shape[0]. x.shape[1] is static. y.shape[1] has range [2, 512], and is unrelated to any other dimension. In this design, we allow relations between dimensions to be specified with univariate linear expressions: A * dim + B can be specified for any dimension. This allows users to specify more complex constraints like integer divisibility for dynamic dimensions: dx = Dim(\"dx\", min=4, max=512) dynamic_shapes = { \"x\": (4 * dx, None) # x.shape[0] has range [16, 2048], and is divisible by 4. } Constraint violations, suggested fixes# One common issue with this specification style (before Dim.AUTO was introduced), is that the specification would often be mismatched with what was produced by model tracing. That would lead to ConstraintViolation errors and export suggested fixes - see for example with this model \u0026 specification, where the model inherently requires equality between dimensions 0 of x and y, and requires dimension 1 to be static. class Foo(torch.nn.Module): def forward(self, x, y): w = x + y return w + torch.ones(4) dx, dy, d1 = torch.export.dims(\"dx\", \"dy\", \"d1\") try: ep = export( Foo(), (torch.randn(6, 4), torch.randn(6, 4)), dynamic_shapes={ \"x\": (dx, d1), \"y\": (dy, d1), }, ) except Exception: tb.print_exc() I1004 00:20:20.418000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:20.419000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s77 = 6 for L[\u0027x\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s77\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:20:20.421000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s27 = 4 for L[\u0027x\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s27\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V1004 00:20:20.422000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] I1004 00:20:20.424000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s17 = 6 for L[\u0027y\u0027].size()[0] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s17\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" I1004 00:20:20.424000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5110] create_symbol s94 = 4 for L[\u0027y\u0027].size()[1] [2, int_oo] (_export/non_strict_utils.py:229 in fakify), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s94\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\" V1004 00:20:20.429000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.430000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.431000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.432000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.433000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.433000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] V1004 00:20:20.435000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] I1004 00:20:20.437000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s27, s94) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s27, s94)\" I1004 00:20:20.438000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s94 = s27 (solve) VR[2, int_oo] I1004 00:20:20.439000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s77, s17) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s77, s17)\" I1004 00:20:20.440000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s77 = s17 (solve) VR[2, int_oo] V1004 00:20:20.442000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == False [statically known] I1004 00:20:20.451000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert Eq(s27, 4) [guard added] (_subclasses/fake_impls.py:922 in infer_size), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"Eq(s27, 4)\" V1004 00:20:20.451000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s27 = VR[4, 4] (update) I1004 00:20:20.452000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s27 = 4 (range_refined_to_singleton) VR[4, 4] I1004 00:20:20.456000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:20:20.456000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range s94 = VR[4, 4] (update) I1004 00:20:20.457000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6776] set_replacement s94 = 4 (find) VR[4, 4] V1004 00:20:20.457000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[0] s17 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo]) V1004 00:20:20.457000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[1] 4 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo]) V1004 00:20:20.459000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[0] 4 None V1004 00:20:20.459000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[1] 1 None V1004 00:20:20.460000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].storage_offset() 0 None V1004 00:20:20.460000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[0] s17 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo]) V1004 00:20:20.460000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[1] 4 StrictMinMaxConstraint(warn_only=False, vr=VR[0, int_oo]) V1004 00:20:20.464000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[0] 4 None V1004 00:20:20.464000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[1] 1 None V1004 00:20:20.464000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].storage_offset() 0 None Traceback (most recent call last): File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1798, in _export_to_aten_ir_make_fx produce_guards_callback(gm) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1944, in _produce_guards_callback return produce_guards_and_solve_constraints( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 549, in produce_guards_and_solve_constraints raise constraint_violation_error File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 514, in produce_guards_and_solve_constraints shape_env.produce_guards( File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5200, in produce_guards return self.produce_guards_verbose(*args, **kwargs, langs=(\"python\",))[0].exprs ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5932, in produce_guards_verbose raise ConstraintViolationError( torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS=\"+dynamic\". - You marked d1 as dynamic but your code specialized it to be a constant (4). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. Framework stack: File \"/opt/conda/bin/sphinx-build\", line 7, in \u003cmodule\u003e sys.exit(main()) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 339, in main return make_main(argv) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 213, in make_main return make_mode.run_make_mode(argv[1:]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 181, in run_make_mode return make.run_generic_build(args[0]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 169, in run_generic_build return build_main(args + opts) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 293, in build_main app = Sphinx(args.sourcedir, args.confdir, args.outputdir, File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 272, in __init__ self._init_builder() File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 343, in _init_builder self.events.emit(\u0027builder-inited\u0027) File \"/opt/conda/lib/python3.11/site-packages/sphinx/events.py\", line 97, in emit results.append(listener.handler(self.app, *args)) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py\", line 757, in generate_gallery_rst ) = generate_dir_rst( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 606, in generate_dir_rst results = parallel( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 607, in \u003cgenexpr\u003e p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator File \"/workspace/tutorials-kr/conf.py\", line 86, in wrapper p.start() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 121, in start self._popen = self._Popen(self) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen return _default_context.get_context().Process._Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 281, in _Popen return Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__ self._launch(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 71, in _launch code = process_obj._bootstrap(parent_sentinel=child_r) File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap self.run() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run self._target(*self._args, **self._kwargs) File \"/workspace/tutorials-kr/conf.py\", line 74, in call_fn result = func(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1374, in generate_file_rst output_blocks, time_elapsed = execute_script( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1192, in execute_script execute_code_block( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1048, in execute_code_block is_last_expr, mem_max = _exec_and_get_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 876, in _exec_and_get_memory mem_max, _ = call_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1725, in _sg_call_memory_noop return 0.0, func() File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 794, in __call__ exec(self.code, self.fake_main.__dict__) File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 557, in \u003cmodule\u003e ep = export( File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1770, in _export_to_aten_ir_make_fx gm, graph_signature = transform(_make_fx_helper)( File \"??\", line 0, in _start File \"??\", line 0, in __libc_start_main File \"??\", line 0, in __libc_init_first File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 738, in Py_BytesMain File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 360, in pymain_run_file_obj File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 79, in _PyRun_AnyFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 440, in _PyRun_SimpleFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1657, in pyrun_file File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1762, in run_mod File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1741, in run_eval_code_obj File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1900, in _aot_export_non_strict gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1685, in _make_fx_helper gm = make_fx( File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in wrapped return make_fx_tracer.trace(f, *args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2250, in trace return self._trace_inner(f, *args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4984, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2221, in _trace_inner t = dispatch_trace( File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 2790, in list___init___impl File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 966, in list_extend File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_compile.py\", line 53, in inner return disable_fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1254, in dispatch_trace graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1835, in trace res = super().trace(root, concrete_args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 850, in trace (self.create_arg(fn(*args)),), File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1312, in wrapped out = f(*tensors) # type:ignore[call-arg] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 5091, in _PyEval_EvalFrameDefault File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e File \"/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c\", line 1077, in builtin_exec_impl File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1589, in wrapped_fn return tuple(flat_fn(*args)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn tree_out = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 906, in functional_call out = mod(*args[params_len:], **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1884, in forward tree_out = mod(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 553, in forward return w + torch.ones(4) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1360, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1407, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 1051, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 950, in handler return torch._library.utils.handle_dispatch_mode( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py\", line 296, in handle_dispatch_mode return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1462, in __torch_dispatch__ return proxy_call(self, func, self.pre_dispatch, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 914, in proxy_call out = func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ return self._op(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 5548, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/abstract.c\", line 893, in binary_op1 File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7420, in slot_nb_add File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in _object* torch::autograd::TypeError_to_NotImplemented_\u003c\u0026torch::autograd::THPVariable_add\u003e(_object*, _object*, _object*) File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__ return self.dispatch(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch return self._cached_dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1487, in _cached_dispatch_impl output = self._dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"??\", line 0, in at::_ops::add_Tensor::call(at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2581, in _dispatch_impl return maybe_propagate_real_tensors(fast_impl(self, *args, **kwargs)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py\", line 962, in fast_binary_impl final_shape = infer_size(final_shape, shape) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py\", line 922, in infer_size torch._check( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 1684, in _check _check_with(RuntimeError, cond, message) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"init.cpp\", line 0, in pybind11::cpp_function::initialize\u003ctorch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}, pybind11::object, pybind11::args const\u0026, pybind11::kwargs const\u0026\u003e(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}\u0026\u0026, pybind11::object (*)(pybind11::args const\u0026, pybind11::kwargs const\u0026))::{lambda(pybind11::detail::function_call\u0026)#1}::_FUN(pybind11::detail::function_call\u0026) File \"init.cpp\", line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}::operator()(pybind11::args const\u0026, pybind11::kwargs const\u0026) const File \"??\", line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, c10::Symbol, pybind11::args const\u0026, pybind11::kwargs const\u0026, bool, std::optional\u003cc10::DispatchKey\u003e) File \"??\", line 0, in torch::jit::invokeOperatorFromPython(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, pybind11::args const\u0026, pybind11::kwargs const\u0026, std::optional\u003cc10::DispatchKey\u003e) File \"register_c10_ops.cpp\", line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const [clone .isra.0] File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in void c10::BoxedKernel::make_boxed_function\u003c\u0026(anonymous namespace)::pythonTLSSnapshotFallback\u003e(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"VariableType_2.cpp\", line 0, in c10::impl::make_boxed_from_unboxed_functor\u003cc10::impl::detail::WrapFunctionIntoFunctor_\u003cc10::CompileTimeFunctionPointer\u003cat::Tensor (c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026), \u0026torch::autograd::VariableType::(anonymous namespace)::add_Tensor\u003e, at::Tensor, c10::guts::typelist::typelist\u003cc10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026\u003e \u003e, false\u003e::call(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"VariableType_2.cpp\", line 0, in torch::autograd::VariableType::(anonymous namespace)::add_Tensor(c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026) File \"??\", line 0, in at::_ops::add_Tensor::redispatch(c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 1647, in _check_with if expect_true(cond): File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 1702, in expect_true return a.node.expect_true( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 559, in expect_true return self.shape_env.guard_or_defer_runtime_assert( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 1021, in bounded_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7730, in guard_or_defer_runtime_assert self._maybe_guard_rel(expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6868, in _maybe_guard_rel self._refine_ranges(expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7819, in _refine_ranges self._set_replacement( File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 1021, in bounded_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6768, in _set_replacement CapturedTraceback.extract(cpp=True) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py\", line 212, in extract torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp), File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"\", line 0, in pybind11::cpp_function::initialize\u003cstd::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v\u003e(std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*)(bool, bool, bool), pybind11::name const\u0026, pybind11::scope const\u0026, pybind11::sibling const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026)::{lambda(pybind11::detail::function_call\u0026)#3}::operator()(pybind11::detail::function_call\u0026) const File \"??\", line 0, in torch::CapturedTraceback::gather(bool, bool, bool) File \"??\", line 0, in torch::unwind::unwind() - You marked d1 as dynamic but your code specialized it to be a constant (4). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. Framework stack: File \"/opt/conda/bin/sphinx-build\", line 7, in \u003cmodule\u003e sys.exit(main()) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 339, in main return make_main(argv) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 213, in make_main return make_mode.run_make_mode(argv[1:]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 181, in run_make_mode return make.run_generic_build(args[0]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 169, in run_generic_build return build_main(args + opts) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 293, in build_main app = Sphinx(args.sourcedir, args.confdir, args.outputdir, File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 272, in __init__ self._init_builder() File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 343, in _init_builder self.events.emit(\u0027builder-inited\u0027) File \"/opt/conda/lib/python3.11/site-packages/sphinx/events.py\", line 97, in emit results.append(listener.handler(self.app, *args)) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py\", line 757, in generate_gallery_rst ) = generate_dir_rst( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 606, in generate_dir_rst results = parallel( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 607, in \u003cgenexpr\u003e p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator File \"/workspace/tutorials-kr/conf.py\", line 86, in wrapper p.start() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 121, in start self._popen = self._Popen(self) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen return _default_context.get_context().Process._Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 281, in _Popen return Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__ self._launch(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 71, in _launch code = process_obj._bootstrap(parent_sentinel=child_r) File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap self.run() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run self._target(*self._args, **self._kwargs) File \"/workspace/tutorials-kr/conf.py\", line 74, in call_fn result = func(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1374, in generate_file_rst output_blocks, time_elapsed = execute_script( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1192, in execute_script execute_code_block( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1048, in execute_code_block is_last_expr, mem_max = _exec_and_get_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 876, in _exec_and_get_memory mem_max, _ = call_memory( File \"??\", line 0, in _start File \"??\", line 0, in __libc_start_main File \"??\", line 0, in __libc_init_first File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 738, in Py_BytesMain File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 360, in pymain_run_file_obj File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 79, in _PyRun_AnyFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 440, in _PyRun_SimpleFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1657, in pyrun_file File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1762, in run_mod File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1741, in run_eval_code_obj File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1725, in _sg_call_memory_noop return 0.0, func() File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 794, in __call__ exec(self.code, self.fake_main.__dict__) File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 557, in \u003cmodule\u003e ep = export( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4984, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 2790, in list___init___impl File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 966, in list_extend File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1798, in _export_to_aten_ir_make_fx produce_guards_callback(gm) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 5091, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1944, in _produce_guards_callback return produce_guards_and_solve_constraints( File \"/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c\", line 1077, in builtin_exec_impl File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 514, in produce_guards_and_solve_constraints shape_env.produce_guards( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5200, in produce_guards return self.produce_guards_verbose(*args, **kwargs, langs=(\"python\",))[0].exprs File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5405, in produce_guards_verbose expr1, expr2 = get_expression(src1), get_expression(src2) # type: ignore[] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5399, in get_expression return symint.node.expr File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 189, in expr return self.shape_env.replace(self._expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Objects/classobject.c\", line 59, in method_vectorcall File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 2539, in wrapper return fn_cache(self, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 3461, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6324, in replace r = self._find(s) File \"/usr/local/src/conda/python-3.11.13/Objects/object.c\", line 1368, in PyObject_GenericGetAttr File \"/usr/local/src/conda/python-3.11.13/Objects/object.c\", line 1278, in _PyObject_GenericGetAttrWithDict File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 2539, in wrapper return fn_cache(self, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 917, in infinite_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 917, in infinite_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6809, in _find self._set_replacement(a, replaced, \"find\") File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6768, in _set_replacement CapturedTraceback.extract(cpp=True) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py\", line 212, in extract torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp), File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"\", line 0, in pybind11::cpp_function::initialize\u003cstd::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v\u003e(std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*)(bool, bool, bool), pybind11::name const\u0026, pybind11::scope const\u0026, pybind11::sibling const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026)::{lambda(pybind11::detail::function_call\u0026)#3}::operator()(pybind11::detail::function_call\u0026) const File \"??\", line 0, in torch::CapturedTraceback::gather(bool, bool, bool) File \"??\", line 0, in torch::unwind::unwind() - The values of dy = L[\u0027y\u0027].size()[0] and dx = L[\u0027x\u0027].size()[0] must always be equal. Suggested fixes: d1 = 4 dy = dx During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 557, in \u003cmodule\u003e ep = export( ^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 319, in export raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( ^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1164, in wrapper raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1164, in wrapper raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( ^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1800, in _export_to_aten_ir_make_fx raise UserError(UserErrorType.CONSTRAINT_VIOLATION, str(e)) # noqa: B904 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ torch._dynamo.exc.UserError: Constraints violated (d1, dy)! For more information, run with TORCH_LOGS=\"+dynamic\". - You marked d1 as dynamic but your code specialized it to be a constant (4). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. Framework stack: File \"/opt/conda/bin/sphinx-build\", line 7, in \u003cmodule\u003e sys.exit(main()) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 339, in main return make_main(argv) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 213, in make_main return make_mode.run_make_mode(argv[1:]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 181, in run_make_mode return make.run_generic_build(args[0]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 169, in run_generic_build return build_main(args + opts) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 293, in build_main app = Sphinx(args.sourcedir, args.confdir, args.outputdir, File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 272, in __init__ self._init_builder() File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 343, in _init_builder self.events.emit(\u0027builder-inited\u0027) File \"/opt/conda/lib/python3.11/site-packages/sphinx/events.py\", line 97, in emit results.append(listener.handler(self.app, *args)) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py\", line 757, in generate_gallery_rst ) = generate_dir_rst( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 606, in generate_dir_rst results = parallel( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 607, in \u003cgenexpr\u003e p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator File \"/workspace/tutorials-kr/conf.py\", line 86, in wrapper p.start() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 121, in start self._popen = self._Popen(self) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen return _default_context.get_context().Process._Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 281, in _Popen return Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__ self._launch(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 71, in _launch code = process_obj._bootstrap(parent_sentinel=child_r) File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap self.run() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run self._target(*self._args, **self._kwargs) File \"/workspace/tutorials-kr/conf.py\", line 74, in call_fn result = func(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1374, in generate_file_rst output_blocks, time_elapsed = execute_script( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1192, in execute_script execute_code_block( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1048, in execute_code_block is_last_expr, mem_max = _exec_and_get_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 876, in _exec_and_get_memory mem_max, _ = call_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1725, in _sg_call_memory_noop return 0.0, func() File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 794, in __call__ exec(self.code, self.fake_main.__dict__) File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 557, in \u003cmodule\u003e ep = export( File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1770, in _export_to_aten_ir_make_fx gm, graph_signature = transform(_make_fx_helper)( File \"??\", line 0, in _start File \"??\", line 0, in __libc_start_main File \"??\", line 0, in __libc_init_first File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 738, in Py_BytesMain File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 360, in pymain_run_file_obj File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 79, in _PyRun_AnyFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 440, in _PyRun_SimpleFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1657, in pyrun_file File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1762, in run_mod File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1741, in run_eval_code_obj File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1900, in _aot_export_non_strict gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1685, in _make_fx_helper gm = make_fx( File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in wrapped return make_fx_tracer.trace(f, *args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2250, in trace return self._trace_inner(f, *args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4984, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2221, in _trace_inner t = dispatch_trace( File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 2790, in list___init___impl File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 966, in list_extend File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_compile.py\", line 53, in inner return disable_fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1254, in dispatch_trace graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1835, in trace res = super().trace(root, concrete_args) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 850, in trace (self.create_arg(fn(*args)),), File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1312, in wrapped out = f(*tensors) # type:ignore[call-arg] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 5091, in _PyEval_EvalFrameDefault File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e File \"/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c\", line 1077, in builtin_exec_impl File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1589, in wrapped_fn return tuple(flat_fn(*args)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn tree_out = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 906, in functional_call out = mod(*args[params_len:], **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1884, in forward tree_out = mod(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 553, in forward return w + torch.ones(4) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1360, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1407, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 1051, in __torch_function__ return func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 950, in handler return torch._library.utils.handle_dispatch_mode( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py\", line 296, in handle_dispatch_mode return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1462, in __torch_dispatch__ return proxy_call(self, func, self.pre_dispatch, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 914, in proxy_call out = func(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ return self._op(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 5548, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/abstract.c\", line 893, in binary_op1 File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7420, in slot_nb_add File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in _object* torch::autograd::TypeError_to_NotImplemented_\u003c\u0026torch::autograd::THPVariable_add\u003e(_object*, _object*, _object*) File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__ return self.dispatch(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch return self._cached_dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"\", line 0, in torch::handle_torch_function(torch::PythonArgs\u0026, _object*, _object*, _object*, _object*, char const*, char const*) File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1487, in _cached_dispatch_impl output = self._dispatch_impl(func, types, args, kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/descrobject.c\", line 364, in method_vectorcall_VARARGS_KEYWORDS File \"python_variable_methods.cpp\", line 0, in torch::autograd::THPVariable_add(_object*, _object*, _object*) File \"??\", line 0, in at::_ops::add_Tensor::call(at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2581, in _dispatch_impl return maybe_propagate_real_tensors(fast_impl(self, *args, **kwargs)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py\", line 962, in fast_binary_impl final_shape = infer_size(final_shape, shape) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py\", line 922, in infer_size torch._check( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 1684, in _check _check_with(RuntimeError, cond, message) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7321, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"init.cpp\", line 0, in pybind11::cpp_function::initialize\u003ctorch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}, pybind11::object, pybind11::args const\u0026, pybind11::kwargs const\u0026\u003e(torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}\u0026\u0026, pybind11::object (*)(pybind11::args const\u0026, pybind11::kwargs const\u0026))::{lambda(pybind11::detail::function_call\u0026)#1}::_FUN(pybind11::detail::function_call\u0026) File \"init.cpp\", line 0, in torch::jit::initJITBindings(_object*)::{lambda(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026)#2}::operator()(std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026, std::__cxx11::basic_string\u003cchar, std::char_traits\u003cchar\u003e, std::allocator\u003cchar\u003e \u003e const\u0026) const::{lambda(pybind11::args const\u0026, pybind11::kwargs const\u0026)#1}::operator()(pybind11::args const\u0026, pybind11::kwargs const\u0026) const File \"??\", line 0, in torch::jit::_get_operation_for_overload_or_packet(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, c10::Symbol, pybind11::args const\u0026, pybind11::kwargs const\u0026, bool, std::optional\u003cc10::DispatchKey\u003e) File \"??\", line 0, in torch::jit::invokeOperatorFromPython(std::vector\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e, std::allocator\u003cstd::shared_ptr\u003ctorch::jit::Operator\u003e \u003e \u003e const\u0026, pybind11::args const\u0026, pybind11::kwargs const\u0026, std::optional\u003cc10::DispatchKey\u003e) File \"register_c10_ops.cpp\", line 0, in c10::Dispatcher::callBoxed(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const [clone .isra.0] File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in void c10::BoxedKernel::make_boxed_function\u003c\u0026(anonymous namespace)::pythonTLSSnapshotFallback\u003e(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"VariableType_2.cpp\", line 0, in c10::impl::make_boxed_from_unboxed_functor\u003cc10::impl::detail::WrapFunctionIntoFunctor_\u003cc10::CompileTimeFunctionPointer\u003cat::Tensor (c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026), \u0026torch::autograd::VariableType::(anonymous namespace)::add_Tensor\u003e, at::Tensor, c10::guts::typelist::typelist\u003cc10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026\u003e \u003e, false\u003e::call(c10::OperatorKernel*, c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"VariableType_2.cpp\", line 0, in torch::autograd::VariableType::(anonymous namespace)::add_Tensor(c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026) File \"??\", line 0, in at::_ops::add_Tensor::redispatch(c10::DispatchKeySet, at::Tensor const\u0026, at::Tensor const\u0026, c10::Scalar const\u0026) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::python_dispatcher(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"\", line 0, in c10::OperatorHandle::callBoxedForDispatchKey(c10::DispatchKey, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e\u0026) const File \"PythonFallbackKernel.cpp\", line 0, in (anonymous namespace)::pythonFallback(c10::OperatorHandle const\u0026, c10::DispatchKeySet, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) File \"PyInterpreter.cpp\", line 0, in torch::detail::(anonymous namespace)::ConcretePyInterpreterVTable::dispatch(c10::OperatorHandle const\u0026, std::vector\u003cc10::IValue, std::allocator\u003cc10::IValue\u003e \u003e*) const File \"??\", line 0, in torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef\u003c_object*\u003e, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 627, in PyObject_CallMethod File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 1647, in _check_with if expect_true(cond): File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 1702, in expect_true return a.node.expect_true( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 559, in expect_true return self.shape_env.guard_or_defer_runtime_assert( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 1021, in bounded_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7730, in guard_or_defer_runtime_assert self._maybe_guard_rel(expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6868, in _maybe_guard_rel self._refine_ranges(expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7819, in _refine_ranges self._set_replacement( File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 1021, in bounded_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6768, in _set_replacement CapturedTraceback.extract(cpp=True) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py\", line 212, in extract torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp), File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"\", line 0, in pybind11::cpp_function::initialize\u003cstd::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v\u003e(std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*)(bool, bool, bool), pybind11::name const\u0026, pybind11::scope const\u0026, pybind11::sibling const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026)::{lambda(pybind11::detail::function_call\u0026)#3}::operator()(pybind11::detail::function_call\u0026) const File \"??\", line 0, in torch::CapturedTraceback::gather(bool, bool, bool) File \"??\", line 0, in torch::unwind::unwind() - You marked d1 as dynamic but your code specialized it to be a constant (4). If you\u0027re using mark_dynamic, either remove it or use maybe_mark_dynamic. If you\u0027re using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO. Framework stack: File \"/opt/conda/bin/sphinx-build\", line 7, in \u003cmodule\u003e sys.exit(main()) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 339, in main return make_main(argv) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 213, in make_main return make_mode.run_make_mode(argv[1:]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 181, in run_make_mode return make.run_generic_build(args[0]) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 169, in run_generic_build return build_main(args + opts) File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 293, in build_main app = Sphinx(args.sourcedir, args.confdir, args.outputdir, File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 272, in __init__ self._init_builder() File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 343, in _init_builder self.events.emit(\u0027builder-inited\u0027) File \"/opt/conda/lib/python3.11/site-packages/sphinx/events.py\", line 97, in emit results.append(listener.handler(self.app, *args)) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py\", line 757, in generate_gallery_rst ) = generate_dir_rst( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 606, in generate_dir_rst results = parallel( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 607, in \u003cgenexpr\u003e p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator File \"/workspace/tutorials-kr/conf.py\", line 86, in wrapper p.start() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 121, in start self._popen = self._Popen(self) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen return _default_context.get_context().Process._Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 281, in _Popen return Popen(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__ self._launch(process_obj) File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 71, in _launch code = process_obj._bootstrap(parent_sentinel=child_r) File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap self.run() File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run self._target(*self._args, **self._kwargs) File \"/workspace/tutorials-kr/conf.py\", line 74, in call_fn result = func(*args, **kwargs) File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1374, in generate_file_rst output_blocks, time_elapsed = execute_script( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1192, in execute_script execute_code_block( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1048, in execute_code_block is_last_expr, mem_max = _exec_and_get_memory( File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 876, in _exec_and_get_memory mem_max, _ = call_memory( File \"??\", line 0, in _start File \"??\", line 0, in __libc_start_main File \"??\", line 0, in __libc_init_first File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 738, in Py_BytesMain File \"/usr/local/src/conda/python-3.11.13/Modules/main.c\", line 360, in pymain_run_file_obj File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 79, in _PyRun_AnyFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 440, in _PyRun_SimpleFileObject File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1657, in pyrun_file File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1762, in run_mod File \"/usr/local/src/conda/python-3.11.13/Python/pythonrun.c\", line 1741, in run_eval_code_obj File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1725, in _sg_call_memory_noop return 0.0, func() File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 794, in __call__ exec(self.code, self.fake_main.__dict__) File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 557, in \u003cmodule\u003e ep = export( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4984, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 2790, in list___init___impl File \"/usr/local/src/conda/python-3.11.13/Objects/listobject.c\", line 966, in list_extend File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 1103, in type_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/typeobject.c\", line 7624, in slot_tp_call File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 482, in _PyObject_Call_Prepend File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 141, in _PyObject_FastCallDictTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1798, in _export_to_aten_ir_make_fx produce_guards_callback(gm) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 5091, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1944, in _produce_guards_callback return produce_guards_and_solve_constraints( File \"/usr/local/src/conda/python-3.11.13/Python/bltinmodule.c\", line 1077, in builtin_exec_impl File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 1148, in PyEval_EvalCode File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 514, in produce_guards_and_solve_constraints shape_env.produce_guards( File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5200, in produce_guards return self.produce_guards_verbose(*args, **kwargs, langs=(\"python\",))[0].exprs File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5405, in produce_guards_verbose expr1, expr2 = get_expression(src1), get_expression(src2) # type: ignore[] File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5399, in get_expression return symint.node.expr File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 189, in expr return self.shape_env.replace(self._expr) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 257, in _PyVectorcall_Call File \"/usr/local/src/conda/python-3.11.13/Objects/classobject.c\", line 59, in method_vectorcall File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 2539, in wrapper return fn_cache(self, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 3461, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6324, in replace r = self._find(s) File \"/usr/local/src/conda/python-3.11.13/Objects/object.c\", line 1368, in PyObject_GenericGetAttr File \"/usr/local/src/conda/python-3.11.13/Objects/object.c\", line 1278, in _PyObject_GenericGetAttrWithDict File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_call.h\", line 92, in _PyObject_VectorcallTstate File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 2539, in wrapper return fn_cache(self, *args, **kwargs) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 917, in infinite_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 343, in _PyObject_Call File \"/usr/local/src/conda/python-3.11.13/Modules/_functoolsmodule.c\", line 917, in infinite_lru_cache_wrapper File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6809, in _find self._set_replacement(a, replaced, \"find\") File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 7349, in do_call_core File \"/usr/local/src/conda/python-3.11.13/Include/internal/pycore_ceval.h\", line 73, in _PyEval_EvalFrame File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6768, in _set_replacement CapturedTraceback.extract(cpp=True) File \"/usr/local/src/conda/python-3.11.13/Python/ceval.c\", line 4769, in _PyEval_EvalFrameDefault File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_traceback.py\", line 212, in extract torch._C._profiler.gather_traceback(python=True, script=script, cpp=cpp), File \"/usr/local/src/conda/python-3.11.13/Objects/call.c\", line 214, in _PyObject_MakeTpCall File \"/usr/local/src/conda/python-3.11.13/Objects/methodobject.c\", line 542, in cfunction_call File \"\", line 0, in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) File \"\", line 0, in pybind11::cpp_function::initialize\u003cstd::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e, bool, bool, bool, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v\u003e(std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*\u0026)(bool, bool, bool), std::shared_ptr\u003ctorch::CapturedTraceback\u003e (*)(bool, bool, bool), pybind11::name const\u0026, pybind11::scope const\u0026, pybind11::sibling const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026, pybind11::arg_v const\u0026)::{lambda(pybind11::detail::function_call\u0026)#3}::operator()(pybind11::detail::function_call\u0026) const File \"??\", line 0, in torch::CapturedTraceback::gather(bool, bool, bool) File \"??\", line 0, in torch::unwind::unwind() - The values of dy = L[\u0027y\u0027].size()[0] and dx = L[\u0027x\u0027].size()[0] must always be equal. Suggested fixes: d1 = 4 dy = dx The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`. The expectation with suggested fixes is that the user can interactively copy-paste the changes into their dynamic shapes specification, and successfully export afterwards. Lastly, there\u2019s couple nice-to-knows about the options for specification: None is a good option for static behavior: - dynamic_shapes=None (default) exports with the entire model being static. - specifying None at an input-level exports with all tensor dimensions static, and is also required for non-tensor inputs. - specifying None at a dimension-level specializes that dimension, though this is deprecated in favor of Dim.STATIC. specifying per-dimension integer values also produces static behavior, and will additionally check that the provided sample input matches the specification. These options are combined in the inputs \u0026 dynamic shapes spec below: inputs = ( torch.randn(4, 4), torch.randn(3, 3), 16, False, ) dynamic_shapes = { \"tensor_0\": (Dim.AUTO, None), \"tensor_1\": None, \"int_val\": None, \"bool_val\": None, } Data-dependent errors# While trying to export models, you have may have encountered errors like \u201cCould not guard on data-dependent expression\u201d, or Could not extract specialized integer from data-dependent expression\u201d. These errors exist because torch.export() compiles programs using FakeTensors, which symbolically represent their real tensor counterparts. While these have equivalent symbolic properties (e.g. sizes, strides, dtypes), they diverge in that FakeTensors do not contain any data values. While this avoids unnecessary memory usage and expensive computation, it does mean that export may be unable to out-of-the-box compile parts of user code where compilation relies on data values. In short, if the compiler requires a concrete, data-dependent value in order to proceed, it will error out, complaining that the value is not available. Data-dependent values appear in many places, and common sources are calls like item(), tolist(), or torch.unbind() that extract scalar values from tensors. How are these values represented in the exported program? In the Constraints/Dynamic Shapes section, we talked about allocating symbols to represent dynamic input dimensions. The same happens here: we allocate symbols for every data-dependent value that appears in the program. The important distinction is that these are \u201cunbacked\u201d symbols, in contrast to the \u201cbacked\u201d symbols allocated for input dimensions. The \u201cbacked/unbacked\u201d nomenclature refers to the presence/absence of a \u201chint\u201d for the symbol: a concrete value backing the symbol, that can inform the compiler on how to proceed. In the input shape symbol case (backed symbols), these hints are simply the sample input shapes provided, which explains why control-flow branching is determined by the sample input properties. For data-dependent values, the symbols are taken from FakeTensor \u201cdata\u201d during tracing, and so the compiler doesn\u2019t know the actual values (hints) that these symbols would take on. Let\u2019s see how these show up in exported programs: class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() b = y.tolist() return b + [a] inps = ( torch.tensor(1), torch.tensor([2, 3]), ) ep = export(Foo(), inps) print(ep) I1004 00:20:20.495000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:20.501000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense) I1004 00:20:20.501000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u0] I1004 00:20:20.506000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u1 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense) I1004 00:20:20.506000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u1] I1004 00:20:20.507000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u2 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense) I1004 00:20:20.508000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u2] I1004 00:20:20.509000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:20:20.510000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].storage_offset() 0 None V1004 00:20:20.510000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[0] 2 None V1004 00:20:20.510000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[0] 1 None V1004 00:20:20.510000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].storage_offset() 0 None ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\", y: \"i64[2]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:618 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:619 in forward, code: b = y.tolist() unbind = torch.ops.aten.unbind.int(y); y = None getitem: \"i64[]\" = unbind[0] getitem_1: \"i64[]\" = unbind[1]; unbind = None item_1: \"Sym(u1)\" = torch.ops.aten.item.default(getitem); getitem = None item_2: \"Sym(u2)\" = torch.ops.aten.item.default(getitem_1); getitem_1 = None return (item_1, item_2, item) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs item_1: USER_OUTPUT item_2: USER_OUTPUT item: USER_OUTPUT Range constraints: {u0: VR[-int_oo, int_oo], u1: VR[-int_oo, int_oo], u2: VR[-int_oo, int_oo]} The result is that 3 unbacked symbols (notice they\u2019re prefixed with \u201cu\u201d, instead of the usual \u201cs\u201d for input shape/backed symbols) are allocated and returned: 1 for the item() call, and 1 for each of the elements of y with the tolist() call. Note from the range constraints field that these take on ranges of [-int_oo, int_oo], not the default [0, int_oo] range allocated to input shape symbols, since we have no information on what these values are - they don\u2019t represent sizes, so don\u2019t necessarily have positive values. Guards, torch._check()# But the case above is easy to export, because the concrete values of these symbols aren\u2019t used in any compiler decision-making; all that\u2019s relevant is that the return values are unbacked symbols. The data-dependent errors highlighted in this section are cases like the following, where data-dependent guards are encountered: class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() if a // 2 \u003e= 5: return y + 2 else: return y * 5 Here we actually need the \u201chint\u201d, or the concrete value of a for the compiler to decide whether to trace return y + 2 or return y * 5 as the output. Because we trace with FakeTensors, we don\u2019t know what a // 2 \u003e= 5 actually evaluates to, and export errors out with \u201cCould not guard on data-dependent expression u0 // 2 \u003e= 5 (unhinted)\u201d. So how do we export this toy model? Unlike torch.compile(), export requires full graph compilation, and we can\u2019t just graph break on this. Here are some basic options: Manual specialization: we could intervene by selecting the branch to trace, either by removing the control-flow code to contain only the specialized branch, or using torch.compiler.is_compiling() to guard what\u2019s traced at compile-time. torch.cond(): we could rewrite the control-flow code to use torch.cond() so we don\u2019t specialize on a branch. While these options are valid, they have their pitfalls. Option 1 sometimes requires drastic, invasive rewrites of the model code to specialize, and torch.cond() is not a comprehensive system for handling data-dependent errors. As we will see, there are data-dependent errors that do not involve control-flow. The generally recommended approach is to start with torch._check() calls. While these give the impression of purely being assert statements, they are in fact a system of informing the compiler on properties of symbols. While a torch._check() call does act as an assertion at runtime, when traced at compile-time, the checked expression is sent to the symbolic shapes subsystem for reasoning, and any symbol properties that follow from the expression being true, are stored as symbol properties (provided it\u2019s smart enough to infer those properties). So even if unbacked symbols don\u2019t have hints, if we\u2019re able to communicate properties that are generally true for these symbols via torch._check() calls, we can potentially bypass data-dependent guards without rewriting the offending model code. For example in the model above, inserting torch._check(a \u003e= 10) would tell the compiler that y + 2 can always be returned, and torch._check(a == 4) tells it to return y * 5. See what happens when we re-export this model. class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() torch._check(a \u003e= 10) torch._check(a \u003c= 60) if a // 2 \u003e= 5: return y + 2 else: return y * 5 inps = ( torch.tensor(32), torch.randn(4), ) ep = export(Foo(), inps) print(ep) I1004 00:20:20.515000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:20.519000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense) I1004 00:20:20.519000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u0] I1004 00:20:20.521000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert u0 \u003e= 10 [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:673 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"u0 \u003e= 10\" V1004 00:20:20.522000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range u0 = VR[10, int_oo] (update) I1004 00:20:20.527000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert u0 \u003c= 60 [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:674 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"u0 \u003c= 60\" V1004 00:20:20.528000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range u0 = VR[10, 60] (update) V1004 00:20:20.534000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == True [statically known] I1004 00:20:20.537000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:20:20.538000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].storage_offset() 0 None V1004 00:20:20.538000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[0] 4 None V1004 00:20:20.538000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[0] 1 None V1004 00:20:20.538000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].storage_offset() 0 None V1004 00:20:20.539000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 \u003e= 10 == True [statically known] V1004 00:20:20.540000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 \u003c= 60 == True [statically known] ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\", y: \"f32[4]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:672 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None ge_2: \"Sym(u0 \u003e= 10)\" = item \u003e= 10 _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_2, \"Runtime assertion failed for expression u0 \u003e= 10 on node \u0027ge_2\u0027\"); ge_2 = _assert_scalar_default = None le_1: \"Sym(u0 \u003c= 60)\" = item \u003c= 60; item = None _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 \u003c= 60 on node \u0027le_1\u0027\"); le_1 = _assert_scalar_default_1 = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:676 in forward, code: return y + 2 add: \"f32[4]\" = torch.ops.aten.add.Tensor(y, 2); y = None return (add,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {u0: VR[10, 60]} Export succeeds, and note from the range constraints field that u0 takes on a range of [10, 60]. So what information do torch._check() calls actually communicate? This varies as the symbolic shapes subsystem gets smarter, but at a fundamental level, these are generally true: Equality with non-data-dependent expressions: torch._check() calls that communicate equalities like u0 == s0 + 4 or u0 == 5. Range refinement: calls that provide lower or upper bounds for symbols, like the above. Some basic reasoning around more complicated expressions: inserting torch._check(a \u003c 4) will typically tell the compiler that a \u003e= 4 is false. Checks on complex expressions like torch._check(a ** 2 - 3 * a \u003c= 10) will typically get you past identical guards. As mentioned previously, torch._check() calls have applicability outside of data-dependent control flow. For example, here\u2019s a model where torch._check() insertion prevails while manual specialization \u0026 torch.cond() do not: class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() return y[a] inps = ( torch.tensor(32), torch.randn(60), ) try: export(Foo(), inps) except Exception: tb.print_exc() I1004 00:20:20.544000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:20.549000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense) I1004 00:20:20.549000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u0] V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] Data dependent variable \u0027u0\u0027 allocated at: V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/bin/sphinx-build\", line 7, in \u003cmodule\u003e V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] sys.exit(main()) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 339, in main V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return make_main(argv) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 213, in make_main V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return make_mode.run_make_mode(argv[1:]) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 181, in run_make_mode V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return make.run_generic_build(args[0]) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 169, in run_generic_build V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return build_main(args + opts) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 293, in build_main V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] app = Sphinx(args.sourcedir, args.confdir, args.outputdir, V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 272, in __init__ V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] self._init_builder() V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 343, in _init_builder V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] self.events.emit(\u0027builder-inited\u0027) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/events.py\", line 97, in emit V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] results.append(listener.handler(self.app, *args)) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py\", line 757, in generate_gallery_rst V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] ) = generate_dir_rst( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 606, in generate_dir_rst V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] results = parallel( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 607, in \u003cgenexpr\u003e V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/workspace/tutorials-kr/conf.py\", line 86, in wrapper V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] p.start() V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 121, in start V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] self._popen = self._Popen(self) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return _default_context.get_context().Process._Popen(process_obj) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 281, in _Popen V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return Popen(process_obj) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__ V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] self._launch(process_obj) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 71, in _launch V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] code = process_obj._bootstrap(parent_sentinel=child_r) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] self.run() V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] self._target(*self._args, **self._kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/workspace/tutorials-kr/conf.py\", line 74, in call_fn V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] result = func(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1374, in generate_file_rst V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] output_blocks, time_elapsed = execute_script( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1192, in execute_script V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] execute_code_block( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1048, in execute_code_block V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] is_last_expr, mem_max = _exec_and_get_memory( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 876, in _exec_and_get_memory V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] mem_max, _ = call_memory( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1725, in _sg_call_memory_noop V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return 0.0, func() V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 794, in __call__ V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] exec(self.code, self.fake_main.__dict__) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 709, in \u003cmodule\u003e V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] export(Foo(), inps) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return _export( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] ep = fn(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return fn(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] ep = _export_for_training( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] ep = fn(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return fn(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] export_artifact = export_func( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] aten_export_artifact = _to_aten_func( # type: ignore[operator] V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1770, in _export_to_aten_ir_make_fx V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] gm, graph_signature = transform(_make_fx_helper)( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1900, in _aot_export_non_strict V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1685, in _make_fx_helper V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] gm = make_fx( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in wrapped V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return make_fx_tracer.trace(f, *args) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2250, in trace V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self._trace_inner(f, *args) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2221, in _trace_inner V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] t = dispatch_trace( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_compile.py\", line 53, in inner V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return disable_fn(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return fn(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1254, in dispatch_trace V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1835, in trace V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] res = super().trace(root, concrete_args) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return fn(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 850, in trace V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] (self.create_arg(fn(*args)),), V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1312, in wrapped V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] out = f(*tensors) # type:ignore[call-arg] V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1589, in wrapped_fn V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return tuple(flat_fn(*args)) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] tree_out = fn(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 906, in functional_call V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] out = mod(*args[params_len:], **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self.call_module(mod, forward, args, kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return Tracer.call_module(self, m, forward, args, kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] ret_val = forward(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return _orig_module_call(mod, *args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self._call_impl(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return forward_call(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1884, in forward V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] tree_out = mod(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self.call_module(mod, forward, args, kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return Tracer.call_module(self, m, forward, args, kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] ret_val = forward(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return _orig_module_call(mod, *args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self._call_impl(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return forward_call(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 701, in forward V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] a = x.item() V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1360, in __torch_function__ V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return func(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1407, in __torch_function__ V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return func(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 1051, in __torch_function__ V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return func(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 950, in handler V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return torch._library.utils.handle_dispatch_mode( V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py\", line 296, in handle_dispatch_mode V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return fn(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1462, in __torch_dispatch__ V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return proxy_call(self, func, self.pre_dispatch, args, kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 914, in proxy_call V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] out = func(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self._op(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return fn(*args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__ V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self.dispatch(func, types, args, kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self._cached_dispatch_impl(func, types, args, kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1474, in _cached_dispatch_impl V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self._dispatch_impl(func, types, args, kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2687, in _dispatch_impl V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] op_impl_out = op_impl(self, func, *args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py\", line 163, in dispatch_to_op_implementations_dict V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return op_implementations_dict[func](fake_mode, func, *args, **kwargs) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py\", line 425, in local_scalar_dense V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] r = fake_mode.shape_env.create_unbacked_symint() V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return retlog(fn(*args, **kwargs)) V1004 00:20:20.553000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] failed while attempting to run meta for aten.select.int E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] Traceback (most recent call last): E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2717, in _dispatch_impl E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] r = func(*args, **kwargs) E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] ^^^^^^^^^^^^^^^^^^^^^ E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] return self._op(*args, **kwargs) E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] ^^^^^^^^^^^^^^^^^^^^^^^^^ E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] File \"/opt/conda/lib/python3.11/site-packages/torch/_meta_registrations.py\", line 5545, in meta_select E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] guard_size_oblivious(-index \u003e size) or guard_size_oblivious(index \u003e= size) E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 473, in guard_size_oblivious E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] return expr.node.guard_size_oblivious(\"\", 0) E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 596, in guard_size_oblivious E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] r = self.evaluate(size_oblivious=True) E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 512, in evaluate E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] return self.shape_env.evaluate_sym_node(self, size_oblivious) E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7223, in evaluate_sym_node E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] return self.evaluate_expr( E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] ^^^^^^^^^^^^^^^^^^^ E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7323, in evaluate_expr E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] return self._inner_evaluate_expr( E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] ^^^^^^^^^^^^^^^^^^^^^^^^^^ E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] return retlog(fn(*args, **kwargs)) E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] ^^^^^^^^^^^^^^^^^^^ E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7346, in _inner_evaluate_expr E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] return self._evaluate_expr( E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] ^^^^^^^^^^^^^^^^^^^^ E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7570, in _evaluate_expr E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] raise self._make_data_dependent_error( E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression -u0 \u003e 60 (unhinted: -u0 \u003e 60). (Size-like symbols: none) E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] Caused by: (_meta_registrations.py:5545 in meta_select) E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] For more information, run with TORCH_LOGS=\"dynamic\" E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\" E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] E1004 00:20:20.557000 3670787 site-packages/torch/_subclasses/fake_tensor.py:2721] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 def forward(self, arg0_1: \"i64[]\", arg1_1: \"f32[60]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:701 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(arg0_1); arg0_1 = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:702 in forward, code: return y[a] select = torch.ops.aten.select.int(arg1_1, 0, item); arg1_1 = item = select = None def forward(self, arg0_1: \"i64[]\", arg1_1: \"f32[60]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:701 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(arg0_1); arg0_1 = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:702 in forward, code: return y[a] select = torch.ops.aten.select.int(arg1_1, 0, item); arg1_1 = item = select = None Traceback (most recent call last): File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 709, in \u003cmodule\u003e export(Foo(), inps) File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 319, in export raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( ^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1164, in wrapper raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1164, in wrapper raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( ^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1770, in _export_to_aten_ir_make_fx gm, graph_signature = transform(_make_fx_helper)( ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1900, in _aot_export_non_strict gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1685, in _make_fx_helper gm = make_fx( ^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in wrapped return make_fx_tracer.trace(f, *args) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2250, in trace return self._trace_inner(f, *args) ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2221, in _trace_inner t = dispatch_trace( ^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_compile.py\", line 53, in inner return disable_fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1254, in dispatch_trace graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1835, in trace res = super().trace(root, concrete_args) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 850, in trace (self.create_arg(fn(*args)),), ^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1312, in wrapped out = f(*tensors) # type:ignore[call-arg] ^^^^^^^^^^^ File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1589, in wrapped_fn return tuple(flat_fn(*args)) ^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn tree_out = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 906, in functional_call out = mod(*args[params_len:], **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1884, in forward tree_out = mod(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 702, in forward return y[a] ~^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1360, in __torch_function__ return func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1407, in __torch_function__ return func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 1051, in __torch_function__ return func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 1026, in run t = _method(t, *_args) ^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 950, in handler return torch._library.utils.handle_dispatch_mode( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py\", line 296, in handle_dispatch_mode return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1462, in __torch_dispatch__ return proxy_call(self, func, self.pre_dispatch, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 914, in proxy_call out = func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ return self._op(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__ return self.dispatch(func, types, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch return self._cached_dispatch_impl(func, types, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1487, in _cached_dispatch_impl output = self._dispatch_impl(func, types, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2717, in _dispatch_impl r = func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ return self._op(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_meta_registrations.py\", line 5545, in meta_select guard_size_oblivious(-index \u003e size) or guard_size_oblivious(index \u003e= size) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 473, in guard_size_oblivious return expr.node.guard_size_oblivious(\"\", 0) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 596, in guard_size_oblivious r = self.evaluate(size_oblivious=True) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 512, in evaluate return self.shape_env.evaluate_sym_node(self, size_oblivious) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7223, in evaluate_sym_node return self.evaluate_expr( ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7323, in evaluate_expr return self._inner_evaluate_expr( ^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7346, in _inner_evaluate_expr return self._evaluate_expr( ^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7570, in _evaluate_expr raise self._make_data_dependent_error( torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not guard on data-dependent expression -u0 \u003e 60 (unhinted: -u0 \u003e 60). (Size-like symbols: none) Caused by: (_meta_registrations.py:5545 in meta_select) For more information, run with TORCH_LOGS=\"dynamic\" For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\" If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 The following call raised this error: File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 702, in forward return y[a] To fix the error, insert one of the following checks before this call: 1. torch._check((-1)*a \u003e 60) 2. torch._check((-1)*a \u003c= 60) (These suggested fixes were derived by replacing `u0` with a in -u0 \u003e 60 and its negation.) The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`. Here is a scenario where torch._check() insertion is required simply to prevent an operation from failing. The export call will fail with \u201cCould not guard on data-dependent expression -u0 \u003e 60\u201d, implying that the compiler doesn\u2019t know if this is a valid indexing operation - if the value of x is out-of-bounds for y or not. Here, manual specialization is too prohibitive, and torch.cond() has no place. Instead, informing the compiler of u0\u2019s range is sufficient: class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() torch._check(a \u003e= 0) torch._check(a \u003c y.shape[0]) return y[a] inps = ( torch.tensor(32), torch.randn(60), ) ep = export(Foo(), inps) print(ep) I1004 00:20:20.569000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:20.573000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense) I1004 00:20:20.574000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u0] I1004 00:20:20.575000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert u0 \u003e= 0 [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:722 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"u0 \u003e= 0\" V1004 00:20:20.575000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range u0 = VR[0, int_oo] (update) I1004 00:20:20.578000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert u0 \u003c 60 [guard added] (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:723 in forward), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"u0 \u003c 60\" V1004 00:20:20.579000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range u0 = VR[0, 59] (update) V1004 00:20:20.582000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval size_oblivious(-u0 \u003e 60) == False [statically known] V1004 00:20:20.582000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval size_oblivious(u0 \u003e= 60) == False [statically known] V1004 00:20:20.583000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == True [statically known] V1004 00:20:20.583000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7475] eval False == True [statically known] I1004 00:20:20.585000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:20:20.586000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].storage_offset() 0 None V1004 00:20:20.586000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[0] 60 None V1004 00:20:20.586000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[0] 1 None V1004 00:20:20.586000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].storage_offset() 0 None V1004 00:20:20.587000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 \u003e= 0 == True [statically known] V1004 00:20:20.589000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 \u003c= 59 == True [statically known] V1004 00:20:20.589000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 \u003c 60 == True [statically known] ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\", y: \"f32[60]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:721 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None ge_1: \"Sym(u0 \u003e= 0)\" = item \u003e= 0 _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 \u003e= 0 on node \u0027ge_1\u0027\"); ge_1 = _assert_scalar_default = None le: \"Sym(u0 \u003c= 59)\" = item \u003c= 59 _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le, \"Runtime assertion failed for expression u0 \u003c= 59 on node \u0027le\u0027\"); le = _assert_scalar_default_1 = None # lt_1: \"Sym(u0 \u003c 60)\" = item \u003c 60 _assert_scalar_default_2 = torch.ops.aten._assert_scalar.default(lt_1, \"Runtime assertion failed for expression u0 \u003c 60 on node \u0027lt_1\u0027\"); lt_1 = _assert_scalar_default_2 = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:724 in forward, code: return y[a] select: \"f32[]\" = torch.ops.aten.select.int(y, 0, item); y = item = None return (select,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs select: USER_OUTPUT Range constraints: {u0: VR[0, 59]} Specialized values# Another category of data-dependent error happens when the program attempts to extract a concrete data-dependent integer/float value while tracing. This looks something like \u201cCould not extract specialized integer from data-dependent expression\u201d, and is analogous to the previous class of errors - if these occur when attempting to evaluate concrete integer/float values, data-dependent guard errors arise with evaluating concrete boolean values. This error typically occurs when there is an explicit or implicit int() cast on a data-dependent expression. For example, this list comprehension has a range() call that implicitly does an int() cast on the size of the list: class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() b = torch.cat([y for y in range(a)], dim=0) return b + int(a) inps = ( torch.tensor(32), torch.randn(60), ) try: export(Foo(), inps, strict=False) except Exception: tb.print_exc() I1004 00:20:20.594000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:20.599000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense) I1004 00:20:20.599000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u0] V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] Data dependent variable \u0027u0\u0027 allocated at: V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/bin/sphinx-build\", line 7, in \u003cmodule\u003e V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] sys.exit(main()) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 339, in main V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return make_main(argv) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 213, in make_main V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return make_mode.run_make_mode(argv[1:]) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 181, in run_make_mode V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return make.run_generic_build(args[0]) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/make_mode.py\", line 169, in run_generic_build V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return build_main(args + opts) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/cmd/build.py\", line 293, in build_main V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] app = Sphinx(args.sourcedir, args.confdir, args.outputdir, V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 272, in __init__ V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] self._init_builder() V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/application.py\", line 343, in _init_builder V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] self.events.emit(\u0027builder-inited\u0027) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx/events.py\", line 97, in emit V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] results.append(listener.handler(self.app, *args)) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_gallery.py\", line 757, in generate_gallery_rst V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] ) = generate_dir_rst( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 606, in generate_dir_rst V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] results = parallel( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 607, in \u003cgenexpr\u003e V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] p_fun(fname, target_dir, src_dir, gallery_conf) for fname in iterator V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/workspace/tutorials-kr/conf.py\", line 86, in wrapper V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] p.start() V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 121, in start V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] self._popen = self._Popen(self) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 224, in _Popen V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return _default_context.get_context().Process._Popen(process_obj) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/context.py\", line 281, in _Popen V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return Popen(process_obj) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 19, in __init__ V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] self._launch(process_obj) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/popen_fork.py\", line 71, in _launch V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] code = process_obj._bootstrap(parent_sentinel=child_r) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] self.run() V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/multiprocessing/process.py\", line 108, in run V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] self._target(*self._args, **self._kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/workspace/tutorials-kr/conf.py\", line 74, in call_fn V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] result = func(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1374, in generate_file_rst V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] output_blocks, time_elapsed = execute_script( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1192, in execute_script V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] execute_code_block( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1048, in execute_code_block V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] is_last_expr, mem_max = _exec_and_get_memory( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 876, in _exec_and_get_memory V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] mem_max, _ = call_memory( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 1725, in _sg_call_memory_noop V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return 0.0, func() V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/sphinx_gallery/gen_rst.py\", line 794, in __call__ V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] exec(self.code, self.fake_main.__dict__) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 756, in \u003cmodule\u003e V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] export(Foo(), inps, strict=False) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return _export( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] ep = fn(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return fn(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] ep = _export_for_training( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] ep = fn(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return fn(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] export_artifact = export_func( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] aten_export_artifact = _to_aten_func( # type: ignore[operator] V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1770, in _export_to_aten_ir_make_fx V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] gm, graph_signature = transform(_make_fx_helper)( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1900, in _aot_export_non_strict V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1685, in _make_fx_helper V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] gm = make_fx( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in wrapped V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return make_fx_tracer.trace(f, *args) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2250, in trace V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self._trace_inner(f, *args) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2221, in _trace_inner V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] t = dispatch_trace( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_compile.py\", line 53, in inner V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return disable_fn(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return fn(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1254, in dispatch_trace V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1835, in trace V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] res = super().trace(root, concrete_args) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return fn(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 850, in trace V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] (self.create_arg(fn(*args)),), V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1312, in wrapped V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] out = f(*tensors) # type:ignore[call-arg] V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1589, in wrapped_fn V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return tuple(flat_fn(*args)) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] tree_out = fn(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 906, in functional_call V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] out = mod(*args[params_len:], **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self.call_module(mod, forward, args, kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return Tracer.call_module(self, m, forward, args, kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] ret_val = forward(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return _orig_module_call(mod, *args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self._call_impl(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return forward_call(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1884, in forward V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] tree_out = mod(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self.call_module(mod, forward, args, kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return Tracer.call_module(self, m, forward, args, kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] ret_val = forward(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return _orig_module_call(mod, *args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self._call_impl(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return forward_call(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 747, in forward V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] a = x.item() V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1360, in __torch_function__ V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return func(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1407, in __torch_function__ V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return func(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_export/non_strict_utils.py\", line 1051, in __torch_function__ V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return func(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 950, in handler V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return torch._library.utils.handle_dispatch_mode( V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_library/utils.py\", line 296, in handle_dispatch_mode V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return fn(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1462, in __torch_dispatch__ V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return proxy_call(self, func, self.pre_dispatch, args, kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 914, in proxy_call V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] out = func(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_ops.py\", line 829, in __call__ V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self._op(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_stats.py\", line 28, in wrapper V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return fn(*args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1352, in __torch_dispatch__ V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self.dispatch(func, types, args, kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2058, in dispatch V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self._cached_dispatch_impl(func, types, args, kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 1474, in _cached_dispatch_impl V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return self._dispatch_impl(func, types, args, kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py\", line 2687, in _dispatch_impl V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] op_impl_out = op_impl(self, func, *args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py\", line 163, in dispatch_to_op_implementations_dict V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return op_implementations_dict[func](fake_mode, func, *args, **kwargs) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/_subclasses/fake_impls.py\", line 425, in local_scalar_dense V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] r = fake_mode.shape_env.create_unbacked_symint() V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] return retlog(fn(*args, **kwargs)) V1004 00:20:20.600000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6519] def forward(self, arg0_1: \"i64[]\", arg1_1: \"f32[60]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:747 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(arg0_1); arg0_1 = item = None def forward(self, arg0_1: \"i64[]\", arg1_1: \"f32[60]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:747 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(arg0_1); arg0_1 = item = None Traceback (most recent call last): File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 756, in \u003cmodule\u003e export(Foo(), inps, strict=False) File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 319, in export raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/__init__.py\", line 286, in export return _export( ^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1164, in wrapper raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2176, in _export ep = _export_for_training( ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1164, in wrapper raise e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1130, in wrapper ep = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/exported_program.py\", line 123, in wrapper return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 2037, in _export_for_training export_artifact = export_func( ^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1979, in _non_strict_export aten_export_artifact = _to_aten_func( # type: ignore[operator] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1770, in _export_to_aten_ir_make_fx gm, graph_signature = transform(_make_fx_helper)( ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1900, in _aot_export_non_strict gm, sig = aot_export(wrapped_mod, args, kwargs=kwargs, **flags) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1685, in _make_fx_helper gm = make_fx( ^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2318, in wrapped return make_fx_tracer.trace(f, *args) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2250, in trace return self._trace_inner(f, *args) ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 2221, in _trace_inner t = dispatch_trace( ^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_compile.py\", line 53, in inner return disable_fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1254, in dispatch_trace graph = tracer.trace(root, concrete_args) # type: ignore[arg-type] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1835, in trace res = super().trace(root, concrete_args) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 850, in trace (self.create_arg(fn(*args)),), ^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1312, in wrapped out = f(*tensors) # type:ignore[call-arg] ^^^^^^^^^^^ File \"\u003cstring\u003e\", line 1, in \u003clambda\u003e File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1589, in wrapped_fn return tuple(flat_fn(*args)) ^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 184, in flat_fn tree_out = fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/traced_function_transforms.py\", line 906, in functional_call out = mod(*args[params_len:], **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/export/_trace.py\", line 1884, in forward tree_out = mod(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 825, in module_call_wrapper return self.call_module(mod, forward, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/proxy_tensor.py\", line 1905, in call_module return Tracer.call_module(self, m, forward, args, kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 542, in call_module ret_val = forward(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/_symbolic_trace.py\", line 818, in forward return _orig_module_call(mod, *args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py\", line 748, in forward b = torch.cat([y for y in range(a)], dim=0) ^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/__init__.py\", line 438, in __index__ return self.node.int_() ^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 468, in int_ return self.guard_int(\"\", 0) # NB: uses Python backtrace ^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 518, in guard_int r = self.evaluate() ^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/sym_node.py\", line 512, in evaluate return self.shape_env.evaluate_sym_node(self, size_oblivious) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7223, in evaluate_sym_node return self.evaluate_expr( ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7323, in evaluate_expr return self._inner_evaluate_expr( ^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/recording.py\", line 272, in wrapper return retlog(fn(*args, **kwargs)) ^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7346, in _inner_evaluate_expr return self._evaluate_expr( ^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 7570, in _evaluate_expr raise self._make_data_dependent_error( torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0). (Size-like symbols: none) Caused by: (orkspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:748 in forward) For more information, run with TORCH_LOGS=\"dynamic\" For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\" If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`. For these errors, some basic options you have are: Avoid unnecessary int() cast calls, in this case the int(a) in the return statement. Use torch._check() calls; unfortunately all you may be able to do in this case is specialize (with torch._check(a == 60)). Rewrite the offending code at a higher level. For example, the list comprehension is semantically a repeat() op, which doesn\u2019t involve an int() cast. The following rewrite avoids data-dependent errors: class Foo(torch.nn.Module): def forward(self, x, y): a = x.item() b = y.unsqueeze(0).repeat(a, 1) return b + a inps = ( torch.tensor(32), torch.randn(60), ) ep = export(Foo(), inps, strict=False) print(ep) I1004 00:20:20.611000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:20.616000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:4776] create_unbacked_symint u0 [-int_oo, int_oo] (_subclasses/fake_impls.py:425 in local_scalar_dense) I1004 00:20:20.616000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:1287] compute_unbacked_bindings [u0] I1004 00:20:20.620000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7197] runtime_assert u0 \u003e= 0 [guard added] (_meta_registrations.py:4247 in meta_repeat), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"u0 \u003e= 0\" V1004 00:20:20.620000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:6606] _update_var_to_range u0 = VR[0, int_oo] (update) V1004 00:20:20.621000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 \u003e= 0 == True [statically known] I1004 00:20:20.623000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7369] could not evaluate Eq(u0, 0) due to data dependency, it was assumed to be False with no runtime assertions (utils/_stats.py:28 in wrapper) I1004 00:20:20.623000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7369] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 I1004 00:20:20.628000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7369] could not evaluate 60*u0 \u003c 2 due to data dependency, it was assumed to be False with no runtime assertions (_prims_common/__init__.py:279 in is_contiguous) I1004 00:20:20.628000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7369] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 I1004 00:20:20.629000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7369] could not evaluate Eq(u0, 1) due to data dependency, it was assumed to be False with no runtime assertions (_prims_common/__init__.py:285 in is_contiguous) I1004 00:20:20.629000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7369] For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1 V1004 00:20:20.631000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert True == True [statically known] I1004 00:20:20.635000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:20:20.636000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].storage_offset() 0 None V1004 00:20:20.636000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].size()[0] 60 None V1004 00:20:20.636000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].stride()[0] 1 None V1004 00:20:20.636000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027y\u0027].storage_offset() 0 None V1004 00:20:20.637000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7694] runtime_assert u0 \u003e= 0 == True [statically known] ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"i64[]\", y: \"f32[60]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:769 in forward, code: a = x.item() item: \"Sym(u0)\" = torch.ops.aten.item.default(x); x = None # sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item); sym_constrain_range_for_size_default = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:769 in forward, code: a = x.item() ge: \"Sym(u0 \u003e= 0)\" = item \u003e= 0 _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge, \"Runtime assertion failed for expression u0 \u003e= 0 on node \u0027ge\u0027\"); ge = _assert_scalar_default = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:770 in forward, code: b = y.unsqueeze(0).repeat(a, 1) unsqueeze: \"f32[1, 60]\" = torch.ops.aten.unsqueeze.default(y, 0); y = None repeat: \"f32[u0, 60]\" = torch.ops.aten.repeat.default(unsqueeze, [item, 1]); unsqueeze = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:771 in forward, code: return b + a add: \"f32[u0, 60]\" = torch.ops.aten.add.Tensor(repeat, item); repeat = item = None return (add,) Graph signature: # inputs x: USER_INPUT y: USER_INPUT # outputs add: USER_OUTPUT Range constraints: {u0: VR[0, int_oo]} Data-dependent errors can be much more involved, and there are many more options in your toolkit to deal with them: torch._check_is_size(), guard_size_oblivious(), or real-tensor tracing, as starters. For more in-depth guides, please refer to the Export Programming Model, or Dealing with GuardOnDataDependentSymNode errors. Custom Ops# torch.export can export PyTorch programs with custom operators. Please refer to this page on how to author a custom operator in either C++ or Python. The following is an example of registering a custom operator in python to be used by torch.export. The important thing to note is that the custom op must have a FakeTensor kernel. @torch.library.custom_op(\"my_custom_library::custom_op\", mutates_args={}) def custom_op(x: torch.Tensor) -\u003e torch.Tensor: print(\"custom_op called!\") return torch.relu(x) @custom_op.register_fake def custom_op_meta(x): # Returns an empty tensor with the same shape as the expected output return torch.empty_like(x) Here is an example of exporting a program with the custom op. class CustomOpExample(torch.nn.Module): def forward(self, x): x = torch.sin(x) x = torch.ops.my_custom_library.custom_op(x) x = torch.cos(x) return x exported_custom_op_example = export(CustomOpExample(), (torch.randn(3, 3),)) print(exported_custom_op_example) print(exported_custom_op_example.module()(torch.randn(3, 3))) I1004 00:20:20.685000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:20.695000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:20:20.695000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[0] 3 None V1004 00:20:20.695000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[1] 3 None V1004 00:20:20.695000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[0] 3 None V1004 00:20:20.695000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[1] 1 None V1004 00:20:20.695000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].storage_offset() 0 None ExportedProgram: class GraphModule(torch.nn.Module): def forward(self, x: \"f32[3, 3]\"): # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:812 in forward, code: x = torch.sin(x) sin: \"f32[3, 3]\" = torch.ops.aten.sin.default(x); x = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:813 in forward, code: x = torch.ops.my_custom_library.custom_op(x) custom_op: \"f32[3, 3]\" = torch.ops.my_custom_library.custom_op.default(sin); sin = None # File: /workspace/tutorials-kr/intermediate_source/torch_export_tutorial.py:814 in forward, code: x = torch.cos(x) cos: \"f32[3, 3]\" = torch.ops.aten.cos.default(custom_op); custom_op = None return (cos,) Graph signature: # inputs x: USER_INPUT # outputs cos: USER_OUTPUT Range constraints: {} custom_op called! tensor([[0.7485, 0.6790, 0.8114], [1.0000, 0.9969, 1.0000], [0.9950, 1.0000, 0.6667]]) Note that in the ExportedProgram, the custom operator is included in the graph. IR/Decompositions# The graph produced by torch.export returns a graph containing only ATen operators, which are the basic unit of computation in PyTorch. As there are over 3000 ATen operators, export provides a way to narrow down the operator set used in the graph based on certain characteristics, creating different IRs. By default, export produces the most generic IR which contains all ATen operators, including both functional and non-functional operators. A functional operator is one that does not contain any mutations or aliasing of the inputs. You can find a list of all ATen operators here and you can inspect if an operator is functional by checking op._schema.is_mutable, for example: print(torch.ops.aten.add.Tensor._schema.is_mutable) print(torch.ops.aten.add_.Tensor._schema.is_mutable) False True This generic IR can be used to train in eager PyTorch Autograd. This IR can be more explicitly reached through the API torch.export.export_for_training, which was introduced in PyTorch 2.5, but calling torch.export.export should produce the same graph as of PyTorch 2.6. class DecompExample(torch.nn.Module): def __init__(self) -\u003e None: super().__init__() self.conv = torch.nn.Conv2d(1, 3, 1, 1) self.bn = torch.nn.BatchNorm2d(3) def forward(self, x): x = self.conv(x) x = self.bn(x) return (x,) ep_for_training = torch.export.export_for_training(DecompExample(), (torch.randn(1, 1, 3, 3),)) print(ep_for_training.graph) I1004 00:20:20.702000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:20.727000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:20:20.727000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[0] 1 None V1004 00:20:20.727000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[1] 1 None V1004 00:20:20.727000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[2] 3 None V1004 00:20:20.727000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[3] 3 None V1004 00:20:20.728000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[0] 9 None V1004 00:20:20.728000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[1] 9 None V1004 00:20:20.728000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[2] 3 None V1004 00:20:20.728000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[3] 1 None V1004 00:20:20.728000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].storage_offset() 0 None graph(): %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight] %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias] %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight] %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias] %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean] %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var] %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked] %x : [num_users=1] = placeholder[target=x] %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv_weight, %p_conv_bias), kwargs = {}) %add_ : [num_users=0] = call_function[target=torch.ops.aten.add_.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {}) %batch_norm : [num_users=1] = call_function[target=torch.ops.aten.batch_norm.default](args = (%conv2d, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05, True), kwargs = {}) return (batch_norm,) We can then lower this exported program to an operator set which only contains functional ATen operators through the API run_decompositions, which decomposes the ATen operators into the ones specified in the decomposition table, and functionalizes the graph. By specifying an empty set, we\u2019re only performing functionalization, and does not do any additional decompositions. This results in an IR which contains ~2000 operators (instead of the 3000 operators above), and is ideal for inference cases. ep_for_inference = ep_for_training.run_decompositions(decomp_table={}) print(ep_for_inference.graph) graph(): %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight] %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias] %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight] %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias] %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean] %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var] %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked] %x : [num_users=1] = placeholder[target=x] %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv_weight, %p_conv_bias), kwargs = {}) %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {}) %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%conv2d, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {}) %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {}) %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {}) %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {}) return (getitem_3, getitem_4, add, getitem) As we can see, the previously mutable operator, torch.ops.aten.add_.default has now been replaced with torch.ops.aten.add.default, a l operator. We can also further lower this exported program to an operator set which only contains the Core ATen Operator Set, which is a collection of only ~180 operators. This IR is optimal for backends who do not want to reimplement all ATen operators. from torch.export import default_decompositions core_aten_decomp_table = default_decompositions() core_aten_ep = ep_for_training.run_decompositions(decomp_table=core_aten_decomp_table) print(core_aten_ep.graph) graph(): %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight] %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias] %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight] %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias] %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean] %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var] %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked] %x : [num_users=1] = placeholder[target=x] %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%x, %p_conv_weight, %p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {}) %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {}) %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%convolution, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {}) %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {}) %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {}) %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {}) return (getitem_3, getitem_4, add, getitem) We now see that torch.ops.aten.conv2d.default has been decomposed into torch.ops.aten.convolution.default. This is because convolution is a more \u201ccore\u201d operator, as operations like conv1d and conv2d can be implemented using the same op. We can also specify our own decomposition behaviors: my_decomp_table = torch.export.default_decompositions() def my_awesome_custom_conv2d_function(x, weight, bias, stride=[1, 1], padding=[0, 0], dilation=[1, 1], groups=1): return 2 * torch.ops.aten.convolution(x, weight, bias, stride, padding, dilation, False, [0, 0], groups) my_decomp_table[torch.ops.aten.conv2d.default] = my_awesome_custom_conv2d_function my_ep = ep_for_training.run_decompositions(my_decomp_table) print(my_ep.graph) graph(): %p_conv_weight : [num_users=1] = placeholder[target=p_conv_weight] %p_conv_bias : [num_users=1] = placeholder[target=p_conv_bias] %p_bn_weight : [num_users=1] = placeholder[target=p_bn_weight] %p_bn_bias : [num_users=1] = placeholder[target=p_bn_bias] %b_bn_running_mean : [num_users=1] = placeholder[target=b_bn_running_mean] %b_bn_running_var : [num_users=1] = placeholder[target=b_bn_running_var] %b_bn_num_batches_tracked : [num_users=1] = placeholder[target=b_bn_num_batches_tracked] %x : [num_users=1] = placeholder[target=x] %convolution : [num_users=1] = call_function[target=torch.ops.aten.convolution.default](args = (%x, %p_conv_weight, %p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), kwargs = {}) %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convolution, 2), kwargs = {}) %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_bn_num_batches_tracked, 1), kwargs = {}) %_native_batch_norm_legit_functional : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit_functional.default](args = (%mul, %p_bn_weight, %p_bn_bias, %b_bn_running_mean, %b_bn_running_var, True, 0.1, 1e-05), kwargs = {}) %getitem : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 0), kwargs = {}) %getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 3), kwargs = {}) %getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%_native_batch_norm_legit_functional, 4), kwargs = {}) return (getitem_3, getitem_4, add, getitem) Notice that instead of torch.ops.aten.conv2d.default being decomposed into torch.ops.aten.convolution.default, it is now decomposed into torch.ops.aten.convolution.default and torch.ops.aten.mul.Tensor, which matches our custom decomposition rule. ExportDB# torch.export will only ever export a single computation graph from a PyTorch program. Because of this requirement, there will be Python or PyTorch features that are not compatible with torch.export, which will require users to rewrite parts of their model code. We have seen examples of this earlier in the tutorial \u2013 for example, rewriting if-statements using cond. ExportDB is the standard reference that documents supported and unsupported Python/PyTorch features for torch.export. It is essentially a list a program samples, each of which represents the usage of one particular Python/PyTorch feature and its interaction with torch.export. Examples are also tagged by category so that they can be more easily searched. For example, let\u2019s use ExportDB to get a better understanding of how the predicate works in the cond operator. We can look at the example called cond_predicate, which has a torch.cond tag. The example code looks like: def cond_predicate(x): \"\"\" The conditional statement (aka predicate) passed to ``cond()`` must be one of the following: - ``torch.Tensor`` with a single element - boolean expression NOTE: If the `pred` is test on a dim with batch size \u003c 2, it will be specialized. \"\"\" pred = x.dim() \u003e 2 and x.shape[2] \u003e 10 return cond(pred, lambda x: x.cos(), lambda y: y.sin(), [x]) More generally, ExportDB can be used as a reference when one of the following occurs: Before attempting torch.export, you know ahead of time that your model uses some tricky Python/PyTorch features and you want to know if torch.export covers that feature. When attempting torch.export, there is a failure and it\u2019s unclear how to work around it. ExportDB is not exhaustive, but is intended to cover all use cases found in typical PyTorch code. Feel free to reach out if there is an important Python/PyTorch feature that should be added to ExportDB or supported by torch.export. Running the Exported Program# As torch.export is only a graph capturing mechanism, calling the artifact produced by torch.export eagerly will be equivalent to running the eager module. To optimize the execution of the Exported Program, we can pass this exported artifact to backends such as Inductor through torch.compile, AOTInductor, or TensorRT. class M(torch.nn.Module): def __init__(self): super().__init__() self.linear = torch.nn.Linear(3, 3) def forward(self, x): x = self.linear(x) return x inp = torch.randn(2, 3, device=\"cuda\") m = M().to(device=\"cuda\") ep = torch.export.export(m, (inp,)) # Run it eagerly res = ep.module()(inp) print(res) # Run it with torch.compile res = torch.compile(ep.module(), backend=\"inductor\")(inp) print(res) I1004 00:20:21.493000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] create_env I1004 00:20:21.503000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] produce_guards V1004 00:20:21.503000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[0] 2 None V1004 00:20:21.504000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].size()[1] 3 None V1004 00:20:21.504000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[0] 3 None V1004 00:20:21.504000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].stride()[1] 1 None V1004 00:20:21.504000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] track_symint L[\u0027x\u0027].storage_offset() 0 None tensor([[-0.8696, 0.6491, 1.1789], [ 1.0454, -1.0737, -0.5275]], device=\u0027cuda:0\u0027, grad_fn=\u003cAddmmBackward0\u003e) I1004 00:20:22.488000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:3767] [2/0] create_env /opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision(\u0027high\u0027)` for better performance. V1004 00:20:24.047000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7461] [2/0] eval 0 [trivial] V1004 00:20:24.052000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:7461] [2/0] eval 1 [trivial] I1004 00:20:24.368000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] [2/0] produce_guards I1004 00:20:24.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5238] [2/0] produce_guards V1004 00:20:24.376000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027x\u0027].size()[0] 2 None V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027x\u0027].size()[1] 3 None V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027x\u0027].stride()[0] 3 None V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027x\u0027].stride()[1] 1 None V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027x\u0027].storage_offset() 0 None V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].size()[0] 3 None V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].size()[1] 3 None V1004 00:20:24.377000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].stride()[0] 3 None V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].stride()[1] 1 None V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].storage_offset() 0 None V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027bias\u0027].size()[0] 3 None V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027bias\u0027].stride()[0] 1 None V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5458] [2/0] track_symint L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027bias\u0027].storage_offset() 0 None V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027x\u0027].size()[0] == 2 V1004 00:20:24.378000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027x\u0027].size()[1] == 3 V1004 00:20:24.379000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027x\u0027].stride()[0] == 3 V1004 00:20:24.379000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027x\u0027].stride()[1] == 1 V1004 00:20:24.379000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027x\u0027].storage_offset() == 0 V1004 00:20:24.379000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].size()[0] == 3 V1004 00:20:24.379000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].size()[1] == 3 V1004 00:20:24.379000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].stride()[0] == 3 V1004 00:20:24.380000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].stride()[1] == 1 V1004 00:20:24.380000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027weight\u0027].storage_offset() == 0 V1004 00:20:24.380000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027bias\u0027].size()[0] == 3 V1004 00:20:24.380000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027bias\u0027].stride()[0] == 1 V1004 00:20:24.380000 3670787 site-packages/torch/fx/experimental/symbolic_shapes.py:5679] [2/0] Skipping guard L[\u0027self\u0027]._modules[\u0027linear\u0027]._parameters[\u0027bias\u0027].storage_offset() == 0 tensor([[-0.8696, 0.6491, 1.1789], [ 1.0454, -1.0737, -0.5275]], device=\u0027cuda:0\u0027, grad_fn=\u003cCompiledFunctionBackward\u003e) import torch._inductor # Note: these APIs are subject to change # Compile the exported program to a PT2 archive using ``AOTInductor`` with torch.no_grad(): pt2_path = torch._inductor.aoti_compile_and_package(ep) # Load and run the .so file in Python. # To load and run it in a C++ environment, see: # https://pytorch.org/docs/main/torch.compiler_aot_inductor.html aoti_compiled = torch._inductor.aoti_load_package(pt2_path) res = aoti_compiled(inp) Conclusion# We introduced torch.export, the new PyTorch 2.X way to export single computation graphs from PyTorch programs. In particular, we demonstrate several code modifications and considerations (control flow ops, constraints, etc.) that need to be made in order to export a graph. Total running time of the script: (0 minutes 42.143 seconds) Download Jupyter notebook: torch_export_tutorial.ipynb Download Python source code: torch_export_tutorial.py Download zipped: torch_export_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/intermediate/torch_export_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>