
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="PyTorch로 분산 어플리케이션 개발하기" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/intermediate/dist_tuto.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Author: Séb Arnold, 번역: 박정환,. 선수과목(Prerequisites): PyTorch Distributed Overview. 이 짧은 튜토리얼에서는 PyTorch의 분산 패키지를 둘러볼 예정입니다. 여기에서는 어떻게 분산 환경을 설정하는지와 서로 다른 통신 방법을 사용하는지를 알아보고, 패키지 내부도 일부 살펴보도록 하겠습니다. 설정(Setup): PyTorch에 포함된 분산 패키지(예. torch.distributed)는 연구자와 실무자가 여러 프로세스와 클러스터의 기기에서 계산을 쉽게 병렬화 할 수 있게..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="Author: Séb Arnold, 번역: 박정환,. 선수과목(Prerequisites): PyTorch Distributed Overview. 이 짧은 튜토리얼에서는 PyTorch의 분산 패키지를 둘러볼 예정입니다. 여기에서는 어떻게 분산 환경을 설정하는지와 서로 다른 통신 방법을 사용하는지를 알아보고, 패키지 내부도 일부 살펴보도록 하겠습니다. 설정(Setup): PyTorch에 포함된 분산 패키지(예. torch.distributed)는 연구자와 실무자가 여러 프로세스와 클러스터의 기기에서 계산을 쉽게 병렬화 할 수 있게..." />
<meta property="og:ignore_canonical" content="true" />

    <title>PyTorch로 분산 어플리케이션 개발하기 &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/dist_tuto';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/intermediate/dist_tuto.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="Getting Started with Fully Sharded Data Parallel (FSDP2)" href="FSDP_tutorial.html" />
    <link rel="prev" title="분산 데이터 병렬 처리 시작하기" href="ddp_tutorial.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/ddp_series_intro.html">PyTorch의 분산 데이터 병렬 처리 - 비디오 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_tutorial.html">분산 데이터 병렬 처리 시작하기</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">PyTorch로 분산 어플리케이션 개발하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="FSDP_tutorial.html">Getting Started with Fully Sharded Data Parallel (FSDP2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="TCPStore_libuv_backend.html">Introduction to Libuv TCPStore Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="TP_tutorial.html">Large Scale Transformer model training with Tensor Parallel (TP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelining_tutorial.html">Introduction to Distributed Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="process_group_cpp_extension_tutorial.html">Cpp 확장을 사용한 프로세스 그룹 백엔드 사용자 정의</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/rpc_ddp_tutorial.html">분산 데이터 병렬(DDP)과 분산 RPC 프레임워크 결합</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/generic_join.html">Distributed Training with Uneven Inputs Using the Join Context Manager</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../distributed.html" class="nav-link">Distributed</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">PyTorch로 분산...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../distributed.html">
        <meta itemprop="name" content="Distributed">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="PyTorch로 분산 어플리케이션 개발하기">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">intermediate/dist_tuto</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="pytorch">
<h1>PyTorch로 분산 어플리케이션 개발하기<a class="headerlink" href="#pytorch" title="Link to this heading">#</a></h1>
<dl class="simple">
<dt><strong>Author</strong>: <a class="reference external" href="https://seba1511.com">Séb Arnold</a></dt><dd><p><strong>번역</strong>: <a class="reference external" href="https://github.com/9bow">박정환</a></p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="../_images/pencil-16.png"><img alt="edit" src="../_images/pencil-16.png" style="width: 16px; height: 16px;" /></a> 이 튜토리얼의 소스 코드는 <a class="reference external" href="https://github.com/pytorchkorea/tutorials-kr/blob/main/intermediate_source/dist_tuto.rst">GitHub</a> 에서 확인하고 변경해 볼 수 있습니다.</p>
</div>
<p>선수과목(Prerequisites):</p>
<ul class="simple">
<li><p><a class="reference external" href="../beginner/dist_overview.html">PyTorch Distributed Overview</a></p></li>
</ul>
<p>이 짧은 튜토리얼에서는 PyTorch의 분산 패키지를 둘러볼 예정입니다.
여기에서는 어떻게 분산 환경을 설정하는지와 서로 다른 통신 방법을 사용하는지를
알아보고, 패키지 내부도 일부 살펴보도록 하겠습니다.</p>
<section id="setup">
<h2>설정(Setup)<a class="headerlink" href="#setup" title="Link to this heading">#</a></h2>
<!--
* Processes & machines
* variables and init_process_group
--><p>PyTorch에 포함된 분산 패키지(예. <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code>)는 연구자와 실무자가
여러 프로세스와 클러스터의 기기에서 계산을 쉽게 병렬화 할 수 있게 합니다.
이를 위해, 각 프로세스가 다른 프로세스와 데이터를 교환할 수 있도록 메시지 교환
규약(messaging passing semantics)을 활용합니다. 멀티프로세싱(<code class="docutils literal notranslate"><span class="pre">torch.multiprocessing</span></code>)
패키지와 달리, 프로세스는 다른 커뮤니케이션 백엔드(backend)를 사용할 수 있으며
동일 기기 상에서 실행되는 것에 제약이 없습니다.</p>
<p>이 튜토리얼을 시작하기 위해 여러 프로세스를 동시에 실행할 수 있어야 합니다.
연산 클러스터에 접근하는 경우에는 시스템 관리자에게 확인하거나 선호하는 코디네이션
도구(coordination tool)를 사용하시면 됩니다. (예. <a class="reference external" href="https://linux.die.net/man/1/pdsh">pdsh</a>,
<a class="reference external" href="https://cea-hpc.github.io/clustershell/">clustershell</a>, 또는
<a class="reference external" href="https://slurm.schedmd.com/">slurm</a>). 이 튜토리얼에서는 다음 템플릿을 사용하여
단일 기기에서 여러 프로세스를 생성(fork)하겠습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;run.py:&quot;&quot;&quot;</span>
<span class="c1">#!/usr/bin/env python</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.multiprocessing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mp</span>

<span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Distributed function to be implemented later. &quot;&quot;&quot;</span>
    <span class="k">pass</span>

<span class="k">def</span><span class="w"> </span><span class="nf">init_process</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;gloo&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Initialize the distributed environment. &quot;&quot;&quot;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_ADDR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;MASTER_PORT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;29500&#39;</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
    <span class="n">fn</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">processes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="s2">&quot;google.colab&quot;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running in Google Colab&quot;</span><span class="p">)</span>
        <span class="n">mp</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;spawn&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mp</span><span class="o">.</span><span class="n">set_start_method</span><span class="p">(</span><span class="s2">&quot;spawn&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">init_process</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">run</span><span class="p">))</span>
        <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="n">processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">processes</span><span class="p">:</span>
        <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
<p>위 스크립트는 2개의 프로세스를 생성(spawn)하여 각자 다른 분산 환경을 설정하고,
프로세스 그룹(<code class="docutils literal notranslate"><span class="pre">dist.init_process_group</span></code>)을 초기화하고, 최종적으로는 <code class="docutils literal notranslate"><span class="pre">run</span></code>
함수를 실행합니다.</p>
<p>이제 <code class="docutils literal notranslate"><span class="pre">init_process</span></code> 함수를 살펴보도록 하겠습니다. 이 함수는 모든 프로세스가
마스터를 통해 조정(coordinate)될 수 있도록 동일한 IP 주소와 포트를 사용합니다.
여기에서는 <code class="docutils literal notranslate"><span class="pre">gloo</span></code> 백엔드를 사용하였으나 다른 백엔드들도 사용이 가능합니다.
(<a class="reference external" href="#communication-backends">섹션 5.1</a> 참고) 이 튜토리얼의 마지막 부분에 있는
<code class="docutils literal notranslate"><span class="pre">dist.init_process_group</span></code> 에서 일어나는 놀라운 일을 살펴볼 것이지만, 기본적으로는
프로세스가 자신의 위치를 공유함으로써 서로 통신할 수 있도록 합니다.</p>
</section>
<section id="point-to-point">
<h2>점-대-점 간(Point-to-Point) 통신<a class="headerlink" href="#point-to-point" title="Link to this heading">#</a></h2>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="../_images/send_recv.png"><img alt="송신과 수신" src="../_images/send_recv.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">송신과 수신</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>하나의 프로세스에서 다른 프로세스로 데이터를 전송하는 것을 점-대-점 간 통신이라고 합니다.
지점간 통신은  <code class="docutils literal notranslate"><span class="pre">send/recv</span></code> 함수 또는 즉시 응답하는(<em>immediate</em> counter-parts)
<code class="docutils literal notranslate"><span class="pre">isend</span></code> 와 <code class="docutils literal notranslate"><span class="pre">irecv</span></code> 를 사용합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;블로킹(blocking) 점-대-점 간 통신&quot;&quot;&quot;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Send the tensor to process 1</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Receive tensor from process 0</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>위 예제에서 두 프로세스는 값이 0인 Tensor로 시작한 후, 0번 프로세스가 Tensor의 값을
증가시킨 후 1번 프로세스로 보내서 둘 다 1.0으로 종료됩니다. 이 때, 프로세스 1은
수신한 데이터를 저장할 메모리를 할당해두어야 합니다.</p>
<p>또한 <code class="docutils literal notranslate"><span class="pre">send/recv</span></code> 는 모두 <strong>블로킹</strong> 입니다: 두 프로세스는 통신이 완료될 때까지
멈춰있습니다. 반면에 즉시 응답하는 것이 <strong>논-블로킹</strong> 입니다; 스크립트는 실행을
계속하고 메소드는 <code class="docutils literal notranslate"><span class="pre">wait()</span></code> 를 선택할 수 있는 <code class="docutils literal notranslate"><span class="pre">Work</span></code> 객체를 반환합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;논-블로킹(non-blocking) 점-대-점 간 통신&quot;&quot;&quot;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">req</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Send the tensor to process 1</span>
        <span class="n">req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank 0 started sending&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Receive tensor from process 0</span>
        <span class="n">req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">irecv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank 1 started receiving&#39;</span><span class="p">)</span>
    <span class="n">req</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>즉시 응답하는 함수들을 사용할 때는 Tensor를 어떻게 주고 받을지를 주의해야 합니다.
데이터가 언제 다른 프로세스로 송수신되는지 모르기 때문에, <code class="docutils literal notranslate"><span class="pre">req.wait()</span></code> 가 완료되기
전에는 전송된 Tensor를 수정하거나 수신된 Tensor에 접근해서는 안됩니다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dist.isend()</span></code> 다음에 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 에 쓰면 정의되지 않은 동작이 발생합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">req.wait()</span></code> 가 실행되기 전까지, <code class="docutils literal notranslate"><span class="pre">dist.irecv()</span></code> 다음에 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 를 읽으면 정의되지 않은 동작이 발생합니다.</p></li>
</ul>
<p>그러나, <code class="docutils literal notranslate"><span class="pre">req.wait()</span></code> 를 실행한 후에는 통신이 이루어진 것을 보장받을 수 있기 때문에,
<code class="docutils literal notranslate"><span class="pre">tensor[0]</span></code> 에 저장된 값은 1.0이 됩니다.</p>
<p>점-대-점 간 통신은 프로세스 간 통신에 대한 더 세밀한 제어를 원할 때 유용합니다.
<a class="reference external" href="https://github.com/baidu-research/baidu-allreduce">바이두(Baidu)의 DeepSpeech</a> 나
<a class="reference external" href="https://research.fb.com/publications/imagenet1kin1h/">페이스북(Facebook)의 대규모 실험</a>
에서 사용하는 것과 같은 멋진 알고리즘을 구현할 때 사용할 수 있습니다.
(<a class="reference external" href="#ring-allreduce">섹션 4.1</a> 참고)</p>
</section>
<section id="collective-communication">
<h2>집합 통신(Collective Communication)<a class="headerlink" href="#collective-communication" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<tbody>
<tr class="row-odd"><td><figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../_images/scatter.png"><img alt="Scatter" src="../_images/scatter.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Scatter</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
</td>
<td><figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../_images/gather.png"><img alt="Gather" src="../_images/gather.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Gather</span><a class="headerlink" href="#id4" title="Link to this image">#</a></p>
</figcaption>
</figure>
</td>
</tr>
<tr class="row-even"><td><figure class="align-center" id="id5">
<a class="reference internal image-reference" href="../_images/reduce.png"><img alt="Reduce" src="../_images/reduce.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Reduce</span><a class="headerlink" href="#id5" title="Link to this image">#</a></p>
</figcaption>
</figure>
</td>
<td><figure class="align-center" id="id6">
<a class="reference internal image-reference" href="../_images/all_reduce.png"><img alt="All-Reduce" src="../_images/all_reduce.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">All-Reduce</span><a class="headerlink" href="#id6" title="Link to this image">#</a></p>
</figcaption>
</figure>
</td>
</tr>
<tr class="row-odd"><td><figure class="align-center" id="id7">
<a class="reference internal image-reference" href="../_images/broadcast.png"><img alt="Broadcast" src="../_images/broadcast.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Broadcast</span><a class="headerlink" href="#id7" title="Link to this image">#</a></p>
</figcaption>
</figure>
</td>
<td><figure class="align-center" id="id8">
<a class="reference internal image-reference" href="../_images/all_gather.png"><img alt="All-Gather" src="../_images/all_gather.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">All-Gather</span><a class="headerlink" href="#id8" title="Link to this image">#</a></p>
</figcaption>
</figure>
</td>
</tr>
</tbody>
</table>
</div>
<p>점-대-점 간 통신과 달리 집합 통신은 <strong>그룹</strong> 의 모든 프로세스에 걸친 통신 패턴을
허용합니다. 그룹은 모든 프로세스의 부분 집합입니다. 그룹을 생성하기 위해서는
<code class="docutils literal notranslate"><span class="pre">dist.new_group(group)</span></code> 에 순서(rank) 목록을 전달합니다. 기본적으로, 집합 통신은
<strong>월드(world)</strong> 라고 부르는 전체 프로세스에서 실행됩니다. 예를 들어, 모든 프로세스에
존재하는 모든 Tensor들의 합을 얻기 위해서는 <code class="docutils literal notranslate"><span class="pre">dist.all_reduce(tensor,</span> <span class="pre">op,</span> <span class="pre">group)</span></code> 을
사용하면 됩니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; All-Reduce 예제 &quot;&quot;&quot;</span>
<span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; 간단한 집합 통신 &quot;&quot;&quot;</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s1">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>그룹 내의 모든 Tensor들의 합이 필요하기 때문에, <code class="docutils literal notranslate"><span class="pre">dist.ReduceOp.SUM</span></code> 을
리듀스(reduce) 연산자로 사용하였습니다. 일반적으로, 교환 법칙이 허용되는(commutative)
모든 수학 연산을 연산자로 사용할 수 있습니다. PyTorch는 요소별(element-wise)로
동작하는 기본적으로 4개의 연산자를 제공합니다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dist.ReduceOp.SUM</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.ReduceOp.PRODUCT</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.ReduceOp.MAX</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.ReduceOp.MIN</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.ReduceOp.BAND</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.ReduceOp.BOR</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.ReduceOp.BXOR</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.ReduceOp.PREMUL_SUM</span></code>.</p></li>
</ul>
<p>지원하는 연산의 전체 목록은
<a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp">여기</a> 에서 확인할 수 있습니다.</p>
<p>PyTorch에는 현재 <code class="docutils literal notranslate"><span class="pre">dist.all_reduce(tensor,</span> <span class="pre">op,</span> <span class="pre">group)</span></code> 외에도 많은 추가적인 집합 통신이
구현되어 있습니다. 여기 몇 가지 지원되는 집합 통신을 소개합니다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dist.broadcast(tensor,</span> <span class="pre">src,</span> <span class="pre">group)</span></code>: <code class="docutils literal notranslate"><span class="pre">src</span></code> 의 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 를 모든 프로세스의 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 에
복사합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.reduce(tensor,</span> <span class="pre">dst,</span> <span class="pre">op,</span> <span class="pre">group)</span></code>: <code class="docutils literal notranslate"><span class="pre">op</span></code> 를 모든 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 에 적용한 뒤
결과를 <code class="docutils literal notranslate"><span class="pre">dst</span></code> 프로세스의 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 에 저장합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.all_reduce(tensor,</span> <span class="pre">op,</span> <span class="pre">group)</span></code>: 리듀스와 동일하지만, 결과가 모든
프로세스의 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 에 저장됩니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.scatter(tensor,</span> <span class="pre">scatter_list,</span> <span class="pre">src,</span> <span class="pre">group)</span></code>: <span class="math">\(i^{\text{번째}}\)</span> Tensor
<code class="docutils literal notranslate"><span class="pre">scatter_list[i]</span></code> 를 <span class="math">\(i^{\text{번째}}\)</span> 프로세스의 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 에 복사합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.gather(tensor,</span> <span class="pre">gather_list,</span> <span class="pre">dst,</span> <span class="pre">group)</span></code>: 모든 프로세스의 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 를 <code class="docutils literal notranslate"><span class="pre">dst</span></code> 프로세스의
<code class="docutils literal notranslate"><span class="pre">gather_list</span></code> 에 복사합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.all_gather(tensor_list,</span> <span class="pre">tensor,</span> <span class="pre">group)</span></code>: 모든 프로세스의 <code class="docutils literal notranslate"><span class="pre">tensor</span></code> 를
모든 프로세스의 <code class="docutils literal notranslate"><span class="pre">tensor_list</span></code> 에 복사합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.barrier(group)</span></code>: <cite>group</cite> 내의 모든 프로세스가 이 함수에 진입할 때까지
<cite>group</cite> 내의 모든 프로세스를 멈춥(block)니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dist.all_to_all(output_tensor_list,</span> <span class="pre">input_tensor_list,</span> <span class="pre">group)</span></code>: <cite>group</cite> 내의 모든 프로세스에
<cite>input_tensor_list</cite> 의 Tensor들을 분산하고, <cite>output_tensor_list</cite> 에 수집된 Tensor들을 반환합니다.</p></li>
</ul>
<p>지원되는 집합 통신의 전체 목록은 PyTorch Distributed의 최신 문서 <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html">(링크)</a>
를 참고하세요.</p>
</section>
<section id="distributed-training">
<h2>분산 학습(Distributed Training)<a class="headerlink" href="#distributed-training" title="Link to this heading">#</a></h2>
<!--
* Gloo Backend
* Simple all_reduce on the gradients
* Point to optimized DistributedDataParallel

TODO: Custom ring-allreduce
--><p><strong>참고:</strong> 이 섹션의 예제 스크립트들은 <a class="reference external" href="https://github.com/seba-1511/dist_tuto.pth/">이 GitHub 저장소</a>
에서 찾아보실 수 있습니다.</p>
<p>이제 분산 모듈이 어떻게 동작하는지 이해했으므로, 유용한 뭔가를 작성해보겠습니다.
<a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel">DistributedDataParallel</a> 의
기능을 복제해보는 것이 목표입니다. 물론, 이것은 교훈적인(didactic) 예제이므로
실제 상황에서는 위에 링크된 잘 테스트되고 최적화된 공식 버전을 사용해야 합니다.</p>
<p>매우 간단하게 확률적 경사 하강법(SGD)의 분산 버전을 구현해보겠습니다. 스크립트는
모든 프로세스가 각자의 데이터 배치(batch)에서 각자의 모델의 변화도(gradient)를
계산한 후 평균을 계산합니다. 프로세스의 수를 변경해도 유사한 수렴 결과를 보장하기
위해, 먼저 데이터셋을 분할해야 합니다.
(아래 코드 대신 <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split">torch.utils.data.random_split</a>
을 사용해도 됩니다.)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; 데이터셋 분할 헬퍼(helper) &quot;&quot;&quot;</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Partition</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">data_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">data_idx</span><span class="p">]</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DataPartitioner</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1234</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">partitions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">Random</span><span class="p">()</span>  <span class="c1"># from random import Random</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">data_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">indexes</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">data_len</span><span class="p">)]</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">frac</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">:</span>
            <span class="n">part_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">frac</span> <span class="o">*</span> <span class="n">data_len</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">partitions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">indexes</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">part_len</span><span class="p">])</span>
            <span class="n">indexes</span> <span class="o">=</span> <span class="n">indexes</span><span class="p">[</span><span class="n">part_len</span><span class="p">:]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">use</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partition</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Partition</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">partitions</span><span class="p">[</span><span class="n">partition</span><span class="p">])</span>
</pre></div>
</div>
<p>위 코드를 사용하여 어떤 데이터셋도 몇 줄의 코드로 간단히 분할할 수 있습니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; MNIST 데이터셋 분할 &quot;&quot;&quot;</span>
<span class="k">def</span><span class="w"> </span><span class="nf">partition_dataset</span><span class="p">():</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                                 <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                 <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                             <span class="p">]))</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
    <span class="n">bsz</span> <span class="o">=</span> <span class="mi">128</span> <span class="o">//</span> <span class="n">size</span>
    <span class="n">partition_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">size</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">)]</span>
    <span class="n">partition</span> <span class="o">=</span> <span class="n">DataPartitioner</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">partition_sizes</span><span class="p">)</span>
    <span class="n">partition</span> <span class="o">=</span> <span class="n">partition</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">())</span>
    <span class="n">train_set</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">partition</span><span class="p">,</span>
                                         <span class="n">batch_size</span><span class="o">=</span><span class="n">bsz</span><span class="p">,</span>
                                         <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_set</span><span class="p">,</span> <span class="n">bsz</span>
</pre></div>
</div>
<p>2개의 복제본이 있다고 가정하고, 각각의 프로세스가 60000 / 2 = 30000 샘플의
<code class="docutils literal notranslate"><span class="pre">train_set</span></code> 을 가질 것입니다. 또한 <strong>전체</strong> 배치 크기를 128로 유지하기 위해
배치 크기를 복제본 수로 나누도록 하겠습니다.</p>
<p>이제 일반적인 순전파-역전파-최적화 학습 코드를 작성하고, 모델의 변화도 평균을
계산하는 함수를 추가하겠습니다. (아래 코드는 공식
<a class="reference external" href="https://github.com/pytorch/examples/blob/master/mnist/main.py">PyTorch MNIST 예제</a>
에서 많은 부분을 차용하였습니다.)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; 분산 동기(synchronous) SGD 예제 &quot;&quot;&quot;</span>
<span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
    <span class="n">train_set</span><span class="p">,</span> <span class="n">bsz</span> <span class="o">=</span> <span class="n">partition_dataset</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                          <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="n">num_batches</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_set</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">bsz</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">epoch_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_set</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">epoch_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">average_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(),</span> <span class="s1">&#39;, epoch &#39;</span><span class="p">,</span>
              <span class="n">epoch</span><span class="p">,</span> <span class="s1">&#39;: &#39;</span><span class="p">,</span> <span class="n">epoch_loss</span> <span class="o">/</span> <span class="n">num_batches</span><span class="p">)</span>
</pre></div>
</div>
<p>모델을 받아 전체 월드(world)의 평균 변화도를 계산하는 <code class="docutils literal notranslate"><span class="pre">average_gradients(model)</span></code>
함수를 구현하는 것이 남았습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; 변화도 평균 계산하기 &quot;&quot;&quot;</span>
<span class="k">def</span><span class="w"> </span><span class="nf">average_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">/=</span> <span class="n">size</span>
</pre></div>
</div>
<p><em>완성(Et voilà)</em>! 분산 동기(synchronous) SGD를 성공적으로 구현했으며 어떤 모델도
대형 연산 클러스터에서 학습할 수 있습니다.</p>
<p><strong>참고:</strong> 마지막 문장은 <em>기술적으로는</em> 참이지만, 동기식 SGD를 상용 수준(production-level)으로
구현하기 위해서는 <a class="reference external" href="https://seba-1511.github.io/dist_blog">더 많은 트릭</a> 이 필요합니다.
다시 말씀드리지만, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">테스트되고 최적화된</a>
것을 사용하십시오.</p>
<section id="ring-allreduce">
<h3>사용자 정의 링-올리듀스(Ring-Allreduce)<a class="headerlink" href="#ring-allreduce" title="Link to this heading">#</a></h3>
<p>추가로 DeepSpeech의 효율적인 링 올리듀스(ring allreduce)를 구현하고 싶다고 가정해보겠습니다.
이것은 점-대-점 집합 통신(point-to-point collectives)으로 쉽게 구현할 수 있습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot; 링-리듀스(ring-reduce) 구현 &quot;&quot;&quot;</span>
<span class="k">def</span><span class="w"> </span><span class="nf">allreduce</span><span class="p">(</span><span class="n">send</span><span class="p">,</span> <span class="n">recv</span><span class="p">):</span>
   <span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
   <span class="n">size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
   <span class="n">send_buff</span> <span class="o">=</span> <span class="n">send</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
   <span class="n">recv_buff</span> <span class="o">=</span> <span class="n">send</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
   <span class="n">accum</span> <span class="o">=</span> <span class="n">send</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

   <span class="n">left</span> <span class="o">=</span> <span class="p">((</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">size</span><span class="p">)</span> <span class="o">%</span> <span class="n">size</span>
   <span class="n">right</span> <span class="o">=</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">size</span>

   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
       <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
           <span class="c1"># Send send_buff</span>
           <span class="n">send_req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">send_buff</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span>
           <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">recv_buff</span><span class="p">,</span> <span class="n">left</span><span class="p">)</span>
           <span class="n">accum</span><span class="p">[:]</span> <span class="o">+=</span> <span class="n">recv_buff</span><span class="p">[:]</span>
       <span class="k">else</span><span class="p">:</span>
           <span class="c1"># Send recv_buff</span>
           <span class="n">send_req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">recv_buff</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span>
           <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">send_buff</span><span class="p">,</span> <span class="n">left</span><span class="p">)</span>
           <span class="n">accum</span><span class="p">[:]</span> <span class="o">+=</span> <span class="n">send_buff</span><span class="p">[:]</span>
       <span class="n">send_req</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
   <span class="n">recv</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">accum</span><span class="p">[:]</span>
</pre></div>
</div>
<p>위 스크립트에서, <code class="docutils literal notranslate"><span class="pre">allreduce(send,</span> <span class="pre">recv)</span></code> 함수는 PyTorch에 있는 것과는 약간
다른 특징을 가지고 있습니다. 이는 <code class="docutils literal notranslate"><span class="pre">recv</span></code> Tensor를 받은 후 모든 <code class="docutils literal notranslate"><span class="pre">send</span></code> Tensor의
합을 저장합니다. 여기에서 구현한 것과 DeepSpeech와는 다른 부분이 여전히 다른 부분이
있는데, 이것은 숙제로 남겨두도록 하겠습니다: DeepSpeech의 구현은 통신 대역폭을
최적으로 확용하기 위해 변화도 Tensor를 <em>덩어리(chunk)</em> 로 나눕니다.
(힌트: <a class="reference external" href="https://pytorch.org/docs/stable/torch.html#torch.chunk">torch.chunk</a>)</p>
</section>
</section>
<section id="advanced-topics">
<h2>고급 주제(Advanced Topics)<a class="headerlink" href="#advanced-topics" title="Link to this heading">#</a></h2>
<p>이제 <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> 보다 진보된 기능들을 살펴볼 준비가 되었습니다.
다루어야 할 주제들이 많으므로, 이 섹션을 다음과 같이 2개의 하위 섹션으로 나누도록
하겠습니다:</p>
<ol class="arabic simple">
<li><p>통신 백엔드: GPU와 GPU 간의 통신을 위해 MPI와 Gloo를 어떻게 사용해야 할지 배웁니다.</p></li>
<li><p>초기화 방법: <code class="docutils literal notranslate"><span class="pre">dist.init_process_group()</span></code> 에서 초기 구성 단계를 잘 설정하는 방법을
이해합니다.</p></li>
</ol>
<section id="communication-backends">
<h3>통신 백엔드(Communication Backends)<a class="headerlink" href="#communication-backends" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> 의 가장 우아한 면 중 하나는 다른 백엔드를 기반으로 추상화하고
구축하는 기능입니다. 앞에서 언급한 것처럼 현재 PyTorch에는 여러 백엔드들이 구현되어 있습니다.
이러한 백엔드들은 서로 다른 가속기(accelerator) 유형들에 해당하는 인터페이스를 제공하는
<a class="reference external" href="https://pytorch.org/docs/stable/torch.html#accelerators">Accelerator API</a> 를
사용하여 쉽게 선택할 수 있습니다. 일반적으로는 Gloo, NCLL 및 MPI를 많이 하숑합니다.
각각은 원하는 사용 사례에 따라 서로 다른 스펙과 트레이드오프(tradeoffs)를 갖습니다. 지원하는 기능의 비교표는
<a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#module-torch.distributed">여기</a>
에서 찾아보실 수 있습니다.</p>
<p><strong>Gloo 백엔드</strong></p>
<p>지금껏 우리는 <a class="reference external" href="https://github.com/facebookincubator/gloo">Gloo backend</a> 를
광범위하게 사용했습니다. 이것은 미리 컴파일된 PyTorch 바이너리가 포함되어 있으며
Linux(0.2 이상)와 macOS(1.3 이상)을 모두 지원하고 있어 개발 플랫폼으로 매우 편리합니다.
또한 CPU에서는 모든 저짐-대-지점 및 집합 연산들을, GPU에서는 집합 연산을 지원합니다.
CUDA Tensor에 대한 집합 연산 구현은 NCCL 백엔드에서 제공하는 것만큼 최적화되어
있지는 않습니다.</p>
<p>알고 계시겠지만, 위에서 만든 분산 SGD 예제는 GPU에 <code class="docutils literal notranslate"><span class="pre">model</span></code> 을 올리면 동작하지
않습니다. 여러 GPU를 사용하기 위해서는 아래와 같이 수정이 필요합니다:</p>
<ol class="arabic simple">
<li><p>가속기(Accelerator) API 사용: <code class="docutils literal notranslate"><span class="pre">device_type</span> <span class="pre">=</span> <span class="pre">torch.accelerator.current_accelerator()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.device(f&quot;{device_type}:{rank}&quot;)</span></code> 사용</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Net()</span></code> <span class="math">\(\rightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Net().to(device)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data,</span> <span class="pre">target</span> <span class="pre">=</span> <span class="pre">data.to(device),</span> <span class="pre">target.to(device)</span></code> 사용</p></li>
</ol>
<p>위와 같이 변경하고 나면 이제 2개의 GPU에서 모델이 학습을 하며, <code class="docutils literal notranslate"><span class="pre">watch</span> <span class="pre">nvidia-smi</span></code>
로 사용률을 모니터링할 수 있습니다.</p>
<p><strong>MPI 백엔드</strong></p>
<p>MPI(Message Passing Interface)는 고성능 컴퓨팅 분야의 표준 도구입니다.
이는 점-대-점 간 통신과 집합 통신을 허용하며 <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> 의 API에
영감을 주었습니다. 다양한 목적에 따라 최적화된 몇몇 MPI 구현체들(예.
<a class="reference external" href="https://www.open-mpi.org/">Open-MPI</a>,
<a class="reference external" href="http://mvapich.cse.ohio-state.edu/">MVAPICH2</a>,
<a class="reference external" href="https://software.intel.com/en-us/intel-mpi-library">Intel MPI</a> )이 있습니다.
MPI 백엔드를 사용하는 이점은 대규모 연산 클러스에서의 MPI의 폭넓은 가용성(과 높은
수준의 최적화)에 있습니다. 또한, <a class="reference external" href="https://developer.nvidia.com/mvapich">일부</a>
<a class="reference external" href="https://developer.nvidia.com/ibm-spectrum-mpi">최신</a>
<a class="reference external" href="https://www.open-mpi.org/">구현체들</a> 은 CPU를 통한 메모리 복사를 방지하기 위해
CUDA IPC와 GPU Direct 기술을 활용하고 있습니다.</p>
<p>불행하게도 PyTorch 바이너리는 MPI 구현을 포함할 수 없으므로 직접 재컴파일해야
합니다. 다행히도 이 과정은 매우 간단해서 PyTorch가 <em>스스로</em> 사용 가능한 MPI 구현체를
찾아볼 것입니다. 다음 단계들은 PyTorch를 <a class="reference external" href="https://github.com/pytorch/pytorch#from-source">소스로부터</a>
설치함으로써 MPI 백엔드를 설치하는 과정입니다.</p>
<ol class="arabic simple">
<li><p>아나콘다(Anaconda) 환경을 생성하고 활성화한 뒤
<a class="reference external" href="https://github.com/pytorch/pytorch#from-source">이 가이드</a> 를 따라서 모든
필요 사항들을 설치하시되, <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span></code> 은 아직 실행하지 <strong>마십시오.</strong></p></li>
<li><p>선호하는 MPI 구현체를 선택하고 설치하십시오. CUDA를 인식하는 MPI를 활성화하기
위해서는 추가적인 단계가 필요할 수 있습니다. 여기에서는 Open-MPI를 GPU <em>없이</em>
사용하도록 하겠습니다: <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">-c</span> <span class="pre">conda-forge</span> <span class="pre">openmpi</span></code></p></li>
<li><p>이제, 복제해둔 PyTorch 저장소로 가서 <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span></code> 을 실행하겠습니다.</p></li>
</ol>
<p>새로 설치한 백엔드를 테스트해보기 위해, 약간의 수정을 해보겠습니다.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">__name__</span> <span class="pre">==</span> <span class="pre">'__main__':</span></code> 아래 내용을 <code class="docutils literal notranslate"><span class="pre">init_process(0,</span> <span class="pre">0,</span> <span class="pre">run,</span> <span class="pre">backend='mpi')</span></code>
으로 변경합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mpirun</span> <span class="pre">-n</span> <span class="pre">4</span> <span class="pre">python</span> <span class="pre">myscript.py</span></code> 을 실행합니다.</p></li>
</ol>
<p>이러한 변경 사항은 MPI가 프로세스를 생성(spawn)하기 전에 자체적인 환경을 만들기
위해 필요합니다. MPI는 자신의 프로세스를 생성하고 <a class="reference external" href="#initialization-methods">초기화 방법</a>
에 설명된 핸드쉐이크(handshake)를 수행하여 <code class="docutils literal notranslate"><span class="pre">init_process_group</span></code> 의 <code class="docutils literal notranslate"><span class="pre">rank</span></code> 와
<code class="docutils literal notranslate"><span class="pre">size</span></code> 인자를 불필요하게 만듭니다. 이는 각 프로세스에 연산 리소스를 조절(tailor)할
수 있도록 추가적인 인자를 <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> 으로 전달할 수 있기 때문에 매우 강력합니다.
(프로세스당 코어 개수, 장비(machine)의 우선 순위 수동 할당 및
<a class="reference external" href="https://www.open-mpi.org/faq/?category=running#mpirun-hostfile">기타 다른 것</a>)
이렇게 함으로써, 다른 통신 백엔드와 같은 유사한 결과를 얻을 수 있습니다.</p>
<p><strong>NCCL 백엔드</strong></p>
<p><a class="reference external" href="https://github.com/nvidia/nccl">NCCL 백엔드</a> 는 CUDA Tensor들에 대한 집합 연산의
최적화된 구현체를 제공합니다. 집합 연산에 CUDA Tensor만 사용하는 경우, 동급 최고
성능을 위해 이 백엔드를 사용하는 것을 고려해보시기 바랍니다. NCCL 백엔드는 미리
빌드(pre-built)된 바이너리에 CUDA 지원과 함께 포함되어 있습니다.</p>
<p><strong>XCCL 백엔드</strong></p>
<p><cite>XCCL 백엔드</cite> 는 XPU Tensor들에 대한 집합 연산의 최적화된 구현체를 제공합니다.
만약 집합 연산에 XPU Tensor만 사용하는 경우, 이 백엔드는 동급 최고(Best-in-Class) 성능을 제공합니다.
XCCL 백엔드는 XPU 지원과 함께 미리 빌드(pre-built)된 바이너리에 포함되어 있습니다.</p>
</section>
<section id="initialization-methods">
<h3>초기화 방법(Initialization Methods)<a class="headerlink" href="#initialization-methods" title="Link to this heading">#</a></h3>
<p>마지막으로, 처음 호출했던 함수를 살펴보겠습니다: <code class="docutils literal notranslate"><span class="pre">dist.init_process_group(backend,</span> <span class="pre">init_method)</span></code>
특히, 각 프로세스 간의 초기 조정(initial coordination) 단계를 담당하는 다양한 초기화
방법들을 살펴보도록 하겠습니다. 이러한 방법들은 어떻게 이러한 조정이 수행되는지를
정의할 수 있게 합니다. 초기화 방법의 선택은 하드웨어 설정에 따라 다르며, 하나의 방법이 다른 방법보다 더 적합할 수 있습니다.
다음 섹션 외에도 <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#initialization">공식 문서</a>
를 참고하실 수 있습니다.</p>
<p><strong>환경 변수</strong></p>
<p>이 튜토리얼에서 지금까지는 환경 변수의 초기화 메소드를 사용해왔습니다. 모든 기기에서
아래 네가지 환경 변수를 설정하게 되면, 모든 프로세스들이 마스터(master)에 적합하게
연결하고, 다른 프로세스들의 정보를 얻은 후 핸드쉐이크까지 할 수 있습니다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MASTER_PORT</span></code>: 0-순위의 프로세스를 호스트할 기기의 비어있는 포트 번호(free port)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code>: 0-순위의 프로세스를 호스트할 기기의 IP 주소</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WORLD_SIZE</span></code>: 전체 프로세스 수 - 마스터가 얼마나 많은 워커들을 기다릴지 알 수 있습니다</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RANK</span></code>: 각 프로세스의 우선순위 - 워커 또는 마스터 여부를 확인할 수 있습니다.</p></li>
</ul>
<p><strong>공유 파일 시스템</strong></p>
<p>공유 파일 시스템은 모든 프로세스가 공유된 파일에의 접근 및 프로세스들간의 공유 파일을
조정(coordinate)하기 위해 필요합니다. 이것은 각 프로세스가 파일을 열고, 정보를 쓰고,
다른 프로세스들이 작업을 완료할 때까지 기다리게 하는 것을 뜻합니다. 필요한 모든
정보는 모든 프로세스들이 쉽게 사용할 수 있도록 합니다. 경쟁 조건(race conditions)을
피하기 위해, 파일 시스템은 반드시 <a class="reference external" href="http://man7.org/linux/man-pages/man2/fcntl.2.html">fcntl</a>
을 이용한 잠금을 지원해야 합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
    <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;file:///mnt/nfs/sharedfile&#39;</span><span class="p">,</span>
    <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
    <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>TCP</strong></p>
<p>0-순위 프로세스의 IP 주소와 접근 가능한 포트 번호가 있으면 TCP를 통한 초기화를 할
수 있습니다. 모든 워커들은 0-순위의 프로세스에 연결하고 서로 정보를 교환하는 방법에
대한 정보를 공유합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
    <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;tcp://10.1.1.20:23456&#39;</span><span class="p">,</span>
    <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
    <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<!--
## Internals
* The magic behind init_process_group:

1. validate and parse the arguments
2. resolve the backend: name2channel.at()
3. Drop GIL & THDProcessGroupInit: instantiate the channel and add address of master from config
4. rank 0 inits master, others workers
5. master: create sockets for all workers -> wait for all workers to connect -> send them each the info about location of other processes
6. worker: create socket to master, send own info, receive info about each worker, and then handshake with each of them
7. By this time everyone has handshake with everyone.
--><center><p><strong>감사의 말</strong></p>
</center><p>PyTorch 개발자분들께 구현, 문서화 및 테스트를 잘해주신 것에 감사드립니다. 코드가
불분명할 때는 언제나 <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html">문서</a>
또는 <a class="reference external" href="https://github.com/pytorch/pytorch/tree/master/test/distributed">테스트</a>
에서 답을 찾을 수 있었습니다. 또한 튜토리얼 초안에 대해 통찰력있는 의견과 질문에
답변을 해주신 Soumith Chintala, Adam Paszke 그리고 Natalia Gimelshei께도 감사드립니다.</p>
</section>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="ddp_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">분산 데이터 병렬 처리 시작하기</p>
      </div>
    </a>
    <a class="right-next"
       href="FSDP_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Getting Started with Fully Sharded Data Parallel (FSDP2)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ddp_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">분산 데이터 병렬 처리 시작하기</p>
      </div>
    </a>
    <a class="right-next"
       href="FSDP_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Getting Started with Fully Sharded Data Parallel (FSDP2)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">설정(Setup)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#point-to-point">점-대-점 간(Point-to-Point) 통신</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#collective-communication">집합 통신(Collective Communication)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-training">분산 학습(Distributed Training)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ring-allreduce">사용자 정의 링-올리듀스(Ring-Allreduce)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-topics">고급 주제(Advanced Topics)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#communication-backends">통신 백엔드(Communication Backends)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initialization-methods">초기화 방법(Initialization Methods)</a></li>
</ul>
</li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "PyTorch\ub85c \ubd84\uc0b0 \uc5b4\ud50c\ub9ac\ucf00\uc774\uc158 \uac1c\ubc1c\ud558\uae30",
       "headline": "PyTorch\ub85c \ubd84\uc0b0 \uc5b4\ud50c\ub9ac\ucf00\uc774\uc158 \uac1c\ubc1c\ud558\uae30",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/intermediate/dist_tuto.html",
       "articleBody": "PyTorch\ub85c \ubd84\uc0b0 \uc5b4\ud50c\ub9ac\ucf00\uc774\uc158 \uac1c\ubc1c\ud558\uae30# Author: S\u00e9b Arnold\ubc88\uc5ed: \ubc15\uc815\ud658 \ucc38\uace0 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \uc18c\uc2a4 \ucf54\ub4dc\ub294 GitHub \uc5d0\uc11c \ud655\uc778\ud558\uace0 \ubcc0\uacbd\ud574 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc120\uc218\uacfc\ubaa9(Prerequisites): PyTorch Distributed Overview \uc774 \uc9e7\uc740 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 PyTorch\uc758 \ubd84\uc0b0 \ud328\ud0a4\uc9c0\ub97c \ub458\ub7ec\ubcfc \uc608\uc815\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 \uc5b4\ub5bb\uac8c \ubd84\uc0b0 \ud658\uacbd\uc744 \uc124\uc815\ud558\ub294\uc9c0\uc640 \uc11c\ub85c \ub2e4\ub978 \ud1b5\uc2e0 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\ub294\uc9c0\ub97c \uc54c\uc544\ubcf4\uace0, \ud328\ud0a4\uc9c0 \ub0b4\ubd80\ub3c4 \uc77c\ubd80 \uc0b4\ud3b4\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc124\uc815(Setup)# PyTorch\uc5d0 \ud3ec\ud568\ub41c \ubd84\uc0b0 \ud328\ud0a4\uc9c0(\uc608. torch.distributed)\ub294 \uc5f0\uad6c\uc790\uc640 \uc2e4\ubb34\uc790\uac00 \uc5ec\ub7ec \ud504\ub85c\uc138\uc2a4\uc640 \ud074\ub7ec\uc2a4\ud130\uc758 \uae30\uae30\uc5d0\uc11c \uacc4\uc0b0\uc744 \uc27d\uac8c \ubcd1\ub82c\ud654 \ud560 \uc218 \uc788\uac8c \ud569\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574, \uac01 \ud504\ub85c\uc138\uc2a4\uac00 \ub2e4\ub978 \ud504\ub85c\uc138\uc2a4\uc640 \ub370\uc774\ud130\ub97c \uad50\ud658\ud560 \uc218 \uc788\ub3c4\ub85d \uba54\uc2dc\uc9c0 \uad50\ud658 \uaddc\uc57d(messaging passing semantics)\uc744 \ud65c\uc6a9\ud569\ub2c8\ub2e4. \uba40\ud2f0\ud504\ub85c\uc138\uc2f1(torch.multiprocessing) \ud328\ud0a4\uc9c0\uc640 \ub2ec\ub9ac, \ud504\ub85c\uc138\uc2a4\ub294 \ub2e4\ub978 \ucee4\ubba4\ub2c8\ucf00\uc774\uc158 \ubc31\uc5d4\ub4dc(backend)\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc73c\uba70 \ub3d9\uc77c \uae30\uae30 \uc0c1\uc5d0\uc11c \uc2e4\ud589\ub418\ub294 \uac83\uc5d0 \uc81c\uc57d\uc774 \uc5c6\uc2b5\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc2dc\uc791\ud558\uae30 \uc704\ud574 \uc5ec\ub7ec \ud504\ub85c\uc138\uc2a4\ub97c \ub3d9\uc2dc\uc5d0 \uc2e4\ud589\ud560 \uc218 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4. \uc5f0\uc0b0 \ud074\ub7ec\uc2a4\ud130\uc5d0 \uc811\uadfc\ud558\ub294 \uacbd\uc6b0\uc5d0\ub294 \uc2dc\uc2a4\ud15c \uad00\ub9ac\uc790\uc5d0\uac8c \ud655\uc778\ud558\uac70\ub098 \uc120\ud638\ud558\ub294 \ucf54\ub514\ub124\uc774\uc158 \ub3c4\uad6c(coordination tool)\ub97c \uc0ac\uc6a9\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4. (\uc608. pdsh, clustershell, \ub610\ub294 slurm). \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 \ub2e4\uc74c \ud15c\ud50c\ub9bf\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e8\uc77c \uae30\uae30\uc5d0\uc11c \uc5ec\ub7ec \ud504\ub85c\uc138\uc2a4\ub97c \uc0dd\uc131(fork)\ud558\uaca0\uc2b5\ub2c8\ub2e4. \"\"\"run.py:\"\"\" #!/usr/bin/env python import os import sys import torch import torch.distributed as dist import torch.multiprocessing as mp def run(rank, size): \"\"\" Distributed function to be implemented later. \"\"\" pass def init_process(rank, size, fn, backend=\u0027gloo\u0027): \"\"\" Initialize the distributed environment. \"\"\" os.environ[\u0027MASTER_ADDR\u0027] = \u0027127.0.0.1\u0027 os.environ[\u0027MASTER_PORT\u0027] = \u002729500\u0027 dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) if __name__ == \"__main__\": world_size = 2 processes = [] if \"google.colab\" in sys.modules: print(\"Running in Google Colab\") mp.get_context(\"spawn\") else: mp.set_start_method(\"spawn\") for rank in range(world_size): p = mp.Process(target=init_process, args=(rank, world_size, run)) p.start() processes.append(p) for p in processes: p.join() \uc704 \uc2a4\ud06c\ub9bd\ud2b8\ub294 2\uac1c\uc758 \ud504\ub85c\uc138\uc2a4\ub97c \uc0dd\uc131(spawn)\ud558\uc5ec \uac01\uc790 \ub2e4\ub978 \ubd84\uc0b0 \ud658\uacbd\uc744 \uc124\uc815\ud558\uace0, \ud504\ub85c\uc138\uc2a4 \uadf8\ub8f9(dist.init_process_group)\uc744 \ucd08\uae30\ud654\ud558\uace0, \ucd5c\uc885\uc801\uc73c\ub85c\ub294 run \ud568\uc218\ub97c \uc2e4\ud589\ud569\ub2c8\ub2e4. \uc774\uc81c init_process \ud568\uc218\ub97c \uc0b4\ud3b4\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774 \ud568\uc218\ub294 \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uac00 \ub9c8\uc2a4\ud130\ub97c \ud1b5\ud574 \uc870\uc815(coordinate)\ub420 \uc218 \uc788\ub3c4\ub85d \ub3d9\uc77c\ud55c IP \uc8fc\uc18c\uc640 \ud3ec\ud2b8\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 gloo \ubc31\uc5d4\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc600\uc73c\ub098 \ub2e4\ub978 \ubc31\uc5d4\ub4dc\ub4e4\ub3c4 \uc0ac\uc6a9\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. (\uc139\uc158 5.1 \ucc38\uace0) \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc758 \ub9c8\uc9c0\ub9c9 \ubd80\ubd84\uc5d0 \uc788\ub294 dist.init_process_group \uc5d0\uc11c \uc77c\uc5b4\ub098\ub294 \ub180\ub77c\uc6b4 \uc77c\uc744 \uc0b4\ud3b4\ubcfc \uac83\uc774\uc9c0\ub9cc, \uae30\ubcf8\uc801\uc73c\ub85c\ub294 \ud504\ub85c\uc138\uc2a4\uac00 \uc790\uc2e0\uc758 \uc704\uce58\ub97c \uacf5\uc720\ud568\uc73c\ub85c\uc368 \uc11c\ub85c \ud1b5\uc2e0\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. \uc810-\ub300-\uc810 \uac04(Point-to-Point) \ud1b5\uc2e0# \uc1a1\uc2e0\uacfc \uc218\uc2e0# \ud558\ub098\uc758 \ud504\ub85c\uc138\uc2a4\uc5d0\uc11c \ub2e4\ub978 \ud504\ub85c\uc138\uc2a4\ub85c \ub370\uc774\ud130\ub97c \uc804\uc1a1\ud558\ub294 \uac83\uc744 \uc810-\ub300-\uc810 \uac04 \ud1b5\uc2e0\uc774\ub77c\uace0 \ud569\ub2c8\ub2e4. \uc9c0\uc810\uac04 \ud1b5\uc2e0\uc740 send/recv \ud568\uc218 \ub610\ub294 \uc989\uc2dc \uc751\ub2f5\ud558\ub294(immediate counter-parts) isend \uc640 irecv \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \"\"\"\ube14\ub85c\ud0b9(blocking) \uc810-\ub300-\uc810 \uac04 \ud1b5\uc2e0\"\"\" def run(rank, size): tensor = torch.zeros(1) if rank == 0: tensor += 1 # Send the tensor to process 1 dist.send(tensor=tensor, dst=1) else: # Receive tensor from process 0 dist.recv(tensor=tensor, src=0) print(\u0027Rank \u0027, rank, \u0027 has data \u0027, tensor[0]) \uc704 \uc608\uc81c\uc5d0\uc11c \ub450 \ud504\ub85c\uc138\uc2a4\ub294 \uac12\uc774 0\uc778 Tensor\ub85c \uc2dc\uc791\ud55c \ud6c4, 0\ubc88 \ud504\ub85c\uc138\uc2a4\uac00 Tensor\uc758 \uac12\uc744 \uc99d\uac00\uc2dc\ud0a8 \ud6c4 1\ubc88 \ud504\ub85c\uc138\uc2a4\ub85c \ubcf4\ub0b4\uc11c \ub458 \ub2e4 1.0\uc73c\ub85c \uc885\ub8cc\ub429\ub2c8\ub2e4. \uc774 \ub54c, \ud504\ub85c\uc138\uc2a4 1\uc740 \uc218\uc2e0\ud55c \ub370\uc774\ud130\ub97c \uc800\uc7a5\ud560 \uba54\ubaa8\ub9ac\ub97c \ud560\ub2f9\ud574\ub450\uc5b4\uc57c \ud569\ub2c8\ub2e4. \ub610\ud55c send/recv \ub294 \ubaa8\ub450 \ube14\ub85c\ud0b9 \uc785\ub2c8\ub2e4: \ub450 \ud504\ub85c\uc138\uc2a4\ub294 \ud1b5\uc2e0\uc774 \uc644\ub8cc\ub420 \ub54c\uae4c\uc9c0 \uba48\ucdb0\uc788\uc2b5\ub2c8\ub2e4. \ubc18\uba74\uc5d0 \uc989\uc2dc \uc751\ub2f5\ud558\ub294 \uac83\uc774 \ub17c-\ube14\ub85c\ud0b9 \uc785\ub2c8\ub2e4; \uc2a4\ud06c\ub9bd\ud2b8\ub294 \uc2e4\ud589\uc744 \uacc4\uc18d\ud558\uace0 \uba54\uc18c\ub4dc\ub294 wait() \ub97c \uc120\ud0dd\ud560 \uc218 \uc788\ub294 Work \uac1d\uccb4\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4. \"\"\"\ub17c-\ube14\ub85c\ud0b9(non-blocking) \uc810-\ub300-\uc810 \uac04 \ud1b5\uc2e0\"\"\" def run(rank, size): tensor = torch.zeros(1) req = None if rank == 0: tensor += 1 # Send the tensor to process 1 req = dist.isend(tensor=tensor, dst=1) print(\u0027Rank 0 started sending\u0027) else: # Receive tensor from process 0 req = dist.irecv(tensor=tensor, src=0) print(\u0027Rank 1 started receiving\u0027) req.wait() print(\u0027Rank \u0027, rank, \u0027 has data \u0027, tensor[0]) \uc989\uc2dc \uc751\ub2f5\ud558\ub294 \ud568\uc218\ub4e4\uc744 \uc0ac\uc6a9\ud560 \ub54c\ub294 Tensor\ub97c \uc5b4\ub5bb\uac8c \uc8fc\uace0 \ubc1b\uc744\uc9c0\ub97c \uc8fc\uc758\ud574\uc57c \ud569\ub2c8\ub2e4. \ub370\uc774\ud130\uac00 \uc5b8\uc81c \ub2e4\ub978 \ud504\ub85c\uc138\uc2a4\ub85c \uc1a1\uc218\uc2e0\ub418\ub294\uc9c0 \ubaa8\ub974\uae30 \ub54c\ubb38\uc5d0, req.wait() \uac00 \uc644\ub8cc\ub418\uae30 \uc804\uc5d0\ub294 \uc804\uc1a1\ub41c Tensor\ub97c \uc218\uc815\ud558\uac70\ub098 \uc218\uc2e0\ub41c Tensor\uc5d0 \uc811\uadfc\ud574\uc11c\ub294 \uc548\ub429\ub2c8\ub2e4. dist.isend() \ub2e4\uc74c\uc5d0 tensor \uc5d0 \uc4f0\uba74 \uc815\uc758\ub418\uc9c0 \uc54a\uc740 \ub3d9\uc791\uc774 \ubc1c\uc0dd\ud569\ub2c8\ub2e4. req.wait() \uac00 \uc2e4\ud589\ub418\uae30 \uc804\uae4c\uc9c0, dist.irecv() \ub2e4\uc74c\uc5d0 tensor \ub97c \uc77d\uc73c\uba74 \uc815\uc758\ub418\uc9c0 \uc54a\uc740 \ub3d9\uc791\uc774 \ubc1c\uc0dd\ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098, req.wait() \ub97c \uc2e4\ud589\ud55c \ud6c4\uc5d0\ub294 \ud1b5\uc2e0\uc774 \uc774\ub8e8\uc5b4\uc9c4 \uac83\uc744 \ubcf4\uc7a5\ubc1b\uc744 \uc218 \uc788\uae30 \ub54c\ubb38\uc5d0, tensor[0] \uc5d0 \uc800\uc7a5\ub41c \uac12\uc740 1.0\uc774 \ub429\ub2c8\ub2e4. \uc810-\ub300-\uc810 \uac04 \ud1b5\uc2e0\uc740 \ud504\ub85c\uc138\uc2a4 \uac04 \ud1b5\uc2e0\uc5d0 \ub300\ud55c \ub354 \uc138\ubc00\ud55c \uc81c\uc5b4\ub97c \uc6d0\ud560 \ub54c \uc720\uc6a9\ud569\ub2c8\ub2e4. \ubc14\uc774\ub450(Baidu)\uc758 DeepSpeech \ub098 \ud398\uc774\uc2a4\ubd81(Facebook)\uc758 \ub300\uaddc\ubaa8 \uc2e4\ud5d8 \uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uac83\uacfc \uac19\uc740 \uba4b\uc9c4 \uc54c\uace0\ub9ac\uc998\uc744 \uad6c\ud604\ud560 \ub54c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\uc139\uc158 4.1 \ucc38\uace0) \uc9d1\ud569 \ud1b5\uc2e0(Collective Communication)# Scatter# Gather# Reduce# All-Reduce# Broadcast# All-Gather# \uc810-\ub300-\uc810 \uac04 \ud1b5\uc2e0\uacfc \ub2ec\ub9ac \uc9d1\ud569 \ud1b5\uc2e0\uc740 \uadf8\ub8f9 \uc758 \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uc5d0 \uac78\uce5c \ud1b5\uc2e0 \ud328\ud134\uc744 \ud5c8\uc6a9\ud569\ub2c8\ub2e4. \uadf8\ub8f9\uc740 \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uc758 \ubd80\ubd84 \uc9d1\ud569\uc785\ub2c8\ub2e4. \uadf8\ub8f9\uc744 \uc0dd\uc131\ud558\uae30 \uc704\ud574\uc11c\ub294 dist.new_group(group) \uc5d0 \uc21c\uc11c(rank) \ubaa9\ub85d\uc744 \uc804\ub2ec\ud569\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc73c\ub85c, \uc9d1\ud569 \ud1b5\uc2e0\uc740 \uc6d4\ub4dc(world) \ub77c\uace0 \ubd80\ub974\ub294 \uc804\uccb4 \ud504\ub85c\uc138\uc2a4\uc5d0\uc11c \uc2e4\ud589\ub429\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uc5d0 \uc874\uc7ac\ud558\ub294 \ubaa8\ub4e0 Tensor\ub4e4\uc758 \ud569\uc744 \uc5bb\uae30 \uc704\ud574\uc11c\ub294 dist.all_reduce(tensor, op, group) \uc744 \uc0ac\uc6a9\ud558\uba74 \ub429\ub2c8\ub2e4. \"\"\" All-Reduce \uc608\uc81c \"\"\" def run(rank, size): \"\"\" \uac04\ub2e8\ud55c \uc9d1\ud569 \ud1b5\uc2e0 \"\"\" group = dist.new_group([0, 1]) tensor = torch.ones(1) dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group) print(\u0027Rank \u0027, rank, \u0027 has data \u0027, tensor[0]) \uadf8\ub8f9 \ub0b4\uc758 \ubaa8\ub4e0 Tensor\ub4e4\uc758 \ud569\uc774 \ud544\uc694\ud558\uae30 \ub54c\ubb38\uc5d0, dist.ReduceOp.SUM \uc744 \ub9ac\ub4c0\uc2a4(reduce) \uc5f0\uc0b0\uc790\ub85c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c, \uad50\ud658 \ubc95\uce59\uc774 \ud5c8\uc6a9\ub418\ub294(commutative) \ubaa8\ub4e0 \uc218\ud559 \uc5f0\uc0b0\uc744 \uc5f0\uc0b0\uc790\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. PyTorch\ub294 \uc694\uc18c\ubcc4(element-wise)\ub85c \ub3d9\uc791\ud558\ub294 \uae30\ubcf8\uc801\uc73c\ub85c 4\uac1c\uc758 \uc5f0\uc0b0\uc790\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. dist.ReduceOp.SUM, dist.ReduceOp.PRODUCT, dist.ReduceOp.MAX, dist.ReduceOp.MIN, dist.ReduceOp.BAND, dist.ReduceOp.BOR, dist.ReduceOp.BXOR, dist.ReduceOp.PREMUL_SUM. \uc9c0\uc6d0\ud558\ub294 \uc5f0\uc0b0\uc758 \uc804\uccb4 \ubaa9\ub85d\uc740 \uc5ec\uae30 \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. PyTorch\uc5d0\ub294 \ud604\uc7ac dist.all_reduce(tensor, op, group) \uc678\uc5d0\ub3c4 \ub9ce\uc740 \ucd94\uac00\uc801\uc778 \uc9d1\ud569 \ud1b5\uc2e0\uc774 \uad6c\ud604\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30 \uba87 \uac00\uc9c0 \uc9c0\uc6d0\ub418\ub294 \uc9d1\ud569 \ud1b5\uc2e0\uc744 \uc18c\uac1c\ud569\ub2c8\ub2e4. dist.broadcast(tensor, src, group): src \uc758 tensor \ub97c \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uc758 tensor \uc5d0 \ubcf5\uc0ac\ud569\ub2c8\ub2e4. dist.reduce(tensor, dst, op, group): op \ub97c \ubaa8\ub4e0 tensor \uc5d0 \uc801\uc6a9\ud55c \ub4a4 \uacb0\uacfc\ub97c dst \ud504\ub85c\uc138\uc2a4\uc758 tensor \uc5d0 \uc800\uc7a5\ud569\ub2c8\ub2e4. dist.all_reduce(tensor, op, group): \ub9ac\ub4c0\uc2a4\uc640 \ub3d9\uc77c\ud558\uc9c0\ub9cc, \uacb0\uacfc\uac00 \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uc758 tensor \uc5d0 \uc800\uc7a5\ub429\ub2c8\ub2e4. dist.scatter(tensor, scatter_list, src, group): \\(i^{\\text{\ubc88\uc9f8}}\\) Tensor scatter_list[i] \ub97c \\(i^{\\text{\ubc88\uc9f8}}\\) \ud504\ub85c\uc138\uc2a4\uc758 tensor \uc5d0 \ubcf5\uc0ac\ud569\ub2c8\ub2e4. dist.gather(tensor, gather_list, dst, group): \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uc758 tensor \ub97c dst \ud504\ub85c\uc138\uc2a4\uc758 gather_list \uc5d0 \ubcf5\uc0ac\ud569\ub2c8\ub2e4. dist.all_gather(tensor_list, tensor, group): \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uc758 tensor \ub97c \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uc758 tensor_list \uc5d0 \ubcf5\uc0ac\ud569\ub2c8\ub2e4. dist.barrier(group): group \ub0b4\uc758 \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uac00 \uc774 \ud568\uc218\uc5d0 \uc9c4\uc785\ud560 \ub54c\uae4c\uc9c0 group \ub0b4\uc758 \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\ub97c \uba48\ucda5(block)\ub2c8\ub2e4. dist.all_to_all(output_tensor_list, input_tensor_list, group): group \ub0b4\uc758 \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uc5d0 input_tensor_list \uc758 Tensor\ub4e4\uc744 \ubd84\uc0b0\ud558\uace0, output_tensor_list \uc5d0 \uc218\uc9d1\ub41c Tensor\ub4e4\uc744 \ubc18\ud658\ud569\ub2c8\ub2e4. \uc9c0\uc6d0\ub418\ub294 \uc9d1\ud569 \ud1b5\uc2e0\uc758 \uc804\uccb4 \ubaa9\ub85d\uc740 PyTorch Distributed\uc758 \ucd5c\uc2e0 \ubb38\uc11c (\ub9c1\ud06c) \ub97c \ucc38\uace0\ud558\uc138\uc694. \ubd84\uc0b0 \ud559\uc2b5(Distributed Training)# \ucc38\uace0: \uc774 \uc139\uc158\uc758 \uc608\uc81c \uc2a4\ud06c\ub9bd\ud2b8\ub4e4\uc740 \uc774 GitHub \uc800\uc7a5\uc18c \uc5d0\uc11c \ucc3e\uc544\ubcf4\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uc81c \ubd84\uc0b0 \ubaa8\ub4c8\uc774 \uc5b4\ub5bb\uac8c \ub3d9\uc791\ud558\ub294\uc9c0 \uc774\ud574\ud588\uc73c\ubbc0\ub85c, \uc720\uc6a9\ud55c \ubb54\uac00\ub97c \uc791\uc131\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. DistributedDataParallel \uc758 \uae30\ub2a5\uc744 \ubcf5\uc81c\ud574\ubcf4\ub294 \uac83\uc774 \ubaa9\ud45c\uc785\ub2c8\ub2e4. \ubb3c\ub860, \uc774\uac83\uc740 \uad50\ud6c8\uc801\uc778(didactic) \uc608\uc81c\uc774\ubbc0\ub85c \uc2e4\uc81c \uc0c1\ud669\uc5d0\uc11c\ub294 \uc704\uc5d0 \ub9c1\ud06c\ub41c \uc798 \ud14c\uc2a4\ud2b8\ub418\uace0 \ucd5c\uc801\ud654\ub41c \uacf5\uc2dd \ubc84\uc804\uc744 \uc0ac\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4. \ub9e4\uc6b0 \uac04\ub2e8\ud558\uac8c \ud655\ub960\uc801 \uacbd\uc0ac \ud558\uac15\ubc95(SGD)\uc758 \ubd84\uc0b0 \ubc84\uc804\uc744 \uad6c\ud604\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc2a4\ud06c\ub9bd\ud2b8\ub294 \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uac00 \uac01\uc790\uc758 \ub370\uc774\ud130 \ubc30\uce58(batch)\uc5d0\uc11c \uac01\uc790\uc758 \ubaa8\ub378\uc758 \ubcc0\ud654\ub3c4(gradient)\ub97c \uacc4\uc0b0\ud55c \ud6c4 \ud3c9\uade0\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4. \ud504\ub85c\uc138\uc2a4\uc758 \uc218\ub97c \ubcc0\uacbd\ud574\ub3c4 \uc720\uc0ac\ud55c \uc218\ub834 \uacb0\uacfc\ub97c \ubcf4\uc7a5\ud558\uae30 \uc704\ud574, \uba3c\uc800 \ub370\uc774\ud130\uc14b\uc744 \ubd84\ud560\ud574\uc57c \ud569\ub2c8\ub2e4. (\uc544\ub798 \ucf54\ub4dc \ub300\uc2e0 torch.utils.data.random_split \uc744 \uc0ac\uc6a9\ud574\ub3c4 \ub429\ub2c8\ub2e4.) \"\"\" \ub370\uc774\ud130\uc14b \ubd84\ud560 \ud5ec\ud37c(helper) \"\"\" class Partition(object): def __init__(self, data, index): self.data = data self.index = index def __len__(self): return len(self.index) def __getitem__(self, index): data_idx = self.index[index] return self.data[data_idx] class DataPartitioner(object): def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234): self.data = data self.partitions = [] rng = Random() # from random import Random rng.seed(seed) data_len = len(data) indexes = [x for x in range(0, data_len)] rng.shuffle(indexes) for frac in sizes: part_len = int(frac * data_len) self.partitions.append(indexes[0:part_len]) indexes = indexes[part_len:] def use(self, partition): return Partition(self.data, self.partitions[partition]) \uc704 \ucf54\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5b4\ub5a4 \ub370\uc774\ud130\uc14b\ub3c4 \uba87 \uc904\uc758 \ucf54\ub4dc\ub85c \uac04\ub2e8\ud788 \ubd84\ud560\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: \"\"\" MNIST \ub370\uc774\ud130\uc14b \ubd84\ud560 \"\"\" def partition_dataset(): dataset = datasets.MNIST(\u0027./data\u0027, train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])) size = dist.get_world_size() bsz = 128 // size partition_sizes = [1.0 / size for _ in range(size)] partition = DataPartitioner(dataset, partition_sizes) partition = partition.use(dist.get_rank()) train_set = torch.utils.data.DataLoader(partition, batch_size=bsz, shuffle=True) return train_set, bsz 2\uac1c\uc758 \ubcf5\uc81c\ubcf8\uc774 \uc788\ub2e4\uace0 \uac00\uc815\ud558\uace0, \uac01\uac01\uc758 \ud504\ub85c\uc138\uc2a4\uac00 60000 / 2 = 30000 \uc0d8\ud50c\uc758 train_set \uc744 \uac00\uc9c8 \uac83\uc785\ub2c8\ub2e4. \ub610\ud55c \uc804\uccb4 \ubc30\uce58 \ud06c\uae30\ub97c 128\ub85c \uc720\uc9c0\ud558\uae30 \uc704\ud574 \ubc30\uce58 \ud06c\uae30\ub97c \ubcf5\uc81c\ubcf8 \uc218\ub85c \ub098\ub204\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774\uc81c \uc77c\ubc18\uc801\uc778 \uc21c\uc804\ud30c-\uc5ed\uc804\ud30c-\ucd5c\uc801\ud654 \ud559\uc2b5 \ucf54\ub4dc\ub97c \uc791\uc131\ud558\uace0, \ubaa8\ub378\uc758 \ubcc0\ud654\ub3c4 \ud3c9\uade0\uc744 \uacc4\uc0b0\ud558\ub294 \ud568\uc218\ub97c \ucd94\uac00\ud558\uaca0\uc2b5\ub2c8\ub2e4. (\uc544\ub798 \ucf54\ub4dc\ub294 \uacf5\uc2dd PyTorch MNIST \uc608\uc81c \uc5d0\uc11c \ub9ce\uc740 \ubd80\ubd84\uc744 \ucc28\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.) \"\"\" \ubd84\uc0b0 \ub3d9\uae30(synchronous) SGD \uc608\uc81c \"\"\" def run(rank, size): torch.manual_seed(1234) train_set, bsz = partition_dataset() model = Net() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) num_batches = ceil(len(train_set.dataset) / float(bsz)) for epoch in range(10): epoch_loss = 0.0 for data, target in train_set: optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) epoch_loss += loss.item() loss.backward() average_gradients(model) optimizer.step() print(\u0027Rank \u0027, dist.get_rank(), \u0027, epoch \u0027, epoch, \u0027: \u0027, epoch_loss / num_batches) \ubaa8\ub378\uc744 \ubc1b\uc544 \uc804\uccb4 \uc6d4\ub4dc(world)\uc758 \ud3c9\uade0 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud558\ub294 average_gradients(model) \ud568\uc218\ub97c \uad6c\ud604\ud558\ub294 \uac83\uc774 \ub0a8\uc558\uc2b5\ub2c8\ub2e4. \"\"\" \ubcc0\ud654\ub3c4 \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30 \"\"\" def average_gradients(model): size = float(dist.get_world_size()) for param in model.parameters(): dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM) param.grad.data /= size \uc644\uc131(Et voil\u00e0)! \ubd84\uc0b0 \ub3d9\uae30(synchronous) SGD\ub97c \uc131\uacf5\uc801\uc73c\ub85c \uad6c\ud604\ud588\uc73c\uba70 \uc5b4\ub5a4 \ubaa8\ub378\ub3c4 \ub300\ud615 \uc5f0\uc0b0 \ud074\ub7ec\uc2a4\ud130\uc5d0\uc11c \ud559\uc2b5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ucc38\uace0: \ub9c8\uc9c0\ub9c9 \ubb38\uc7a5\uc740 \uae30\uc220\uc801\uc73c\ub85c\ub294 \ucc38\uc774\uc9c0\ub9cc, \ub3d9\uae30\uc2dd SGD\ub97c \uc0c1\uc6a9 \uc218\uc900(production-level)\uc73c\ub85c \uad6c\ud604\ud558\uae30 \uc704\ud574\uc11c\ub294 \ub354 \ub9ce\uc740 \ud2b8\ub9ad \uc774 \ud544\uc694\ud569\ub2c8\ub2e4. \ub2e4\uc2dc \ub9d0\uc500\ub4dc\ub9ac\uc9c0\ub9cc, \ud14c\uc2a4\ud2b8\ub418\uace0 \ucd5c\uc801\ud654\ub41c \uac83\uc744 \uc0ac\uc6a9\ud558\uc2ed\uc2dc\uc624. \uc0ac\uc6a9\uc790 \uc815\uc758 \ub9c1-\uc62c\ub9ac\ub4c0\uc2a4(Ring-Allreduce)# \ucd94\uac00\ub85c DeepSpeech\uc758 \ud6a8\uc728\uc801\uc778 \ub9c1 \uc62c\ub9ac\ub4c0\uc2a4(ring allreduce)\ub97c \uad6c\ud604\ud558\uace0 \uc2f6\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc740 \uc810-\ub300-\uc810 \uc9d1\ud569 \ud1b5\uc2e0(point-to-point collectives)\uc73c\ub85c \uc27d\uac8c \uad6c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \"\"\" \ub9c1-\ub9ac\ub4c0\uc2a4(ring-reduce) \uad6c\ud604 \"\"\" def allreduce(send, recv): rank = dist.get_rank() size = dist.get_world_size() send_buff = send.clone() recv_buff = send.clone() accum = send.clone() left = ((rank - 1) + size) % size right = (rank + 1) % size for i in range(size - 1): if i % 2 == 0: # Send send_buff send_req = dist.isend(send_buff, right) dist.recv(recv_buff, left) accum[:] += recv_buff[:] else: # Send recv_buff send_req = dist.isend(recv_buff, right) dist.recv(send_buff, left) accum[:] += send_buff[:] send_req.wait() recv[:] = accum[:] \uc704 \uc2a4\ud06c\ub9bd\ud2b8\uc5d0\uc11c, allreduce(send, recv) \ud568\uc218\ub294 PyTorch\uc5d0 \uc788\ub294 \uac83\uacfc\ub294 \uc57d\uac04 \ub2e4\ub978 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 recv Tensor\ub97c \ubc1b\uc740 \ud6c4 \ubaa8\ub4e0 send Tensor\uc758 \ud569\uc744 \uc800\uc7a5\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\uc11c \uad6c\ud604\ud55c \uac83\uacfc DeepSpeech\uc640\ub294 \ub2e4\ub978 \ubd80\ubd84\uc774 \uc5ec\uc804\ud788 \ub2e4\ub978 \ubd80\ubd84\uc774 \uc788\ub294\ub370, \uc774\uac83\uc740 \uc219\uc81c\ub85c \ub0a8\uaca8\ub450\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4: DeepSpeech\uc758 \uad6c\ud604\uc740 \ud1b5\uc2e0 \ub300\uc5ed\ud3ed\uc744 \ucd5c\uc801\uc73c\ub85c \ud655\uc6a9\ud558\uae30 \uc704\ud574 \ubcc0\ud654\ub3c4 Tensor\ub97c \ub369\uc5b4\ub9ac(chunk) \ub85c \ub098\ub215\ub2c8\ub2e4. (\ud78c\ud2b8: torch.chunk) \uace0\uae09 \uc8fc\uc81c(Advanced Topics)# \uc774\uc81c torch.distributed \ubcf4\ub2e4 \uc9c4\ubcf4\ub41c \uae30\ub2a5\ub4e4\uc744 \uc0b4\ud3b4\ubcfc \uc900\ube44\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4. \ub2e4\ub8e8\uc5b4\uc57c \ud560 \uc8fc\uc81c\ub4e4\uc774 \ub9ce\uc73c\ubbc0\ub85c, \uc774 \uc139\uc158\uc744 \ub2e4\uc74c\uacfc \uac19\uc774 2\uac1c\uc758 \ud558\uc704 \uc139\uc158\uc73c\ub85c \ub098\ub204\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4: \ud1b5\uc2e0 \ubc31\uc5d4\ub4dc: GPU\uc640 GPU \uac04\uc758 \ud1b5\uc2e0\uc744 \uc704\ud574 MPI\uc640 Gloo\ub97c \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud574\uc57c \ud560\uc9c0 \ubc30\uc6c1\ub2c8\ub2e4. \ucd08\uae30\ud654 \ubc29\ubc95: dist.init_process_group() \uc5d0\uc11c \ucd08\uae30 \uad6c\uc131 \ub2e8\uacc4\ub97c \uc798 \uc124\uc815\ud558\ub294 \ubc29\ubc95\uc744 \uc774\ud574\ud569\ub2c8\ub2e4. \ud1b5\uc2e0 \ubc31\uc5d4\ub4dc(Communication Backends)# torch.distributed \uc758 \uac00\uc7a5 \uc6b0\uc544\ud55c \uba74 \uc911 \ud558\ub098\ub294 \ub2e4\ub978 \ubc31\uc5d4\ub4dc\ub97c \uae30\ubc18\uc73c\ub85c \ucd94\uc0c1\ud654\ud558\uace0 \uad6c\ucd95\ud558\ub294 \uae30\ub2a5\uc785\ub2c8\ub2e4. \uc55e\uc5d0\uc11c \uc5b8\uae09\ud55c \uac83\ucc98\ub7fc \ud604\uc7ac PyTorch\uc5d0\ub294 \uc5ec\ub7ec \ubc31\uc5d4\ub4dc\ub4e4\uc774 \uad6c\ud604\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubc31\uc5d4\ub4dc\ub4e4\uc740 \uc11c\ub85c \ub2e4\ub978 \uac00\uc18d\uae30(accelerator) \uc720\ud615\ub4e4\uc5d0 \ud574\ub2f9\ud558\ub294 \uc778\ud130\ud398\uc774\uc2a4\ub97c \uc81c\uacf5\ud558\ub294 Accelerator API \ub97c \uc0ac\uc6a9\ud558\uc5ec \uc27d\uac8c \uc120\ud0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c\ub294 Gloo, NCLL \ubc0f MPI\ub97c \ub9ce\uc774 \ud558\uc211\ud569\ub2c8\ub2e4. \uac01\uac01\uc740 \uc6d0\ud558\ub294 \uc0ac\uc6a9 \uc0ac\ub840\uc5d0 \ub530\ub77c \uc11c\ub85c \ub2e4\ub978 \uc2a4\ud399\uacfc \ud2b8\ub808\uc774\ub4dc\uc624\ud504(tradeoffs)\ub97c \uac16\uc2b5\ub2c8\ub2e4. \uc9c0\uc6d0\ud558\ub294 \uae30\ub2a5\uc758 \ube44\uad50\ud45c\ub294 \uc5ec\uae30 \uc5d0\uc11c \ucc3e\uc544\ubcf4\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Gloo \ubc31\uc5d4\ub4dc \uc9c0\uae08\uaecf \uc6b0\ub9ac\ub294 Gloo backend \ub97c \uad11\ubc94\uc704\ud558\uac8c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc740 \ubbf8\ub9ac \ucef4\ud30c\uc77c\ub41c PyTorch \ubc14\uc774\ub108\ub9ac\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70 Linux(0.2 \uc774\uc0c1)\uc640 macOS(1.3 \uc774\uc0c1)\uc744 \ubaa8\ub450 \uc9c0\uc6d0\ud558\uace0 \uc788\uc5b4 \uac1c\ubc1c \ud50c\ub7ab\ud3fc\uc73c\ub85c \ub9e4\uc6b0 \ud3b8\ub9ac\ud569\ub2c8\ub2e4. \ub610\ud55c CPU\uc5d0\uc11c\ub294 \ubaa8\ub4e0 \uc800\uc9d0-\ub300-\uc9c0\uc810 \ubc0f \uc9d1\ud569 \uc5f0\uc0b0\ub4e4\uc744, GPU\uc5d0\uc11c\ub294 \uc9d1\ud569 \uc5f0\uc0b0\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. CUDA Tensor\uc5d0 \ub300\ud55c \uc9d1\ud569 \uc5f0\uc0b0 \uad6c\ud604\uc740 NCCL \ubc31\uc5d4\ub4dc\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 \uac83\ub9cc\ud07c \ucd5c\uc801\ud654\ub418\uc5b4 \uc788\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4. \uc54c\uace0 \uacc4\uc2dc\uaca0\uc9c0\ub9cc, \uc704\uc5d0\uc11c \ub9cc\ub4e0 \ubd84\uc0b0 SGD \uc608\uc81c\ub294 GPU\uc5d0 model \uc744 \uc62c\ub9ac\uba74 \ub3d9\uc791\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc5ec\ub7ec GPU\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc544\ub798\uc640 \uac19\uc774 \uc218\uc815\uc774 \ud544\uc694\ud569\ub2c8\ub2e4: \uac00\uc18d\uae30(Accelerator) API \uc0ac\uc6a9: device_type = torch.accelerator.current_accelerator() torch.device(f\"{device_type}:{rank}\") \uc0ac\uc6a9 model = Net() \\(\\rightarrow\\) model = Net().to(device) data, target = data.to(device), target.to(device) \uc0ac\uc6a9 \uc704\uc640 \uac19\uc774 \ubcc0\uacbd\ud558\uace0 \ub098\uba74 \uc774\uc81c 2\uac1c\uc758 GPU\uc5d0\uc11c \ubaa8\ub378\uc774 \ud559\uc2b5\uc744 \ud558\uba70, watch nvidia-smi \ub85c \uc0ac\uc6a9\ub960\uc744 \ubaa8\ub2c8\ud130\ub9c1\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. MPI \ubc31\uc5d4\ub4dc MPI(Message Passing Interface)\ub294 \uace0\uc131\ub2a5 \ucef4\ud4e8\ud305 \ubd84\uc57c\uc758 \ud45c\uc900 \ub3c4\uad6c\uc785\ub2c8\ub2e4. \uc774\ub294 \uc810-\ub300-\uc810 \uac04 \ud1b5\uc2e0\uacfc \uc9d1\ud569 \ud1b5\uc2e0\uc744 \ud5c8\uc6a9\ud558\uba70 torch.distributed \uc758 API\uc5d0 \uc601\uac10\uc744 \uc8fc\uc5c8\uc2b5\ub2c8\ub2e4. \ub2e4\uc591\ud55c \ubaa9\uc801\uc5d0 \ub530\ub77c \ucd5c\uc801\ud654\ub41c \uba87\uba87 MPI \uad6c\ud604\uccb4\ub4e4(\uc608. Open-MPI, MVAPICH2, Intel MPI )\uc774 \uc788\uc2b5\ub2c8\ub2e4. MPI \ubc31\uc5d4\ub4dc\ub97c \uc0ac\uc6a9\ud558\ub294 \uc774\uc810\uc740 \ub300\uaddc\ubaa8 \uc5f0\uc0b0 \ud074\ub7ec\uc2a4\uc5d0\uc11c\uc758 MPI\uc758 \ud3ed\ub113\uc740 \uac00\uc6a9\uc131(\uacfc \ub192\uc740 \uc218\uc900\uc758 \ucd5c\uc801\ud654)\uc5d0 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c, \uc77c\ubd80 \ucd5c\uc2e0 \uad6c\ud604\uccb4\ub4e4 \uc740 CPU\ub97c \ud1b5\ud55c \uba54\ubaa8\ub9ac \ubcf5\uc0ac\ub97c \ubc29\uc9c0\ud558\uae30 \uc704\ud574 CUDA IPC\uc640 GPU Direct \uae30\uc220\uc744 \ud65c\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ubd88\ud589\ud558\uac8c\ub3c4 PyTorch \ubc14\uc774\ub108\ub9ac\ub294 MPI \uad6c\ud604\uc744 \ud3ec\ud568\ud560 \uc218 \uc5c6\uc73c\ubbc0\ub85c \uc9c1\uc811 \uc7ac\ucef4\ud30c\uc77c\ud574\uc57c \ud569\ub2c8\ub2e4. \ub2e4\ud589\ud788\ub3c4 \uc774 \uacfc\uc815\uc740 \ub9e4\uc6b0 \uac04\ub2e8\ud574\uc11c PyTorch\uac00 \uc2a4\uc2a4\ub85c \uc0ac\uc6a9 \uac00\ub2a5\ud55c MPI \uad6c\ud604\uccb4\ub97c \ucc3e\uc544\ubcfc \uac83\uc785\ub2c8\ub2e4. \ub2e4\uc74c \ub2e8\uacc4\ub4e4\uc740 PyTorch\ub97c \uc18c\uc2a4\ub85c\ubd80\ud130 \uc124\uce58\ud568\uc73c\ub85c\uc368 MPI \ubc31\uc5d4\ub4dc\ub97c \uc124\uce58\ud558\ub294 \uacfc\uc815\uc785\ub2c8\ub2e4. \uc544\ub098\ucf58\ub2e4(Anaconda) \ud658\uacbd\uc744 \uc0dd\uc131\ud558\uace0 \ud65c\uc131\ud654\ud55c \ub4a4 \uc774 \uac00\uc774\ub4dc \ub97c \ub530\ub77c\uc11c \ubaa8\ub4e0 \ud544\uc694 \uc0ac\ud56d\ub4e4\uc744 \uc124\uce58\ud558\uc2dc\ub418, python setup.py install \uc740 \uc544\uc9c1 \uc2e4\ud589\ud558\uc9c0 \ub9c8\uc2ed\uc2dc\uc624. \uc120\ud638\ud558\ub294 MPI \uad6c\ud604\uccb4\ub97c \uc120\ud0dd\ud558\uace0 \uc124\uce58\ud558\uc2ed\uc2dc\uc624. CUDA\ub97c \uc778\uc2dd\ud558\ub294 MPI\ub97c \ud65c\uc131\ud654\ud558\uae30 \uc704\ud574\uc11c\ub294 \ucd94\uac00\uc801\uc778 \ub2e8\uacc4\uac00 \ud544\uc694\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 Open-MPI\ub97c GPU \uc5c6\uc774 \uc0ac\uc6a9\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4: conda install -c conda-forge openmpi \uc774\uc81c, \ubcf5\uc81c\ud574\ub454 PyTorch \uc800\uc7a5\uc18c\ub85c \uac00\uc11c python setup.py install \uc744 \uc2e4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc0c8\ub85c \uc124\uce58\ud55c \ubc31\uc5d4\ub4dc\ub97c \ud14c\uc2a4\ud2b8\ud574\ubcf4\uae30 \uc704\ud574, \uc57d\uac04\uc758 \uc218\uc815\uc744 \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. if __name__ == \u0027__main__\u0027: \uc544\ub798 \ub0b4\uc6a9\uc744 init_process(0, 0, run, backend=\u0027mpi\u0027) \uc73c\ub85c \ubcc0\uacbd\ud569\ub2c8\ub2e4. mpirun -n 4 python myscript.py \uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubcc0\uacbd \uc0ac\ud56d\uc740 MPI\uac00 \ud504\ub85c\uc138\uc2a4\ub97c \uc0dd\uc131(spawn)\ud558\uae30 \uc804\uc5d0 \uc790\uccb4\uc801\uc778 \ud658\uacbd\uc744 \ub9cc\ub4e4\uae30 \uc704\ud574 \ud544\uc694\ud569\ub2c8\ub2e4. MPI\ub294 \uc790\uc2e0\uc758 \ud504\ub85c\uc138\uc2a4\ub97c \uc0dd\uc131\ud558\uace0 \ucd08\uae30\ud654 \ubc29\ubc95 \uc5d0 \uc124\uba85\ub41c \ud578\ub4dc\uc250\uc774\ud06c(handshake)\ub97c \uc218\ud589\ud558\uc5ec init_process_group \uc758 rank \uc640 size \uc778\uc790\ub97c \ubd88\ud544\uc694\ud558\uac8c \ub9cc\ub4ed\ub2c8\ub2e4. \uc774\ub294 \uac01 \ud504\ub85c\uc138\uc2a4\uc5d0 \uc5f0\uc0b0 \ub9ac\uc18c\uc2a4\ub97c \uc870\uc808(tailor)\ud560 \uc218 \uc788\ub3c4\ub85d \ucd94\uac00\uc801\uc778 \uc778\uc790\ub97c mpirun \uc73c\ub85c \uc804\ub2ec\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc5d0 \ub9e4\uc6b0 \uac15\ub825\ud569\ub2c8\ub2e4. (\ud504\ub85c\uc138\uc2a4\ub2f9 \ucf54\uc5b4 \uac1c\uc218, \uc7a5\ube44(machine)\uc758 \uc6b0\uc120 \uc21c\uc704 \uc218\ub3d9 \ud560\ub2f9 \ubc0f \uae30\ud0c0 \ub2e4\ub978 \uac83) \uc774\ub807\uac8c \ud568\uc73c\ub85c\uc368, \ub2e4\ub978 \ud1b5\uc2e0 \ubc31\uc5d4\ub4dc\uc640 \uac19\uc740 \uc720\uc0ac\ud55c \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. NCCL \ubc31\uc5d4\ub4dc NCCL \ubc31\uc5d4\ub4dc \ub294 CUDA Tensor\ub4e4\uc5d0 \ub300\ud55c \uc9d1\ud569 \uc5f0\uc0b0\uc758 \ucd5c\uc801\ud654\ub41c \uad6c\ud604\uccb4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc9d1\ud569 \uc5f0\uc0b0\uc5d0 CUDA Tensor\ub9cc \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0, \ub3d9\uae09 \ucd5c\uace0 \uc131\ub2a5\uc744 \uc704\ud574 \uc774 \ubc31\uc5d4\ub4dc\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc744 \uace0\ub824\ud574\ubcf4\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4. NCCL \ubc31\uc5d4\ub4dc\ub294 \ubbf8\ub9ac \ube4c\ub4dc(pre-built)\ub41c \ubc14\uc774\ub108\ub9ac\uc5d0 CUDA \uc9c0\uc6d0\uacfc \ud568\uaed8 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. XCCL \ubc31\uc5d4\ub4dc XCCL \ubc31\uc5d4\ub4dc \ub294 XPU Tensor\ub4e4\uc5d0 \ub300\ud55c \uc9d1\ud569 \uc5f0\uc0b0\uc758 \ucd5c\uc801\ud654\ub41c \uad6c\ud604\uccb4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \ub9cc\uc57d \uc9d1\ud569 \uc5f0\uc0b0\uc5d0 XPU Tensor\ub9cc \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0, \uc774 \ubc31\uc5d4\ub4dc\ub294 \ub3d9\uae09 \ucd5c\uace0(Best-in-Class) \uc131\ub2a5\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. XCCL \ubc31\uc5d4\ub4dc\ub294 XPU \uc9c0\uc6d0\uacfc \ud568\uaed8 \ubbf8\ub9ac \ube4c\ub4dc(pre-built)\ub41c \ubc14\uc774\ub108\ub9ac\uc5d0 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ucd08\uae30\ud654 \ubc29\ubc95(Initialization Methods)# \ub9c8\uc9c0\ub9c9\uc73c\ub85c, \ucc98\uc74c \ud638\ucd9c\ud588\ub358 \ud568\uc218\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4: dist.init_process_group(backend, init_method) \ud2b9\ud788, \uac01 \ud504\ub85c\uc138\uc2a4 \uac04\uc758 \ucd08\uae30 \uc870\uc815(initial coordination) \ub2e8\uacc4\ub97c \ub2f4\ub2f9\ud558\ub294 \ub2e4\uc591\ud55c \ucd08\uae30\ud654 \ubc29\ubc95\ub4e4\uc744 \uc0b4\ud3b4\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubc29\ubc95\ub4e4\uc740 \uc5b4\ub5bb\uac8c \uc774\ub7ec\ud55c \uc870\uc815\uc774 \uc218\ud589\ub418\ub294\uc9c0\ub97c \uc815\uc758\ud560 \uc218 \uc788\uac8c \ud569\ub2c8\ub2e4. \ucd08\uae30\ud654 \ubc29\ubc95\uc758 \uc120\ud0dd\uc740 \ud558\ub4dc\uc6e8\uc5b4 \uc124\uc815\uc5d0 \ub530\ub77c \ub2e4\ub974\uba70, \ud558\ub098\uc758 \ubc29\ubc95\uc774 \ub2e4\ub978 \ubc29\ubc95\ubcf4\ub2e4 \ub354 \uc801\ud569\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \uc139\uc158 \uc678\uc5d0\ub3c4 \uacf5\uc2dd \ubb38\uc11c \ub97c \ucc38\uace0\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud658\uacbd \ubcc0\uc218 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c \uc9c0\uae08\uae4c\uc9c0\ub294 \ud658\uacbd \ubcc0\uc218\uc758 \ucd08\uae30\ud654 \uba54\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud574\uc654\uc2b5\ub2c8\ub2e4. \ubaa8\ub4e0 \uae30\uae30\uc5d0\uc11c \uc544\ub798 \ub124\uac00\uc9c0 \ud658\uacbd \ubcc0\uc218\ub97c \uc124\uc815\ud558\uac8c \ub418\uba74, \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\ub4e4\uc774 \ub9c8\uc2a4\ud130(master)\uc5d0 \uc801\ud569\ud558\uac8c \uc5f0\uacb0\ud558\uace0, \ub2e4\ub978 \ud504\ub85c\uc138\uc2a4\ub4e4\uc758 \uc815\ubcf4\ub97c \uc5bb\uc740 \ud6c4 \ud578\ub4dc\uc250\uc774\ud06c\uae4c\uc9c0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. MASTER_PORT: 0-\uc21c\uc704\uc758 \ud504\ub85c\uc138\uc2a4\ub97c \ud638\uc2a4\ud2b8\ud560 \uae30\uae30\uc758 \ube44\uc5b4\uc788\ub294 \ud3ec\ud2b8 \ubc88\ud638(free port) MASTER_ADDR: 0-\uc21c\uc704\uc758 \ud504\ub85c\uc138\uc2a4\ub97c \ud638\uc2a4\ud2b8\ud560 \uae30\uae30\uc758 IP \uc8fc\uc18c WORLD_SIZE: \uc804\uccb4 \ud504\ub85c\uc138\uc2a4 \uc218 - \ub9c8\uc2a4\ud130\uac00 \uc5bc\ub9c8\ub098 \ub9ce\uc740 \uc6cc\ucee4\ub4e4\uc744 \uae30\ub2e4\ub9b4\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4 RANK: \uac01 \ud504\ub85c\uc138\uc2a4\uc758 \uc6b0\uc120\uc21c\uc704 - \uc6cc\ucee4 \ub610\ub294 \ub9c8\uc2a4\ud130 \uc5ec\ubd80\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uacf5\uc720 \ud30c\uc77c \uc2dc\uc2a4\ud15c \uacf5\uc720 \ud30c\uc77c \uc2dc\uc2a4\ud15c\uc740 \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\uac00 \uacf5\uc720\ub41c \ud30c\uc77c\uc5d0\uc758 \uc811\uadfc \ubc0f \ud504\ub85c\uc138\uc2a4\ub4e4\uac04\uc758 \uacf5\uc720 \ud30c\uc77c\uc744 \uc870\uc815(coordinate)\ud558\uae30 \uc704\ud574 \ud544\uc694\ud569\ub2c8\ub2e4. \uc774\uac83\uc740 \uac01 \ud504\ub85c\uc138\uc2a4\uac00 \ud30c\uc77c\uc744 \uc5f4\uace0, \uc815\ubcf4\ub97c \uc4f0\uace0, \ub2e4\ub978 \ud504\ub85c\uc138\uc2a4\ub4e4\uc774 \uc791\uc5c5\uc744 \uc644\ub8cc\ud560 \ub54c\uae4c\uc9c0 \uae30\ub2e4\ub9ac\uac8c \ud558\ub294 \uac83\uc744 \ub73b\ud569\ub2c8\ub2e4. \ud544\uc694\ud55c \ubaa8\ub4e0 \uc815\ubcf4\ub294 \ubaa8\ub4e0 \ud504\ub85c\uc138\uc2a4\ub4e4\uc774 \uc27d\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ud569\ub2c8\ub2e4. \uacbd\uc7c1 \uc870\uac74(race conditions)\uc744 \ud53c\ud558\uae30 \uc704\ud574, \ud30c\uc77c \uc2dc\uc2a4\ud15c\uc740 \ubc18\ub4dc\uc2dc fcntl \uc744 \uc774\uc6a9\ud55c \uc7a0\uae08\uc744 \uc9c0\uc6d0\ud574\uc57c \ud569\ub2c8\ub2e4. dist.init_process_group( init_method=\u0027file:///mnt/nfs/sharedfile\u0027, rank=args.rank, world_size=4) TCP 0-\uc21c\uc704 \ud504\ub85c\uc138\uc2a4\uc758 IP \uc8fc\uc18c\uc640 \uc811\uadfc \uac00\ub2a5\ud55c \ud3ec\ud2b8 \ubc88\ud638\uac00 \uc788\uc73c\uba74 TCP\ub97c \ud1b5\ud55c \ucd08\uae30\ud654\ub97c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubaa8\ub4e0 \uc6cc\ucee4\ub4e4\uc740 0-\uc21c\uc704\uc758 \ud504\ub85c\uc138\uc2a4\uc5d0 \uc5f0\uacb0\ud558\uace0 \uc11c\ub85c \uc815\ubcf4\ub97c \uad50\ud658\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uacf5\uc720\ud569\ub2c8\ub2e4. dist.init_process_group( init_method=\u0027tcp://10.1.1.20:23456\u0027, rank=args.rank, world_size=4) \uac10\uc0ac\uc758 \ub9d0 PyTorch \uac1c\ubc1c\uc790\ubd84\ub4e4\uaed8 \uad6c\ud604, \ubb38\uc11c\ud654 \ubc0f \ud14c\uc2a4\ud2b8\ub97c \uc798\ud574\uc8fc\uc2e0 \uac83\uc5d0 \uac10\uc0ac\ub4dc\ub9bd\ub2c8\ub2e4. \ucf54\ub4dc\uac00 \ubd88\ubd84\uba85\ud560 \ub54c\ub294 \uc5b8\uc81c\ub098 \ubb38\uc11c \ub610\ub294 \ud14c\uc2a4\ud2b8 \uc5d0\uc11c \ub2f5\uc744 \ucc3e\uc744 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \ub610\ud55c \ud29c\ud1a0\ub9ac\uc5bc \ucd08\uc548\uc5d0 \ub300\ud574 \ud1b5\ucc30\ub825\uc788\ub294 \uc758\uacac\uacfc \uc9c8\ubb38\uc5d0 \ub2f5\ubcc0\uc744 \ud574\uc8fc\uc2e0 Soumith Chintala, Adam Paszke \uadf8\ub9ac\uace0 Natalia Gimelshei\uaed8\ub3c4 \uac10\uc0ac\ub4dc\ub9bd\ub2c8\ub2e4.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/intermediate/dist_tuto.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>