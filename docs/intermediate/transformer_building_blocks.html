
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta content="Learn how to optimize transformer models by replacing nn.Transformer with Nested Tensors and torch.compile() for significant performance gains in PyTorch." name="description" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile()" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/intermediate/transformer_building_blocks.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Author: Mikayla Gawarecki What you will learn Learn about the low-level building blocks PyTorch provides to build custom transformer layers ( nested tensors, scaled_dot_product_attention, torch.compile(), and FlexAttention), Discover how the above improve memory usage and performance using MultiH..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta property="og:ignore_canonical" content="true" />

    <title>Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile() &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/transformer_building_blocks';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/intermediate/transformer_building_blocks.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Accelerating...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Accelerating PyTorch Transformers by replacing <code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> with Nested Tensors and <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code>">
        <meta itemprop="position" content="1">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">intermediate/transformer_building_blocks</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-intermediate-transformer-building-blocks-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="accelerating-pytorch-transformers-by-replacing-nn-transformer-with-nested-tensors-and-torch-compile">
<span id="sphx-glr-intermediate-transformer-building-blocks-py"></span><h1>Accelerating PyTorch Transformers by replacing <code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> with Nested Tensors and <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code><a class="headerlink" href="#accelerating-pytorch-transformers-by-replacing-nn-transformer-with-nested-tensors-and-torch-compile" title="Link to this heading">#</a></h1>
<p><strong>Author:</strong> <a class="reference external" href="https://github.com/mikaylagawarecki">Mikayla Gawarecki</a></p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-mortar-board" viewBox="0 0 16 16" aria-hidden="true"><path d="M7.693 1.066a.747.747 0 0 1 .614 0l7.25 3.25a.75.75 0 0 1 0 1.368L13 6.831v2.794c0 1.024-.81 1.749-1.66 2.173-.893.447-2.075.702-3.34.702-.278 0-.55-.012-.816-.036a.75.75 0 0 1 .133-1.494c.22.02.45.03.683.03 1.082 0 2.025-.221 2.67-.543.69-.345.83-.682.83-.832V7.503L8.307 8.934a.747.747 0 0 1-.614 0L4 7.28v1.663c.296.105.575.275.812.512.438.438.688 1.059.688 1.796v3a.75.75 0 0 1-.75.75h-3a.75.75 0 0 1-.75-.75v-3c0-.737.25-1.358.688-1.796.237-.237.516-.407.812-.512V6.606L.443 5.684a.75.75 0 0 1 0-1.368ZM2.583 5 8 7.428 13.416 5 8 2.572ZM2.5 11.25v2.25H4v-2.25c0-.388-.125-.611-.25-.735a.697.697 0 0 0-.5-.203.707.707 0 0 0-.5.203c-.125.124-.25.347-.25.735Z"></path></svg> What you will learn</div>
<ul class="simple">
<li><p class="sd-card-text">Learn about the low-level building blocks PyTorch provides to build custom transformer layers (
nested tensors, <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code>, and <code class="docutils literal notranslate"><span class="pre">FlexAttention</span></code>)</p></li>
<li><p class="sd-card-text">Discover how the above improve memory usage and performance using MultiHeadAttention as an example</p></li>
<li><p class="sd-card-text">Explore advanced customizations using the aforementioned building blocks</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-list-unordered" viewBox="0 0 16 16" aria-hidden="true"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Prerequisites</div>
<ul class="simple">
<li><p class="sd-card-text">PyTorch v.2.6.0 or later</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Over the past few years, the PyTorch team has developed various lower level
features that, when composed, can create a variety of transformer variants. These
include:</p>
<ul class="simple">
<li><p>Nested Tensors with the <code class="docutils literal notranslate"><span class="pre">torch.jagged</span></code> layout (AKA NJTs)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FlexAttention</span></code></p></li>
</ul>
<p>This tutorial will give a brief overview of the above technologies and
demonstrate how they can be composed to yield flexible and performant transformer layers with improved user experience.</p>
<p>One may observe that the <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> module currently provides various <code class="docutils literal notranslate"><span class="pre">Transformer</span></code>-related layers.
In particular, it includes <code class="docutils literal notranslate"><span class="pre">TransformerEncoderLayer</span></code>, <code class="docutils literal notranslate"><span class="pre">TransformerEncoder</span></code>, <code class="docutils literal notranslate"><span class="pre">TransformerDecoderLayer</span></code>,
<code class="docutils literal notranslate"><span class="pre">TransformerDecoder</span></code>, <code class="docutils literal notranslate"><span class="pre">Transformer</span></code> and <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code>. This family
of layers was initially implemented following the <a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention is All
You Need</a> paper. The components discussed in
this tutorial provide improved user experience, flexibility and performance over
the existing <code class="docutils literal notranslate"><span class="pre">nn</span></code> layers.</p>
</section>
<section id="is-this-tutorial-for-me">
<h1>Is this tutorial for me?<a class="headerlink" href="#is-this-tutorial-for-me" title="Link to this heading">#</a></h1>
<p>If you are wondering about what building blocks the <code class="docutils literal notranslate"><span class="pre">torch</span></code> library provides
for writing your own transformer layers and best practices, you are in the
right place. Please keep reading!</p>
<p>If you are looking for an out-of-the-box implementation of a popular transformer
architecture, note that there are many open-source libraries that provide them,
including:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/huggingface/transformers">HuggingFace transformers</a></p></li>
<li><p><a class="reference external" href="https://github.com/facebookresearch/xformers">xformers</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/torchtune">torchtune</a></p></li>
</ul>
<p>If you are only interested in performant attention score modifications, please
check out the <a class="reference external" href="https://pytorch.org/blog/flexattention/">FlexAttention blog</a> that
contains a <a class="reference external" href="https://github.com/meta-pytorch/attention-gym">gym of masks</a>.</p>
</section>
<section id="introducing-the-building-blocks">
<h1>Introducing the Building Blocks<a class="headerlink" href="#introducing-the-building-blocks" title="Link to this heading">#</a></h1>
<p>First, we will briefly introduce the four technologies mentioned in the introduction</p>
<ul class="simple">
<li><p><a class="reference external" href="https://tutorials.pytorch.kr/unstable/nestedtensor.html">torch.nested</a></p></li>
</ul>
<p>Nested tensors generalize the shape of regular dense tensors, allowing for
representation of ragged-sized data with the same tensor UX. In the context of
transformers, we can think of nested tensors as a tool for representing variable
sequence lengths. They eliminate the need for the bug-prone practices of explicit
padding and masking (think <code class="docutils literal notranslate"><span class="pre">key_padding_mask</span></code> in <code class="docutils literal notranslate"><span class="pre">nn.MultiHeadAttention</span></code>).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://tutorials.pytorch.kr/intermediate/scaled_dot_product_attention_tutorial.html">scaled_dot_product_attention</a></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code> is a primitive for
<span class="math">\(\text{softmax}(\frac{QK^T}{\sqrt{E}} + B)V\)</span> that dispatches into either fused
implementations of the operator or a fallback implementation. It works out of
the box in eager mode (i.e. the default mode of using PyTorch where operations
are executed on the fly as they are encountered) and also integrates seamlessly
with <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code>. As of 2.6, it will also offer grouped query attention
natively.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://tutorials.pytorch.kr/intermediate/torch_compile_tutorial.html">torch.compile()</a></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code> is a compiler introduced in version 2.0 that is able to
capture a graph of PyTorch code and perform various optimizations on it, such as
fusing together sequences of ops. Nested tensors with the <code class="docutils literal notranslate"><span class="pre">torch.jagged</span></code> layout
and <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code> work seamlessly with compile. In the
context of transformers, the value add of using compile with nested tensor
and SDPA is that compile can remove framework overhead ones sees in eager mode
and fuse sequences of ops in transformers together, such as projection and
activation.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/blog/flexattention/">FlexAttention</a></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">FlexAttention</span></code> is a primitive that allows users to modify attention scores
prior to the softmax operation. It generalizes the additive <code class="docutils literal notranslate"><span class="pre">B</span></code> term above
for <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code>, allowing for arbitrary calculation. It
requires compile to achieve good performance.</p>
</section>
<section id="the-above-building-blocks-are-all-you-need-as-of-october-2024">
<h1>The above building blocks are “All You Need” (as of October 2024)<a class="headerlink" href="#the-above-building-blocks-are-all-you-need-as-of-october-2024" title="Link to this heading">#</a></h1>
<p>The main premise in this section is that most transformer variations are
GPT-style, consisting of layers like Embedding, Positional Encoding, Attention
Blocks and Feed Forward networks. If we were to try to classify the differences
in this space, we might land on something like:</p>
<ol class="arabic simple">
<li><p>Layer type (activation functions such as <code class="docutils literal notranslate"><span class="pre">SwiGLU</span></code> and others, normalization functions
such as <code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code> and others, positional encodings, such as Sinusoidal, Rotary.)</p></li>
<li><p>Layer ordering, such as where to apply norms and positional encoding.</p></li>
<li><p>Modifications to attention score, such as <code class="docutils literal notranslate"><span class="pre">ALiBi</span></code>, Relative Positional Bias and so on.</p></li>
</ol>
<p>In a pre-compiler environment, you might write a custom transformer and notice
that it functions correctly but is slow. To address this, you might develop a
custom fused kernel for the specific series of operations. In a compiler environment,
you can simply perform the initial step and then compile and benefit from improved performance.</p>
<section id="multiheadattention">
<h2>MultiheadAttention<a class="headerlink" href="#multiheadattention" title="Link to this heading">#</a></h2>
<p>Remember that MultiheadAttention takes in a query, key, and value, and consists
of an input projection, a <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code> operator and an
output projection. The main takeaway we want to demonstrate here is the
improvement yielded when we replaced padded/masked inputs with nested tensors.
The improvements are threefold:</p>
<ul class="simple">
<li><p><strong>User Experience</strong>
Remember that <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code> requires <code class="docutils literal notranslate"><span class="pre">query</span></code>, <code class="docutils literal notranslate"><span class="pre">key</span></code>, and
<code class="docutils literal notranslate"><span class="pre">value</span></code> to be dense <code class="docutils literal notranslate"><span class="pre">torch.Tensors</span></code>. It also provides a
<code class="docutils literal notranslate"><span class="pre">key_padding_mask</span></code> that is used to mask out padding tokens in the <code class="docutils literal notranslate"><span class="pre">key</span></code>
that arise due to different sequence lengths within a batch. Since there is
no <code class="docutils literal notranslate"><span class="pre">query_padding_mask</span></code> in <code class="docutils literal notranslate"><span class="pre">nn.MHA</span></code>, users have to take care to mask/slice
the outputs appropriately to account for query sequence lengths. <code class="docutils literal notranslate"><span class="pre">NestedTensor</span></code>
cleanly removes the need for this sort of error-prone padding masks.</p></li>
<li><p><strong>Memory</strong>
Instead of materializing a dense <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">S,</span> <span class="pre">D]</span></code> tensor with a <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">S]</span></code>
padding mask (where <code class="docutils literal notranslate"><span class="pre">B</span></code> is batch size, <code class="docutils literal notranslate"><span class="pre">S</span></code> is max sequence length in the
batch and <code class="docutils literal notranslate"><span class="pre">D</span></code> is embedding size), nested tensors allow you to cleanly
represent the batch of varying sequence lengths. As a result, the input and
intermediate activations will use less memory.</p></li>
<li><p><strong>Performance</strong>
Since padding is not materialized and unnecessary computation on padding is
skipped, performance and memory usage improve.</p></li>
</ul>
<p>We’ll demonstrate the above by building upon the <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code> layer in the
<a class="reference external" href="https://tutorials.pytorch.kr/unstable/nestedtensor.html">Nested Tensor tutorial</a>
and comparing it to the <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code> layer.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>


<span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes multi-head attention. Supports nested or padded tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        E_q (int): Size of embedding dim for query</span>
<span class="sd">        E_k (int): Size of embedding dim for key</span>
<span class="sd">        E_v (int): Size of embedding dim for value</span>
<span class="sd">        E_total (int): Total embedding dim of combined heads post input projection. Each head</span>
<span class="sd">            has dim E_total // nheads</span>
<span class="sd">        nheads (int): Number of heads</span>
<span class="sd">        dropout (float, optional): Dropout probability. Default: 0.0</span>
<span class="sd">        bias (bool, optional): Whether to add bias to input projection. Default: True</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">E_q</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">E_k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">E_v</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">E_total</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">nheads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nheads</span> <span class="o">=</span> <span class="n">nheads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_qkv_same_embed_dim</span> <span class="o">=</span> <span class="n">E_q</span> <span class="o">==</span> <span class="n">E_k</span> <span class="ow">and</span> <span class="n">E_q</span> <span class="o">==</span> <span class="n">E_v</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qkv_same_embed_dim</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">packed_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">E_k</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">E_v</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="n">E_out</span> <span class="o">=</span> <span class="n">E_q</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">E_total</span><span class="p">,</span> <span class="n">E_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">E_total</span> <span class="o">%</span> <span class="n">nheads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Embedding dim is not divisible by nheads&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">E_head</span> <span class="o">=</span> <span class="n">E_total</span> <span class="o">//</span> <span class="n">nheads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass; runs the following process:</span>
<span class="sd">            1. Apply input projection</span>
<span class="sd">            2. Split heads and prepare for SDPA</span>
<span class="sd">            3. Run SDPA</span>
<span class="sd">            4. Apply output projection</span>

<span class="sd">        Args:</span>
<span class="sd">            query (torch.Tensor): query of shape (``N``, ``L_q``, ``E_qk``)</span>
<span class="sd">            key (torch.Tensor): key of shape (``N``, ``L_kv``, ``E_qk``)</span>
<span class="sd">            value (torch.Tensor): value of shape (``N``, ``L_kv``, ``E_v``)</span>
<span class="sd">            attn_mask (torch.Tensor, optional): attention mask of shape (``N``, ``L_q``, ``L_kv``) to pass to SDPA. Default: None</span>
<span class="sd">            is_causal (bool, optional): Whether to apply causal mask. Default: False</span>

<span class="sd">        Returns:</span>
<span class="sd">            attn_output (torch.Tensor): output of shape (N, L_t, E_q)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Step 1. Apply input projection</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_qkv_same_embed_dim</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">query</span> <span class="ow">is</span> <span class="n">key</span> <span class="ow">and</span> <span class="n">key</span> <span class="ow">is</span> <span class="n">value</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">packed_proj</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">q_weight</span><span class="p">,</span> <span class="n">k_weight</span><span class="p">,</span> <span class="n">v_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">packed_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
                    <span class="n">q_bias</span><span class="p">,</span> <span class="n">k_bias</span><span class="p">,</span> <span class="n">v_bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">packed_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">q_bias</span><span class="p">,</span> <span class="n">k_bias</span><span class="p">,</span> <span class="n">v_bias</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">q_weight</span><span class="p">,</span> <span class="n">q_bias</span><span class="p">),</span>
                    <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">k_weight</span><span class="p">,</span> <span class="n">k_bias</span><span class="p">),</span>
                    <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">v_weight</span><span class="p">,</span> <span class="n">v_bias</span><span class="p">),</span>
                <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
            <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="c1"># Step 2. Split heads and prepare for SDPA</span>
        <span class="c1"># reshape query, key, value to separate by head</span>
        <span class="c1"># (N, L_t, E_total) -&gt; (N, L_t, nheads, E_head) -&gt; (N, nheads, L_t, E_head)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">nheads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">E_head</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># (N, L_s, E_total) -&gt; (N, L_s, nheads, E_head) -&gt; (N, nheads, L_s, E_head)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">nheads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">E_head</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># (N, L_s, E_total) -&gt; (N, L_s, nheads, E_head) -&gt; (N, nheads, L_s, E_head)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">nheads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">E_head</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Step 3. Run SDPA</span>
        <span class="c1"># (N, nheads, L_t, E_head)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span>
        <span class="p">)</span>
        <span class="c1"># (N, nheads, L_t, E_head) -&gt; (N, L_t, nheads, E_head) -&gt; (N, L_t, E_total)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Step 4. Apply output projection</span>
        <span class="c1"># (N, L_t, E_total) -&gt; (N, L_t, E_out)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">attn_output</span>
</pre></div>
</div>
<section id="utilities">
<h3>Utilities<a class="headerlink" href="#utilities" title="Link to this heading">#</a></h3>
<p>In this section, we include a utility to generate semi-realistic data using
<code class="docutils literal notranslate"><span class="pre">Zipf</span></code> distribution for sentence lengths. This is used to generate the nested
query, key, and value tensors. We also include a benchmark utility.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>


<span class="k">def</span><span class="w"> </span><span class="nf">zipf_sentence_lengths</span><span class="p">(</span><span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># generate fake corpus by unigram Zipf distribution</span>
    <span class="c1"># from wikitext-2 corpus, we get rank &quot;.&quot; = 3, &quot;!&quot; = 386, &quot;?&quot; = 858</span>
    <span class="n">sentence_lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ibatch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">sentence_lengths</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">zipf</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
        <span class="k">while</span> <span class="n">word</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">word</span> <span class="o">!=</span> <span class="mi">386</span> <span class="ow">and</span> <span class="n">word</span> <span class="o">!=</span> <span class="mi">858</span><span class="p">:</span>
            <span class="n">sentence_lengths</span><span class="p">[</span><span class="n">ibatch</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">zipf</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sentence_lengths</span><span class="p">)</span>


<span class="c1"># Generate a batch of semi-realistic data using Zipf distribution for sentence lengths</span>
<span class="c1"># in the form of nested tensors with the jagged layout.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">query_seq_len_1</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># generate semi-realistic data using Zipf distribution for sentence lengths</span>
    <span class="n">sentence_lengths</span> <span class="o">=</span> <span class="n">zipf_sentence_lengths</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Note: the torch.jagged layout is a nested tensor layout that supports a single ragged</span>
    <span class="c1"># dimension and works with torch.compile. The batch items each have shape (B, S*, D)</span>
    <span class="c1"># where B = batch size, S* = ragged sequence length, and D = embedding dimension.</span>
    <span class="k">if</span> <span class="n">query_seq_len_1</span><span class="p">:</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">(</span>
            <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">sentence_lengths</span><span class="p">],</span>
            <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">sentence_lengths</span>
            <span class="p">],</span>
            <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">key</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentence_lengths</span>
        <span class="p">],</span>
        <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentence_lengths</span>
        <span class="p">],</span>
        <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">sentence_lengths</span>


<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">timeit</span>


<span class="k">def</span><span class="w"> </span><span class="nf">benchmark</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
    <span class="n">begin</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">default_timer</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">begin</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">()</span>
</pre></div>
</div>
<p>We will now demonstrate the performance improvements of using nested tensors
in the <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code> layer + compile for self attention. We compare this against
the traditional <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code> + compile with padding and masking.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">E_total</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span>
<span class="n">E_out</span> <span class="o">=</span> <span class="n">E_q</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="n">E_q</span>
<span class="n">nheads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">bias</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">sentence_lengths</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">sentence_lengths</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Total sequence length in nested query </span><span class="si">{</span><span class="n">sentence_lengths</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">, max sequence length </span><span class="si">{</span><span class="n">S</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="n">padded_query</span><span class="p">,</span> <span class="n">padded_key</span><span class="p">,</span> <span class="n">padded_value</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">t</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">mha_layer</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
    <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span>
<span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">vanilla_mha_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span>
    <span class="n">E_q</span><span class="p">,</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span>
<span class="p">)</span>

<span class="c1"># ``nn.MultiheadAttention`` uses a non conventional initialization for layers, so do this for exact parity :(</span>
<span class="n">mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
    <span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">mha_layer</span><span class="o">.</span><span class="n">packed_proj</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
    <span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
<span class="n">mha_layer</span><span class="o">.</span><span class="n">packed_proj</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
    <span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">new_mha_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">mha_layer</span><span class="p">)</span>
<span class="c1"># warmup compile</span>
<span class="n">nested_result_warmup</span> <span class="o">=</span> <span class="n">new_mha_layer</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># benchmark</span>
<span class="n">nested_result</span><span class="p">,</span> <span class="n">nested_time</span><span class="p">,</span> <span class="n">nested_peak_memory</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span>
    <span class="n">new_mha_layer</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">padded_nested_result</span> <span class="o">=</span> <span class="n">nested_result</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

<span class="c1"># For the vanilla ``nn.MultiheadAttention``, we need to construct the ``key_padding_mask``</span>
<span class="c1"># Further, ``nn.MultiheadAttention`` forces one to materialize the ``attn_mask`` even if using ``is_causal``</span>
<span class="n">src_key_padding_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">padded_query</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="mi">0</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">attn_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentence_lengths</span><span class="p">):</span>
    <span class="n">attn_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">s</span><span class="p">,</span> <span class="p">:</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Transformer</span><span class="o">.</span><span class="n">generate_square_subsequent_mask</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>

<span class="n">vanilla_mha_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">vanilla_mha_layer</span><span class="p">)</span>
<span class="c1"># warmup compile</span>
<span class="n">warmup_vanilla_result</span> <span class="o">=</span> <span class="n">vanilla_mha_layer</span><span class="p">(</span>
    <span class="n">padded_query</span><span class="p">,</span>
    <span class="n">padded_query</span><span class="p">,</span>
    <span class="n">padded_query</span><span class="p">,</span>
    <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
    <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">src_key_padding_mask</span><span class="p">,</span>
    <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># benchmark</span>
<span class="p">(</span><span class="n">padded_result</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">padded_time</span><span class="p">,</span> <span class="n">padded_peak_memory</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span>
    <span class="n">vanilla_mha_layer</span><span class="p">,</span>
    <span class="n">padded_query</span><span class="p">,</span>
    <span class="n">padded_query</span><span class="p">,</span>
    <span class="n">padded_query</span><span class="p">,</span>
    <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">src_key_padding_mask</span><span class="p">,</span>
    <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
    <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">padded_time</span><span class="si">=:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, padded_peak_memory=</span><span class="si">{</span><span class="n">padded_peak_memory</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">nested_time</span><span class="si">=:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, nested_peak_memory=</span><span class="si">{</span><span class="n">nested_peak_memory</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Max difference between vanilla and nested result&quot;</span><span class="p">,</span>
    <span class="p">(</span><span class="n">padded_result</span> <span class="o">-</span> <span class="n">padded_nested_result</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Nested speedup: </span><span class="si">{</span><span class="p">(</span><span class="n">padded_time</span><span class="o">/</span><span class="n">nested_time</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Nested peak memory reduction </span><span class="si">{</span><span class="p">((</span><span class="n">padded_peak_memory</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">nested_peak_memory</span><span class="p">)</span><span class="o">/</span><span class="mf">1e9</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Total sequence length in nested query 9721, max sequence length 90
/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:282: UserWarning:

TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision(&#39;high&#39;)` for better performance.

padded_time=0.00371, padded_peak_memory=2.52 GB
nested_time=0.00138, nested_peak_memory=0.59 GB
Max difference between vanilla and nested result 0.0
Nested speedup: 2.68
Nested peak memory reduction 1.93 GB
</pre></div>
</div>
<p>For reference, here are some sample outputs on A100:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">padded_time</span><span class="o">=</span><span class="mf">0.03454</span><span class="p">,</span> <span class="n">padded_peak_memory</span><span class="o">=</span><span class="mf">4.14</span> <span class="n">GB</span>
<span class="n">nested_time</span><span class="o">=</span><span class="mf">0.00612</span><span class="p">,</span> <span class="n">nested_peak_memory</span><span class="o">=</span><span class="mf">0.76</span> <span class="n">GB</span>
<span class="n">Max</span> <span class="n">difference</span> <span class="n">between</span> <span class="n">vanilla</span> <span class="ow">and</span> <span class="n">nested</span> <span class="n">result</span> <span class="mf">0.0</span>
<span class="n">Nested</span> <span class="n">speedup</span><span class="p">:</span> <span class="mf">5.65</span>
<span class="n">Nested</span> <span class="n">peak</span> <span class="n">memory</span> <span class="n">reduction</span> <span class="mf">3.39</span> <span class="n">GB</span>
</pre></div>
</div>
<p>We can also see the same for backward pass</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">entry_length</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentence_lengths</span><span class="p">):</span>
    <span class="c1"># padding-specific step: remove output projection bias from padded entries for fair comparison</span>
    <span class="n">padded_result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">entry_length</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="n">_</span><span class="p">,</span> <span class="n">padded_bw_time</span><span class="p">,</span> <span class="n">padded_bw_peak_mem</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span>
    <span class="k">lambda</span><span class="p">:</span> <span class="n">padded_result</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">nested_bw_time</span><span class="p">,</span> <span class="n">nested_bw_peak_mem</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span>
    <span class="k">lambda</span><span class="p">:</span> <span class="n">padded_nested_result</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">padded_bw_time</span><span class="si">=:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, padded_bw_peak_mem=</span><span class="si">{</span><span class="n">padded_bw_peak_mem</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">nested_bw_time</span><span class="si">=:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">, nested_bw_peak_mem=</span><span class="si">{</span><span class="n">nested_bw_peak_mem</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Nested backward speedup: </span><span class="si">{</span><span class="p">(</span><span class="n">padded_bw_time</span><span class="o">/</span><span class="n">nested_bw_time</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Nested backward peak memory reduction </span><span class="si">{</span><span class="p">((</span><span class="n">padded_bw_peak_mem</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">nested_bw_peak_mem</span><span class="p">)</span><span class="o">/</span><span class="mf">1e9</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Difference in out_proj.weight.grad&quot;</span><span class="p">,</span>
    <span class="p">(</span><span class="n">mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span> <span class="o">-</span> <span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="o">.</span><span class="n">abs</span><span class="p">()</span>
    <span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Difference in packed_proj.weight.grad&quot;</span><span class="p">,</span>
    <span class="p">(</span><span class="n">mha_layer</span><span class="o">.</span><span class="n">packed_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span> <span class="o">-</span> <span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="o">.</span><span class="n">abs</span><span class="p">()</span>
    <span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Difference in out_proj.bias.grad&quot;</span><span class="p">,</span>
    <span class="p">(</span><span class="n">mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="o">-</span> <span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="o">.</span><span class="n">abs</span><span class="p">()</span>
    <span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Difference in packed_proj.bias.grad&quot;</span><span class="p">,</span>
    <span class="p">(</span><span class="n">mha_layer</span><span class="o">.</span><span class="n">packed_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="o">-</span> <span class="n">vanilla_mha_layer</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="o">.</span><span class="n">abs</span><span class="p">()</span>
    <span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>padded_bw_time=0.99221, padded_bw_peak_mem=3.16 GB
nested_bw_time=0.02065, nested_bw_peak_mem=2.08 GB
Nested backward speedup: 48.04
Nested backward peak memory reduction 1.08 GB
Difference in out_proj.weight.grad 0.0001983642578125
Difference in packed_proj.weight.grad 0.001708984375
Difference in out_proj.bias.grad 0.0
Difference in packed_proj.bias.grad 0.001953125
</pre></div>
</div>
<p>Sample outputs on A100:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">padded_bw_time</span><span class="o">=</span><span class="mf">2.09337</span><span class="p">,</span> <span class="n">padded_bw_peak_mem</span><span class="o">=</span><span class="mf">5.10</span> <span class="n">GB</span>
<span class="n">nested_bw_time</span><span class="o">=</span><span class="mf">0.01452</span><span class="p">,</span> <span class="n">nested_bw_peak_mem</span><span class="o">=</span><span class="mf">3.24</span> <span class="n">GB</span>
<span class="n">Nested</span> <span class="n">backward</span> <span class="n">speedup</span><span class="p">:</span> <span class="mf">144.13</span>
<span class="n">Nested</span> <span class="n">backward</span> <span class="n">peak</span> <span class="n">memory</span> <span class="n">reduction</span> <span class="mf">1.86</span> <span class="n">GB</span>
<span class="n">Difference</span> <span class="ow">in</span> <span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span> <span class="mf">0.000244140625</span>
<span class="n">Difference</span> <span class="ow">in</span> <span class="n">packed_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span> <span class="mf">0.001556396484375</span>
<span class="n">Difference</span> <span class="ow">in</span> <span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="mf">0.0</span>
<span class="n">Difference</span> <span class="ow">in</span> <span class="n">packed_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span> <span class="mf">0.001953125</span>
</pre></div>
</div>
</section>
</section>
<section id="gpt-style-layer">
<h2>GPT-style layer<a class="headerlink" href="#gpt-style-layer" title="Link to this heading">#</a></h2>
<p>A basic GPT-style transformer layer consists of a causal self-attention layer
followed by a feed-forward network (FFN) with skip connections. Implementing
this is fairly straightforward using the <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code> layer above and
gives equivalent results to an <code class="docutils literal notranslate"><span class="pre">nn.TransformerEncoderLayer</span></code> with
<code class="docutils literal notranslate"><span class="pre">is_causal=True</span></code>.</p>
<p>We  demonstrate examples of implementing the rest of the <code class="docutils literal notranslate"><span class="pre">nn</span></code> layers
<a class="reference external" href="https://github.com/mikaylagawarecki/transformer_tutorial_accompaniment">here</a>
but omit that from this tutorial for brevity.</p>
</section>
<section id="going-one-step-further">
<h2>Going one step further<a class="headerlink" href="#going-one-step-further" title="Link to this heading">#</a></h2>
<p>So far, we have demonstrated how to implement a performant <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code>
layer that follows the traditional <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code>. Going back to our
classification of modifications to the transformer architecture, remember that we
classified the modifications into layer type, layer ordering, and modifications
to the attention score. We trust that changing layer type and layer ordering
(such as swapping <code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code> for <code class="docutils literal notranslate"><span class="pre">RMSNorm</span></code>) is fairly straightforward.</p>
<p>In this section, we will discuss various functionalities using the
aforementioned building blocks, including the following:</p>
<ul class="simple">
<li><p>Cross Attention</p></li>
<li><p>Fully masked rows no longer cause NaNs</p></li>
<li><p>Modifying attention score: ALiBi with FlexAttention and NJT</p></li>
<li><p>Packed Projection</p></li>
</ul>
</section>
<section id="cross-attention">
<h2>Cross Attention<a class="headerlink" href="#cross-attention" title="Link to this heading">#</a></h2>
<p>Cross attention is a form of attention where the query and key/value tensors
are from different sequences.</p>
<p>One example of this is in <code class="docutils literal notranslate"><span class="pre">nn.TransformerDecoderLayer</span></code> where the query comes
from the decoder and the key/value come from the encoder.</p>
<p>The above MultiheadAttention layer nicely generalizes to this case with nested
tensors for both query and key/value.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">query</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">q_len</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">kv_len</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Total sequence length in nested query </span><span class="si">{</span><span class="n">q_len</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">, max sequence length </span><span class="si">{</span><span class="n">q_len</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Total sequence length in nested key/value </span><span class="si">{</span><span class="n">kv_len</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">, max sequence length </span><span class="si">{</span><span class="n">kv_len</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">new_mha_layer</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Total sequence length in nested query 11088, max sequence length 132
Total sequence length in nested key/value 11397, max sequence length 150
</pre></div>
</div>
<p>As above, we can compare this against the vanilla compiled <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">query</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">q_len</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">kv_len</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">padded_query</span><span class="p">,</span> <span class="n">padded_key</span><span class="p">,</span> <span class="n">padded_value</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">t</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">key_padding_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">padded_key</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="mi">0</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># warmup compile</span>
<span class="n">warmup_nested_result</span> <span class="o">=</span> <span class="n">new_mha_layer</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">warmup_vanilla_result</span> <span class="o">=</span> <span class="n">vanilla_mha_layer</span><span class="p">(</span>
    <span class="n">padded_query</span><span class="p">,</span>
    <span class="n">padded_key</span><span class="p">,</span>
    <span class="n">padded_value</span><span class="p">,</span>
    <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span>
    <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">nested_result</span><span class="p">,</span> <span class="n">nested_time</span><span class="p">,</span> <span class="n">nested_peak_memory</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span>
    <span class="n">new_mha_layer</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="p">(</span><span class="n">padded_result</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">padded_time</span><span class="p">,</span> <span class="n">padded_peak_memory</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span>
    <span class="n">vanilla_mha_layer</span><span class="p">,</span>
    <span class="n">padded_query</span><span class="p">,</span>
    <span class="n">padded_key</span><span class="p">,</span>
    <span class="n">padded_value</span><span class="p">,</span>
    <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span>
    <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">padded_nested_result</span> <span class="o">=</span> <span class="n">nested_result</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">entry_length</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">q_len</span><span class="p">):</span>
    <span class="c1"># padding-specific step: remove output projection bias from padded entries for fair comparison</span>
    <span class="n">padded_result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">entry_length</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Max difference between vanilla and nested result&quot;</span><span class="p">,</span>
    <span class="p">(</span><span class="n">padded_result</span> <span class="o">-</span> <span class="n">padded_nested_result</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Nested speedup: </span><span class="si">{</span><span class="p">(</span><span class="n">padded_time</span><span class="o">/</span><span class="n">nested_time</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Nested peak memory reduction </span><span class="si">{</span><span class="p">((</span><span class="n">padded_peak_memory</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">nested_peak_memory</span><span class="p">)</span><span class="o">/</span><span class="mf">1e9</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Max difference between vanilla and nested result 0.0
Nested speedup: 19.70
Nested peak memory reduction 1.11 GB
</pre></div>
</div>
<p>Sample outputs on A100:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Max</span> <span class="n">difference</span> <span class="n">between</span> <span class="n">vanilla</span> <span class="ow">and</span> <span class="n">nested</span> <span class="n">result</span> <span class="mf">0.0</span>
<span class="n">Nested</span> <span class="n">speedup</span><span class="p">:</span> <span class="mf">4.01</span>
<span class="n">Nested</span> <span class="n">peak</span> <span class="n">memory</span> <span class="n">reduction</span> <span class="mf">1.40</span> <span class="n">GB</span>
</pre></div>
</div>
</section>
<section id="fully-masked-rows-no-longer-cause-nans">
<h2>Fully masked rows no longer cause NaNs<a class="headerlink" href="#fully-masked-rows-no-longer-cause-nans" title="Link to this heading">#</a></h2>
<p>There has been a long standing issue with <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code> and
<code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code> where if a row was fully masked out, the output
of the attention layer would be NaN. See <a class="reference external" href="https://github.com/pytorch/pytorch/issues/41508">issue</a>.
This is because the softmax over an empty set is undefined.</p>
<p>Thanks to <a class="reference external" href="https://github.com/pytorch/pytorch/pull/133882">this PR</a>
this is no longer the case. Instead, the output corresponding to fully masked rows
in <code class="docutils literal notranslate"><span class="pre">scaled_dot_product_attention</span></code> will be 0. For cases where <code class="docutils literal notranslate"><span class="pre">nn.MHA</span></code> does
not employ the “fast-path”, this will also apply.</p>
<p>Using a custom MHA layer with NJTs is strongly recommended over the
existing “fast-path” in <code class="docutils literal notranslate"><span class="pre">nn.MultiheadAttention</span></code> as NJT’s ability to model raggedness
appropriately makes it possible to properly express empty sequences.</p>
</section>
<section id="flexattention-njt">
<h2>FlexAttention + NJT<a class="headerlink" href="#flexattention-njt" title="Link to this heading">#</a></h2>
<p>NJT also composes with the <code class="docutils literal notranslate"><span class="pre">FlexAttention</span></code> module. This is a generalization
of the <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code> layer that allows for arbitrary modifications
to the attention score. The example below takes the <code class="docutils literal notranslate"><span class="pre">alibi_mod</span></code>
that implements <a class="reference external" href="https://arxiv.org/abs/2108.12409">ALiBi</a> from
<a class="reference external" href="https://github.com/meta-pytorch/attention-gym">attention gym</a> and uses it
with nested input tensors.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.attention.flex_attention</span><span class="w"> </span><span class="kn">import</span> <span class="n">flex_attention</span>


<span class="k">def</span><span class="w"> </span><span class="nf">generate_alibi_bias</span><span class="p">(</span><span class="n">H</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns an alibi bias score_mod given the number of heads H</span>
<span class="sd">    Args:</span>
<span class="sd">        H: number of heads</span>
<span class="sd">    Returns:</span>
<span class="sd">        alibi_bias: alibi bias score_mod</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">alibi_mod</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp2</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">8.0</span> <span class="o">/</span> <span class="n">H</span><span class="p">))</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="p">(</span><span class="n">q_idx</span> <span class="o">-</span> <span class="n">kv_idx</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
        <span class="k">return</span> <span class="n">score</span> <span class="o">+</span> <span class="n">bias</span>

    <span class="k">return</span> <span class="n">alibi_mod</span>


<span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">E_q</span> <span class="o">//</span> <span class="mi">8</span>
<span class="n">alibi_score_mod</span> <span class="o">=</span> <span class="n">generate_alibi_bias</span><span class="p">(</span><span class="n">n_heads</span><span class="p">)</span>
<span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">out_flex2</span> <span class="o">=</span> <span class="n">flex_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">score_mod</span><span class="o">=</span><span class="n">alibi_score_mod</span><span class="p">)</span>
</pre></div>
</div>
<p>In addition, one can also use the <code class="docutils literal notranslate"><span class="pre">block_mask</span></code> utility of <code class="docutils literal notranslate"><span class="pre">FlexAttention</span></code>
with NJTs via the <code class="docutils literal notranslate"><span class="pre">create_nested_block_mask</span></code> function. This is useful for
taking advantage of the sparsity of the mask to speed up the attention computation.
In particular, the function creates a sparse block mask for a “stacked sequence” of all
the variable length sequences in the NJT combined into one, while properly masking out
inter-sequence attention. In the following example, we show how to create a
causal block mask using this utility.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.attention.flex_attention</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_nested_block_mask</span>


<span class="k">def</span><span class="w"> </span><span class="nf">causal_mask</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">q_idx</span><span class="p">,</span> <span class="n">kv_idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">q_idx</span> <span class="o">&gt;=</span> <span class="n">kv_idx</span>


<span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_k</span><span class="p">,</span> <span class="n">E_v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">block_mask</span> <span class="o">=</span> <span class="n">create_nested_block_mask</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">_compile</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">D</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">out_flex</span> <span class="o">=</span> <span class="n">flex_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">block_mask</span><span class="o">=</span><span class="n">block_mask</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="packed-projection">
<h2>Packed Projection<a class="headerlink" href="#packed-projection" title="Link to this heading">#</a></h2>
<p>Packed projection is a technique that makes use of the fact that when the input
for projection (matrix multiplications) are the same (self-attention), we can pack the projection
weights and biases into single tensors. It is especially useful when the individual
projections are memory bound rather than compute bound. There are
two examples that we will demonstrate here:</p>
<ul class="simple">
<li><p>Input projection for MultiheadAttention</p></li>
<li><p>SwiGLU activation in feed-forward network of Transformer Layer</p></li>
</ul>
<section id="input-projection-for-multiheadattention">
<h3>Input projection for MultiheadAttention<a class="headerlink" href="#input-projection-for-multiheadattention" title="Link to this heading">#</a></h3>
<p>When doing self-attention, the <code class="docutils literal notranslate"><span class="pre">query</span></code>, <code class="docutils literal notranslate"><span class="pre">key</span></code>, and <code class="docutils literal notranslate"><span class="pre">value</span></code>
are the same tensor. Each of these tensors is projected with a
<code class="docutils literal notranslate"><span class="pre">Linear(E_q,</span> <span class="pre">E_total)</span></code> layer. Instead, we can pack this into one layer,
which is what we do in the MultiheadAttention layer above.</p>
<p>Let us compare the performance of the packed projection against the usual method:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">InputProjection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">PackedInputProjection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">packed_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">E_q</span><span class="p">,</span> <span class="n">E_total</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">packed_proj</span><span class="p">(</span><span class="n">query</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="n">B</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">8192</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>

<span class="n">torch</span><span class="o">.</span><span class="n">set_float32_matmul_precision</span><span class="p">(</span><span class="s2">&quot;high&quot;</span><span class="p">)</span>
<span class="n">in_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">InputProjection</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">))</span>
<span class="n">packed_in_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">PackedInputProjection</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">q</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">sequence_lengths</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

<span class="c1"># warmup</span>
<span class="n">in_proj</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">packed_in_proj</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

<span class="c1"># benchmark</span>
<span class="p">(</span><span class="n">q_out</span><span class="p">,</span> <span class="n">k_out</span><span class="p">,</span> <span class="n">v_out</span><span class="p">),</span> <span class="n">time</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">in_proj</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
<span class="p">(</span><span class="n">q_out</span><span class="p">,</span> <span class="n">k_out</span><span class="p">,</span> <span class="n">v_out</span><span class="p">),</span> <span class="n">time_packed</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">packed_in_proj</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
<span class="c1"># On my A100 prints 1.05x speedup</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;InputProjection: </span><span class="si">{</span><span class="n">time</span><span class="si">:</span><span class="s2">5f</span><span class="si">}</span><span class="s2"> s, PackedInputProjection: </span><span class="si">{</span><span class="n">time_packed</span><span class="si">:</span><span class="s2">5f</span><span class="si">}</span><span class="s2"> s, speedup: </span><span class="si">{</span><span class="n">time</span><span class="o">/</span><span class="n">time_packed</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>InputProjection: 0.003266 s, PackedInputProjection: 0.003204 s, speedup: 1.02x
</pre></div>
</div>
</section>
<section id="swiglu-feed-forward-network-of-transformer-layer">
<h3>SwiGLU feed forward network of Transformer Layer<a class="headerlink" href="#swiglu-feed-forward-network-of-transformer-layer" title="Link to this heading">#</a></h3>
<p>Swish-Gated Linear Unit (SwiGLU) is a non-linear activation function that is increasingly popular in the feed-forward
network of the transformer layer (e.g. Llama). A feed-forward network with SwiGLU activation is defined as:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SwiGLUFFN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dim</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="n">multiple_of</span><span class="p">,</span>
        <span class="n">ffn_dim_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_dim</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span>
        <span class="c1"># custom dim factor multiplier</span>
        <span class="k">if</span> <span class="n">ffn_dim_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ffn_dim_multiplier</span> <span class="o">*</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">multiple_of</span> <span class="o">*</span> <span class="p">((</span><span class="n">hidden_dim</span> <span class="o">+</span> <span class="n">multiple_of</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">multiple_of</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">w3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>An alternative way of implementing this that uses packed projection is</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">PackedSwiGLUFFN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dim</span><span class="p">,</span>
        <span class="n">hidden_dim</span><span class="p">,</span>
        <span class="n">multiple_of</span><span class="p">,</span>
        <span class="n">ffn_dim_multiplier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_dim</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span>
        <span class="c1"># custom dim factor multiplier</span>
        <span class="k">if</span> <span class="n">ffn_dim_multiplier</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ffn_dim_multiplier</span> <span class="o">*</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">multiple_of</span> <span class="o">*</span> <span class="p">((</span><span class="n">hidden_dim</span> <span class="o">+</span> <span class="n">multiple_of</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">multiple_of</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">w13</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x1</span><span class="p">,</span> <span class="n">x3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w13</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x3</span><span class="p">)</span>
</pre></div>
</div>
<p>We can compare the performance of the two implementations as follows
Depending on your hardware, you might see different results. On an A100 I see
1.12x speedup for D=128.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">swigluffn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">SwiGLUFFN</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">))</span>
<span class="n">packed_swigluffn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">PackedSwiGLUFFN</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">q</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">sentence_lengths</span> <span class="o">=</span> <span class="n">gen_batch</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

<span class="c1"># warmup</span>
<span class="n">swigluffn</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">packed_swigluffn</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

<span class="c1"># benchmark</span>
<span class="n">_</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">swigluffn</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">time_packed</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">packed_swigluffn</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
<span class="c1"># On my A100 prints 1.08x speedup</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;SwiGLUFFN: </span><span class="si">{</span><span class="n">time</span><span class="si">}</span><span class="s2"> s, PackedSwiGLUFFN: </span><span class="si">{</span><span class="n">time_packed</span><span class="si">}</span><span class="s2"> s, speedup: </span><span class="si">{</span><span class="n">time</span><span class="o">/</span><span class="n">time_packed</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>SwiGLUFFN: 0.0008159049903042614 s, PackedSwiGLUFFN: 0.000746919002267532 s, speedup: 1.09x
</pre></div>
</div>
</section>
</section>
<section id="extended-examples">
<h2>Extended examples<a class="headerlink" href="#extended-examples" title="Link to this heading">#</a></h2>
<p>We intend to update this tutorial to demonstrate more examples of how to use
the various performant building blocks such as KV-Caching, Grouped Query Attention
etc. Further, there are several good examples of using various performant building blocks to
implement various transformer architectures. Some examples include</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/meta-pytorch/gpt-fast">gpt-fast</a></p></li>
<li><p><a class="reference external" href="https://github.com/meta-pytorch/segment-anything-fast">segment-anything-fast</a></p></li>
<li><p><a class="reference external" href="https://github.com/lucidrains/vit-pytorch/blob/73199ab486e0fad9eced2e3350a11681db08b61b/vit_pytorch/na_vit_nested_tensor.py">lucidrains implementation of NaViT with nested tensors</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/torchtune/blob/a8a64ec6a99a6ea2be4fdaf0cd5797b03a2567cf/torchtune/modules/vision_transformer.py#L16">torchtune’s implementation of VisionTransformer</a></p></li>
</ul>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In this tutorial, we have introduced the low level building blocks PyTorch
provides for writing transformer layers and demonstrated examples how to compose
them. It is our hope that this tutorial has educated the reader on the ease with
which flexible and performant transformer layers can be implemented by users of PyTorch.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (1 minutes 19.616 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-transformer-building-blocks-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/e201125c39959609ca168c306995205c/transformer_building_blocks.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">transformer_building_blocks.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/57114670a041b4c96ed6eb9fc17a6b3f/transformer_building_blocks.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">transformer_building_blocks.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/9e1bf792ffacdc6aa490d8ac2246cba7/transformer_building_blocks.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">transformer_building_blocks.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Accelerating PyTorch Transformers by replacing <code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> with Nested Tensors and <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#is-this-tutorial-for-me">Is this tutorial for me?</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introducing-the-building-blocks">Introducing the Building Blocks</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-above-building-blocks-are-all-you-need-as-of-october-2024">The above building blocks are “All You Need” (as of October 2024)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiheadattention">MultiheadAttention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#utilities">Utilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-style-layer">GPT-style layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#going-one-step-further">Going one step further</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-attention">Cross Attention</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fully-masked-rows-no-longer-cause-nans">Fully masked rows no longer cause NaNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flexattention-njt">FlexAttention + NJT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#packed-projection">Packed Projection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-projection-for-multiheadattention">Input projection for MultiheadAttention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#swiglu-feed-forward-network-of-transformer-layer">SwiGLU feed forward network of Transformer Layer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extended-examples">Extended examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>

  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile()",
       "headline": "Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile()",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/intermediate/transformer_building_blocks.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile()# Author: Mikayla Gawarecki What you will learn Learn about the low-level building blocks PyTorch provides to build custom transformer layers ( nested tensors, scaled_dot_product_attention, torch.compile(), and FlexAttention) Discover how the above improve memory usage and performance using MultiHeadAttention as an example Explore advanced customizations using the aforementioned building blocks Prerequisites PyTorch v.2.6.0 or later Over the past few years, the PyTorch team has developed various lower level features that, when composed, can create a variety of transformer variants. These include: Nested Tensors with the torch.jagged layout (AKA NJTs) scaled_dot_product_attention torch.compile() FlexAttention This tutorial will give a brief overview of the above technologies and demonstrate how they can be composed to yield flexible and performant transformer layers with improved user experience. One may observe that the torch.nn module currently provides various Transformer-related layers. In particular, it includes TransformerEncoderLayer, TransformerEncoder, TransformerDecoderLayer, TransformerDecoder, Transformer and MultiheadAttention. This family of layers was initially implemented following the Attention is All You Need paper. The components discussed in this tutorial provide improved user experience, flexibility and performance over the existing nn layers. Is this tutorial for me?# If you are wondering about what building blocks the torch library provides for writing your own transformer layers and best practices, you are in the right place. Please keep reading! If you are looking for an out-of-the-box implementation of a popular transformer architecture, note that there are many open-source libraries that provide them, including: HuggingFace transformers xformers torchtune If you are only interested in performant attention score modifications, please check out the FlexAttention blog that contains a gym of masks. Introducing the Building Blocks# First, we will briefly introduce the four technologies mentioned in the introduction torch.nested Nested tensors generalize the shape of regular dense tensors, allowing for representation of ragged-sized data with the same tensor UX. In the context of transformers, we can think of nested tensors as a tool for representing variable sequence lengths. They eliminate the need for the bug-prone practices of explicit padding and masking (think key_padding_mask in nn.MultiHeadAttention). scaled_dot_product_attention scaled_dot_product_attention is a primitive for \\(\\text{softmax}(\\frac{QK^T}{\\sqrt{E}} + B)V\\) that dispatches into either fused implementations of the operator or a fallback implementation. It works out of the box in eager mode (i.e. the default mode of using PyTorch where operations are executed on the fly as they are encountered) and also integrates seamlessly with torch.compile(). As of 2.6, it will also offer grouped query attention natively. torch.compile() torch.compile() is a compiler introduced in version 2.0 that is able to capture a graph of PyTorch code and perform various optimizations on it, such as fusing together sequences of ops. Nested tensors with the torch.jagged layout and scaled_dot_product_attention work seamlessly with compile. In the context of transformers, the value add of using compile with nested tensor and SDPA is that compile can remove framework overhead ones sees in eager mode and fuse sequences of ops in transformers together, such as projection and activation. FlexAttention FlexAttention is a primitive that allows users to modify attention scores prior to the softmax operation. It generalizes the additive B term above for scaled_dot_product_attention, allowing for arbitrary calculation. It requires compile to achieve good performance. The above building blocks are \u201cAll You Need\u201d (as of October 2024)# The main premise in this section is that most transformer variations are GPT-style, consisting of layers like Embedding, Positional Encoding, Attention Blocks and Feed Forward networks. If we were to try to classify the differences in this space, we might land on something like: Layer type (activation functions such as SwiGLU and others, normalization functions such as RMSNorm and others, positional encodings, such as Sinusoidal, Rotary.) Layer ordering, such as where to apply norms and positional encoding. Modifications to attention score, such as ALiBi, Relative Positional Bias and so on. In a pre-compiler environment, you might write a custom transformer and notice that it functions correctly but is slow. To address this, you might develop a custom fused kernel for the specific series of operations. In a compiler environment, you can simply perform the initial step and then compile and benefit from improved performance. MultiheadAttention# Remember that MultiheadAttention takes in a query, key, and value, and consists of an input projection, a scaled_dot_product_attention operator and an output projection. The main takeaway we want to demonstrate here is the improvement yielded when we replaced padded/masked inputs with nested tensors. The improvements are threefold: User Experience Remember that nn.MultiheadAttention requires query, key, and value to be dense torch.Tensors. It also provides a key_padding_mask that is used to mask out padding tokens in the key that arise due to different sequence lengths within a batch. Since there is no query_padding_mask in nn.MHA, users have to take care to mask/slice the outputs appropriately to account for query sequence lengths. NestedTensor cleanly removes the need for this sort of error-prone padding masks. Memory Instead of materializing a dense [B, S, D] tensor with a [B, S] padding mask (where B is batch size, S is max sequence length in the batch and D is embedding size), nested tensors allow you to cleanly represent the batch of varying sequence lengths. As a result, the input and intermediate activations will use less memory. Performance Since padding is not materialized and unnecessary computation on padding is skipped, performance and memory usage improve. We\u2019ll demonstrate the above by building upon the MultiheadAttention layer in the Nested Tensor tutorial and comparing it to the nn.MultiheadAttention layer. import torch import torch.nn as nn import torch.nn.functional as F class MultiHeadAttention(nn.Module): \"\"\" Computes multi-head attention. Supports nested or padded tensors. Args: E_q (int): Size of embedding dim for query E_k (int): Size of embedding dim for key E_v (int): Size of embedding dim for value E_total (int): Total embedding dim of combined heads post input projection. Each head has dim E_total // nheads nheads (int): Number of heads dropout (float, optional): Dropout probability. Default: 0.0 bias (bool, optional): Whether to add bias to input projection. Default: True \"\"\" def __init__( self, E_q: int, E_k: int, E_v: int, E_total: int, nheads: int, dropout: float = 0.0, bias=True, device=None, dtype=None, ): factory_kwargs = {\"device\": device, \"dtype\": dtype} super().__init__() self.nheads = nheads self.dropout = dropout self._qkv_same_embed_dim = E_q == E_k and E_q == E_v if self._qkv_same_embed_dim: self.packed_proj = nn.Linear(E_q, E_total * 3, bias=bias, **factory_kwargs) else: self.q_proj = nn.Linear(E_q, E_total, bias=bias, **factory_kwargs) self.k_proj = nn.Linear(E_k, E_total, bias=bias, **factory_kwargs) self.v_proj = nn.Linear(E_v, E_total, bias=bias, **factory_kwargs) E_out = E_q self.out_proj = nn.Linear(E_total, E_out, bias=bias, **factory_kwargs) assert E_total % nheads == 0, \"Embedding dim is not divisible by nheads\" self.E_head = E_total // nheads self.bias = bias def forward( self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask=None, is_causal=False, ) -\u003e torch.Tensor: \"\"\" Forward pass; runs the following process: 1. Apply input projection 2. Split heads and prepare for SDPA 3. Run SDPA 4. Apply output projection Args: query (torch.Tensor): query of shape (``N``, ``L_q``, ``E_qk``) key (torch.Tensor): key of shape (``N``, ``L_kv``, ``E_qk``) value (torch.Tensor): value of shape (``N``, ``L_kv``, ``E_v``) attn_mask (torch.Tensor, optional): attention mask of shape (``N``, ``L_q``, ``L_kv``) to pass to SDPA. Default: None is_causal (bool, optional): Whether to apply causal mask. Default: False Returns: attn_output (torch.Tensor): output of shape (N, L_t, E_q) \"\"\" # Step 1. Apply input projection if self._qkv_same_embed_dim: if query is key and key is value: result = self.packed_proj(query) query, key, value = torch.chunk(result, 3, dim=-1) else: q_weight, k_weight, v_weight = torch.chunk( self.packed_proj.weight, 3, dim=0 ) if self.bias: q_bias, k_bias, v_bias = torch.chunk( self.packed_proj.bias, 3, dim=0 ) else: q_bias, k_bias, v_bias = None, None, None query, key, value = ( F.linear(query, q_weight, q_bias), F.linear(key, k_weight, k_bias), F.linear(value, v_weight, v_bias), ) else: query = self.q_proj(query) key = self.k_proj(key) value = self.v_proj(value) # Step 2. Split heads and prepare for SDPA # reshape query, key, value to separate by head # (N, L_t, E_total) -\u003e (N, L_t, nheads, E_head) -\u003e (N, nheads, L_t, E_head) query = query.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2) # (N, L_s, E_total) -\u003e (N, L_s, nheads, E_head) -\u003e (N, nheads, L_s, E_head) key = key.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2) # (N, L_s, E_total) -\u003e (N, L_s, nheads, E_head) -\u003e (N, nheads, L_s, E_head) value = value.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2) # Step 3. Run SDPA # (N, nheads, L_t, E_head) attn_output = F.scaled_dot_product_attention( query, key, value, dropout_p=self.dropout, is_causal=is_causal ) # (N, nheads, L_t, E_head) -\u003e (N, L_t, nheads, E_head) -\u003e (N, L_t, E_total) attn_output = attn_output.transpose(1, 2).flatten(-2) # Step 4. Apply output projection # (N, L_t, E_total) -\u003e (N, L_t, E_out) attn_output = self.out_proj(attn_output) return attn_output Utilities# In this section, we include a utility to generate semi-realistic data using Zipf distribution for sentence lengths. This is used to generate the nested query, key, and value tensors. We also include a benchmark utility. import numpy as np def zipf_sentence_lengths(alpha: float, batch_size: int) -\u003e torch.Tensor: # generate fake corpus by unigram Zipf distribution # from wikitext-2 corpus, we get rank \".\" = 3, \"!\" = 386, \"?\" = 858 sentence_lengths = np.empty(batch_size, dtype=int) for ibatch in range(batch_size): sentence_lengths[ibatch] = 1 word = np.random.zipf(alpha) while word != 3 and word != 386 and word != 858: sentence_lengths[ibatch] += 1 word = np.random.zipf(alpha) return torch.tensor(sentence_lengths) # Generate a batch of semi-realistic data using Zipf distribution for sentence lengths # in the form of nested tensors with the jagged layout. def gen_batch(N, E_q, E_k, E_v, device, dtype=torch.float32, query_seq_len_1=False): # generate semi-realistic data using Zipf distribution for sentence lengths sentence_lengths = zipf_sentence_lengths(alpha=1.2, batch_size=N) # Note: the torch.jagged layout is a nested tensor layout that supports a single ragged # dimension and works with torch.compile. The batch items each have shape (B, S*, D) # where B = batch size, S* = ragged sequence length, and D = embedding dimension. if query_seq_len_1: query = torch.nested.nested_tensor( [torch.randn(1, E_q, dtype=dtype, device=device) for l in sentence_lengths], layout=torch.jagged, ) else: query = torch.nested.nested_tensor( [ torch.randn(l.item(), E_q, dtype=dtype, device=device) for l in sentence_lengths ], layout=torch.jagged, ) key = torch.nested.nested_tensor( [ torch.randn(s.item(), E_k, dtype=dtype, device=device) for s in sentence_lengths ], layout=torch.jagged, ) value = torch.nested.nested_tensor( [ torch.randn(s.item(), E_v, dtype=dtype, device=device) for s in sentence_lengths ], layout=torch.jagged, ) return query, key, value, sentence_lengths import math import timeit def benchmark(func, *args, **kwargs): torch.cuda.synchronize() torch.cuda.reset_peak_memory_stats() begin = timeit.default_timer() output = func(*args, **kwargs) torch.cuda.synchronize() end = timeit.default_timer() return output, (end - begin), torch.cuda.max_memory_allocated() We will now demonstrate the performance improvements of using nested tensors in the MultiheadAttention layer + compile for self attention. We compare this against the traditional nn.MultiheadAttention + compile with padding and masking. N, E_q, E_k, E_v, E_total = 512, 512, 512, 512, 512 E_out = E_q d_model = E_q nheads = 8 dropout = 0.0 bias = True device = \"cuda\" torch.manual_seed(6) query, key, value, sentence_lengths = gen_batch(N, E_q, E_k, E_v, device) S = sentence_lengths.max().item() print( f\"Total sequence length in nested query {sentence_lengths.sum().item()}, max sequence length {S}\" ) padded_query, padded_key, padded_value = ( t.to_padded_tensor(0.0) for t in (query, key, value) ) torch.manual_seed(6) mha_layer = MultiHeadAttention( E_q, E_k, E_v, E_total, nheads, dropout=dropout, bias=bias, device=\"cuda\" ) torch.manual_seed(6) vanilla_mha_layer = nn.MultiheadAttention( E_q, nheads, dropout=dropout, batch_first=True, bias=bias, device=\"cuda\" ) # ``nn.MultiheadAttention`` uses a non conventional initialization for layers, so do this for exact parity :( mha_layer.out_proj.weight = nn.Parameter( vanilla_mha_layer.out_proj.weight.clone().detach() ) mha_layer.packed_proj.weight = nn.Parameter( vanilla_mha_layer.in_proj_weight.clone().detach() ) mha_layer.out_proj.bias = nn.Parameter(vanilla_mha_layer.out_proj.bias.clone().detach()) mha_layer.packed_proj.bias = nn.Parameter( vanilla_mha_layer.in_proj_bias.clone().detach() ) new_mha_layer = torch.compile(mha_layer) # warmup compile nested_result_warmup = new_mha_layer(query, query, query, is_causal=True) # benchmark nested_result, nested_time, nested_peak_memory = benchmark( new_mha_layer, query, query, query, is_causal=True ) padded_nested_result = nested_result.to_padded_tensor(0.0) # For the vanilla ``nn.MultiheadAttention``, we need to construct the ``key_padding_mask`` # Further, ``nn.MultiheadAttention`` forces one to materialize the ``attn_mask`` even if using ``is_causal`` src_key_padding_mask = torch.where(padded_query == 0.0, -math.inf, 0)[:, :, 0] attn_mask = torch.empty((N, S, S), device=device).fill_(float(\"-inf\")) for i, s in enumerate(sentence_lengths): attn_mask[i, :s, :s] = nn.Transformer.generate_square_subsequent_mask(s) attn_mask = attn_mask.unsqueeze(1).expand(N, nheads, S, S).reshape(N * nheads, S, S) vanilla_mha_layer = torch.compile(vanilla_mha_layer) # warmup compile warmup_vanilla_result = vanilla_mha_layer( padded_query, padded_query, padded_query, attn_mask=attn_mask, key_padding_mask=src_key_padding_mask, need_weights=False, is_causal=True, ) # benchmark (padded_result, _), padded_time, padded_peak_memory = benchmark( vanilla_mha_layer, padded_query, padded_query, padded_query, key_padding_mask=src_key_padding_mask, need_weights=False, attn_mask=attn_mask, is_causal=True, ) print(f\"{padded_time=:.5f}, padded_peak_memory={padded_peak_memory/1e9:.2f} GB\") print(f\"{nested_time=:.5f}, nested_peak_memory={nested_peak_memory/1e9:.2f} GB\") print( \"Max difference between vanilla and nested result\", (padded_result - padded_nested_result).abs().max().item(), ) print(f\"Nested speedup: {(padded_time/nested_time):.2f}\") print( f\"Nested peak memory reduction {((padded_peak_memory - nested_peak_memory)/1e9):.2f} GB\" ) Total sequence length in nested query 9721, max sequence length 90 /opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision(\u0027high\u0027)` for better performance. padded_time=0.00371, padded_peak_memory=2.52 GB nested_time=0.00138, nested_peak_memory=0.59 GB Max difference between vanilla and nested result 0.0 Nested speedup: 2.68 Nested peak memory reduction 1.93 GB For reference, here are some sample outputs on A100: padded_time=0.03454, padded_peak_memory=4.14 GB nested_time=0.00612, nested_peak_memory=0.76 GB Max difference between vanilla and nested result 0.0 Nested speedup: 5.65 Nested peak memory reduction 3.39 GB We can also see the same for backward pass for i, entry_length in enumerate(sentence_lengths): # padding-specific step: remove output projection bias from padded entries for fair comparison padded_result[i, entry_length:, :] = 0.0 _, padded_bw_time, padded_bw_peak_mem = benchmark( lambda: padded_result.sum().backward() ) _, nested_bw_time, nested_bw_peak_mem = benchmark( lambda: padded_nested_result.sum().backward() ) print(f\"{padded_bw_time=:.5f}, padded_bw_peak_mem={padded_bw_peak_mem/1e9:.2f} GB\") print(f\"{nested_bw_time=:.5f}, nested_bw_peak_mem={nested_bw_peak_mem/1e9:.2f} GB\") print(f\"Nested backward speedup: {(padded_bw_time/nested_bw_time):.2f}\") print( f\"Nested backward peak memory reduction {((padded_bw_peak_mem - nested_bw_peak_mem)/1e9):.2f} GB\" ) print( \"Difference in out_proj.weight.grad\", (mha_layer.out_proj.weight.grad - vanilla_mha_layer.out_proj.weight.grad) .abs() .max() .item(), ) print( \"Difference in packed_proj.weight.grad\", (mha_layer.packed_proj.weight.grad - vanilla_mha_layer.in_proj_weight.grad) .abs() .max() .item(), ) print( \"Difference in out_proj.bias.grad\", (mha_layer.out_proj.bias.grad - vanilla_mha_layer.out_proj.bias.grad) .abs() .max() .item(), ) print( \"Difference in packed_proj.bias.grad\", (mha_layer.packed_proj.bias.grad - vanilla_mha_layer.in_proj_bias.grad) .abs() .max() .item(), ) padded_bw_time=0.99221, padded_bw_peak_mem=3.16 GB nested_bw_time=0.02065, nested_bw_peak_mem=2.08 GB Nested backward speedup: 48.04 Nested backward peak memory reduction 1.08 GB Difference in out_proj.weight.grad 0.0001983642578125 Difference in packed_proj.weight.grad 0.001708984375 Difference in out_proj.bias.grad 0.0 Difference in packed_proj.bias.grad 0.001953125 Sample outputs on A100: padded_bw_time=2.09337, padded_bw_peak_mem=5.10 GB nested_bw_time=0.01452, nested_bw_peak_mem=3.24 GB Nested backward speedup: 144.13 Nested backward peak memory reduction 1.86 GB Difference in out_proj.weight.grad 0.000244140625 Difference in packed_proj.weight.grad 0.001556396484375 Difference in out_proj.bias.grad 0.0 Difference in packed_proj.bias.grad 0.001953125 GPT-style layer# A basic GPT-style transformer layer consists of a causal self-attention layer followed by a feed-forward network (FFN) with skip connections. Implementing this is fairly straightforward using the MultiheadAttention layer above and gives equivalent results to an nn.TransformerEncoderLayer with is_causal=True. We demonstrate examples of implementing the rest of the nn layers here but omit that from this tutorial for brevity. Going one step further# So far, we have demonstrated how to implement a performant MultiheadAttention layer that follows the traditional nn.MultiheadAttention. Going back to our classification of modifications to the transformer architecture, remember that we classified the modifications into layer type, layer ordering, and modifications to the attention score. We trust that changing layer type and layer ordering (such as swapping LayerNorm for RMSNorm) is fairly straightforward. In this section, we will discuss various functionalities using the aforementioned building blocks, including the following: Cross Attention Fully masked rows no longer cause NaNs Modifying attention score: ALiBi with FlexAttention and NJT Packed Projection Cross Attention# Cross attention is a form of attention where the query and key/value tensors are from different sequences. One example of this is in nn.TransformerDecoderLayer where the query comes from the decoder and the key/value come from the encoder. The above MultiheadAttention layer nicely generalizes to this case with nested tensors for both query and key/value. query, _, _, q_len = gen_batch(N, E_q, E_k, E_v, device) _, key, value, kv_len = gen_batch(N, E_q, E_k, E_v, device) print( f\"Total sequence length in nested query {q_len.sum().item()}, max sequence length {q_len.max().item()}\" ) print( f\"Total sequence length in nested key/value {kv_len.sum().item()}, max sequence length {kv_len.max().item()}\" ) out = new_mha_layer(query, key, value, is_causal=False) Total sequence length in nested query 11088, max sequence length 132 Total sequence length in nested key/value 11397, max sequence length 150 As above, we can compare this against the vanilla compiled nn.MultiheadAttention. torch.manual_seed(6) query, _, _, q_len = gen_batch(N, E_q, E_k, E_v, device) _, key, value, kv_len = gen_batch(N, E_q, E_k, E_v, device) padded_query, padded_key, padded_value = ( t.to_padded_tensor(0.0) for t in (query, key, value) ) key_padding_mask = torch.where(padded_key == 0.0, -math.inf, 0)[:, :, 0] # warmup compile warmup_nested_result = new_mha_layer(query, key, value, is_causal=False) warmup_vanilla_result = vanilla_mha_layer( padded_query, padded_key, padded_value, key_padding_mask=key_padding_mask, need_weights=False, is_causal=False, ) nested_result, nested_time, nested_peak_memory = benchmark( new_mha_layer, query, key, value, is_causal=False ) (padded_result, _), padded_time, padded_peak_memory = benchmark( vanilla_mha_layer, padded_query, padded_key, padded_value, key_padding_mask=key_padding_mask, need_weights=False, is_causal=False, ) padded_nested_result = nested_result.to_padded_tensor(0.0) for i, entry_length in enumerate(q_len): # padding-specific step: remove output projection bias from padded entries for fair comparison padded_result[i, entry_length:, :] = 0.0 print( \"Max difference between vanilla and nested result\", (padded_result - padded_nested_result).abs().max().item(), ) print(f\"Nested speedup: {(padded_time/nested_time):.2f}\") print( f\"Nested peak memory reduction {((padded_peak_memory - nested_peak_memory)/1e9):.2f} GB\" ) Max difference between vanilla and nested result 0.0 Nested speedup: 19.70 Nested peak memory reduction 1.11 GB Sample outputs on A100: Max difference between vanilla and nested result 0.0 Nested speedup: 4.01 Nested peak memory reduction 1.40 GB Fully masked rows no longer cause NaNs# There has been a long standing issue with nn.MultiheadAttention and scaled_dot_product_attention where if a row was fully masked out, the output of the attention layer would be NaN. See issue. This is because the softmax over an empty set is undefined. Thanks to this PR this is no longer the case. Instead, the output corresponding to fully masked rows in scaled_dot_product_attention will be 0. For cases where nn.MHA does not employ the \u201cfast-path\u201d, this will also apply. Using a custom MHA layer with NJTs is strongly recommended over the existing \u201cfast-path\u201d in nn.MultiheadAttention as NJT\u2019s ability to model raggedness appropriately makes it possible to properly express empty sequences. FlexAttention + NJT# NJT also composes with the FlexAttention module. This is a generalization of the MultiheadAttention layer that allows for arbitrary modifications to the attention score. The example below takes the alibi_mod that implements ALiBi from attention gym and uses it with nested input tensors. from torch.nn.attention.flex_attention import flex_attention def generate_alibi_bias(H: int): \"\"\"Returns an alibi bias score_mod given the number of heads H Args: H: number of heads Returns: alibi_bias: alibi bias score_mod \"\"\" def alibi_mod(score, b, h, q_idx, kv_idx): scale = torch.exp2(-((h + 1) * 8.0 / H)) bias = (q_idx - kv_idx) * scale return score + bias return alibi_mod query, key, value, _ = gen_batch(N, E_q, E_k, E_v, device) n_heads, D = 8, E_q // 8 alibi_score_mod = generate_alibi_bias(n_heads) query = query.unflatten(-1, [n_heads, D]).transpose(1, 2).detach().requires_grad_() key = key.unflatten(-1, [n_heads, D]).transpose(1, 2).detach().requires_grad_() value = value.unflatten(-1, [n_heads, D]).transpose(1, 2).detach().requires_grad_() out_flex2 = flex_attention(query, key, value, score_mod=alibi_score_mod) In addition, one can also use the block_mask utility of FlexAttention with NJTs via the create_nested_block_mask function. This is useful for taking advantage of the sparsity of the mask to speed up the attention computation. In particular, the function creates a sparse block mask for a \u201cstacked sequence\u201d of all the variable length sequences in the NJT combined into one, while properly masking out inter-sequence attention. In the following example, we show how to create a causal block mask using this utility. from torch.nn.attention.flex_attention import create_nested_block_mask def causal_mask(b, h, q_idx, kv_idx): return q_idx \u003e= kv_idx query, key, value, _ = gen_batch(N, E_q, E_k, E_v, device) block_mask = create_nested_block_mask(causal_mask, 1, 1, query, _compile=True) query = query.unflatten(-1, [n_heads, D]).transpose(1, 2).detach().requires_grad_() key = key.unflatten(-1, [n_heads, D]).transpose(1, 2).detach().requires_grad_() value = value.unflatten(-1, [n_heads, D]).transpose(1, 2).detach().requires_grad_() out_flex = flex_attention(query, key, value, block_mask=block_mask) Packed Projection# Packed projection is a technique that makes use of the fact that when the input for projection (matrix multiplications) are the same (self-attention), we can pack the projection weights and biases into single tensors. It is especially useful when the individual projections are memory bound rather than compute bound. There are two examples that we will demonstrate here: Input projection for MultiheadAttention SwiGLU activation in feed-forward network of Transformer Layer Input projection for MultiheadAttention# When doing self-attention, the query, key, and value are the same tensor. Each of these tensors is projected with a Linear(E_q, E_total) layer. Instead, we can pack this into one layer, which is what we do in the MultiheadAttention layer above. Let us compare the performance of the packed projection against the usual method: class InputProjection(nn.Module): def __init__(self, E_q, E_total, bias=False, device=None, dtype=None): factory_kwargs = {\"device\": device, \"dtype\": dtype} super().__init__() self.q_proj = nn.Linear(E_q, E_total, bias=bias, **factory_kwargs) self.k_proj = nn.Linear(E_q, E_total, bias=bias, **factory_kwargs) self.v_proj = nn.Linear(E_q, E_total, bias=bias, **factory_kwargs) def forward(self, x): return self.q_proj(x), self.k_proj(x), self.v_proj(x) class PackedInputProjection(nn.Module): def __init__(self, E_q, E_total, bias=False, device=None, dtype=None): factory_kwargs = {\"device\": device, \"dtype\": dtype} super().__init__() self.packed_proj = nn.Linear(E_q, E_total * 3, bias=bias, **factory_kwargs) def forward(self, query): return torch.chunk(self.packed_proj(query), 3, dim=-1) B, D, dtype = 256, 8192, torch.bfloat16 torch.set_float32_matmul_precision(\"high\") in_proj = torch.compile(InputProjection(D, D, device=\"cuda\", dtype=torch.bfloat16)) packed_in_proj = torch.compile( PackedInputProjection(D, D, device=\"cuda\", dtype=torch.bfloat16) ) q, _, _, sequence_lengths = gen_batch(B, D, D, D, device=\"cuda\", dtype=torch.bfloat16) # warmup in_proj(q) packed_in_proj(q) # benchmark (q_out, k_out, v_out), time, _ = benchmark(in_proj, q) (q_out, k_out, v_out), time_packed, _ = benchmark(packed_in_proj, q) # On my A100 prints 1.05x speedup print( f\"InputProjection: {time:5f} s, PackedInputProjection: {time_packed:5f} s, speedup: {time/time_packed:.2f}x\" ) InputProjection: 0.003266 s, PackedInputProjection: 0.003204 s, speedup: 1.02x SwiGLU feed forward network of Transformer Layer# Swish-Gated Linear Unit (SwiGLU) is a non-linear activation function that is increasingly popular in the feed-forward network of the transformer layer (e.g. Llama). A feed-forward network with SwiGLU activation is defined as: class SwiGLUFFN(nn.Module): def __init__( self, dim, hidden_dim, multiple_of, ffn_dim_multiplier=None, device=None, dtype=None, ): factory_kwargs = {\"device\": device, \"dtype\": dtype} super().__init__() hidden_dim = int(2 * hidden_dim / 3) # custom dim factor multiplier if ffn_dim_multiplier is not None: hidden_dim = int(ffn_dim_multiplier * hidden_dim) hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of) self.w1 = nn.Linear(dim, hidden_dim, bias=False, **factory_kwargs) self.w2 = nn.Linear(hidden_dim, dim, bias=False, **factory_kwargs) self.w3 = nn.Linear(dim, hidden_dim, bias=False, **factory_kwargs) def forward(self, x): return self.w2(F.silu(self.w1(x)) * self.w3(x)) An alternative way of implementing this that uses packed projection is class PackedSwiGLUFFN(nn.Module): def __init__( self, dim, hidden_dim, multiple_of, ffn_dim_multiplier=None, device=None, dtype=None, ): factory_kwargs = {\"device\": device, \"dtype\": dtype} super().__init__() hidden_dim = int(2 * hidden_dim / 3) # custom dim factor multiplier if ffn_dim_multiplier is not None: hidden_dim = int(ffn_dim_multiplier * hidden_dim) hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of) self.w13 = nn.Linear(dim, 2 * hidden_dim, bias=False, **factory_kwargs) self.w2 = nn.Linear(hidden_dim, dim, bias=False, **factory_kwargs) def forward(self, x): x1, x3 = torch.chunk(self.w13(x), 2, dim=-1) return self.w2(F.silu(x1) * x3) We can compare the performance of the two implementations as follows Depending on your hardware, you might see different results. On an A100 I see 1.12x speedup for D=128. D = 128 swigluffn = torch.compile(SwiGLUFFN(D, D * 4, 256, device=\"cuda\", dtype=torch.bfloat16)) packed_swigluffn = torch.compile( PackedSwiGLUFFN(D, D * 4, 256, device=\"cuda\", dtype=torch.bfloat16) ) q, _, _, sentence_lengths = gen_batch(D, D, D, D, device=\"cuda\", dtype=torch.bfloat16) # warmup swigluffn(q) packed_swigluffn(q) # benchmark _, time, _ = benchmark(swigluffn, q) _, time_packed, _ = benchmark(packed_swigluffn, q) # On my A100 prints 1.08x speedup print( f\"SwiGLUFFN: {time} s, PackedSwiGLUFFN: {time_packed} s, speedup: {time/time_packed:.2f}x\" ) SwiGLUFFN: 0.0008159049903042614 s, PackedSwiGLUFFN: 0.000746919002267532 s, speedup: 1.09x Extended examples# We intend to update this tutorial to demonstrate more examples of how to use the various performant building blocks such as KV-Caching, Grouped Query Attention etc. Further, there are several good examples of using various performant building blocks to implement various transformer architectures. Some examples include gpt-fast segment-anything-fast lucidrains implementation of NaViT with nested tensors torchtune\u2019s implementation of VisionTransformer Conclusion# In this tutorial, we have introduced the low level building blocks PyTorch provides for writing transformer layers and demonstrated examples how to compose them. It is our hope that this tutorial has educated the reader on the ease with which flexible and performant transformer layers can be implemented by users of PyTorch. Total running time of the script: (1 minutes 19.616 seconds) Download Jupyter notebook: transformer_building_blocks.ipynb Download Python source code: transformer_building_blocks.py Download zipped: transformer_building_blocks.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/intermediate/transformer_building_blocks.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>