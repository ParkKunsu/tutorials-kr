
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/intermediate/char_rnn_classification_tutorial.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Author: Sean Robertson, 번역: 황성수, 김제필,. 이 튜토리얼은 3부로 구성된 시리즈의 일부입니다: 기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기, 기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 생성하기, 기초부터 시작하는 NLP: Sequence to Sequence 네트워크와 Attention을 이용한 번역. 여기에서는 단어를 분류하기 위해 기초적인 문자-단위의 순환 신경망(RNN, Recurrent Nueral Network)을 구축하고 학습할 예정입니다. 이 튜토리얼 및 이후 ..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="Author: Sean Robertson, 번역: 황성수, 김제필,. 이 튜토리얼은 3부로 구성된 시리즈의 일부입니다: 기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기, 기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 생성하기, 기초부터 시작하는 NLP: Sequence to Sequence 네트워크와 Attention을 이용한 번역. 여기에서는 단어를 분류하기 위해 기초적인 문자-단위의 순환 신경망(RNN, Recurrent Nueral Network)을 구축하고 학습할 예정입니다. 이 튜토리얼 및 이후 ..." />
<meta property="og:ignore_canonical" content="true" />

    <title>기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기 &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/char_rnn_classification_tutorial';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/intermediate/char_rnn_classification_tutorial.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">기초부터 시작하는...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기">
        <meta itemprop="position" content="1">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">intermediate/char_rnn_classification_tutorial</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-intermediate-char-rnn-classification-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="nlp-rnn">
<span id="sphx-glr-intermediate-char-rnn-classification-tutorial-py"></span><h1>기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기<a class="headerlink" href="#nlp-rnn" title="Link to this heading">#</a></h1>
<dl class="simple">
<dt><strong>Author</strong>: <a class="reference external" href="https://github.com/spro">Sean Robertson</a></dt><dd><p><strong>번역</strong>: <a class="reference external" href="https://github.com/adonisues">황성수</a>, <a class="reference external" href="https://github.com/garlicvread">김제필</a></p>
</dd>
</dl>
<p>이 튜토리얼은 3부로 구성된 시리즈의 일부입니다:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://tutorials.pytorch.kr/intermediate/char_rnn_classification_tutorial.html">기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기</a></p></li>
<li><p><a class="reference external" href="https://tutorials.pytorch.kr/intermediate/char_rnn_generation_tutorial.html">기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 생성하기</a></p></li>
<li><p><a class="reference external" href="https://tutorials.pytorch.kr/intermediate/seq2seq_translation_tutorial.html">기초부터 시작하는 NLP: Sequence to Sequence 네트워크와 Attention을 이용한 번역</a></p></li>
</ul>
<p>여기에서는 단어를 분류하기 위해 기초적인 문자-단위의 순환 신경망(RNN, Recurrent Nueral Network)을
구축하고 학습할 예정입니다. 이 튜토리얼 및 이후 2개 튜토리얼인 <a class="reference internal" href="char_rnn_generation_tutorial.html"><span class="doc">기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 생성하기</span></a>
및 <a class="reference internal" href="seq2seq_translation_tutorial.html"><span class="doc">기초부터 시작하는 NLP: Sequence to Sequence 네트워크와 Attention을 이용한 번역</span></a> 에서는 자연어 처리(NLP, Natural Language Processing)
분야에서 어떻게 데이터를 전처리하고 NLP 모델을 구축하는지를 밑바닥부터(from scratch) 설명합니다.
이를 위해 이 튜토리얼 시리즈에서는 NLP 모델링을 위한 데이터 전처리가 밑바닥(low-level)에서 어떻게 진행되는지 알 수 있습니다.</p>
<p>문자-단위 RNN은 단어를 문자의 연속으로 읽어 들여서 각 단계의 예측과
“은닉 상태(Hidden State)”를 출력하고, 다음 단계에 이전 단계의 은닉 상태를 전달합니다.
단어가 속한 클래스로 출력되도록 최종 예측으로 선택합니다.</p>
<p>구체적으로, 18개 언어로 된 수천 개의 성(姓)을 훈련시키고,
철자에 따라 이름이 어떤 언어인지 예측합니다.</p>
<section id="torch">
<h2>Torch 준비<a class="headerlink" href="#torch" title="Link to this heading">#</a></h2>
<p>하드웨어(CPU 또는 CUDA)에 맞춰 GPU 가속을 사용할 수 있도록 적절한 장치를 기본 장치로 설정합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Check if CUDA is available</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">set_default_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using device = </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">get_default_device</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Using device = cuda:0
</pre></div>
</div>
</section>
<section id="id3">
<h2>데이터 준비<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://download.pytorch.org/tutorial/data.zip">여기</a> 에서 데이터를 다운로드 받고
현재 디렉토리에 압축을 풉니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">data/names</span></code> 디렉토리에는 <code class="docutils literal notranslate"><span class="pre">[Language].txt</span></code> 라는 18 개의 텍스트 파일이 있습니다.
각 파일에는 한 줄에 하나의 이름이 포함되어 있으며 대부분 로마자로 되어 있습니다.
(하지만 유니코드에서 ASCII로 변환은 해야 합니다)</p>
<p>첫번째 단계는 데이터를 정의하고 정리하는 것입니다. 초기에는 유니코드를 일반 ASCII로 변환하여
RNN 입력 레이어를 제한해야 합니다. 이는 유니코드 문자열을 ASCII로 변환하고 허용된 문자의 작은 집합만을 허용하여 이루어집니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">string</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">unicodedata</span>

<span class="c1"># &quot;_&quot; 를 사용하여 어휘집(Vocabulary)에 없는 문자를 표현할 수 있습니다. 즉, 모델에서 처리하지 않는 모든 문자를 표현할 수 있습니다.</span>
<span class="n">allowed_characters</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_letters</span> <span class="o">+</span> <span class="s2">&quot; .,;&#39;&quot;</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span>
<span class="n">n_letters</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">allowed_characters</span><span class="p">)</span>

<span class="c1"># 유니코드 문자열을 일반 ASCII로 변환하기: https://stackoverflow.com/a/518232/2809427</span>
<span class="k">def</span><span class="w"> </span><span class="nf">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">&#39;NFD&#39;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">!=</span> <span class="s1">&#39;Mn&#39;</span>
        <span class="ow">and</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">allowed_characters</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>유니코드 알파벳 이름을 일반 ASCII로 변환하는 예시입니다. 이렇게 하면 입력 레이어를 단순화할 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;converting &#39;Ślusàrski&#39; to </span><span class="si">{</span><span class="n">unicodeToAscii</span><span class="p">(</span><span class="s1">&#39;Ślusàrski&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>converting &#39;Ślusàrski&#39; to Slusarski
</pre></div>
</div>
</section>
<section id="tensor">
<h2>이름을 Tensor로 변경<a class="headerlink" href="#tensor" title="Link to this heading">#</a></h2>
<p>이제 모든 이름을 체계화했으므로, 이를 활용하기 위해 Tensor로
변환해야 합니다.</p>
<p>하나의 문자를 표현하기 위해 크기가 <code class="docutils literal notranslate"><span class="pre">&lt;1</span> <span class="pre">x</span> <span class="pre">n_letters&gt;</span></code> 인
“One-Hot 벡터”를 사용합니다. One-Hot 벡터는 현재 문자의
주소에는 1이, 그 외 나머지 주소에는 0이 채워진 벡터입니다.
예시 <code class="docutils literal notranslate"><span class="pre">&quot;b&quot;</span> <span class="pre">=</span> <span class="pre">&lt;0</span> <span class="pre">1</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">0</span> <span class="pre">...&gt;</span></code> .</p>
<p>단어를 만들기 위해 One-Hot 벡터들을 2차원 행렬
<code class="docutils literal notranslate"><span class="pre">&lt;line_length</span> <span class="pre">x</span> <span class="pre">1</span> <span class="pre">x</span> <span class="pre">n_letters&gt;</span></code> 에 결합시킵니다.</p>
<p>위에서 보이는 추가적인 1차원은 PyTorch에서 모든 것이 배치(batch)에 있다고 가정하기
때문에 발생합니다. 여기서는 배치 크기 1을 사용하고 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># .. note::</span>
<span class="c1">#    역자 주: One-Hot 벡터는 언어 및 범주형 데이터를 다룰 때 주로 사용하며,</span>
<span class="c1">#    단어, 글자 등을 벡터로 표현할 때 단어, 글자 사이의 상관 관계를 미리 알 수 없을 경우,</span>
<span class="c1">#    One-Hot으로 표현하여 서로 직교한다고 가정하고 학습을 시작합니다.</span>
<span class="c1">#    이와 동일하게, 상관 관계를 알 수 없는 다른 데이터의 경우에도 One-Hot 벡터를 활용할 수 있습니다.</span>
<span class="c1">#</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># all_letters 로 문자의 주소 찾기, 예시 &quot;a&quot; = 0</span>
<span class="k">def</span><span class="w"> </span><span class="nf">letterToIndex</span><span class="p">(</span><span class="n">letter</span><span class="p">):</span>
    <span class="c1"># 모델이 모르는 글자를 만나면, 어휘집에 존재하지 않는 문자(&quot;_&quot;)를 반환합니다.</span>
    <span class="k">if</span> <span class="n">letter</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">allowed_characters</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">allowed_characters</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">allowed_characters</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">letter</span><span class="p">)</span>

<span class="c1"># 검증을 위해서 한 개의 문자를 &lt;1 x n_letters&gt; Tensor로 변환</span>
<span class="k">def</span><span class="w"> </span><span class="nf">letterToTensor</span><span class="p">(</span><span class="n">letter</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_letters</span><span class="p">)</span>
    <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">letterToIndex</span><span class="p">(</span><span class="n">letter</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">tensor</span>

<span class="c1"># 한 줄(이름)을  &lt;line_length x 1 x n_letters&gt;,</span>
<span class="c1"># 또는 One-Hot 문자 벡터의 Array로 변경</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lineToTensor</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_letters</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">li</span><span class="p">,</span> <span class="n">letter</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
        <span class="n">tensor</span><span class="p">[</span><span class="n">li</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">letterToIndex</span><span class="p">(</span><span class="n">letter</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">tensor</span>

<span class="nb">print</span><span class="p">(</span><span class="n">letterToTensor</span><span class="p">(</span><span class="s1">&#39;J&#39;</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">lineToTensor</span><span class="p">(</span><span class="s1">&#39;Jones&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0.]], device=&#39;cuda:0&#39;)
torch.Size([5, 1, 58])
</pre></div>
</div>
<p>Here are some examples of how to use <code class="docutils literal notranslate"><span class="pre">lineToTensor()</span></code> for a single and multiple character string.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The letter &#39;a&#39; becomes </span><span class="si">{</span><span class="n">lineToTensor</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1">#notice that the first position in the tensor = 1</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The name &#39;Ahn&#39; becomes </span><span class="si">{</span><span class="n">lineToTensor</span><span class="p">(</span><span class="s1">&#39;Ahn&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1">#notice &#39;A&#39; sets the 27th index to 1</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>The letter &#39;a&#39; becomes tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0.]]], device=&#39;cuda:0&#39;)
The name &#39;Ahn&#39; becomes tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0.]]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
<p>Congratulations, you have built the foundational tensor objects for this learning task! You can use a similar approach
for other RNN tasks with text.</p>
<p>Next, we need to combine all our examples into a dataset so we can train, test and validate our models. For this,
we will use the <a class="reference external" href="https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html">Dataset and DataLoader</a> classes
to hold our dataset. Each Dataset needs to implement three functions: <code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">__len__</span></code>, and <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">io</span><span class="w"> </span><span class="kn">import</span> <span class="nb">open</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">glob</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span><span class="w"> </span><span class="nc">NamesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_dir</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_dir</span> <span class="o">=</span> <span class="n">data_dir</span> <span class="c1">#for provenance of the dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">localtime</span> <span class="c1">#for provenance of the dataset</span>
        <span class="n">labels_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span> <span class="c1">#set of all classes</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_tensors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels_tensors</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1">#read all the ``.txt`` files in the specified directory</span>
        <span class="n">text_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;*.txt&#39;</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">text_files</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filename</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">labels_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
            <span class="n">lines</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lineToTensor</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

        <span class="c1">#Cache the tensor representation of the labels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels_uniq</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">labels_set</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)):</span>
            <span class="n">temp_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">labels_uniq</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">])],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">labels_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_tensor</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">data_item</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">data_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">data_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_tensors</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">label_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels_tensors</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">label_tensor</span><span class="p">,</span> <span class="n">data_tensor</span><span class="p">,</span> <span class="n">data_label</span><span class="p">,</span> <span class="n">data_item</span>
</pre></div>
</div>
<p>Here we can load our example data into the <code class="docutils literal notranslate"><span class="pre">NamesDataset</span></code></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">alldata</span> <span class="o">=</span> <span class="n">NamesDataset</span><span class="p">(</span><span class="s2">&quot;data/names&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">alldata</span><span class="p">)</span><span class="si">}</span><span class="s2"> items of data&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;example = </span><span class="si">{</span><span class="n">alldata</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>loaded 20074 items of data
example = (tensor([1], device=&#39;cuda:0&#39;), tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0.]],

        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0.]],

        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0.]],

        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0.]]], device=&#39;cuda:0&#39;), &#39;English&#39;, &#39;Abbas&#39;)
</pre></div>
</div>
<p>Using the dataset object allows us to easily split the data into train and test sets. Here we create a 80/20
split but the <code class="docutils literal notranslate"><span class="pre">torch.utils.data</span></code> has more useful utilities. Here we specify a generator since we need to use the
same device as PyTorch defaults to above.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">train_set</span><span class="p">,</span> <span class="n">test_set</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">alldata</span><span class="p">,</span> <span class="p">[</span><span class="mf">.85</span><span class="p">,</span> <span class="mf">.15</span><span class="p">],</span> <span class="n">generator</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">2024</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;train examples = </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span><span class="si">}</span><span class="s2">, validation examples = </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_set</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>train examples = 17063, validation examples = 3011
</pre></div>
</div>
<p>Now we have a basic dataset containing <strong>20074</strong> examples where each example is a pairing of label and name. We have also
split the dataset into training and testing so we can validate the model that we build.</p>
</section>
<section id="id4">
<h2>네트워크 생성<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<p>Autograd 전에, Torch에서 RNN(recurrent neural network) 생성은
여러 시간 단계 걸쳐서 계층의 매개변수를 복제하는 작업을 포함합니다.
계층은 은닉 상태와 변화도(Gradient)를 가지며, 이제 이것들은 그래프 자체에서
완전히 처리됩니다. 이는 feed-forward 계층과
같은 매우 “순수한” 방법으로 RNN을 구현할 수 있음을 의미합니다.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>역자 주: 여기서는 학습 목적으로 nn.RNN 대신 직접 RNN을 사용합니다.</p>
</div>
<p>이 RNN 모듈은 “기본(vanilla)적인 RNN”을 구현하며, 입력과 은닉 상태(hidden state),
그리고 출력 뒤 동작하는 <code class="docutils literal notranslate"><span class="pre">LogSoftmax</span></code> 계층이 있는 3개의 선형 계층만을 가집니다.</p>
<p>This CharRNN class implements an RNN with three components.
First, we use the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.html">nn.RNN implementation</a>.
Next, we define a layer that maps the RNN hidden layers to our output. And finally, we apply a <code class="docutils literal notranslate"><span class="pre">softmax</span></code> function. Using <code class="docutils literal notranslate"><span class="pre">nn.RNN</span></code>
leads to a significant improvement in performance, such as cuDNN-accelerated kernels, versus implementing
each layer as a <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code>. It also simplifies the implementation in <code class="docutils literal notranslate"><span class="pre">forward()</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">class</span><span class="w"> </span><span class="nc">CharRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CharRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">h2o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">line_tensor</span><span class="p">):</span>
        <span class="n">rnn_out</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">line_tensor</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">h2o</span><span class="p">(</span><span class="n">hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>We can then create an RNN with 58 input nodes, 128 hidden nodes, and 18 outputs:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">rnn</span> <span class="o">=</span> <span class="n">CharRNN</span><span class="p">(</span><span class="n">n_letters</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">alldata</span><span class="o">.</span><span class="n">labels_uniq</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rnn</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>CharRNN(
  (rnn): RNN(58, 128)
  (h2o): Linear(in_features=128, out_features=18, bias=True)
  (softmax): LogSoftmax(dim=1)
)
</pre></div>
</div>
<p>After that we can pass our Tensor to the RNN to obtain a predicted output. Subsequently,
we use a helper function, <code class="docutils literal notranslate"><span class="pre">label_from_output</span></code>, to derive a text label for the class.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">label_from_output</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">output_labels</span><span class="p">):</span>
    <span class="n">top_n</span><span class="p">,</span> <span class="n">top_i</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">label_i</span> <span class="o">=</span> <span class="n">top_i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">output_labels</span><span class="p">[</span><span class="n">label_i</span><span class="p">],</span> <span class="n">label_i</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">lineToTensor</span><span class="p">(</span><span class="s1">&#39;Albert&#39;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1">#this is equivalent to ``output = rnn.forward(input)``</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">label_from_output</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">alldata</span><span class="o">.</span><span class="n">labels_uniq</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[-2.9395, -3.0940, -2.8990, -2.9367, -2.8716, -2.7607, -2.9501, -2.8651,
         -2.8425, -2.8339, -2.8015, -2.9372, -2.9002, -2.9046, -2.8947, -2.8643,
         -2.9327, -2.8415]], device=&#39;cuda:0&#39;, grad_fn=&lt;LogSoftmaxBackward0&gt;)
(&#39;Greek&#39;, 5)
</pre></div>
</div>
</section>
<section id="id5">
<h2>학습<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<section id="id6">
<h3>신경망 학습<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>이제 이 네트워크를 학습하는 데 필요한 예시(학습 데이터)를 보여주고 추정합니다.
만일 틀렸다면 알려 줍니다.</p>
<p>We do this by defining a <code class="docutils literal notranslate"><span class="pre">train()</span></code> function which trains the model on a given dataset using minibatches. RNNs
RNNs are trained similarly to other networks; therefore, for completeness, we include a batched training method here.
The loop (<code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">batch</span></code>) computes the losses for each of the items in the batch before adjusting the
weights. This operation is repeated until the number of epochs is reached.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">n_epoch</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n_batch_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">report_every</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Learn on a batch of training_data for a specified number of iterations and reporting thresholds</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Keep track of losses for plotting</span>
    <span class="n">current_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">all_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rnn</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;training on data set with n = </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">training_data</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">rnn</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># clear the gradients</span>

        <span class="c1"># create some minibatches</span>
        <span class="c1"># we cannot use dataloaders because each of our names is a different length</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_data</span><span class="p">)))</span>
        <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">batches</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span> <span class="o">//</span><span class="n">n_batch_size</span> <span class="p">)</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>
            <span class="n">batch_loss</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span> <span class="c1">#for each example in this batch</span>
                <span class="p">(</span><span class="n">label_tensor</span><span class="p">,</span> <span class="n">text_tensor</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span> <span class="o">=</span> <span class="n">training_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">text_tensor</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label_tensor</span><span class="p">)</span>
                <span class="n">batch_loss</span> <span class="o">+=</span> <span class="n">loss</span>

            <span class="c1"># optimize parameters</span>
            <span class="n">batch_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mi">3</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="n">current_loss</span> <span class="o">+=</span> <span class="n">batch_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="n">all_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span> <span class="p">)</span>
        <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="n">report_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">iter</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="nb">iter</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">n_epoch</span><span class="si">:</span><span class="s2">.0%</span><span class="si">}</span><span class="s2">): </span><span class="se">\t</span><span class="s2"> average batch loss = </span><span class="si">{</span><span class="n">all_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">current_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">return</span> <span class="n">all_losses</span>
</pre></div>
</div>
<p>We can now train a dataset with minibatches for a specified number of epochs. The number of epochs for this
example is reduced to speed up the build. You can get better results with different parameters.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">all_losses</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">train_set</span><span class="p">,</span> <span class="n">n_epoch</span><span class="o">=</span><span class="mi">27</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">report_every</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;training took </span><span class="si">{</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>training on data set with n = 17063
5 (19%):         average batch loss = 0.8897688550849814
10 (37%):        average batch loss = 0.6987682116383276
15 (56%):        average batch loss = 0.5792883489287404
20 (74%):        average batch loss = 0.4965955759470279
25 (93%):        average batch loss = 0.43885368771157285
training took 270.788941860199s
</pre></div>
</div>
</section>
<section id="id7">
<h3>결과 도식화<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">all_losses</span></code> 를 이용한 손실 도식화는
네트워크의 학습을 보여줍니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.ticker</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ticker</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">all_losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_char_rnn_classification_tutorial_001.png" srcset="../_images/sphx_glr_char_rnn_classification_tutorial_001.png" alt="char rnn classification tutorial" class = "sphx-glr-single-img"/></section>
</section>
<section id="id8">
<h2>결과 평가<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<p>네트워크가 다른 카테고리에서 얼마나 잘 작동하는지 보기 위해
모든 실제 언어(행)가 네트워크에서 어떤 언어로 추측(열)되는지 나타내는
혼란 행렬(confusion matrix)을 만듭니다. 혼란 행렬을 계산하기 위해
<code class="docutils literal notranslate"><span class="pre">evaluate()</span></code> 로 많은 수의 샘플을 네트워크에 실행합니다.
<code class="docutils literal notranslate"><span class="pre">evaluate()</span></code> 은 <code class="docutils literal notranslate"><span class="pre">train</span> <span class="pre">()</span></code> 과 역전파를 빼면 동일합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">testing_data</span><span class="p">,</span> <span class="n">classes</span><span class="p">):</span>
    <span class="n">confusion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>

    <span class="n">rnn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="c1">#set to eval mode</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># do not record the gradients during eval phase</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">testing_data</span><span class="p">)):</span>
            <span class="p">(</span><span class="n">label_tensor</span><span class="p">,</span> <span class="n">text_tensor</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span> <span class="o">=</span> <span class="n">testing_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">text_tensor</span><span class="p">)</span>
            <span class="n">guess</span><span class="p">,</span> <span class="n">guess_i</span> <span class="o">=</span> <span class="n">label_from_output</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>
            <span class="n">label_i</span> <span class="o">=</span> <span class="n">classes</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
            <span class="n">confusion</span><span class="p">[</span><span class="n">label_i</span><span class="p">][</span><span class="n">guess_i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Normalize by dividing every row by its sum</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">)):</span>
        <span class="n">denom</span> <span class="o">=</span> <span class="n">confusion</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">denom</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">confusion</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">confusion</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">denom</span>

    <span class="c1"># Set up plot</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
    <span class="n">cax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">confusion</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="c1">#numpy uses cpu here so we need to use a cpu version</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">cax</span><span class="p">)</span>

    <span class="c1"># Set up axes</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">)),</span> <span class="n">labels</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">)),</span> <span class="n">labels</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>

    <span class="c1"># Force label at every tick</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">ticker</span><span class="o">.</span><span class="n">MultipleLocator</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">ticker</span><span class="o">.</span><span class="n">MultipleLocator</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># sphinx_gallery_thumbnail_number = 2</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>



<span class="n">evaluate</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">test_set</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="n">alldata</span><span class="o">.</span><span class="n">labels_uniq</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_char_rnn_classification_tutorial_002.png" srcset="../_images/sphx_glr_char_rnn_classification_tutorial_002.png" alt="char rnn classification tutorial" class = "sphx-glr-single-img"/><p>주축에서 벗어난 밝은 점을 선택하여 잘못 추측한 언어를 표시할 수 있습니다.
예를 들어 한국어는 중국어로 이탈리아어로 스페인어로.
그리스어는 매우 잘되는 것으로 영어는 매우 나쁜 것으로 보입니다.
(다른 언어들과의 중첩 때문으로 추정)</p>
</section>
<section id="id9">
<h2>연습<a class="headerlink" href="#id9" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Get better results with a bigger and/or better shaped network</p>
<ul>
<li><p>Adjust the hyperparameters to enhance performance, such as changing the number of epochs, batch size, and learning rate</p></li>
<li><p>Try the <code class="docutils literal notranslate"><span class="pre">nn.LSTM</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.GRU</span></code> layers</p></li>
<li><p>Modify the size of the layers, such as increasing or decreasing the number of hidden nodes or adding additional linear layers</p></li>
<li><p>Combine multiple of these RNNs as a higher level network</p></li>
</ul>
</li>
<li><p>“line -&gt; label” 의 다른 데이터 집합으로 시도해 보십시오, 예를 들어:</p>
<ul>
<li><p>단어 -&gt; 언어</p></li>
<li><p>이름 -&gt; 성별</p></li>
<li><p>캐릭터 이름 -&gt; 작가</p></li>
<li><p>페이지 제목 -&gt; 블로그 또는 서브레딧</p></li>
</ul>
</li>
<li><p>더 크고 더 나은 모양의 네트워크로 더 나은 결과를 얻으십시오.</p>
<ul>
<li><p>더 많은 선형 계층을 추가해 보십시오.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nn.LSTM</span></code> 과 <code class="docutils literal notranslate"><span class="pre">nn.GRU</span></code> 계층을 추가해 보십시오.</p></li>
<li><p>위와 같은 RNN 여러 개를 상위 수준 네트워크로 결합해 보십시오.</p></li>
</ul>
</li>
</ul>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (4 minutes 41.329 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-char-rnn-classification-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/13b143c2380f4768d9432d808ad50799/char_rnn_classification_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">char_rnn_classification_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/37c8905519d3fd3f437b783a48d06eac/char_rnn_classification_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">char_rnn_classification_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/9e1d6a0463b9cd1a32b6e306664a0744/char_rnn_classification_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">char_rnn_classification_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#torch">Torch 준비</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">데이터 준비</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor">이름을 Tensor로 변경</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">네트워크 생성</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">학습</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">신경망 학습</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">결과 도식화</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">결과 평가</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">연습</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "\uae30\ucd08\ubd80\ud130 \uc2dc\uc791\ud558\ub294 NLP: \ubb38\uc790-\ub2e8\uc704 RNN\uc73c\ub85c \uc774\ub984 \ubd84\ub958\ud558\uae30",
       "headline": "\uae30\ucd08\ubd80\ud130 \uc2dc\uc791\ud558\ub294 NLP: \ubb38\uc790-\ub2e8\uc704 RNN\uc73c\ub85c \uc774\ub984 \ubd84\ub958\ud558\uae30",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/intermediate/char_rnn_classification_tutorial.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. \uae30\ucd08\ubd80\ud130 \uc2dc\uc791\ud558\ub294 NLP: \ubb38\uc790-\ub2e8\uc704 RNN\uc73c\ub85c \uc774\ub984 \ubd84\ub958\ud558\uae30# Author: Sean Robertson\ubc88\uc5ed: \ud669\uc131\uc218, \uae40\uc81c\ud544 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 3\ubd80\ub85c \uad6c\uc131\ub41c \uc2dc\ub9ac\uc988\uc758 \uc77c\ubd80\uc785\ub2c8\ub2e4: \uae30\ucd08\ubd80\ud130 \uc2dc\uc791\ud558\ub294 NLP: \ubb38\uc790-\ub2e8\uc704 RNN\uc73c\ub85c \uc774\ub984 \ubd84\ub958\ud558\uae30 \uae30\ucd08\ubd80\ud130 \uc2dc\uc791\ud558\ub294 NLP: \ubb38\uc790-\ub2e8\uc704 RNN\uc73c\ub85c \uc774\ub984 \uc0dd\uc131\ud558\uae30 \uae30\ucd08\ubd80\ud130 \uc2dc\uc791\ud558\ub294 NLP: Sequence to Sequence \ub124\ud2b8\uc6cc\ud06c\uc640 Attention\uc744 \uc774\uc6a9\ud55c \ubc88\uc5ed \uc5ec\uae30\uc5d0\uc11c\ub294 \ub2e8\uc5b4\ub97c \ubd84\ub958\ud558\uae30 \uc704\ud574 \uae30\ucd08\uc801\uc778 \ubb38\uc790-\ub2e8\uc704\uc758 \uc21c\ud658 \uc2e0\uacbd\ub9dd(RNN, Recurrent Nueral Network)\uc744 \uad6c\ucd95\ud558\uace0 \ud559\uc2b5\ud560 \uc608\uc815\uc785\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc \ubc0f \uc774\ud6c4 2\uac1c \ud29c\ud1a0\ub9ac\uc5bc\uc778 \uae30\ucd08\ubd80\ud130 \uc2dc\uc791\ud558\ub294 NLP: \ubb38\uc790-\ub2e8\uc704 RNN\uc73c\ub85c \uc774\ub984 \uc0dd\uc131\ud558\uae30 \ubc0f \uae30\ucd08\ubd80\ud130 \uc2dc\uc791\ud558\ub294 NLP: Sequence to Sequence \ub124\ud2b8\uc6cc\ud06c\uc640 Attention\uc744 \uc774\uc6a9\ud55c \ubc88\uc5ed \uc5d0\uc11c\ub294 \uc790\uc5f0\uc5b4 \ucc98\ub9ac(NLP, Natural Language Processing) \ubd84\uc57c\uc5d0\uc11c \uc5b4\ub5bb\uac8c \ub370\uc774\ud130\ub97c \uc804\ucc98\ub9ac\ud558\uace0 NLP \ubaa8\ub378\uc744 \uad6c\ucd95\ud558\ub294\uc9c0\ub97c \ubc11\ubc14\ub2e5\ubd80\ud130(from scratch) \uc124\uba85\ud569\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574 \uc774 \ud29c\ud1a0\ub9ac\uc5bc \uc2dc\ub9ac\uc988\uc5d0\uc11c\ub294 NLP \ubaa8\ub378\ub9c1\uc744 \uc704\ud55c \ub370\uc774\ud130 \uc804\ucc98\ub9ac\uac00 \ubc11\ubc14\ub2e5(low-level)\uc5d0\uc11c \uc5b4\ub5bb\uac8c \uc9c4\ud589\ub418\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubb38\uc790-\ub2e8\uc704 RNN\uc740 \ub2e8\uc5b4\ub97c \ubb38\uc790\uc758 \uc5f0\uc18d\uc73c\ub85c \uc77d\uc5b4 \ub4e4\uc5ec\uc11c \uac01 \ub2e8\uacc4\uc758 \uc608\uce21\uacfc \u201c\uc740\ub2c9 \uc0c1\ud0dc(Hidden State)\u201d\ub97c \ucd9c\ub825\ud558\uace0, \ub2e4\uc74c \ub2e8\uacc4\uc5d0 \uc774\uc804 \ub2e8\uacc4\uc758 \uc740\ub2c9 \uc0c1\ud0dc\ub97c \uc804\ub2ec\ud569\ub2c8\ub2e4. \ub2e8\uc5b4\uac00 \uc18d\ud55c \ud074\ub798\uc2a4\ub85c \ucd9c\ub825\ub418\ub3c4\ub85d \ucd5c\uc885 \uc608\uce21\uc73c\ub85c \uc120\ud0dd\ud569\ub2c8\ub2e4. \uad6c\uccb4\uc801\uc73c\ub85c, 18\uac1c \uc5b8\uc5b4\ub85c \ub41c \uc218\ucc9c \uac1c\uc758 \uc131(\u59d3)\uc744 \ud6c8\ub828\uc2dc\ud0a4\uace0, \ucca0\uc790\uc5d0 \ub530\ub77c \uc774\ub984\uc774 \uc5b4\ub5a4 \uc5b8\uc5b4\uc778\uc9c0 \uc608\uce21\ud569\ub2c8\ub2e4. Torch \uc900\ube44# \ud558\ub4dc\uc6e8\uc5b4(CPU \ub610\ub294 CUDA)\uc5d0 \ub9de\ucdb0 GPU \uac00\uc18d\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \uc801\uc808\ud55c \uc7a5\uce58\ub97c \uae30\ubcf8 \uc7a5\uce58\ub85c \uc124\uc815\ud569\ub2c8\ub2e4. import torch # Check if CUDA is available device = torch.device(\u0027cpu\u0027) if torch.cuda.is_available(): device = torch.device(\u0027cuda\u0027) torch.set_default_device(device) print(f\"Using device = {torch.get_default_device()}\") Using device = cuda:0 \ub370\uc774\ud130 \uc900\ube44# \uc5ec\uae30 \uc5d0\uc11c \ub370\uc774\ud130\ub97c \ub2e4\uc6b4\ub85c\ub4dc \ubc1b\uace0 \ud604\uc7ac \ub514\ub809\ud1a0\ub9ac\uc5d0 \uc555\ucd95\uc744 \ud489\ub2c8\ub2e4. data/names \ub514\ub809\ud1a0\ub9ac\uc5d0\ub294 [Language].txt \ub77c\ub294 18 \uac1c\uc758 \ud14d\uc2a4\ud2b8 \ud30c\uc77c\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ud30c\uc77c\uc5d0\ub294 \ud55c \uc904\uc5d0 \ud558\ub098\uc758 \uc774\ub984\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc73c\uba70 \ub300\ubd80\ubd84 \ub85c\ub9c8\uc790\ub85c \ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. (\ud558\uc9c0\ub9cc \uc720\ub2c8\ucf54\ub4dc\uc5d0\uc11c ASCII\ub85c \ubcc0\ud658\uc740 \ud574\uc57c \ud569\ub2c8\ub2e4) \uccab\ubc88\uc9f8 \ub2e8\uacc4\ub294 \ub370\uc774\ud130\ub97c \uc815\uc758\ud558\uace0 \uc815\ub9ac\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \ucd08\uae30\uc5d0\ub294 \uc720\ub2c8\ucf54\ub4dc\ub97c \uc77c\ubc18 ASCII\ub85c \ubcc0\ud658\ud558\uc5ec RNN \uc785\ub825 \ub808\uc774\uc5b4\ub97c \uc81c\ud55c\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\ub294 \uc720\ub2c8\ucf54\ub4dc \ubb38\uc790\uc5f4\uc744 ASCII\ub85c \ubcc0\ud658\ud558\uace0 \ud5c8\uc6a9\ub41c \ubb38\uc790\uc758 \uc791\uc740 \uc9d1\ud569\ub9cc\uc744 \ud5c8\uc6a9\ud558\uc5ec \uc774\ub8e8\uc5b4\uc9d1\ub2c8\ub2e4. import string import unicodedata # \"_\" \ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5b4\ud718\uc9d1(Vocabulary)\uc5d0 \uc5c6\ub294 \ubb38\uc790\ub97c \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc989, \ubaa8\ub378\uc5d0\uc11c \ucc98\ub9ac\ud558\uc9c0 \uc54a\ub294 \ubaa8\ub4e0 \ubb38\uc790\ub97c \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. allowed_characters = string.ascii_letters + \" .,;\u0027\" + \"_\" n_letters = len(allowed_characters) # \uc720\ub2c8\ucf54\ub4dc \ubb38\uc790\uc5f4\uc744 \uc77c\ubc18 ASCII\ub85c \ubcc0\ud658\ud558\uae30: https://stackoverflow.com/a/518232/2809427 def unicodeToAscii(s): return \u0027\u0027.join( c for c in unicodedata.normalize(\u0027NFD\u0027, s) if unicodedata.category(c) != \u0027Mn\u0027 and c in allowed_characters ) \uc720\ub2c8\ucf54\ub4dc \uc54c\ud30c\ubcb3 \uc774\ub984\uc744 \uc77c\ubc18 ASCII\ub85c \ubcc0\ud658\ud558\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4. \uc774\ub807\uac8c \ud558\uba74 \uc785\ub825 \ub808\uc774\uc5b4\ub97c \ub2e8\uc21c\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. print (f\"converting \u0027\u015alus\u00e0rski\u0027 to {unicodeToAscii(\u0027\u015alus\u00e0rski\u0027)}\") converting \u0027\u015alus\u00e0rski\u0027 to Slusarski \uc774\ub984\uc744 Tensor\ub85c \ubcc0\uacbd# \uc774\uc81c \ubaa8\ub4e0 \uc774\ub984\uc744 \uccb4\uacc4\ud654\ud588\uc73c\ubbc0\ub85c, \uc774\ub97c \ud65c\uc6a9\ud558\uae30 \uc704\ud574 Tensor\ub85c \ubcc0\ud658\ud574\uc57c \ud569\ub2c8\ub2e4. \ud558\ub098\uc758 \ubb38\uc790\ub97c \ud45c\ud604\ud558\uae30 \uc704\ud574 \ud06c\uae30\uac00 \u003c1 x n_letters\u003e \uc778 \u201cOne-Hot \ubca1\ud130\u201d\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. One-Hot \ubca1\ud130\ub294 \ud604\uc7ac \ubb38\uc790\uc758 \uc8fc\uc18c\uc5d0\ub294 1\uc774, \uadf8 \uc678 \ub098\uba38\uc9c0 \uc8fc\uc18c\uc5d0\ub294 0\uc774 \ucc44\uc6cc\uc9c4 \ubca1\ud130\uc785\ub2c8\ub2e4. \uc608\uc2dc \"b\" = \u003c0 1 0 0 0 ...\u003e . \ub2e8\uc5b4\ub97c \ub9cc\ub4e4\uae30 \uc704\ud574 One-Hot \ubca1\ud130\ub4e4\uc744 2\ucc28\uc6d0 \ud589\ub82c \u003cline_length x 1 x n_letters\u003e \uc5d0 \uacb0\ud569\uc2dc\ud0b5\ub2c8\ub2e4. \uc704\uc5d0\uc11c \ubcf4\uc774\ub294 \ucd94\uac00\uc801\uc778 1\ucc28\uc6d0\uc740 PyTorch\uc5d0\uc11c \ubaa8\ub4e0 \uac83\uc774 \ubc30\uce58(batch)\uc5d0 \uc788\ub2e4\uace0 \uac00\uc815\ud558\uae30 \ub54c\ubb38\uc5d0 \ubc1c\uc0dd\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 \ubc30\uce58 \ud06c\uae30 1\uc744 \uc0ac\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. # .. note:: # \uc5ed\uc790 \uc8fc: One-Hot \ubca1\ud130\ub294 \uc5b8\uc5b4 \ubc0f \ubc94\uc8fc\ud615 \ub370\uc774\ud130\ub97c \ub2e4\ub8f0 \ub54c \uc8fc\ub85c \uc0ac\uc6a9\ud558\uba70, # \ub2e8\uc5b4, \uae00\uc790 \ub4f1\uc744 \ubca1\ud130\ub85c \ud45c\ud604\ud560 \ub54c \ub2e8\uc5b4, \uae00\uc790 \uc0ac\uc774\uc758 \uc0c1\uad00 \uad00\uacc4\ub97c \ubbf8\ub9ac \uc54c \uc218 \uc5c6\uc744 \uacbd\uc6b0, # One-Hot\uc73c\ub85c \ud45c\ud604\ud558\uc5ec \uc11c\ub85c \uc9c1\uad50\ud55c\ub2e4\uace0 \uac00\uc815\ud558\uace0 \ud559\uc2b5\uc744 \uc2dc\uc791\ud569\ub2c8\ub2e4. # \uc774\uc640 \ub3d9\uc77c\ud558\uac8c, \uc0c1\uad00 \uad00\uacc4\ub97c \uc54c \uc218 \uc5c6\ub294 \ub2e4\ub978 \ub370\uc774\ud130\uc758 \uacbd\uc6b0\uc5d0\ub3c4 One-Hot \ubca1\ud130\ub97c \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. # import torch # all_letters \ub85c \ubb38\uc790\uc758 \uc8fc\uc18c \ucc3e\uae30, \uc608\uc2dc \"a\" = 0 def letterToIndex(letter): # \ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \uae00\uc790\ub97c \ub9cc\ub098\uba74, \uc5b4\ud718\uc9d1\uc5d0 \uc874\uc7ac\ud558\uc9c0 \uc54a\ub294 \ubb38\uc790(\"_\")\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4. if letter not in allowed_characters: return allowed_characters.find(\"_\") else: return allowed_characters.find(letter) # \uac80\uc99d\uc744 \uc704\ud574\uc11c \ud55c \uac1c\uc758 \ubb38\uc790\ub97c \u003c1 x n_letters\u003e Tensor\ub85c \ubcc0\ud658 def letterToTensor(letter): tensor = torch.zeros(1, n_letters) tensor[0][letterToIndex(letter)] = 1 return tensor # \ud55c \uc904(\uc774\ub984)\uc744 \u003cline_length x 1 x n_letters\u003e, # \ub610\ub294 One-Hot \ubb38\uc790 \ubca1\ud130\uc758 Array\ub85c \ubcc0\uacbd def lineToTensor(line): tensor = torch.zeros(len(line), 1, n_letters) for li, letter in enumerate(line): tensor[li][0][letterToIndex(letter)] = 1 return tensor print(letterToTensor(\u0027J\u0027)) print(lineToTensor(\u0027Jones\u0027).size()) tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device=\u0027cuda:0\u0027) torch.Size([5, 1, 58]) Here are some examples of how to use lineToTensor() for a single and multiple character string. print (f\"The letter \u0027a\u0027 becomes {lineToTensor(\u0027a\u0027)}\") #notice that the first position in the tensor = 1 print (f\"The name \u0027Ahn\u0027 becomes {lineToTensor(\u0027Ahn\u0027)}\") #notice \u0027A\u0027 sets the 27th index to 1 The letter \u0027a\u0027 becomes tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], device=\u0027cuda:0\u0027) The name \u0027Ahn\u0027 becomes tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], device=\u0027cuda:0\u0027) Congratulations, you have built the foundational tensor objects for this learning task! You can use a similar approach for other RNN tasks with text. Next, we need to combine all our examples into a dataset so we can train, test and validate our models. For this, we will use the Dataset and DataLoader classes to hold our dataset. Each Dataset needs to implement three functions: __init__, __len__, and __getitem__. from io import open import glob import os import time import torch from torch.utils.data import Dataset class NamesDataset(Dataset): def __init__(self, data_dir): self.data_dir = data_dir #for provenance of the dataset self.load_time = time.localtime #for provenance of the dataset labels_set = set() #set of all classes self.data = [] self.data_tensors = [] self.labels = [] self.labels_tensors = [] #read all the ``.txt`` files in the specified directory text_files = glob.glob(os.path.join(data_dir, \u0027*.txt\u0027)) for filename in text_files: label = os.path.splitext(os.path.basename(filename))[0] labels_set.add(label) lines = open(filename, encoding=\u0027utf-8\u0027).read().strip().split(\u0027\\n\u0027) for name in lines: self.data.append(name) self.data_tensors.append(lineToTensor(name)) self.labels.append(label) #Cache the tensor representation of the labels self.labels_uniq = list(labels_set) for idx in range(len(self.labels)): temp_tensor = torch.tensor([self.labels_uniq.index(self.labels[idx])], dtype=torch.long) self.labels_tensors.append(temp_tensor) def __len__(self): return len(self.data) def __getitem__(self, idx): data_item = self.data[idx] data_label = self.labels[idx] data_tensor = self.data_tensors[idx] label_tensor = self.labels_tensors[idx] return label_tensor, data_tensor, data_label, data_item Here we can load our example data into the NamesDataset alldata = NamesDataset(\"data/names\") print(f\"loaded {len(alldata)} items of data\") print(f\"example = {alldata[0]}\") loaded 20074 items of data example = (tensor([1], device=\u0027cuda:0\u0027), tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], device=\u0027cuda:0\u0027), \u0027English\u0027, \u0027Abbas\u0027) Using the dataset object allows us to easily split the data into train and test sets. Here we create a 80/20 split but the torch.utils.data has more useful utilities. Here we specify a generator since we need to use the same device as PyTorch defaults to above. train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], generator=torch.Generator(device=device).manual_seed(2024)) print(f\"train examples = {len(train_set)}, validation examples = {len(test_set)}\") train examples = 17063, validation examples = 3011 Now we have a basic dataset containing 20074 examples where each example is a pairing of label and name. We have also split the dataset into training and testing so we can validate the model that we build. \ub124\ud2b8\uc6cc\ud06c \uc0dd\uc131# Autograd \uc804\uc5d0, Torch\uc5d0\uc11c RNN(recurrent neural network) \uc0dd\uc131\uc740 \uc5ec\ub7ec \uc2dc\uac04 \ub2e8\uacc4 \uac78\uccd0\uc11c \uacc4\uce35\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \ubcf5\uc81c\ud558\ub294 \uc791\uc5c5\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4. \uacc4\uce35\uc740 \uc740\ub2c9 \uc0c1\ud0dc\uc640 \ubcc0\ud654\ub3c4(Gradient)\ub97c \uac00\uc9c0\uba70, \uc774\uc81c \uc774\uac83\ub4e4\uc740 \uadf8\ub798\ud504 \uc790\uccb4\uc5d0\uc11c \uc644\uc804\ud788 \ucc98\ub9ac\ub429\ub2c8\ub2e4. \uc774\ub294 feed-forward \uacc4\uce35\uacfc \uac19\uc740 \ub9e4\uc6b0 \u201c\uc21c\uc218\ud55c\u201d \ubc29\ubc95\uc73c\ub85c RNN\uc744 \uad6c\ud604\ud560 \uc218 \uc788\uc74c\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \ucc38\uace0 \uc5ed\uc790 \uc8fc: \uc5ec\uae30\uc11c\ub294 \ud559\uc2b5 \ubaa9\uc801\uc73c\ub85c nn.RNN \ub300\uc2e0 \uc9c1\uc811 RNN\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774 RNN \ubaa8\ub4c8\uc740 \u201c\uae30\ubcf8(vanilla)\uc801\uc778 RNN\u201d\uc744 \uad6c\ud604\ud558\uba70, \uc785\ub825\uacfc \uc740\ub2c9 \uc0c1\ud0dc(hidden state), \uadf8\ub9ac\uace0 \ucd9c\ub825 \ub4a4 \ub3d9\uc791\ud558\ub294 LogSoftmax \uacc4\uce35\uc774 \uc788\ub294 3\uac1c\uc758 \uc120\ud615 \uacc4\uce35\ub9cc\uc744 \uac00\uc9d1\ub2c8\ub2e4. This CharRNN class implements an RNN with three components. First, we use the nn.RNN implementation. Next, we define a layer that maps the RNN hidden layers to our output. And finally, we apply a softmax function. Using nn.RNN leads to a significant improvement in performance, such as cuDNN-accelerated kernels, versus implementing each layer as a nn.Linear. It also simplifies the implementation in forward(). import torch.nn as nn import torch.nn.functional as F class CharRNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(CharRNN, self).__init__() self.rnn = nn.RNN(input_size, hidden_size) self.h2o = nn.Linear(hidden_size, output_size) self.softmax = nn.LogSoftmax(dim=1) def forward(self, line_tensor): rnn_out, hidden = self.rnn(line_tensor) output = self.h2o(hidden[0]) output = self.softmax(output) return output We can then create an RNN with 58 input nodes, 128 hidden nodes, and 18 outputs: n_hidden = 128 rnn = CharRNN(n_letters, n_hidden, len(alldata.labels_uniq)) print(rnn) CharRNN( (rnn): RNN(58, 128) (h2o): Linear(in_features=128, out_features=18, bias=True) (softmax): LogSoftmax(dim=1) ) After that we can pass our Tensor to the RNN to obtain a predicted output. Subsequently, we use a helper function, label_from_output, to derive a text label for the class. def label_from_output(output, output_labels): top_n, top_i = output.topk(1) label_i = top_i[0].item() return output_labels[label_i], label_i input = lineToTensor(\u0027Albert\u0027) output = rnn(input) #this is equivalent to ``output = rnn.forward(input)`` print(output) print(label_from_output(output, alldata.labels_uniq)) tensor([[-2.9395, -3.0940, -2.8990, -2.9367, -2.8716, -2.7607, -2.9501, -2.8651, -2.8425, -2.8339, -2.8015, -2.9372, -2.9002, -2.9046, -2.8947, -2.8643, -2.9327, -2.8415]], device=\u0027cuda:0\u0027, grad_fn=\u003cLogSoftmaxBackward0\u003e) (\u0027Greek\u0027, 5) \ud559\uc2b5# \uc2e0\uacbd\ub9dd \ud559\uc2b5# \uc774\uc81c \uc774 \ub124\ud2b8\uc6cc\ud06c\ub97c \ud559\uc2b5\ud558\ub294 \ub370 \ud544\uc694\ud55c \uc608\uc2dc(\ud559\uc2b5 \ub370\uc774\ud130)\ub97c \ubcf4\uc5ec\uc8fc\uace0 \ucd94\uc815\ud569\ub2c8\ub2e4. \ub9cc\uc77c \ud2c0\ub838\ub2e4\uba74 \uc54c\ub824 \uc90d\ub2c8\ub2e4. We do this by defining a train() function which trains the model on a given dataset using minibatches. RNNs RNNs are trained similarly to other networks; therefore, for completeness, we include a batched training method here. The loop (for i in batch) computes the losses for each of the items in the batch before adjusting the weights. This operation is repeated until the number of epochs is reached. import random import numpy as np def train(rnn, training_data, n_epoch = 10, n_batch_size = 64, report_every = 50, learning_rate = 0.2, criterion = nn.NLLLoss()): \"\"\" Learn on a batch of training_data for a specified number of iterations and reporting thresholds \"\"\" # Keep track of losses for plotting current_loss = 0 all_losses = [] rnn.train() optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate) start = time.time() print(f\"training on data set with n = {len(training_data)}\") for iter in range(1, n_epoch + 1): rnn.zero_grad() # clear the gradients # create some minibatches # we cannot use dataloaders because each of our names is a different length batches = list(range(len(training_data))) random.shuffle(batches) batches = np.array_split(batches, len(batches) //n_batch_size ) for idx, batch in enumerate(batches): batch_loss = 0 for i in batch: #for each example in this batch (label_tensor, text_tensor, label, text) = training_data[i] output = rnn.forward(text_tensor) loss = criterion(output, label_tensor) batch_loss += loss # optimize parameters batch_loss.backward() nn.utils.clip_grad_norm_(rnn.parameters(), 3) optimizer.step() optimizer.zero_grad() current_loss += batch_loss.item() / len(batch) all_losses.append(current_loss / len(batches) ) if iter % report_every == 0: print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\") current_loss = 0 return all_losses We can now train a dataset with minibatches for a specified number of epochs. The number of epochs for this example is reduced to speed up the build. You can get better results with different parameters. start = time.time() all_losses = train(rnn, train_set, n_epoch=27, learning_rate=0.15, report_every=5) end = time.time() print(f\"training took {end-start}s\") training on data set with n = 17063 5 (19%): average batch loss = 0.8897688550849814 10 (37%): average batch loss = 0.6987682116383276 15 (56%): average batch loss = 0.5792883489287404 20 (74%): average batch loss = 0.4965955759470279 25 (93%): average batch loss = 0.43885368771157285 training took 270.788941860199s \uacb0\uacfc \ub3c4\uc2dd\ud654# all_losses \ub97c \uc774\uc6a9\ud55c \uc190\uc2e4 \ub3c4\uc2dd\ud654\ub294 \ub124\ud2b8\uc6cc\ud06c\uc758 \ud559\uc2b5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4: import matplotlib.pyplot as plt import matplotlib.ticker as ticker plt.figure() plt.plot(all_losses) plt.show() \uacb0\uacfc \ud3c9\uac00# \ub124\ud2b8\uc6cc\ud06c\uac00 \ub2e4\ub978 \uce74\ud14c\uace0\ub9ac\uc5d0\uc11c \uc5bc\ub9c8\ub098 \uc798 \uc791\ub3d9\ud558\ub294\uc9c0 \ubcf4\uae30 \uc704\ud574 \ubaa8\ub4e0 \uc2e4\uc81c \uc5b8\uc5b4(\ud589)\uac00 \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c \uc5b4\ub5a4 \uc5b8\uc5b4\ub85c \ucd94\uce21(\uc5f4)\ub418\ub294\uc9c0 \ub098\ud0c0\ub0b4\ub294 \ud63c\ub780 \ud589\ub82c(confusion matrix)\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. \ud63c\ub780 \ud589\ub82c\uc744 \uacc4\uc0b0\ud558\uae30 \uc704\ud574 evaluate() \ub85c \ub9ce\uc740 \uc218\uc758 \uc0d8\ud50c\uc744 \ub124\ud2b8\uc6cc\ud06c\uc5d0 \uc2e4\ud589\ud569\ub2c8\ub2e4. evaluate() \uc740 train () \uacfc \uc5ed\uc804\ud30c\ub97c \ube7c\uba74 \ub3d9\uc77c\ud569\ub2c8\ub2e4. def evaluate(rnn, testing_data, classes): confusion = torch.zeros(len(classes), len(classes)) rnn.eval() #set to eval mode with torch.no_grad(): # do not record the gradients during eval phase for i in range(len(testing_data)): (label_tensor, text_tensor, label, text) = testing_data[i] output = rnn(text_tensor) guess, guess_i = label_from_output(output, classes) label_i = classes.index(label) confusion[label_i][guess_i] += 1 # Normalize by dividing every row by its sum for i in range(len(classes)): denom = confusion[i].sum() if denom \u003e 0: confusion[i] = confusion[i] / denom # Set up plot fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(confusion.cpu().numpy()) #numpy uses cpu here so we need to use a cpu version fig.colorbar(cax) # Set up axes ax.set_xticks(np.arange(len(classes)), labels=classes, rotation=90) ax.set_yticks(np.arange(len(classes)), labels=classes) # Force label at every tick ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) # sphinx_gallery_thumbnail_number = 2 plt.show() evaluate(rnn, test_set, classes=alldata.labels_uniq) \uc8fc\ucd95\uc5d0\uc11c \ubc97\uc5b4\ub09c \ubc1d\uc740 \uc810\uc744 \uc120\ud0dd\ud558\uc5ec \uc798\ubabb \ucd94\uce21\ud55c \uc5b8\uc5b4\ub97c \ud45c\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \ud55c\uad6d\uc5b4\ub294 \uc911\uad6d\uc5b4\ub85c \uc774\ud0c8\ub9ac\uc544\uc5b4\ub85c \uc2a4\ud398\uc778\uc5b4\ub85c. \uadf8\ub9ac\uc2a4\uc5b4\ub294 \ub9e4\uc6b0 \uc798\ub418\ub294 \uac83\uc73c\ub85c \uc601\uc5b4\ub294 \ub9e4\uc6b0 \ub098\uc05c \uac83\uc73c\ub85c \ubcf4\uc785\ub2c8\ub2e4. (\ub2e4\ub978 \uc5b8\uc5b4\ub4e4\uacfc\uc758 \uc911\ucca9 \ub54c\ubb38\uc73c\ub85c \ucd94\uc815) \uc5f0\uc2b5# Get better results with a bigger and/or better shaped network Adjust the hyperparameters to enhance performance, such as changing the number of epochs, batch size, and learning rate Try the nn.LSTM and nn.GRU layers Modify the size of the layers, such as increasing or decreasing the number of hidden nodes or adding additional linear layers Combine multiple of these RNNs as a higher level network \u201cline -\u003e label\u201d \uc758 \ub2e4\ub978 \ub370\uc774\ud130 \uc9d1\ud569\uc73c\ub85c \uc2dc\ub3c4\ud574 \ubcf4\uc2ed\uc2dc\uc624, \uc608\ub97c \ub4e4\uc5b4: \ub2e8\uc5b4 -\u003e \uc5b8\uc5b4 \uc774\ub984 -\u003e \uc131\ubcc4 \uce90\ub9ad\ud130 \uc774\ub984 -\u003e \uc791\uac00 \ud398\uc774\uc9c0 \uc81c\ubaa9 -\u003e \ube14\ub85c\uadf8 \ub610\ub294 \uc11c\ube0c\ub808\ub527 \ub354 \ud06c\uace0 \ub354 \ub098\uc740 \ubaa8\uc591\uc758 \ub124\ud2b8\uc6cc\ud06c\ub85c \ub354 \ub098\uc740 \uacb0\uacfc\ub97c \uc5bb\uc73c\uc2ed\uc2dc\uc624. \ub354 \ub9ce\uc740 \uc120\ud615 \uacc4\uce35\uc744 \ucd94\uac00\ud574 \ubcf4\uc2ed\uc2dc\uc624. nn.LSTM \uacfc nn.GRU \uacc4\uce35\uc744 \ucd94\uac00\ud574 \ubcf4\uc2ed\uc2dc\uc624. \uc704\uc640 \uac19\uc740 RNN \uc5ec\ub7ec \uac1c\ub97c \uc0c1\uc704 \uc218\uc900 \ub124\ud2b8\uc6cc\ud06c\ub85c \uacb0\ud569\ud574 \ubcf4\uc2ed\uc2dc\uc624. Total running time of the script: (4 minutes 41.329 seconds) Download Jupyter notebook: char_rnn_classification_tutorial.ipynb Download Python source code: char_rnn_classification_tutorial.py Download zipped: char_rnn_classification_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/intermediate/char_rnn_classification_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>