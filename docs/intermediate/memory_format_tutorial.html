
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="(베타) PyTorch를 사용한 Channels Last 메모리 형식" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/intermediate/memory_format_tutorial.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Author: Vitaly Fedyunin 번역: Choi Yoonjeong 무엇을 배울 수 있나요? PyTorch에서의 Channels last 메모리 형식은 무엇인가요?, 특정 연산자에서 성능을 향상시키기 위해 어떻게 사용할 수 있나요?. 전제 조건 PyTorch v1.5.0, CUDA 지원 GPU. Channels last 메모리 형식(memory format)은 차원 순서를 유지하면서 메모리 상의 NCHW 텐서(tensor)를 정렬하는 또 다른 방식입니다. Channels last 텐서는 채널(Channel)이 가장 밀..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="Author: Vitaly Fedyunin 번역: Choi Yoonjeong 무엇을 배울 수 있나요? PyTorch에서의 Channels last 메모리 형식은 무엇인가요?, 특정 연산자에서 성능을 향상시키기 위해 어떻게 사용할 수 있나요?. 전제 조건 PyTorch v1.5.0, CUDA 지원 GPU. Channels last 메모리 형식(memory format)은 차원 순서를 유지하면서 메모리 상의 NCHW 텐서(tensor)를 정렬하는 또 다른 방식입니다. Channels last 텐서는 채널(Channel)이 가장 밀..." />
<meta property="og:ignore_canonical" content="true" />

    <title>(베타) PyTorch를 사용한 Channels Last 메모리 형식 &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/memory_format_tutorial';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/intermediate/memory_format_tutorial.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="Forward-mode Automatic Differentiation (Beta)" href="forward_ad_usage.html" />
    <link rel="prev" title="Knowledge Distillation Tutorial" href="../beginner/knowledge_distillation_tutorial.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../beginner/profiler.html">PyTorch 모듈 프로파일링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="parametrizations.html">Parametrizations Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="pruning_tutorial.html">가지치기 기법(Pruning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="scaled_dot_product_attention_tutorial.html">(Beta) Scaled Dot Product Attention (SDPA)로 고성능 트랜스포머(Transformers) 구현하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/knowledge_distillation_tutorial.html">Knowledge Distillation Tutorial</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Frontend APIs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">(베타) PyTorch를 사용한 Channels Last 메모리 형식</a></li>
<li class="toctree-l1"><a class="reference internal" href="forward_ad_usage.html">Forward-mode Automatic Differentiation (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing function transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="ensembling.html">모델 앙상블</a></li>
<li class="toctree-l1"><a class="reference internal" href="per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_frontend.html">PyTorch C++ 프론트엔드 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_autograd.html">C++ 프론트엔드의 자동 미분 (autograd)</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../deep-dive.html" class="nav-link">Deep Dive</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">(베타)...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../deep-dive.html">
        <meta itemprop="name" content="Deep Dive">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="(베타) PyTorch를 사용한 Channels Last 메모리 형식">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">intermediate/memory_format_tutorial</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-intermediate-memory-format-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="pytorch-channels-last">
<span id="sphx-glr-intermediate-memory-format-tutorial-py"></span><h1>(베타) PyTorch를 사용한 Channels Last 메모리 형식<a class="headerlink" href="#pytorch-channels-last" title="Link to this heading">#</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/VitalyFedyunin">Vitaly Fedyunin</a>
<strong>번역</strong>: <a class="reference external" href="https://github.com/potatochips178">Choi Yoonjeong</a></p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-mortar-board" viewBox="0 0 16 16" aria-hidden="true"><path d="M7.693 1.066a.747.747 0 0 1 .614 0l7.25 3.25a.75.75 0 0 1 0 1.368L13 6.831v2.794c0 1.024-.81 1.749-1.66 2.173-.893.447-2.075.702-3.34.702-.278 0-.55-.012-.816-.036a.75.75 0 0 1 .133-1.494c.22.02.45.03.683.03 1.082 0 2.025-.221 2.67-.543.69-.345.83-.682.83-.832V7.503L8.307 8.934a.747.747 0 0 1-.614 0L4 7.28v1.663c.296.105.575.275.812.512.438.438.688 1.059.688 1.796v3a.75.75 0 0 1-.75.75h-3a.75.75 0 0 1-.75-.75v-3c0-.737.25-1.358.688-1.796.237-.237.516-.407.812-.512V6.606L.443 5.684a.75.75 0 0 1 0-1.368ZM2.583 5 8 7.428 13.416 5 8 2.572ZM2.5 11.25v2.25H4v-2.25c0-.388-.125-.611-.25-.735a.697.697 0 0 0-.5-.203.707.707 0 0 0-.5.203c-.125.124-.25.347-.25.735Z"></path></svg> 무엇을 배울 수 있나요?</div>
<ul class="simple">
<li><p class="sd-card-text">PyTorch에서의 Channels last 메모리 형식은 무엇인가요?</p></li>
<li><p class="sd-card-text">특정 연산자에서 성능을 향상시키기 위해 어떻게 사용할 수 있나요?</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-list-unordered" viewBox="0 0 16 16" aria-hidden="true"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> 전제 조건</div>
<ul class="simple">
<li><p class="sd-card-text">PyTorch v1.5.0</p></li>
<li><p class="sd-card-text">CUDA 지원 GPU</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Channels last 메모리 형식(memory format)은 차원 순서를 유지하면서 메모리 상의 NCHW 텐서(tensor)를 정렬하는 또 다른 방식입니다.
Channels last 텐서는 채널(Channel)이 가장 밀도가 높은(densest) 차원으로 정렬(예. 이미지를 픽셀x픽셀로 저장)됩니다.</p>
<p>예를 들어, (2개의 4 x 4 이미지에 3개의 채널이 존재하는 경우) 전형적인(연속적인) NCHW 텐서의 저장 방식은 다음과 같습니다:</p>
<figure class="align-default">
<img alt="classic_memory_format" src="../_images/classic_memory_format.png" />
</figure>
<p>Channels last 메모리 형식은 데이터를 다르게 정렬합니다:</p>
<figure class="align-default">
<img alt="channels_last_memory_format" src="../_images/channels_last_memory_format.png" />
</figure>
<p>PyTorch는 기존의 스트라이드(strides) 구조를 사용함으로써 메모리 형식을 지원합니다.
예를 들어, Channels last 형식에서 10x3x16x16 배치(batch)는 (768, 1, 48, 3)와 같은 폭(strides)을 가지고 있게 됩니다.</p>
<p>Channels last 메모리 형식은 오직 4D NCHW Tensors에서만 실행할 수 있습니다.</p>
<section id="memory-format-api">
<h2>메모리 형식(Memory Format) API<a class="headerlink" href="#memory-format-api" title="Link to this heading">#</a></h2>
<p>연속 메모리 형식과 channels last 메모리 형식 간에 텐서를 변환하는 방법은 다음과 같습니다.</p>
<p>전형적인 PyTorch의 연속적인 텐서(tensor)</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span> <span class="c1"># 결과: (3072, 1024, 32, 1)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(3072, 1024, 32, 1)
</pre></div>
</div>
<p>변환 연산자</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># 결과: (10, 3, 32, 32) 차원 순서는 보존함</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span> <span class="c1"># 결과: (3072, 1, 96, 3)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([10, 3, 32, 32])
(3072, 1, 96, 3)
</pre></div>
</div>
<p>연속적인 형식으로 되돌리기</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">contiguous_format</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span> <span class="c1"># 결과: (3072, 1024, 32, 1)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(3072, 1024, 32, 1)
</pre></div>
</div>
<p>다른 방식</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span> <span class="c1"># 결과: (3072, 1, 96, 3)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(3072, 1, 96, 3)
</pre></div>
</div>
<p>형식(format) 확인</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">))</span> <span class="c1"># 결과: True</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">to</span></code> 와 <code class="docutils literal notranslate"><span class="pre">contiguous</span></code> 에는 작은 차이(minor difference)가 있습니다.
명시적으로 텐서(tensor)의 메모리 형식을 변환할 때는 <code class="docutils literal notranslate"><span class="pre">to</span></code> 를 사용하는 것을
권장합니다.</p>
<p>대부분의 경우 두 API는 동일하게 동작합니다. 하지만 <code class="docutils literal notranslate"><span class="pre">C==1</span></code> 이거나
<code class="docutils literal notranslate"><span class="pre">H</span> <span class="pre">==</span> <span class="pre">1</span> <span class="pre">&amp;&amp;</span> <span class="pre">W</span> <span class="pre">==</span> <span class="pre">1</span></code> 인 <code class="docutils literal notranslate"><span class="pre">NCHW</span></code> 4D 텐서의 특수한 경우에는 <code class="docutils literal notranslate"><span class="pre">to</span></code> 만이
Channel last 메모리 형식으로 표현된 적절한 폭(stride)을 생성합니다.</p>
<p>이는 위의 두가지 경우에 텐서의 메모리 형식이 모호하기 때문입니다.
예를 들어, 크기가 <code class="docutils literal notranslate"><span class="pre">N1HW</span></code> 인 연속적인 텐서(contiguous tensor)는
<code class="docutils literal notranslate"><span class="pre">연속적</span></code> 이면서 Channel last 형식으로 메모리에 저장됩니다.
따라서, 주어진 메모리 형식에 대해 이미 <code class="docutils literal notranslate"><span class="pre">is_contiguous</span></code> 로 간주되어
<code class="docutils literal notranslate"><span class="pre">contiguous</span></code> 호출은 동작하지 않게(no-op) 되어, 폭(stride)을 갱신하지
않게 됩니다. 반면에, <code class="docutils literal notranslate"><span class="pre">to</span></code> 는 의도한 메모리 형식으로 적절하게 표현하기 위해
크기가 1인 차원에서 의미있는 폭(stride)으로 재배열(restride)합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">special_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">special_x</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">))</span> <span class="c1"># Ouputs: True</span>
<span class="nb">print</span><span class="p">(</span><span class="n">special_x</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">contiguous_format</span><span class="p">))</span> <span class="c1"># Ouputs: True</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>True
True
</pre></div>
</div>
<p>명시적 치환(permutation) API인 <code class="docutils literal notranslate"><span class="pre">permute</span></code> 에서도 동일하게 적용됩니다.
모호성이 발생할 수 있는 특별한 경우에, <code class="docutils literal notranslate"><span class="pre">permute</span></code> 는 의도한 메모리
형식으로 전달되는 폭(stride)을 생성하는 것이 보장되지 않습니다.
<code class="docutils literal notranslate"><span class="pre">to</span></code> 로 명시적으로 메모리 형식을 지정하여 의도치 않은 동작을 피할
것을 권장합니다.</p>
<dl class="simple">
<dt>또한, 3개의 비-배치(non-batch) 차원이 모두 <code class="docutils literal notranslate"><span class="pre">1</span></code> 인 극단적인 경우</dt><dd><p>(<code class="docutils literal notranslate"><span class="pre">C==1</span> <span class="pre">&amp;&amp;</span> <span class="pre">H==1</span> <span class="pre">&amp;&amp;</span> <span class="pre">W==1</span></code>), 현재 구현은 텐서를 Channels last 메모리</p>
</dd>
</dl>
<p>형식으로 표시할 수 없음을 알려드립니다.</p>
<p>Channels last 방식으로 생성하기</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span> <span class="c1"># 결과: (3072, 1, 96, 3)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(3072, 1, 96, 3)
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">clone</span></code> 은 메모리 형식을 보존합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span> <span class="c1"># 결과: (3072, 1, 96, 3)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(3072, 1, 96, 3)
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">to</span></code>, <code class="docutils literal notranslate"><span class="pre">cuda</span></code>, <code class="docutils literal notranslate"><span class="pre">float</span></code> … 등도 메모리 형식을 보존합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span> <span class="c1"># 결과: (3072, 1, 96, 3)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(3072, 1, 96, 3)
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">empty_like</span></code>, <code class="docutils literal notranslate"><span class="pre">*_like</span></code> 연산자도 메모리 형식을 보존합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span> <span class="c1"># 결과: (3072, 1, 96, 3)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(3072, 1, 96, 3)
</pre></div>
</div>
<p>Pointwise 연산자도 메모리 형식을 보존합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span> <span class="c1"># 결과: (3072, 1, 96, 3)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(3072, 1, 96, 3)
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">`cudnn</span></code> 백엔드를 사용하는 <cite>Conv`</cite>, <code class="docutils literal notranslate"><span class="pre">Batchnorm</span></code> 모듈은 Channels last를 지원합니다.
(단, CudNN &gt;=7.6 에서만 동작)
합성곱(convolution) 모듈은 이진 p-wise 연산자(binary p-wise operator)와는 다르게
Channels last가 주된 메모리 형식입니다. 모든 입력은 연속적인 메모리 형식이며,
연산자는 연속된 메모리 형식으로 출력을 생성합니다. 그렇지 않으면, 출력은
channels last 메모리 형식입니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">version</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">7603</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span> <span class="c1"># 모듈 인자들은 Channels last로 변환이 필요합니다</span>

    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">))</span> <span class="c1"># 결과: True</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
<p>입력 텐서가 Channels last를 지원하지 않는 연산자를 만나면
치환(permutation)이 커널에 자동으로 적용되어 입력 텐서를 연속적인 형식으로
복원합니다. 이 경우 과부하가 발생하여 channel last 메모리 형식의 전파가
중단됩니다. 그럼에도 불구하고, 올바른 출력은 보장됩니다.</p>
</section>
<section id="id1">
<h2>성능 향상<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>Channels last 메모리 형식 최적화는 GPU와 CPU에서 모두 사용 가능합니다.
GPU에서는 정밀도를 줄인(reduced precision <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code>) 상태에서 Tensor Cores를 지원하는 NVIDIA의
하드웨어에서 가장 의미심장한 성능 향상을 보였습니다. <cite>AMP (Automated Mixed Precision)</cite> 학습 스크립트를
활용하여 연속적인 형식에 비해 Channels last 방식이 22% 이상의 성능 향승을 확인할 수 있었습니다.
이 때, NVIDIA가 제공하는 AMP를 사용했습니다. <a class="github reference external" href="https://github.com/NVIDIA/apex">NVIDIA/apex</a></p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">main_amp.py</span> <span class="pre">-a</span> <span class="pre">resnet50</span> <span class="pre">--b</span> <span class="pre">200</span> <span class="pre">--workers</span> <span class="pre">16</span> <span class="pre">--opt-level</span> <span class="pre">O2</span>&#160; <span class="pre">./data</span></code></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># opt_level = O2</span>
<span class="c1"># keep_batchnorm_fp32 = None &lt;class &#39;NoneType&#39;&gt;</span>
<span class="c1"># loss_scale = None &lt;class &#39;NoneType&#39;&gt;</span>
<span class="c1"># CUDNN VERSION: 7603</span>
<span class="c1"># =&gt; creating model &#39;resnet50&#39;</span>
<span class="c1"># Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.</span>
<span class="c1"># Defaults for this optimization level are:</span>
<span class="c1"># enabled                : True</span>
<span class="c1"># opt_level              : O2</span>
<span class="c1"># cast_model_type        : torch.float16</span>
<span class="c1"># patch_torch_functions  : False</span>
<span class="c1"># keep_batchnorm_fp32    : True</span>
<span class="c1"># master_weights         : True</span>
<span class="c1"># loss_scale             : dynamic</span>
<span class="c1"># Processing user overrides (additional kwargs that are not None)...</span>
<span class="c1"># After processing overrides, optimization options are:</span>
<span class="c1"># enabled                : True</span>
<span class="c1"># opt_level              : O2</span>
<span class="c1"># cast_model_type        : torch.float16</span>
<span class="c1"># patch_torch_functions  : False</span>
<span class="c1"># keep_batchnorm_fp32    : True</span>
<span class="c1"># master_weights         : True</span>
<span class="c1"># loss_scale             : dynamic</span>
<span class="c1"># Epoch: [0][10/125] Time 0.866 (0.866) Speed 230.949 (230.949) Loss 0.6735125184 (0.6735) Prec@1 61.000 (61.000) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][20/125] Time 0.259 (0.562) Speed 773.481 (355.693) Loss 0.6968704462 (0.6852) Prec@1 55.000 (58.000) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][30/125] Time 0.258 (0.461) Speed 775.089 (433.965) Loss 0.7877287269 (0.7194) Prec@1 51.500 (55.833) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][40/125] Time 0.259 (0.410) Speed 771.710 (487.281) Loss 0.8285319805 (0.7467) Prec@1 48.500 (54.000) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][50/125] Time 0.260 (0.380) Speed 770.090 (525.908) Loss 0.7370464802 (0.7447) Prec@1 56.500 (54.500) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][60/125] Time 0.258 (0.360) Speed 775.623 (555.728) Loss 0.7592862844 (0.7472) Prec@1 51.000 (53.917) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][70/125] Time 0.258 (0.345) Speed 774.746 (579.115) Loss 1.9698858261 (0.9218) Prec@1 49.500 (53.286) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][80/125] Time 0.260 (0.335) Speed 770.324 (597.659) Loss 2.2505953312 (1.0879) Prec@1 50.500 (52.938) Prec@5 100.000 (100.000)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--channels-last</span> <span class="pre">true</span></code> 인자를 전달하여 Channels last 형식으로 모델을 실행하면 22%의 성능 향상을 보입니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">main_amp.py</span> <span class="pre">-a</span> <span class="pre">resnet50</span> <span class="pre">--b</span> <span class="pre">200</span> <span class="pre">--workers</span> <span class="pre">16</span> <span class="pre">--opt-level</span> <span class="pre">O2</span> <span class="pre">--channels-last</span> <span class="pre">true</span> <span class="pre">./data</span></code></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># opt_level = O2</span>
<span class="c1"># keep_batchnorm_fp32 = None &lt;class &#39;NoneType&#39;&gt;</span>
<span class="c1"># loss_scale = None &lt;class &#39;NoneType&#39;&gt;</span>
<span class="c1">#</span>
<span class="c1"># CUDNN VERSION: 7603</span>
<span class="c1">#</span>
<span class="c1"># =&gt; creating model &#39;resnet50&#39;</span>
<span class="c1"># Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.</span>
<span class="c1">#</span>
<span class="c1"># Defaults for this optimization level are:</span>
<span class="c1"># enabled                : True</span>
<span class="c1"># opt_level              : O2</span>
<span class="c1"># cast_model_type        : torch.float16</span>
<span class="c1"># patch_torch_functions  : False</span>
<span class="c1"># keep_batchnorm_fp32    : True</span>
<span class="c1"># master_weights         : True</span>
<span class="c1"># loss_scale             : dynamic</span>
<span class="c1"># Processing user overrides (additional kwargs that are not None)...</span>
<span class="c1"># After processing overrides, optimization options are:</span>
<span class="c1"># enabled                : True</span>
<span class="c1"># opt_level              : O2</span>
<span class="c1"># cast_model_type        : torch.float16</span>
<span class="c1"># patch_torch_functions  : False</span>
<span class="c1"># keep_batchnorm_fp32    : True</span>
<span class="c1"># master_weights         : True</span>
<span class="c1"># loss_scale             : dynamic</span>
<span class="c1">#</span>
<span class="c1"># Epoch: [0][10/125] Time 0.767 (0.767) Speed 260.785 (260.785) Loss 0.7579724789 (0.7580) Prec@1 53.500 (53.500) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][20/125] Time 0.198 (0.482) Speed 1012.135 (414.716) Loss 0.7007197738 (0.7293) Prec@1 49.000 (51.250) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][30/125] Time 0.198 (0.387) Speed 1010.977 (516.198) Loss 0.7113101482 (0.7233) Prec@1 55.500 (52.667) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][40/125] Time 0.197 (0.340) Speed 1013.023 (588.333) Loss 0.8943189979 (0.7661) Prec@1 54.000 (53.000) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][50/125] Time 0.198 (0.312) Speed 1010.541 (641.977) Loss 1.7113249302 (0.9551) Prec@1 51.000 (52.600) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][60/125] Time 0.198 (0.293) Speed 1011.163 (683.574) Loss 5.8537774086 (1.7716) Prec@1 50.500 (52.250) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][70/125] Time 0.198 (0.279) Speed 1011.453 (716.767) Loss 5.7595844269 (2.3413) Prec@1 46.500 (51.429) Prec@5 100.000 (100.000)</span>
<span class="c1"># Epoch: [0][80/125] Time 0.198 (0.269) Speed 1011.827 (743.883) Loss 2.8196096420 (2.4011) Prec@1 47.500 (50.938) Prec@5 100.000 (100.000)</span>
</pre></div>
</div>
<p>아래 목록의 모델들은 Channels last 형식을 전적으로 지원(full support)하며 Volta 장비에서 8%-35%의 성능 향상을 보입니다:
<code class="docutils literal notranslate"><span class="pre">alexnet</span></code>, <code class="docutils literal notranslate"><span class="pre">mnasnet0_5</span></code>, <code class="docutils literal notranslate"><span class="pre">mnasnet0_75</span></code>, <code class="docutils literal notranslate"><span class="pre">mnasnet1_0</span></code>, <code class="docutils literal notranslate"><span class="pre">mnasnet1_3</span></code>, <code class="docutils literal notranslate"><span class="pre">mobilenet_v2</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet101</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet152</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet18</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet34</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet50</span></code>, <code class="docutils literal notranslate"><span class="pre">resnext50_32x4d</span></code>, <code class="docutils literal notranslate"><span class="pre">shufflenet_v2_x0_5</span></code>, <code class="docutils literal notranslate"><span class="pre">shufflenet_v2_x1_0</span></code>, <code class="docutils literal notranslate"><span class="pre">shufflenet_v2_x1_5</span></code>, <code class="docutils literal notranslate"><span class="pre">shufflenet_v2_x2_0</span></code>, <code class="docutils literal notranslate"><span class="pre">squeezenet1_0</span></code>, <code class="docutils literal notranslate"><span class="pre">squeezenet1_1</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg11</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg11_bn</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg13</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg13_bn</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg16</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg16_bn</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg19</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg19_bn</span></code>, <code class="docutils literal notranslate"><span class="pre">wide_resnet101_2</span></code>, <code class="docutils literal notranslate"><span class="pre">wide_resnet50_2</span></code></p>
<p>아래 목록의 모델들은 Channels last 형식을 전적으로 지원하며 Intel(R) Xeon(R) Ice Lake (또는 최신) CPU에서 26%-76% 성능 향상을 보여줍니다:
<code class="docutils literal notranslate"><span class="pre">alexnet</span></code>, <code class="docutils literal notranslate"><span class="pre">densenet121</span></code>, <code class="docutils literal notranslate"><span class="pre">densenet161</span></code>, <code class="docutils literal notranslate"><span class="pre">densenet169</span></code>, <code class="docutils literal notranslate"><span class="pre">googlenet</span></code>, <code class="docutils literal notranslate"><span class="pre">inception_v3</span></code>, <code class="docutils literal notranslate"><span class="pre">mnasnet0_5</span></code>, <code class="docutils literal notranslate"><span class="pre">mnasnet1_0</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet101</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet152</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet18</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet34</span></code>, <code class="docutils literal notranslate"><span class="pre">resnet50</span></code>, <code class="docutils literal notranslate"><span class="pre">resnext101_32x8d</span></code>, <code class="docutils literal notranslate"><span class="pre">resnext50_32x4d</span></code>, <code class="docutils literal notranslate"><span class="pre">shufflenet_v2_x0_5</span></code>, <code class="docutils literal notranslate"><span class="pre">shufflenet_v2_x1_0</span></code>, <code class="docutils literal notranslate"><span class="pre">squeezenet1_0</span></code>, <code class="docutils literal notranslate"><span class="pre">squeezenet1_1</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg11</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg11_bn</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg13</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg13_bn</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg16</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg16_bn</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg19</span></code>, <code class="docutils literal notranslate"><span class="pre">vgg19_bn</span></code>, <code class="docutils literal notranslate"><span class="pre">wide_resnet101_2</span></code>, <code class="docutils literal notranslate"><span class="pre">wide_resnet50_2</span></code></p>
</section>
<section id="id2">
<h2>기존 모델들 변환하기<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>Channels last 지원은 기존 모델이 무엇이냐에 따라 제한되지 않습니다.
어떠한 모델도 Channels last로 변환할 수 있으며
입력(또는 특정 가중치)의 형식만 맞춰주면 (신경망) 그래프를 통해 바로 전파(propagate)할 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 모델을 초기화한(또는 불러온) 이후, 한 번 실행이 필요합니다.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span> <span class="c1"># 원하는 모델로 교체하기</span>

<span class="c1"># 모든 입력에 대해서 실행이 필요합니다.</span>
<span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span> <span class="c1"># 원하는 입력으로 교체하기</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>그러나, 모든 연산자들이 Channels last를 지원하도록 완전히 바뀐 것은 아닙니다(일반적으로는 연속적인 출력을 대신 반환합니다).
위의 예시들에서 Channels last를 지원하지 않는 계층(layer)은 메모리 형식 전파를 멈추게 됩니다.
그럼에도 불구하고, 모델을 channels last 형식으로 변환했으므로, Channels last 메모리 형식으로 4차원의 가중치를 갖는
각 합성곱 계층(convolution layer)에서는 Channels last 형식으로 복원되고 더 빠른 커널(faster kernel)의 이점을 누릴 수 있게 됩니다.</p>
<p>하지만 Channels last를 지원하지 않는 연산자들은 치환(permutation)에 의해 과부하가 발생하게 됩니다.
선택적으로, 변환된 모델의 성능을 향상시키고 싶은 경우 모델의 연산자들 중 channel last를 지원하지 않는 연산자를 조사하고 식별할 수 있습니다.</p>
<p>이는 Channel Last 지원 연산자 목록 <a class="github reference external" href="https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support">pytorch/pytorch</a> 에서 사용한 연산자들이 존재하는지 확인하거나,
eager 실행 모드에서 메모리 형식 검사를 도입하고 모델을 실행해야 합니다.</p>
<p>아래 코드에서, 연산자들의 출력이 입력의 메모리 형식과 일치하지 않으면 예외(exception)를 발생시킵니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">contains_cl</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">():</span>
                <span class="k">return</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">contains_cl</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">t</span><span class="p">)):</span>
                <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">print_inputs</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">indent</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">indent</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
            <span class="n">print_inputs</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">indent</span><span class="o">=</span><span class="n">indent</span> <span class="o">+</span> <span class="s2">&quot;    &quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">indent</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">check_wrapper</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_cl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">was_cl</span> <span class="o">=</span> <span class="n">contains_cl</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;`</span><span class="si">{}</span><span class="s2">` inputs are:&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
            <span class="n">print_inputs</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------------------&quot;</span><span class="p">)</span>
            <span class="k">raise</span> <span class="n">e</span>
        <span class="n">failed</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">was_cl</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">result</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">):</span>
                    <span class="nb">print</span><span class="p">(</span>
                        <span class="s2">&quot;`</span><span class="si">{}</span><span class="s2">` got channels_last input, but output is not channels_last:&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">),</span>
                        <span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                        <span class="n">result</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                        <span class="n">result</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                        <span class="n">result</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">failed</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">failed</span> <span class="ow">and</span> <span class="kc">True</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;`</span><span class="si">{}</span><span class="s2">` inputs are:&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
            <span class="n">print_inputs</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Operator `</span><span class="si">{}</span><span class="s2">` lost channels_last property&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">return</span> <span class="n">check_cl</span>


<span class="n">old_attrs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">attribute</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="n">old_attrs</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">e</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">exclude_functions</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;is_cuda&quot;</span><span class="p">,</span> <span class="s2">&quot;has_names&quot;</span><span class="p">,</span> <span class="s2">&quot;numel&quot;</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;is_contiguous&quot;</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">exclude_functions</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">i</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;__call__&quot;</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">old_attrs</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">e</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">check_wrapper</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>


<span class="n">attribute</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
<span class="n">attribute</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="p">)</span>
<span class="n">attribute</span><span class="p">(</span><span class="n">torch</span><span class="p">)</span>
</pre></div>
</div>
<p>만약 Channels last 텐서를 지원하지 않는 연산자를 발견하였고, 기여하기를 원한다면
다음 개발 문서를 참고해주세요.
<a class="github reference external" href="https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators">pytorch/pytorch</a></p>
<p>아래 코드는 torch의 속성(attributes)를 복원합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">attrs</span><span class="p">)</span> <span class="ow">in</span> <span class="n">old_attrs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">)</span> <span class="ow">in</span> <span class="n">attrs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="nb">setattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id3">
<h2>해야할 일<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>다음과 같이 여전히 해야 할 일이 많이 남아있습니다:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">N1HW</span></code> 와 <code class="docutils literal notranslate"><span class="pre">NC11</span></code> Tensors의 모호성 해결하기;</p></li>
<li><p>분산 학습을 지원하는지 확인하기;</p></li>
<li><p>연산자 범위(operators coverage) 개선(improve)하기</p></li>
</ul>
<p>개선할 부분에 대한 피드백 또는 제안이 있다면 <a class="reference external" href="https://github.com/pytorch/pytorch/issues">이슈를 만들어</a> 알려주세요.</p>
</section>
<section id="id5">
<h2>결론<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>이 튜토리얼에서는 “channels last” 메모리 형식을 소개하고 성능 향상을 위해 어떻게 사용할 수 있는지 살펴보았습니다.
Channels last를 사용하여 비전 모델을 가속화하는 실질적인 예시는
<a class="reference external" href="https://pytorch.org/blog/accelerating-pytorch-vision-models-with-channels-last-on-cpu/">여기</a>
를 참고하세요.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 5.257 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-memory-format-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/f11c58c36c9b8a5daf09d3f9a792ef84/memory_format_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">memory_format_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/591028d309d0401740cd71eb6b14bf93/memory_format_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">memory_format_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/5597f45e0443e4b44273c796119bef90/memory_format_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">memory_format_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="../beginner/knowledge_distillation_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Knowledge Distillation Tutorial</p>
      </div>
    </a>
    <a class="right-next"
       href="forward_ad_usage.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Forward-mode Automatic Differentiation (Beta)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../beginner/knowledge_distillation_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Knowledge Distillation Tutorial</p>
      </div>
    </a>
    <a class="right-next"
       href="forward_ad_usage.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Forward-mode Automatic Differentiation (Beta)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-format-api">메모리 형식(Memory Format) API</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">성능 향상</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">기존 모델들 변환하기</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">해야할 일</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">결론</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "(\ubca0\ud0c0) PyTorch\ub97c \uc0ac\uc6a9\ud55c Channels Last \uba54\ubaa8\ub9ac \ud615\uc2dd",
       "headline": "(\ubca0\ud0c0) PyTorch\ub97c \uc0ac\uc6a9\ud55c Channels Last \uba54\ubaa8\ub9ac \ud615\uc2dd",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/intermediate/memory_format_tutorial.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. (\ubca0\ud0c0) PyTorch\ub97c \uc0ac\uc6a9\ud55c Channels Last \uba54\ubaa8\ub9ac \ud615\uc2dd# Author: Vitaly Fedyunin \ubc88\uc5ed: Choi Yoonjeong \ubb34\uc5c7\uc744 \ubc30\uc6b8 \uc218 \uc788\ub098\uc694? PyTorch\uc5d0\uc11c\uc758 Channels last \uba54\ubaa8\ub9ac \ud615\uc2dd\uc740 \ubb34\uc5c7\uc778\uac00\uc694? \ud2b9\uc815 \uc5f0\uc0b0\uc790\uc5d0\uc11c \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uae30 \uc704\ud574 \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub098\uc694? \uc804\uc81c \uc870\uac74 PyTorch v1.5.0 CUDA \uc9c0\uc6d0 GPU Channels last \uba54\ubaa8\ub9ac \ud615\uc2dd(memory format)\uc740 \ucc28\uc6d0 \uc21c\uc11c\ub97c \uc720\uc9c0\ud558\uba74\uc11c \uba54\ubaa8\ub9ac \uc0c1\uc758 NCHW \ud150\uc11c(tensor)\ub97c \uc815\ub82c\ud558\ub294 \ub610 \ub2e4\ub978 \ubc29\uc2dd\uc785\ub2c8\ub2e4. Channels last \ud150\uc11c\ub294 \ucc44\ub110(Channel)\uc774 \uac00\uc7a5 \ubc00\ub3c4\uac00 \ub192\uc740(densest) \ucc28\uc6d0\uc73c\ub85c \uc815\ub82c(\uc608. \uc774\ubbf8\uc9c0\ub97c \ud53d\uc140x\ud53d\uc140\ub85c \uc800\uc7a5)\ub429\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, (2\uac1c\uc758 4 x 4 \uc774\ubbf8\uc9c0\uc5d0 3\uac1c\uc758 \ucc44\ub110\uc774 \uc874\uc7ac\ud558\ub294 \uacbd\uc6b0) \uc804\ud615\uc801\uc778(\uc5f0\uc18d\uc801\uc778) NCHW \ud150\uc11c\uc758 \uc800\uc7a5 \ubc29\uc2dd\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4: Channels last \uba54\ubaa8\ub9ac \ud615\uc2dd\uc740 \ub370\uc774\ud130\ub97c \ub2e4\ub974\uac8c \uc815\ub82c\ud569\ub2c8\ub2e4: PyTorch\ub294 \uae30\uc874\uc758 \uc2a4\ud2b8\ub77c\uc774\ub4dc(strides) \uad6c\uc870\ub97c \uc0ac\uc6a9\ud568\uc73c\ub85c\uc368 \uba54\ubaa8\ub9ac \ud615\uc2dd\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, Channels last \ud615\uc2dd\uc5d0\uc11c 10x3x16x16 \ubc30\uce58(batch)\ub294 (768, 1, 48, 3)\uc640 \uac19\uc740 \ud3ed(strides)\uc744 \uac00\uc9c0\uace0 \uc788\uac8c \ub429\ub2c8\ub2e4. Channels last \uba54\ubaa8\ub9ac \ud615\uc2dd\uc740 \uc624\uc9c1 4D NCHW Tensors\uc5d0\uc11c\ub9cc \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uba54\ubaa8\ub9ac \ud615\uc2dd(Memory Format) API# \uc5f0\uc18d \uba54\ubaa8\ub9ac \ud615\uc2dd\uacfc channels last \uba54\ubaa8\ub9ac \ud615\uc2dd \uac04\uc5d0 \ud150\uc11c\ub97c \ubcc0\ud658\ud558\ub294 \ubc29\ubc95\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. \uc804\ud615\uc801\uc778 PyTorch\uc758 \uc5f0\uc18d\uc801\uc778 \ud150\uc11c(tensor) import torch N, C, H, W = 10, 3, 32, 32 x = torch.empty(N, C, H, W) print(x.stride()) # \uacb0\uacfc: (3072, 1024, 32, 1) (3072, 1024, 32, 1) \ubcc0\ud658 \uc5f0\uc0b0\uc790 x = x.to(memory_format=torch.channels_last) print(x.shape) # \uacb0\uacfc: (10, 3, 32, 32) \ucc28\uc6d0 \uc21c\uc11c\ub294 \ubcf4\uc874\ud568 print(x.stride()) # \uacb0\uacfc: (3072, 1, 96, 3) torch.Size([10, 3, 32, 32]) (3072, 1, 96, 3) \uc5f0\uc18d\uc801\uc778 \ud615\uc2dd\uc73c\ub85c \ub418\ub3cc\ub9ac\uae30 x = x.to(memory_format=torch.contiguous_format) print(x.stride()) # \uacb0\uacfc: (3072, 1024, 32, 1) (3072, 1024, 32, 1) \ub2e4\ub978 \ubc29\uc2dd x = x.contiguous(memory_format=torch.channels_last) print(x.stride()) # \uacb0\uacfc: (3072, 1, 96, 3) (3072, 1, 96, 3) \ud615\uc2dd(format) \ud655\uc778 print(x.is_contiguous(memory_format=torch.channels_last)) # \uacb0\uacfc: True True to \uc640 contiguous \uc5d0\ub294 \uc791\uc740 \ucc28\uc774(minor difference)\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uba85\uc2dc\uc801\uc73c\ub85c \ud150\uc11c(tensor)\uc758 \uba54\ubaa8\ub9ac \ud615\uc2dd\uc744 \ubcc0\ud658\ud560 \ub54c\ub294 to \ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc744 \uad8c\uc7a5\ud569\ub2c8\ub2e4. \ub300\ubd80\ubd84\uc758 \uacbd\uc6b0 \ub450 API\ub294 \ub3d9\uc77c\ud558\uac8c \ub3d9\uc791\ud569\ub2c8\ub2e4. \ud558\uc9c0\ub9cc C==1 \uc774\uac70\ub098 H == 1 \u0026\u0026 W == 1 \uc778 NCHW 4D \ud150\uc11c\uc758 \ud2b9\uc218\ud55c \uacbd\uc6b0\uc5d0\ub294 to \ub9cc\uc774 Channel last \uba54\ubaa8\ub9ac \ud615\uc2dd\uc73c\ub85c \ud45c\ud604\ub41c \uc801\uc808\ud55c \ud3ed(stride)\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774\ub294 \uc704\uc758 \ub450\uac00\uc9c0 \uacbd\uc6b0\uc5d0 \ud150\uc11c\uc758 \uba54\ubaa8\ub9ac \ud615\uc2dd\uc774 \ubaa8\ud638\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \ud06c\uae30\uac00 N1HW \uc778 \uc5f0\uc18d\uc801\uc778 \ud150\uc11c(contiguous tensor)\ub294 \uc5f0\uc18d\uc801 \uc774\uba74\uc11c Channel last \ud615\uc2dd\uc73c\ub85c \uba54\ubaa8\ub9ac\uc5d0 \uc800\uc7a5\ub429\ub2c8\ub2e4. \ub530\ub77c\uc11c, \uc8fc\uc5b4\uc9c4 \uba54\ubaa8\ub9ac \ud615\uc2dd\uc5d0 \ub300\ud574 \uc774\ubbf8 is_contiguous \ub85c \uac04\uc8fc\ub418\uc5b4 contiguous \ud638\ucd9c\uc740 \ub3d9\uc791\ud558\uc9c0 \uc54a\uac8c(no-op) \ub418\uc5b4, \ud3ed(stride)\uc744 \uac31\uc2e0\ud558\uc9c0 \uc54a\uac8c \ub429\ub2c8\ub2e4. \ubc18\uba74\uc5d0, to \ub294 \uc758\ub3c4\ud55c \uba54\ubaa8\ub9ac \ud615\uc2dd\uc73c\ub85c \uc801\uc808\ud558\uac8c \ud45c\ud604\ud558\uae30 \uc704\ud574 \ud06c\uae30\uac00 1\uc778 \ucc28\uc6d0\uc5d0\uc11c \uc758\ubbf8\uc788\ub294 \ud3ed(stride)\uc73c\ub85c \uc7ac\ubc30\uc5f4(restride)\ud569\ub2c8\ub2e4. special_x = torch.empty(4, 1, 4, 4) print(special_x.is_contiguous(memory_format=torch.channels_last)) # Ouputs: True print(special_x.is_contiguous(memory_format=torch.contiguous_format)) # Ouputs: True True True \uba85\uc2dc\uc801 \uce58\ud658(permutation) API\uc778 permute \uc5d0\uc11c\ub3c4 \ub3d9\uc77c\ud558\uac8c \uc801\uc6a9\ub429\ub2c8\ub2e4. \ubaa8\ud638\uc131\uc774 \ubc1c\uc0dd\ud560 \uc218 \uc788\ub294 \ud2b9\ubcc4\ud55c \uacbd\uc6b0\uc5d0, permute \ub294 \uc758\ub3c4\ud55c \uba54\ubaa8\ub9ac \ud615\uc2dd\uc73c\ub85c \uc804\ub2ec\ub418\ub294 \ud3ed(stride)\uc744 \uc0dd\uc131\ud558\ub294 \uac83\uc774 \ubcf4\uc7a5\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. to \ub85c \uba85\uc2dc\uc801\uc73c\ub85c \uba54\ubaa8\ub9ac \ud615\uc2dd\uc744 \uc9c0\uc815\ud558\uc5ec \uc758\ub3c4\uce58 \uc54a\uc740 \ub3d9\uc791\uc744 \ud53c\ud560 \uac83\uc744 \uad8c\uc7a5\ud569\ub2c8\ub2e4. \ub610\ud55c, 3\uac1c\uc758 \ube44-\ubc30\uce58(non-batch) \ucc28\uc6d0\uc774 \ubaa8\ub450 1 \uc778 \uadf9\ub2e8\uc801\uc778 \uacbd\uc6b0(C==1 \u0026\u0026 H==1 \u0026\u0026 W==1), \ud604\uc7ac \uad6c\ud604\uc740 \ud150\uc11c\ub97c Channels last \uba54\ubaa8\ub9ac \ud615\uc2dd\uc73c\ub85c \ud45c\uc2dc\ud560 \uc218 \uc5c6\uc74c\uc744 \uc54c\ub824\ub4dc\ub9bd\ub2c8\ub2e4. Channels last \ubc29\uc2dd\uc73c\ub85c \uc0dd\uc131\ud558\uae30 x = torch.empty(N, C, H, W, memory_format=torch.channels_last) print(x.stride()) # \uacb0\uacfc: (3072, 1, 96, 3) (3072, 1, 96, 3) clone \uc740 \uba54\ubaa8\ub9ac \ud615\uc2dd\uc744 \ubcf4\uc874\ud569\ub2c8\ub2e4. y = x.clone() print(y.stride()) # \uacb0\uacfc: (3072, 1, 96, 3) (3072, 1, 96, 3) to, cuda, float \u2026 \ub4f1\ub3c4 \uba54\ubaa8\ub9ac \ud615\uc2dd\uc744 \ubcf4\uc874\ud569\ub2c8\ub2e4. if torch.cuda.is_available(): y = x.cuda() print(y.stride()) # \uacb0\uacfc: (3072, 1, 96, 3) (3072, 1, 96, 3) empty_like, *_like \uc5f0\uc0b0\uc790\ub3c4 \uba54\ubaa8\ub9ac \ud615\uc2dd\uc744 \ubcf4\uc874\ud569\ub2c8\ub2e4. y = torch.empty_like(x) print(y.stride()) # \uacb0\uacfc: (3072, 1, 96, 3) (3072, 1, 96, 3) Pointwise \uc5f0\uc0b0\uc790\ub3c4 \uba54\ubaa8\ub9ac \ud615\uc2dd\uc744 \ubcf4\uc874\ud569\ub2c8\ub2e4. z = x + y print(z.stride()) # \uacb0\uacfc: (3072, 1, 96, 3) (3072, 1, 96, 3) `cudnn \ubc31\uc5d4\ub4dc\ub97c \uc0ac\uc6a9\ud558\ub294 Conv`, Batchnorm \ubaa8\ub4c8\uc740 Channels last\ub97c \uc9c0\uc6d0\ud569\ub2c8\ub2e4. (\ub2e8, CudNN \u003e=7.6 \uc5d0\uc11c\ub9cc \ub3d9\uc791) \ud569\uc131\uacf1(convolution) \ubaa8\ub4c8\uc740 \uc774\uc9c4 p-wise \uc5f0\uc0b0\uc790(binary p-wise operator)\uc640\ub294 \ub2e4\ub974\uac8c Channels last\uac00 \uc8fc\ub41c \uba54\ubaa8\ub9ac \ud615\uc2dd\uc785\ub2c8\ub2e4. \ubaa8\ub4e0 \uc785\ub825\uc740 \uc5f0\uc18d\uc801\uc778 \uba54\ubaa8\ub9ac \ud615\uc2dd\uc774\uba70, \uc5f0\uc0b0\uc790\ub294 \uc5f0\uc18d\ub41c \uba54\ubaa8\ub9ac \ud615\uc2dd\uc73c\ub85c \ucd9c\ub825\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74, \ucd9c\ub825\uc740 channels last \uba54\ubaa8\ub9ac \ud615\uc2dd\uc785\ub2c8\ub2e4. if torch.backends.cudnn.is_available() and torch.backends.cudnn.version() \u003e= 7603: model = torch.nn.Conv2d(8, 4, 3).cuda().half() model = model.to(memory_format=torch.channels_last) # \ubaa8\ub4c8 \uc778\uc790\ub4e4\uc740 Channels last\ub85c \ubcc0\ud658\uc774 \ud544\uc694\ud569\ub2c8\ub2e4 input = torch.randint(1, 10, (2, 8, 4, 4), dtype=torch.float32, requires_grad=True) input = input.to(device=\"cuda\", memory_format=torch.channels_last, dtype=torch.float16) out = model(input) print(out.is_contiguous(memory_format=torch.channels_last)) # \uacb0\uacfc: True True \uc785\ub825 \ud150\uc11c\uac00 Channels last\ub97c \uc9c0\uc6d0\ud558\uc9c0 \uc54a\ub294 \uc5f0\uc0b0\uc790\ub97c \ub9cc\ub098\uba74 \uce58\ud658(permutation)\uc774 \ucee4\ub110\uc5d0 \uc790\ub3d9\uc73c\ub85c \uc801\uc6a9\ub418\uc5b4 \uc785\ub825 \ud150\uc11c\ub97c \uc5f0\uc18d\uc801\uc778 \ud615\uc2dd\uc73c\ub85c \ubcf5\uc6d0\ud569\ub2c8\ub2e4. \uc774 \uacbd\uc6b0 \uacfc\ubd80\ud558\uac00 \ubc1c\uc0dd\ud558\uc5ec channel last \uba54\ubaa8\ub9ac \ud615\uc2dd\uc758 \uc804\ud30c\uac00 \uc911\ub2e8\ub429\ub2c8\ub2e4. \uadf8\ub7fc\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0, \uc62c\ubc14\ub978 \ucd9c\ub825\uc740 \ubcf4\uc7a5\ub429\ub2c8\ub2e4. \uc131\ub2a5 \ud5a5\uc0c1# Channels last \uba54\ubaa8\ub9ac \ud615\uc2dd \ucd5c\uc801\ud654\ub294 GPU\uc640 CPU\uc5d0\uc11c \ubaa8\ub450 \uc0ac\uc6a9 \uac00\ub2a5\ud569\ub2c8\ub2e4. GPU\uc5d0\uc11c\ub294 \uc815\ubc00\ub3c4\ub97c \uc904\uc778(reduced precision torch.float16) \uc0c1\ud0dc\uc5d0\uc11c Tensor Cores\ub97c \uc9c0\uc6d0\ud558\ub294 NVIDIA\uc758 \ud558\ub4dc\uc6e8\uc5b4\uc5d0\uc11c \uac00\uc7a5 \uc758\ubbf8\uc2ec\uc7a5\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. AMP (Automated Mixed Precision) \ud559\uc2b5 \uc2a4\ud06c\ub9bd\ud2b8\ub97c \ud65c\uc6a9\ud558\uc5ec \uc5f0\uc18d\uc801\uc778 \ud615\uc2dd\uc5d0 \ube44\ud574 Channels last \ubc29\uc2dd\uc774 22% \uc774\uc0c1\uc758 \uc131\ub2a5 \ud5a5\uc2b9\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \ub54c, NVIDIA\uac00 \uc81c\uacf5\ud558\ub294 AMP\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4. NVIDIA/apex python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2\u00a0 ./data # opt_level = O2 # keep_batchnorm_fp32 = None \u003cclass \u0027NoneType\u0027\u003e # loss_scale = None \u003cclass \u0027NoneType\u0027\u003e # CUDNN VERSION: 7603 # =\u003e creating model \u0027resnet50\u0027 # Selected optimization level O2: FP16 training with FP32 batchnorm and FP32 master weights. # Defaults for this optimization level are: # enabled : True # opt_level : O2 # cast_model_type : torch.float16 # patch_torch_functions : False # keep_batchnorm_fp32 : True # master_weights : True # loss_scale : dynamic # Processing user overrides (additional kwargs that are not None)... # After processing overrides, optimization options are: # enabled : True # opt_level : O2 # cast_model_type : torch.float16 # patch_torch_functions : False # keep_batchnorm_fp32 : True # master_weights : True # loss_scale : dynamic # Epoch: [0][10/125] Time 0.866 (0.866) Speed 230.949 (230.949) Loss 0.6735125184 (0.6735) Prec@1 61.000 (61.000) Prec@5 100.000 (100.000) # Epoch: [0][20/125] Time 0.259 (0.562) Speed 773.481 (355.693) Loss 0.6968704462 (0.6852) Prec@1 55.000 (58.000) Prec@5 100.000 (100.000) # Epoch: [0][30/125] Time 0.258 (0.461) Speed 775.089 (433.965) Loss 0.7877287269 (0.7194) Prec@1 51.500 (55.833) Prec@5 100.000 (100.000) # Epoch: [0][40/125] Time 0.259 (0.410) Speed 771.710 (487.281) Loss 0.8285319805 (0.7467) Prec@1 48.500 (54.000) Prec@5 100.000 (100.000) # Epoch: [0][50/125] Time 0.260 (0.380) Speed 770.090 (525.908) Loss 0.7370464802 (0.7447) Prec@1 56.500 (54.500) Prec@5 100.000 (100.000) # Epoch: [0][60/125] Time 0.258 (0.360) Speed 775.623 (555.728) Loss 0.7592862844 (0.7472) Prec@1 51.000 (53.917) Prec@5 100.000 (100.000) # Epoch: [0][70/125] Time 0.258 (0.345) Speed 774.746 (579.115) Loss 1.9698858261 (0.9218) Prec@1 49.500 (53.286) Prec@5 100.000 (100.000) # Epoch: [0][80/125] Time 0.260 (0.335) Speed 770.324 (597.659) Loss 2.2505953312 (1.0879) Prec@1 50.500 (52.938) Prec@5 100.000 (100.000) --channels-last true \uc778\uc790\ub97c \uc804\ub2ec\ud558\uc5ec Channels last \ud615\uc2dd\uc73c\ub85c \ubaa8\ub378\uc744 \uc2e4\ud589\ud558\uba74 22%\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc785\ub2c8\ub2e4. python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2 --channels-last true ./data # opt_level = O2 # keep_batchnorm_fp32 = None \u003cclass \u0027NoneType\u0027\u003e # loss_scale = None \u003cclass \u0027NoneType\u0027\u003e # # CUDNN VERSION: 7603 # # =\u003e creating model \u0027resnet50\u0027 # Selected optimization level O2: FP16 training with FP32 batchnorm and FP32 master weights. # # Defaults for this optimization level are: # enabled : True # opt_level : O2 # cast_model_type : torch.float16 # patch_torch_functions : False # keep_batchnorm_fp32 : True # master_weights : True # loss_scale : dynamic # Processing user overrides (additional kwargs that are not None)... # After processing overrides, optimization options are: # enabled : True # opt_level : O2 # cast_model_type : torch.float16 # patch_torch_functions : False # keep_batchnorm_fp32 : True # master_weights : True # loss_scale : dynamic # # Epoch: [0][10/125] Time 0.767 (0.767) Speed 260.785 (260.785) Loss 0.7579724789 (0.7580) Prec@1 53.500 (53.500) Prec@5 100.000 (100.000) # Epoch: [0][20/125] Time 0.198 (0.482) Speed 1012.135 (414.716) Loss 0.7007197738 (0.7293) Prec@1 49.000 (51.250) Prec@5 100.000 (100.000) # Epoch: [0][30/125] Time 0.198 (0.387) Speed 1010.977 (516.198) Loss 0.7113101482 (0.7233) Prec@1 55.500 (52.667) Prec@5 100.000 (100.000) # Epoch: [0][40/125] Time 0.197 (0.340) Speed 1013.023 (588.333) Loss 0.8943189979 (0.7661) Prec@1 54.000 (53.000) Prec@5 100.000 (100.000) # Epoch: [0][50/125] Time 0.198 (0.312) Speed 1010.541 (641.977) Loss 1.7113249302 (0.9551) Prec@1 51.000 (52.600) Prec@5 100.000 (100.000) # Epoch: [0][60/125] Time 0.198 (0.293) Speed 1011.163 (683.574) Loss 5.8537774086 (1.7716) Prec@1 50.500 (52.250) Prec@5 100.000 (100.000) # Epoch: [0][70/125] Time 0.198 (0.279) Speed 1011.453 (716.767) Loss 5.7595844269 (2.3413) Prec@1 46.500 (51.429) Prec@5 100.000 (100.000) # Epoch: [0][80/125] Time 0.198 (0.269) Speed 1011.827 (743.883) Loss 2.8196096420 (2.4011) Prec@1 47.500 (50.938) Prec@5 100.000 (100.000) \uc544\ub798 \ubaa9\ub85d\uc758 \ubaa8\ub378\ub4e4\uc740 Channels last \ud615\uc2dd\uc744 \uc804\uc801\uc73c\ub85c \uc9c0\uc6d0(full support)\ud558\uba70 Volta \uc7a5\ube44\uc5d0\uc11c 8%-35%\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc785\ub2c8\ub2e4: alexnet, mnasnet0_5, mnasnet0_75, mnasnet1_0, mnasnet1_3, mobilenet_v2, resnet101, resnet152, resnet18, resnet34, resnet50, resnext50_32x4d, shufflenet_v2_x0_5, shufflenet_v2_x1_0, shufflenet_v2_x1_5, shufflenet_v2_x2_0, squeezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, vgg19_bn, wide_resnet101_2, wide_resnet50_2 \uc544\ub798 \ubaa9\ub85d\uc758 \ubaa8\ub378\ub4e4\uc740 Channels last \ud615\uc2dd\uc744 \uc804\uc801\uc73c\ub85c \uc9c0\uc6d0\ud558\uba70 Intel(R) Xeon(R) Ice Lake (\ub610\ub294 \ucd5c\uc2e0) CPU\uc5d0\uc11c 26%-76% \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4: alexnet, densenet121, densenet161, densenet169, googlenet, inception_v3, mnasnet0_5, mnasnet1_0, resnet101, resnet152, resnet18, resnet34, resnet50, resnext101_32x8d, resnext50_32x4d, shufflenet_v2_x0_5, shufflenet_v2_x1_0, squeezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, vgg19_bn, wide_resnet101_2, wide_resnet50_2 \uae30\uc874 \ubaa8\ub378\ub4e4 \ubcc0\ud658\ud558\uae30# Channels last \uc9c0\uc6d0\uc740 \uae30\uc874 \ubaa8\ub378\uc774 \ubb34\uc5c7\uc774\ub0d0\uc5d0 \ub530\ub77c \uc81c\ud55c\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc5b4\ub5a0\ud55c \ubaa8\ub378\ub3c4 Channels last\ub85c \ubcc0\ud658\ud560 \uc218 \uc788\uc73c\uba70 \uc785\ub825(\ub610\ub294 \ud2b9\uc815 \uac00\uc911\uce58)\uc758 \ud615\uc2dd\ub9cc \ub9de\ucdb0\uc8fc\uba74 (\uc2e0\uacbd\ub9dd) \uadf8\ub798\ud504\ub97c \ud1b5\ud574 \ubc14\ub85c \uc804\ud30c(propagate)\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. # \ubaa8\ub378\uc744 \ucd08\uae30\ud654\ud55c(\ub610\ub294 \ubd88\ub7ec\uc628) \uc774\ud6c4, \ud55c \ubc88 \uc2e4\ud589\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. model = model.to(memory_format=torch.channels_last) # \uc6d0\ud558\ub294 \ubaa8\ub378\ub85c \uad50\uccb4\ud558\uae30 # \ubaa8\ub4e0 \uc785\ub825\uc5d0 \ub300\ud574\uc11c \uc2e4\ud589\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. input = input.to(memory_format=torch.channels_last) # \uc6d0\ud558\ub294 \uc785\ub825\uc73c\ub85c \uad50\uccb4\ud558\uae30 output = model(input) \uadf8\ub7ec\ub098, \ubaa8\ub4e0 \uc5f0\uc0b0\uc790\ub4e4\uc774 Channels last\ub97c \uc9c0\uc6d0\ud558\ub3c4\ub85d \uc644\uc804\ud788 \ubc14\ub010 \uac83\uc740 \uc544\ub2d9\ub2c8\ub2e4(\uc77c\ubc18\uc801\uc73c\ub85c\ub294 \uc5f0\uc18d\uc801\uc778 \ucd9c\ub825\uc744 \ub300\uc2e0 \ubc18\ud658\ud569\ub2c8\ub2e4). \uc704\uc758 \uc608\uc2dc\ub4e4\uc5d0\uc11c Channels last\ub97c \uc9c0\uc6d0\ud558\uc9c0 \uc54a\ub294 \uacc4\uce35(layer)\uc740 \uba54\ubaa8\ub9ac \ud615\uc2dd \uc804\ud30c\ub97c \uba48\ucd94\uac8c \ub429\ub2c8\ub2e4. \uadf8\ub7fc\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0, \ubaa8\ub378\uc744 channels last \ud615\uc2dd\uc73c\ub85c \ubcc0\ud658\ud588\uc73c\ubbc0\ub85c, Channels last \uba54\ubaa8\ub9ac \ud615\uc2dd\uc73c\ub85c 4\ucc28\uc6d0\uc758 \uac00\uc911\uce58\ub97c \uac16\ub294 \uac01 \ud569\uc131\uacf1 \uacc4\uce35(convolution layer)\uc5d0\uc11c\ub294 Channels last \ud615\uc2dd\uc73c\ub85c \ubcf5\uc6d0\ub418\uace0 \ub354 \ube60\ub978 \ucee4\ub110(faster kernel)\uc758 \uc774\uc810\uc744 \ub204\ub9b4 \uc218 \uc788\uac8c \ub429\ub2c8\ub2e4. \ud558\uc9c0\ub9cc Channels last\ub97c \uc9c0\uc6d0\ud558\uc9c0 \uc54a\ub294 \uc5f0\uc0b0\uc790\ub4e4\uc740 \uce58\ud658(permutation)\uc5d0 \uc758\ud574 \uacfc\ubd80\ud558\uac00 \ubc1c\uc0dd\ud558\uac8c \ub429\ub2c8\ub2e4. \uc120\ud0dd\uc801\uc73c\ub85c, \ubcc0\ud658\ub41c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uace0 \uc2f6\uc740 \uacbd\uc6b0 \ubaa8\ub378\uc758 \uc5f0\uc0b0\uc790\ub4e4 \uc911 channel last\ub97c \uc9c0\uc6d0\ud558\uc9c0 \uc54a\ub294 \uc5f0\uc0b0\uc790\ub97c \uc870\uc0ac\ud558\uace0 \uc2dd\ubcc4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 Channel Last \uc9c0\uc6d0 \uc5f0\uc0b0\uc790 \ubaa9\ub85d pytorch/pytorch \uc5d0\uc11c \uc0ac\uc6a9\ud55c \uc5f0\uc0b0\uc790\ub4e4\uc774 \uc874\uc7ac\ud558\ub294\uc9c0 \ud655\uc778\ud558\uac70\ub098, eager \uc2e4\ud589 \ubaa8\ub4dc\uc5d0\uc11c \uba54\ubaa8\ub9ac \ud615\uc2dd \uac80\uc0ac\ub97c \ub3c4\uc785\ud558\uace0 \ubaa8\ub378\uc744 \uc2e4\ud589\ud574\uc57c \ud569\ub2c8\ub2e4. \uc544\ub798 \ucf54\ub4dc\uc5d0\uc11c, \uc5f0\uc0b0\uc790\ub4e4\uc758 \ucd9c\ub825\uc774 \uc785\ub825\uc758 \uba54\ubaa8\ub9ac \ud615\uc2dd\uacfc \uc77c\uce58\ud558\uc9c0 \uc54a\uc73c\uba74 \uc608\uc678(exception)\ub97c \ubc1c\uc0dd\uc2dc\ud0b5\ub2c8\ub2e4. def contains_cl(args): for t in args: if isinstance(t, torch.Tensor): if t.is_contiguous(memory_format=torch.channels_last) and not t.is_contiguous(): return True elif isinstance(t, list) or isinstance(t, tuple): if contains_cl(list(t)): return True return False def print_inputs(args, indent=\"\"): for t in args: if isinstance(t, torch.Tensor): print(indent, t.stride(), t.shape, t.device, t.dtype) elif isinstance(t, list) or isinstance(t, tuple): print(indent, type(t)) print_inputs(list(t), indent=indent + \" \") else: print(indent, t) def check_wrapper(fn): name = fn.__name__ def check_cl(*args, **kwargs): was_cl = contains_cl(args) try: result = fn(*args, **kwargs) except Exception as e: print(\"`{}` inputs are:\".format(name)) print_inputs(args) print(\"-------------------\") raise e failed = False if was_cl: if isinstance(result, torch.Tensor): if result.dim() == 4 and not result.is_contiguous(memory_format=torch.channels_last): print( \"`{}` got channels_last input, but output is not channels_last:\".format(name), result.shape, result.stride(), result.device, result.dtype, ) failed = True if failed and True: print(\"`{}` inputs are:\".format(name)) print_inputs(args) raise Exception(\"Operator `{}` lost channels_last property\".format(name)) return result return check_cl old_attrs = dict() def attribute(m): old_attrs[m] = dict() for i in dir(m): e = getattr(m, i) exclude_functions = [\"is_cuda\", \"has_names\", \"numel\", \"stride\", \"Tensor\", \"is_contiguous\", \"__class__\"] if i not in exclude_functions and not i.startswith(\"_\") and \"__call__\" in dir(e): try: old_attrs[m][i] = e setattr(m, i, check_wrapper(e)) except Exception as e: print(i) print(e) attribute(torch.Tensor) attribute(torch.nn.functional) attribute(torch) \ub9cc\uc57d Channels last \ud150\uc11c\ub97c \uc9c0\uc6d0\ud558\uc9c0 \uc54a\ub294 \uc5f0\uc0b0\uc790\ub97c \ubc1c\uacac\ud558\uc600\uace0, \uae30\uc5ec\ud558\uae30\ub97c \uc6d0\ud55c\ub2e4\uba74 \ub2e4\uc74c \uac1c\ubc1c \ubb38\uc11c\ub97c \ucc38\uace0\ud574\uc8fc\uc138\uc694. pytorch/pytorch \uc544\ub798 \ucf54\ub4dc\ub294 torch\uc758 \uc18d\uc131(attributes)\ub97c \ubcf5\uc6d0\ud569\ub2c8\ub2e4. for (m, attrs) in old_attrs.items(): for (k,v) in attrs.items(): setattr(m, k, v) \ud574\uc57c\ud560 \uc77c# \ub2e4\uc74c\uacfc \uac19\uc774 \uc5ec\uc804\ud788 \ud574\uc57c \ud560 \uc77c\uc774 \ub9ce\uc774 \ub0a8\uc544\uc788\uc2b5\ub2c8\ub2e4: N1HW \uc640 NC11 Tensors\uc758 \ubaa8\ud638\uc131 \ud574\uacb0\ud558\uae30; \ubd84\uc0b0 \ud559\uc2b5\uc744 \uc9c0\uc6d0\ud558\ub294\uc9c0 \ud655\uc778\ud558\uae30; \uc5f0\uc0b0\uc790 \ubc94\uc704(operators coverage) \uac1c\uc120(improve)\ud558\uae30 \uac1c\uc120\ud560 \ubd80\ubd84\uc5d0 \ub300\ud55c \ud53c\ub4dc\ubc31 \ub610\ub294 \uc81c\uc548\uc774 \uc788\ub2e4\uba74 \uc774\uc288\ub97c \ub9cc\ub4e4\uc5b4 \uc54c\ub824\uc8fc\uc138\uc694. \uacb0\ub860# \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 \u201cchannels last\u201d \uba54\ubaa8\ub9ac \ud615\uc2dd\uc744 \uc18c\uac1c\ud558\uace0 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc704\ud574 \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294\uc9c0 \uc0b4\ud3b4\ubcf4\uc558\uc2b5\ub2c8\ub2e4. Channels last\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\uc804 \ubaa8\ub378\uc744 \uac00\uc18d\ud654\ud558\ub294 \uc2e4\uc9c8\uc801\uc778 \uc608\uc2dc\ub294 \uc5ec\uae30 \ub97c \ucc38\uace0\ud558\uc138\uc694. Total running time of the script: (0 minutes 5.257 seconds) Download Jupyter notebook: memory_format_tutorial.ipynb Download Python source code: memory_format_tutorial.py Download zipped: memory_format_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/intermediate/memory_format_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>