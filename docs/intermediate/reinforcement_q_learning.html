
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2023-05-06T02:05:46+00:00" /><meta property="og:title" content="강화 학습 (DQN) 튜토리얼" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/intermediate/reinforcement_q_learning.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Author: Adam Paszke, Mark Towers, 번역: 황성수, 박정환,. 이 튜토리얼에서는 Gymnasium 의 CartPole-v1 태스크에서 DQN (Deep Q Learning) 에이전트를 학습하는데 PyTorch를 사용하는 방법을 보여드립니다. Deep Q Learning (DQN) 논문을 읽어보는 것도 도움이 될 수 있습니다. 태스크 에이전트는 연결된 막대가 똑바로 서 있도록 카트를 왼쪽이나 오른쪽으로 움직이는 두 가지 동작 중 하나를 선택해야 합니다. 환경 설정과 다른 더 까다로운 환경에 대한 자세한 ..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="Author: Adam Paszke, Mark Towers, 번역: 황성수, 박정환,. 이 튜토리얼에서는 Gymnasium 의 CartPole-v1 태스크에서 DQN (Deep Q Learning) 에이전트를 학습하는데 PyTorch를 사용하는 방법을 보여드립니다. Deep Q Learning (DQN) 논문을 읽어보는 것도 도움이 될 수 있습니다. 태스크 에이전트는 연결된 막대가 똑바로 서 있도록 카트를 왼쪽이나 오른쪽으로 움직이는 두 가지 동작 중 하나를 선택해야 합니다. 환경 설정과 다른 더 까다로운 환경에 대한 자세한 ..." />
<meta property="og:ignore_canonical" content="true" />

    <title>강화 학습 (DQN) 튜토리얼 &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/reinforcement_q_learning';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/intermediate/reinforcement_q_learning.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />
    <link rel="next" title="Reinforcement Learning (PPO) with TorchRL Tutorial" href="reinforcement_ppo.html" />
    <link rel="prev" title="공간 변형 네트워크(Spatial Transformer Networks) 튜토리얼" href="spatial_transformer_tutorial.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2023년 05월 06일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2023년 05월 06일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">Image and Video</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="torchvision_tutorial.html">TorchVision Object Detection Finetuning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">컴퓨터 비전(Vision)을 위한 전이학습(Transfer Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/fgsm_tutorial.html">적대적 예제 생성(Adversarial Example Generation)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dcgan_faces_tutorial.html">DCGAN 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="spatial_transformer_tutorial.html">공간 변형 네트워크(Spatial Transformer Networks) 튜토리얼</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reinforcement Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">강화 학습 (DQN) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="mario_rl_tutorial.html">마리오 게임 RL 에이전트로 학습하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Recommendation Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="torchrec_intro_tutorial.html">Introduction to TorchRec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/sharding.html">Exploring TorchRec sharding</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other Domains</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio/stable/index.html">See Audio tutorials on the audio website</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/index.html">See ExecuTorch tutorials on the ExecuTorch website</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../domains.html" class="nav-link">Domains</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">강화 학습 (DQN) 튜토리얼</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../domains.html">
        <meta itemprop="name" content="Domains">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="강화 학습 (DQN) 튜토리얼">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">intermediate/reinforcement_q_learning</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-intermediate-reinforcement-q-learning-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="dqn">
<span id="sphx-glr-intermediate-reinforcement-q-learning-py"></span><h1>강화 학습 (DQN) 튜토리얼<a class="headerlink" href="#dqn" title="Link to this heading">#</a></h1>
<dl class="simple">
<dt><strong>Author</strong>: <a class="reference external" href="https://github.com/apaszke">Adam Paszke</a>, <a class="reference external" href="https://github.com/pseudo-rnd-thoughts">Mark Towers</a></dt><dd><p><strong>번역</strong>: <a class="reference external" href="https://github.com/adonisues">황성수</a>, <a class="reference external" href="https://github.com/9bow">박정환</a></p>
</dd>
</dl>
<p>이 튜토리얼에서는 <a class="reference external" href="https://gymnasium.farama.org">Gymnasium</a> 의
CartPole-v1 태스크에서 DQN (Deep Q Learning) 에이전트를 학습하는데
PyTorch를 사용하는 방법을 보여드립니다.</p>
<p><a class="reference external" href="https://arxiv.org/abs/1312.5602">Deep Q Learning (DQN)</a> 논문을 읽어보는 것도 도움이 될 수 있습니다.</p>
<p><strong>태스크</strong></p>
<p>에이전트는 연결된 막대가 똑바로 서 있도록 카트를 왼쪽이나 오른쪽으로
움직이는 두 가지 동작 중 하나를 선택해야 합니다.
환경 설정과 다른 더 까다로운 환경에 대한 자세한 내용은
<a class="reference external" href="https://gymnasium.farama.org/environments/classic_control/cart_pole/">Gymnasium 웹사이트</a>
에서 찾아볼 수 있습니다.</p>
<figure class="align-default" id="id7">
<img alt="CartPole" src="../_images/cartpole.gif" />
<figcaption>
<p><span class="caption-text">CartPole</span><a class="headerlink" href="#id7" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>에이전트가 현재 환경 상태를 관찰하고 행동을 선택하면,
환경이 새로운 상태로 <em>전환</em> 되고 작업의 결과를 나타내는 보상도 반환됩니다.
이 태스크에서 매 타임스텝 증가마다 보상이 +1이 되고, 만약 막대가 너무 멀리
떨어지거나 카트가 중심에서 2.4 유닛 이상 멀어지면 환경이 중단됩니다.
이것은 더 좋은 시나리오가 더 오랫동안 더 많은 보상을 축적하는 것을 의미합니다.</p>
<p>카트폴 태스크는 에이전트에 대한 입력이 환경 상태(위치, 속도 등)를 나타내는
4개의 실제 값이 되도록 설계되었습니다. 스케일링 없이 이 4개의 입력을 받아
각 동작에 대해 하나씩, 총 2개의 출력을 가진 완전히 연결된 작은 신경망에 통과시킵니다.
신경망은 주어진 입력에 대해, 각 동작에 대한 예상값을 예측하도록 훈련됩니다.
가장 높은 예측값을 갖는 동작이 선택됩니다.</p>
<p><strong>패키지</strong></p>
<p>먼저 필요한 패키지를 가져옵니다. 첫째, 환경 구성을 위해
pip를 사용해 설치한 <a class="reference external" href="https://gymnasium.farama.org/">gymnasium</a> 이 필요합니다.
이는 OpenAI Gym로부터 파생(fork)된 것으로, Gym v0.19부터 같은 팀에서 유지보수를 하고 있습니다.
Google Colab에서 이 튜토리얼을 실행하고 있다면, 다음을 실행해 설치할 수 있습니다:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%%bash
pip3<span class="w"> </span>install<span class="w"> </span>gym<span class="o">[</span>classic_control<span class="o">]</span>
</pre></div>
</div>
<p>또한 PyTorch에서 다음을 사용합니다:</p>
<ul class="simple">
<li><p>신경망 (<code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>)</p></li>
<li><p>최적화 (<code class="docutils literal notranslate"><span class="pre">torch.optim</span></code>)</p></li>
<li><p>자동 미분 (<code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code>)</p></li>
<li><p>시각 태스크를 위한 유틸리티들 (<code class="docutils literal notranslate"><span class="pre">torchvision</span></code> - <a class="reference external" href="https://github.com/pytorch/vision">a separate
package</a>).</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">gymnasium</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gym</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">namedtuple</span><span class="p">,</span> <span class="n">deque</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">itertools</span><span class="w"> </span><span class="kn">import</span> <span class="n">count</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>

<span class="c1"># matplotlib 설정</span>
<span class="n">is_ipython</span> <span class="o">=</span> <span class="s1">&#39;inline&#39;</span> <span class="ow">in</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">get_backend</span><span class="p">()</span>
<span class="k">if</span> <span class="n">is_ipython</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">IPython</span><span class="w"> </span><span class="kn">import</span> <span class="n">display</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ion</span><span class="p">()</span>

<span class="c1"># GPU를 사용하는 경우</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>
    <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span>
    <span class="s2">&quot;mps&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span>
    <span class="s2">&quot;cpu&quot;</span>
<span class="p">)</span>


<span class="c1"># 학습 중에 재현성(reproducibility)을 보장하기 위해, 아래 코드를 주석 해제하여</span>
<span class="c1"># 무작위 시드 값(random seed)을 고정할 수 있습니다.</span>
<span class="c1"># 이렇게 하면 결과가 일관성있게 유지되며, 디버깅 또는 다른 접근 방법을 비교하는 데 도움이 됩니다.</span>
<span class="c1">#</span>
<span class="c1"># 하지만 무작위성(randomness)을 허용하는 것은 실제로 유용할 수 있습니다,</span>
<span class="c1"># 왜냐하면 모델이 다른 학습 경로를 탐색할 수 있기 때문입니다.</span>


<span class="c1"># seed = 42</span>
<span class="c1"># random.seed(seed)</span>
<span class="c1"># torch.manual_seed(seed)</span>
<span class="c1"># env.reset(seed=seed)</span>
<span class="c1"># env.action_space.seed(seed)</span>
<span class="c1"># env.observation_space.seed(seed)</span>
<span class="c1"># if torch.cuda.is_available():</span>
<span class="c1">#     torch.cuda.manual_seed(seed)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
Users of this version of Gym should be able to simply replace &#39;import gym&#39; with &#39;import gymnasium as gym&#39; in the vast majority of cases.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
</pre></div>
</div>
<section id="replay-memory">
<h2>재현 메모리(Replay Memory)<a class="headerlink" href="#replay-memory" title="Link to this heading">#</a></h2>
<p>우리는 DQN 학습을 위해 경험 재현 메모리를 사용할 것입니다.
에이전트가 관찰한 전환(transition)을 저장하고 나중에 이 데이터를
재사용할 수 있습니다. 무작위로 샘플링하면 배치를 구성하는 전환들이
비상관(decorrelated)하게 됩니다. 이것이 DQN 학습 절차를 크게 안정시키고
향상시키는 것으로 나타났습니다.</p>
<p>이를 위해서 두개의 클래스가 필요합니다:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Transition</span></code> - 우리 환경에서 단일 전환을 나타내도록 명명된 튜플.
그것은 화면의 차이인 state로 (state, action) 쌍을 (next_state, reward) 결과로 매핑합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ReplayMemory</span></code> - 최근 관찰된 전이를 보관 유지하는 제한된 크기의 순환 버퍼.
또한 학습을 위한 전환의 무작위 배치를 선택하기위한
<code class="docutils literal notranslate"><span class="pre">.sample</span> <span class="pre">()</span></code> 메소드를 구현합니다.</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">Transition</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Transition&#39;</span><span class="p">,</span>
                        <span class="p">(</span><span class="s1">&#39;state&#39;</span><span class="p">,</span> <span class="s1">&#39;action&#39;</span><span class="p">,</span> <span class="s1">&#39;next_state&#39;</span><span class="p">,</span> <span class="s1">&#39;reward&#39;</span><span class="p">))</span>


<span class="k">class</span><span class="w"> </span><span class="nc">ReplayMemory</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">capacity</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">([],</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">capacity</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;transition 저장&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Transition</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span>
</pre></div>
</div>
<p>이제 모델을 정의합시다. 그러나 먼저 DQN이 무엇인지 간단히 요약해 보겠습니다.</p>
</section>
<section id="id3">
<h2>DQN 알고리즘<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>우리의 환경은 결정론적이므로 여기에 제시된 모든 방정식은 단순화를 위해
결정론적으로 공식화됩니다. 강화 학습 자료은 환경에서 확률론적 전환에
대한 기대값(expectation)도 포함할 것입니다.</p>
<p>우리의 목표는 할인된 누적 보상 (discounted cumulative reward)을
극대화하려는 정책(policy)을 학습하는 것입니다.
<span class="math">\(R_{t_0} = \sum_{t=t_0}^{\infty} \gamma^{t - t_0} r_t\)</span>, 여기서
<span class="math">\(R_{t_0}\)</span> 는 <em>반환(return)</em> 입니다. 할인 상수,
<span class="math">\(\gamma\)</span>, 는 <span class="math">\(0\)</span> 과 <span class="math">\(1\)</span> 의 상수여야 합니다.
<span class="math">\(\gamma\)</span> 가 낮을수록 에이전트에게는 불확실한 먼 미래의 보상은
상당히 확신할 수 있는 가까운 미래의 보상보다 덜 중요해집니다.
또한, 에이전트가 시간적으로 가까운 시점의 보상을, 동일한 양의 먼 미래의
보상보다 먼저 수집하도록 장려합니다.</p>
<p>Q-learning의 주요 아이디어는 만일 함수 <span class="math">\(Q^*: State \times Action \rightarrow \mathbb{R}\)</span> 를
가지고 있다면 반환이 어떻게 될지 알려줄 수 있고,
만약 주어진 상태(state)에서 행동(action)을 한다면, 보상을 최대화하는
정책을 쉽게 구축할 수 있습니다:</p>
<div class="math">
\[\pi^*(s) = \arg\!\max_a \ Q^*(s, a)

\]</div>
<p>그러나 세계(world)에 관한 모든 것을 알지 못하기 때문에,
<span class="math">\(Q^*\)</span> 에 도달할 수 없습니다. 그러나 신경망은
범용 함수 근사자(universal function approximator)이기 때문에
간단하게 생성하고 <span class="math">\(Q^*\)</span> 를 닮도록 학습할 수 있습니다.</p>
<p>학습 업데이트 규칙으로, 일부 정책을 위한 모든 <span class="math">\(Q\)</span> 함수가
Bellman 방정식을 준수한다는 사실을 사용할 것입니다:</p>
<div class="math">
\[Q^{\pi}(s, a) = r + \gamma Q^{\pi}(s', \pi(s'))

\]</div>
<p>평등(equality)의 두 측면 사이의 차이는
시간차 오류(temporal difference error), <span class="math">\(\delta\)</span> 입니다.:</p>
<div class="math">
\[\delta = Q(s, a) - (r + \gamma \max_a' Q(s', a))

\]</div>
<p>오류를 최소화하기 위해서 <a class="reference external" href="https://en.wikipedia.org/wiki/Huber_loss">Huber
loss</a> 를 사용합니다.
Huber loss 는 오류가 작으면 평균 제곱 오차( mean squared error)와 같이
동작하고 오류가 클 때는 평균 절대 오류와 유사합니다.
- 이것은 <span class="math">\(Q\)</span> 의 추정이 매우 혼란스러울 때 이상 값에 더 강건하게 합니다.
재현 메모리에서 샘플링한 전환 배치 <span class="math">\(B\)</span> 에서 이것을 계산합니다:</p>
<div class="math">
\[\mathcal{L} = \frac{1}{|B|}\sum_{(s, a, s', r) \ \in \ B} \mathcal{L}(\delta)\]</div>
<div class="math">
\[\text{where} \quad \mathcal{L}(\delta) = \begin{cases}
  \frac{1}{2}{\delta^2}  & \text{for } |\delta| \le 1, \\
  |\delta| - \frac{1}{2} & \text{otherwise.}
\end{cases}\]</div>
<section id="q">
<h3>Q-네트워크<a class="headerlink" href="#q" title="Link to this heading">#</a></h3>
<p>우리 모델은 현재와 이전 스크린 패치의 차이를 취하는
순연결(feed-forward) 신경망입니다. 두가지 출력 <span class="math">\(Q(s, \mathrm{left})\)</span> 와
<span class="math">\(Q(s, \mathrm{right})\)</span> 가 있습니다. (여기서 <span class="math">\(s\)</span> 는 네트워크의 입력입니다)
결과적으로 네트워크는 주어진 현재 입력에서 각 행동의 <em>기대값</em> 을 예측하려고 합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">DQN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_observations</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DQN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_observations</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>

    <span class="c1"># 최적화 중에 다음 행동을 결정하기 위해서 하나의 요소 또는 배치를 이용해 호촐됩니다.</span>
    <span class="c1"># ([[left0exp,right0exp]...]) 를 반환합니다.</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id4">
<h2>학습<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<section id="id5">
<h3>하이퍼 파라미터와 유틸리티<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>이 셀은 모델과 최적화기를 인스턴스화하고 일부 유틸리티를 정의합니다:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">select_action</span></code> - Epsilon Greedy 정책에 따라 행동을 선택합니다.
간단히 말해서, 가끔 모델을 사용하여 행동을 선택하고 때로는 단지 하나를
균일하게 샘플링할 것입니다. 임의의 액션을 선택할 확률은
<code class="docutils literal notranslate"><span class="pre">EPS_START</span></code> 에서 시작해서 <code class="docutils literal notranslate"><span class="pre">EPS_END</span></code> 를 향해 지수적으로 감소할 것입니다.
<code class="docutils literal notranslate"><span class="pre">EPS_DECAY</span></code> 는 감쇠 속도를 제어합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">plot_durations</span></code> - 지난 100개 에피소드의 평균(공식 평가에서 사용 된 수치)에 따른
에피소드의 지속을 도표로 그리기 위한 헬퍼. 도표는 기본 훈련 루프가
포함된 셀 밑에 있으며, 매 에피소드마다 업데이트됩니다.</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># BATCH_SIZE는 리플레이 버퍼에서 샘플링된 트랜지션의 수입니다.</span>
<span class="c1"># GAMMA는 이전 섹션에서 언급한 할인 계수입니다.</span>
<span class="c1"># EPS_START는 엡실론의 시작 값입니다.</span>
<span class="c1"># EPS_END는 엡실론의 최종 값입니다.</span>
<span class="c1"># EPS_DECAY는 엡실론의 지수 감쇠(exponential decay) 속도 제어하며, 높을수록 감쇠 속도가 느립니다.</span>
<span class="c1"># TAU는 목표 네트워크의 업데이트 속도입니다.</span>
<span class="c1"># LR은 ``AdamW`` 옵티마이저의 학습율(learning rate)입니다.</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">EPS_START</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">EPS_END</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">EPS_DECAY</span> <span class="o">=</span> <span class="mi">2500</span>
<span class="n">TAU</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">3e-4</span>

<span class="c1"># gym 행동 공간에서 행동의 숫자를 얻습니다.</span>
<span class="n">n_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
<span class="c1"># 상태 관측 횟수를 얻습니다.</span>
<span class="n">state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">n_observations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<span class="n">policy_net</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="n">n_observations</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">target_net</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="n">n_observations</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">target_net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">policy_net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">policy_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">ReplayMemory</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>


<span class="n">steps_done</span> <span class="o">=</span> <span class="mi">0</span>


<span class="k">def</span><span class="w"> </span><span class="nf">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">steps_done</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
    <span class="n">eps_threshold</span> <span class="o">=</span> <span class="n">EPS_END</span> <span class="o">+</span> <span class="p">(</span><span class="n">EPS_START</span> <span class="o">-</span> <span class="n">EPS_END</span><span class="p">)</span> <span class="o">*</span> \
        <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span> <span class="o">*</span> <span class="n">steps_done</span> <span class="o">/</span> <span class="n">EPS_DECAY</span><span class="p">)</span>
    <span class="n">steps_done</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">sample</span> <span class="o">&gt;</span> <span class="n">eps_threshold</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="c1"># t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.</span>
            <span class="c1"># 최대 결과의 두번째 열은 최대 요소의 주소값이므로,</span>
            <span class="c1"># 기대 보상이 더 큰 행동을 선택할 수 있습니다.</span>
            <span class="k">return</span> <span class="n">policy_net</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()]],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>


<span class="n">episode_durations</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">plot_durations</span><span class="p">(</span><span class="n">show_result</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">durations_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">episode_durations</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">show_result</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Result&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training...&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Episode&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Duration&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">durations_t</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="c1"># 100개의 에피소드 평균을 가져 와서 도표 그리기</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">durations_t</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">100</span><span class="p">:</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">durations_t</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">99</span><span class="p">),</span> <span class="n">means</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">means</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">pause</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># 도표가 업데이트되도록 잠시 멈춤</span>
    <span class="k">if</span> <span class="n">is_ipython</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">show_result</span><span class="p">:</span>
            <span class="n">display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">())</span>
            <span class="n">display</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="id6">
<h3>학습 루프<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>최종적으로 모델 학습을 위한 코드.</p>
<p>여기서, 최적화의 한 단계를 수행하는 <code class="docutils literal notranslate"><span class="pre">optimize_model</span></code> 함수를 찾을 수 있습니다.
먼저 배치 하나를 샘플링하고 모든 Tensor를 하나로 연결하고
<span class="math">\(Q(s_t, a_t)\)</span> 와  <span class="math">\(V(s_{t+1}) = \max_a Q(s_{t+1}, a)\)</span> 를 계산하고
그것들을 손실로 합칩니다. 우리가 설정한 정의에 따르면 만약 <span class="math">\(s\)</span> 가
마지막 상태라면 <span class="math">\(V(s) = 0\)</span> 입니다.
또한 안정성 추가 위한 <span class="math">\(V(s_{t+1})\)</span> 계산을 위해 목표 네트워크를 사용합니다.
대상 네트워크는 이전에 정의한 하이퍼파라미터 <code class="docutils literal notranslate"><span class="pre">TAU</span></code> 에 의해 제어되는
<a class="reference external" href="https://arxiv.org/pdf/1509.02971.pdf">소프트 업데이트</a>
로 모든 단계에서 업데이트됩니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">optimize_model</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">transitions</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="c1"># Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for</span>
    <span class="c1"># detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로</span>
    <span class="c1"># 전환합니다.</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">Transition</span><span class="p">(</span><span class="o">*</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">transitions</span><span class="p">))</span>

    <span class="c1"># 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다</span>
    <span class="c1"># (최종 상태는 시뮬레이션이 종료 된 이후의 상태)</span>
    <span class="n">non_final_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
                                          <span class="n">batch</span><span class="o">.</span><span class="n">next_state</span><span class="p">)),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
    <span class="n">non_final_next_states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">next_state</span>
                                                <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">])</span>
    <span class="n">state_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
    <span class="n">action_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>
    <span class="n">reward_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">reward</span><span class="p">)</span>

    <span class="c1"># Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.</span>
    <span class="c1"># 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.</span>
    <span class="n">state_action_values</span> <span class="o">=</span> <span class="n">policy_net</span><span class="p">(</span><span class="n">state_batch</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">action_batch</span><span class="p">)</span>

    <span class="c1"># 모든 다음 상태를 위한 V(s_{t+1}) 계산</span>
    <span class="c1"># non_final_next_states의 행동들에 대한 기대값은 &quot;이전&quot; target_net을 기반으로 계산됩니다.</span>
    <span class="c1"># max(1).values로 최고의 보상을 선택하십시오.</span>
    <span class="c1"># 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.</span>
    <span class="n">next_state_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">next_state_values</span><span class="p">[</span><span class="n">non_final_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">target_net</span><span class="p">(</span><span class="n">non_final_next_states</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="c1"># 기대 Q 값 계산</span>
    <span class="n">expected_state_action_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_state_values</span> <span class="o">*</span> <span class="n">GAMMA</span><span class="p">)</span> <span class="o">+</span> <span class="n">reward_batch</span>

    <span class="c1"># Huber 손실 계산</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">state_action_values</span><span class="p">,</span> <span class="n">expected_state_action_values</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># 모델 최적화</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># 변화도 클리핑 바꿔치기</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_value_</span><span class="p">(</span><span class="n">policy_net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>아래에서 주요 학습 루프를 찾을 수 있습니다. 처음으로 환경을
재설정하고 초기 <code class="docutils literal notranslate"><span class="pre">state</span></code> Tensor를 얻습니다. 그런 다음 행동을
샘플링하고, 그것을 실행하고, 다음 상태와 보상(항상 1)을 관찰하고,
모델을 한 번 최적화합니다. 에피소드가 끝나면 (모델이 실패)
루프를 다시 시작합니다.</p>
<p>아래에서 <cite>num_episodes</cite> 는 GPU를 사용할 수 있는 경우 600으로,
그렇지 않은 경우 50개의 에피소드를 설정하여 학습이 너무 오래 걸리지는 않습니다.
하지만 50개의 에피소드만으로는 CartPole에서 좋은 성능을 관찰하기에는 충분치 않습니다.
600개의 학습 에피소드 내에서 모델이 지속적으로 500개의 스텝을 달성하는 것을
볼 수 있어야 합니다. RL 에이전트 학습 과정에는 노이즈가 많을 수 있으므로,
수렴(convergence)이 관찰되지 않으면 학습을 재시작하는 것이 더 나은 결과를 얻을 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">600</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">50</span>

<span class="k">for</span> <span class="n">i_episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="c1"># 환경과 상태 초기화</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">count</span><span class="p">():</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">reward</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>

        <span class="k">if</span> <span class="n">terminated</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 메모리에 변이 저장</span>
        <span class="n">memory</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>

        <span class="c1"># 다음 상태로 이동</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="c1"># (정책 네트워크에서) 최적화 한단계 수행</span>
        <span class="n">optimize_model</span><span class="p">()</span>

        <span class="c1"># 목표 네트워크의 가중치를 소프트 업데이트</span>
        <span class="c1"># θ′ ← τ θ + (1 −τ )θ′</span>
        <span class="n">target_net_state_dict</span> <span class="o">=</span> <span class="n">target_net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">policy_net_state_dict</span> <span class="o">=</span> <span class="n">policy_net</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">policy_net_state_dict</span><span class="p">:</span>
            <span class="n">target_net_state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">policy_net_state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">*</span><span class="n">TAU</span> <span class="o">+</span> <span class="n">target_net_state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">TAU</span><span class="p">)</span>
        <span class="n">target_net</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">target_net_state_dict</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">episode_durations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">plot_durations</span><span class="p">()</span>
            <span class="k">break</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Complete&#39;</span><span class="p">)</span>
<span class="n">plot_durations</span><span class="p">(</span><span class="n">show_result</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ioff</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_reinforcement_q_learning_001.png" srcset="../_images/sphx_glr_reinforcement_q_learning_001.png" alt="Result" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/opt/conda/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:249: DeprecationWarning:

`np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)

Complete
</pre></div>
</div>
<p>다음은 전체 결과 데이터 흐름을 보여주는 다이어그램입니다.</p>
<figure class="align-default">
<img alt="../_images/reinforcement_learning_diagram.jpg" src="../_images/reinforcement_learning_diagram.jpg" />
</figure>
<p>행동은 무작위 또는 정책에 따라 선택되어, gym 환경에서 다음 단계 샘플을 가져옵니다.
결과를 재현 메모리에 저장하고 모든 반복에서 최적화 단계를 실행합니다.
최적화는 재현 메모리에서 무작위 배치를 선택하여 새 정책을 학습합니다.
“이전”의 target_net은 최적화에서 기대 Q 값을 계산하는 데에도 사용됩니다.
목표 네트워크 가중치의 소프트 업데이트는 매 단계(step)마다 수행됩니다.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (9 minutes 47.742 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-reinforcement-q-learning-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/9da0471a9eeb2351a488cd4b44fc6bbf/reinforcement_q_learning.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">reinforcement_q_learning.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/6ea0e49c7d0da2713588ef1a3b64eb35/reinforcement_q_learning.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">reinforcement_q_learning.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/4688eaf2bedafbdc28e62bfc388d2b6d/reinforcement_q_learning.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">reinforcement_q_learning.zip</span></code></a></p>
</div>
</div>
</section>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="spatial_transformer_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">공간 변형 네트워크(Spatial Transformer Networks) 튜토리얼</p>
      </div>
    </a>
    <a class="right-next"
       href="reinforcement_ppo.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Reinforcement Learning (PPO) with TorchRL Tutorial</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="spatial_transformer_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">공간 변형 네트워크(Spatial Transformer Networks) 튜토리얼</p>
      </div>
    </a>
    <a class="right-next"
       href="reinforcement_ppo.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Reinforcement Learning (PPO) with TorchRL Tutorial</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#replay-memory">재현 메모리(Replay Memory)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">DQN 알고리즘</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#q">Q-네트워크</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">학습</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">하이퍼 파라미터와 유틸리티</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">학습 루프</a></li>
</ul>
</li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "\uac15\ud654 \ud559\uc2b5 (DQN) \ud29c\ud1a0\ub9ac\uc5bc",
       "headline": "\uac15\ud654 \ud559\uc2b5 (DQN) \ud29c\ud1a0\ub9ac\uc5bc",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/intermediate/reinforcement_q_learning.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. \uac15\ud654 \ud559\uc2b5 (DQN) \ud29c\ud1a0\ub9ac\uc5bc# Author: Adam Paszke, Mark Towers\ubc88\uc5ed: \ud669\uc131\uc218, \ubc15\uc815\ud658 \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 Gymnasium \uc758 CartPole-v1 \ud0dc\uc2a4\ud06c\uc5d0\uc11c DQN (Deep Q Learning) \uc5d0\uc774\uc804\ud2b8\ub97c \ud559\uc2b5\ud558\ub294\ub370 PyTorch\ub97c \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\ub4dc\ub9bd\ub2c8\ub2e4. Deep Q Learning (DQN) \ub17c\ubb38\uc744 \uc77d\uc5b4\ubcf4\ub294 \uac83\ub3c4 \ub3c4\uc6c0\uc774 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud0dc\uc2a4\ud06c \uc5d0\uc774\uc804\ud2b8\ub294 \uc5f0\uacb0\ub41c \ub9c9\ub300\uac00 \ub611\ubc14\ub85c \uc11c \uc788\ub3c4\ub85d \uce74\ud2b8\ub97c \uc67c\ucabd\uc774\ub098 \uc624\ub978\ucabd\uc73c\ub85c \uc6c0\uc9c1\uc774\ub294 \ub450 \uac00\uc9c0 \ub3d9\uc791 \uc911 \ud558\ub098\ub97c \uc120\ud0dd\ud574\uc57c \ud569\ub2c8\ub2e4. \ud658\uacbd \uc124\uc815\uacfc \ub2e4\ub978 \ub354 \uae4c\ub2e4\ub85c\uc6b4 \ud658\uacbd\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 Gymnasium \uc6f9\uc0ac\uc774\ud2b8 \uc5d0\uc11c \ucc3e\uc544\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. CartPole# \uc5d0\uc774\uc804\ud2b8\uac00 \ud604\uc7ac \ud658\uacbd \uc0c1\ud0dc\ub97c \uad00\ucc30\ud558\uace0 \ud589\ub3d9\uc744 \uc120\ud0dd\ud558\uba74, \ud658\uacbd\uc774 \uc0c8\ub85c\uc6b4 \uc0c1\ud0dc\ub85c \uc804\ud658 \ub418\uace0 \uc791\uc5c5\uc758 \uacb0\uacfc\ub97c \ub098\ud0c0\ub0b4\ub294 \ubcf4\uc0c1\ub3c4 \ubc18\ud658\ub429\ub2c8\ub2e4. \uc774 \ud0dc\uc2a4\ud06c\uc5d0\uc11c \ub9e4 \ud0c0\uc784\uc2a4\ud15d \uc99d\uac00\ub9c8\ub2e4 \ubcf4\uc0c1\uc774 +1\uc774 \ub418\uace0, \ub9cc\uc57d \ub9c9\ub300\uac00 \ub108\ubb34 \uba40\ub9ac \ub5a8\uc5b4\uc9c0\uac70\ub098 \uce74\ud2b8\uac00 \uc911\uc2ec\uc5d0\uc11c 2.4 \uc720\ub2db \uc774\uc0c1 \uba40\uc5b4\uc9c0\uba74 \ud658\uacbd\uc774 \uc911\ub2e8\ub429\ub2c8\ub2e4. \uc774\uac83\uc740 \ub354 \uc88b\uc740 \uc2dc\ub098\ub9ac\uc624\uac00 \ub354 \uc624\ub7ab\ub3d9\uc548 \ub354 \ub9ce\uc740 \ubcf4\uc0c1\uc744 \ucd95\uc801\ud558\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uce74\ud2b8\ud3f4 \ud0dc\uc2a4\ud06c\ub294 \uc5d0\uc774\uc804\ud2b8\uc5d0 \ub300\ud55c \uc785\ub825\uc774 \ud658\uacbd \uc0c1\ud0dc(\uc704\uce58, \uc18d\ub3c4 \ub4f1)\ub97c \ub098\ud0c0\ub0b4\ub294 4\uac1c\uc758 \uc2e4\uc81c \uac12\uc774 \ub418\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc2a4\ucf00\uc77c\ub9c1 \uc5c6\uc774 \uc774 4\uac1c\uc758 \uc785\ub825\uc744 \ubc1b\uc544 \uac01 \ub3d9\uc791\uc5d0 \ub300\ud574 \ud558\ub098\uc529, \ucd1d 2\uac1c\uc758 \ucd9c\ub825\uc744 \uac00\uc9c4 \uc644\uc804\ud788 \uc5f0\uacb0\ub41c \uc791\uc740 \uc2e0\uacbd\ub9dd\uc5d0 \ud1b5\uacfc\uc2dc\ud0b5\ub2c8\ub2e4. \uc2e0\uacbd\ub9dd\uc740 \uc8fc\uc5b4\uc9c4 \uc785\ub825\uc5d0 \ub300\ud574, \uac01 \ub3d9\uc791\uc5d0 \ub300\ud55c \uc608\uc0c1\uac12\uc744 \uc608\uce21\ud558\ub3c4\ub85d \ud6c8\ub828\ub429\ub2c8\ub2e4. \uac00\uc7a5 \ub192\uc740 \uc608\uce21\uac12\uc744 \uac16\ub294 \ub3d9\uc791\uc774 \uc120\ud0dd\ub429\ub2c8\ub2e4. \ud328\ud0a4\uc9c0 \uba3c\uc800 \ud544\uc694\ud55c \ud328\ud0a4\uc9c0\ub97c \uac00\uc838\uc635\ub2c8\ub2e4. \uccab\uc9f8, \ud658\uacbd \uad6c\uc131\uc744 \uc704\ud574 pip\ub97c \uc0ac\uc6a9\ud574 \uc124\uce58\ud55c gymnasium \uc774 \ud544\uc694\ud569\ub2c8\ub2e4. \uc774\ub294 OpenAI Gym\ub85c\ubd80\ud130 \ud30c\uc0dd(fork)\ub41c \uac83\uc73c\ub85c, Gym v0.19\ubd80\ud130 \uac19\uc740 \ud300\uc5d0\uc11c \uc720\uc9c0\ubcf4\uc218\ub97c \ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. Google Colab\uc5d0\uc11c \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc2e4\ud589\ud558\uace0 \uc788\ub2e4\uba74, \ub2e4\uc74c\uc744 \uc2e4\ud589\ud574 \uc124\uce58\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: %%bash pip3 install gym[classic_control] \ub610\ud55c PyTorch\uc5d0\uc11c \ub2e4\uc74c\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4: \uc2e0\uacbd\ub9dd (torch.nn) \ucd5c\uc801\ud654 (torch.optim) \uc790\ub3d9 \ubbf8\ubd84 (torch.autograd) \uc2dc\uac01 \ud0dc\uc2a4\ud06c\ub97c \uc704\ud55c \uc720\ud2f8\ub9ac\ud2f0\ub4e4 (torchvision - a separate package). import gymnasium as gym import math import random import matplotlib import matplotlib.pyplot as plt from collections import namedtuple, deque from itertools import count import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F env = gym.make(\"CartPole-v1\") # matplotlib \uc124\uc815 is_ipython = \u0027inline\u0027 in matplotlib.get_backend() if is_ipython: from IPython import display plt.ion() # GPU\ub97c \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0 device = torch.device( \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\" ) # \ud559\uc2b5 \uc911\uc5d0 \uc7ac\ud604\uc131(reproducibility)\uc744 \ubcf4\uc7a5\ud558\uae30 \uc704\ud574, \uc544\ub798 \ucf54\ub4dc\ub97c \uc8fc\uc11d \ud574\uc81c\ud558\uc5ec # \ubb34\uc791\uc704 \uc2dc\ub4dc \uac12(random seed)\uc744 \uace0\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. # \uc774\ub807\uac8c \ud558\uba74 \uacb0\uacfc\uac00 \uc77c\uad00\uc131\uc788\uac8c \uc720\uc9c0\ub418\uba70, \ub514\ubc84\uae45 \ub610\ub294 \ub2e4\ub978 \uc811\uadfc \ubc29\ubc95\uc744 \ube44\uad50\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4. # # \ud558\uc9c0\ub9cc \ubb34\uc791\uc704\uc131(randomness)\uc744 \ud5c8\uc6a9\ud558\ub294 \uac83\uc740 \uc2e4\uc81c\ub85c \uc720\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4, # \uc65c\ub0d0\ud558\uba74 \ubaa8\ub378\uc774 \ub2e4\ub978 \ud559\uc2b5 \uacbd\ub85c\ub97c \ud0d0\uc0c9\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. # seed = 42 # random.seed(seed) # torch.manual_seed(seed) # env.reset(seed=seed) # env.action_space.seed(seed) # env.observation_space.seed(seed) # if torch.cuda.is_available(): # torch.cuda.manual_seed(seed) Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality. Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade. Users of this version of Gym should be able to simply replace \u0027import gym\u0027 with \u0027import gymnasium as gym\u0027 in the vast majority of cases. See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information. \uc7ac\ud604 \uba54\ubaa8\ub9ac(Replay Memory)# \uc6b0\ub9ac\ub294 DQN \ud559\uc2b5\uc744 \uc704\ud574 \uacbd\ud5d8 \uc7ac\ud604 \uba54\ubaa8\ub9ac\ub97c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \uc5d0\uc774\uc804\ud2b8\uac00 \uad00\ucc30\ud55c \uc804\ud658(transition)\uc744 \uc800\uc7a5\ud558\uace0 \ub098\uc911\uc5d0 \uc774 \ub370\uc774\ud130\ub97c \uc7ac\uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubb34\uc791\uc704\ub85c \uc0d8\ud50c\ub9c1\ud558\uba74 \ubc30\uce58\ub97c \uad6c\uc131\ud558\ub294 \uc804\ud658\ub4e4\uc774 \ube44\uc0c1\uad00(decorrelated)\ud558\uac8c \ub429\ub2c8\ub2e4. \uc774\uac83\uc774 DQN \ud559\uc2b5 \uc808\ucc28\ub97c \ud06c\uac8c \uc548\uc815\uc2dc\ud0a4\uace0 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574\uc11c \ub450\uac1c\uc758 \ud074\ub798\uc2a4\uac00 \ud544\uc694\ud569\ub2c8\ub2e4: Transition - \uc6b0\ub9ac \ud658\uacbd\uc5d0\uc11c \ub2e8\uc77c \uc804\ud658\uc744 \ub098\ud0c0\ub0b4\ub3c4\ub85d \uba85\uba85\ub41c \ud29c\ud50c. \uadf8\uac83\uc740 \ud654\uba74\uc758 \ucc28\uc774\uc778 state\ub85c (state, action) \uc30d\uc744 (next_state, reward) \uacb0\uacfc\ub85c \ub9e4\ud551\ud569\ub2c8\ub2e4. ReplayMemory - \ucd5c\uadfc \uad00\ucc30\ub41c \uc804\uc774\ub97c \ubcf4\uad00 \uc720\uc9c0\ud558\ub294 \uc81c\ud55c\ub41c \ud06c\uae30\uc758 \uc21c\ud658 \ubc84\ud37c. \ub610\ud55c \ud559\uc2b5\uc744 \uc704\ud55c \uc804\ud658\uc758 \ubb34\uc791\uc704 \ubc30\uce58\ub97c \uc120\ud0dd\ud558\uae30\uc704\ud55c .sample () \uba54\uc18c\ub4dc\ub97c \uad6c\ud604\ud569\ub2c8\ub2e4. Transition = namedtuple(\u0027Transition\u0027, (\u0027state\u0027, \u0027action\u0027, \u0027next_state\u0027, \u0027reward\u0027)) class ReplayMemory(object): def __init__(self, capacity): self.memory = deque([], maxlen=capacity) def push(self, *args): \"\"\"transition \uc800\uc7a5\"\"\" self.memory.append(Transition(*args)) def sample(self, batch_size): return random.sample(self.memory, batch_size) def __len__(self): return len(self.memory) \uc774\uc81c \ubaa8\ub378\uc744 \uc815\uc758\ud569\uc2dc\ub2e4. \uadf8\ub7ec\ub098 \uba3c\uc800 DQN\uc774 \ubb34\uc5c7\uc778\uc9c0 \uac04\ub2e8\ud788 \uc694\uc57d\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. DQN \uc54c\uace0\ub9ac\uc998# \uc6b0\ub9ac\uc758 \ud658\uacbd\uc740 \uacb0\uc815\ub860\uc801\uc774\ubbc0\ub85c \uc5ec\uae30\uc5d0 \uc81c\uc2dc\ub41c \ubaa8\ub4e0 \ubc29\uc815\uc2dd\uc740 \ub2e8\uc21c\ud654\ub97c \uc704\ud574 \uacb0\uc815\ub860\uc801\uc73c\ub85c \uacf5\uc2dd\ud654\ub429\ub2c8\ub2e4. \uac15\ud654 \ud559\uc2b5 \uc790\ub8cc\uc740 \ud658\uacbd\uc5d0\uc11c \ud655\ub960\ub860\uc801 \uc804\ud658\uc5d0 \ub300\ud55c \uae30\ub300\uac12(expectation)\ub3c4 \ud3ec\ud568\ud560 \uac83\uc785\ub2c8\ub2e4. \uc6b0\ub9ac\uc758 \ubaa9\ud45c\ub294 \ud560\uc778\ub41c \ub204\uc801 \ubcf4\uc0c1 (discounted cumulative reward)\uc744 \uadf9\ub300\ud654\ud558\ub824\ub294 \uc815\ucc45(policy)\uc744 \ud559\uc2b5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \\(R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t\\), \uc5ec\uae30\uc11c \\(R_{t_0}\\) \ub294 \ubc18\ud658(return) \uc785\ub2c8\ub2e4. \ud560\uc778 \uc0c1\uc218, \\(\\gamma\\), \ub294 \\(0\\) \uacfc \\(1\\) \uc758 \uc0c1\uc218\uc5ec\uc57c \ud569\ub2c8\ub2e4. \\(\\gamma\\) \uac00 \ub0ae\uc744\uc218\ub85d \uc5d0\uc774\uc804\ud2b8\uc5d0\uac8c\ub294 \ubd88\ud655\uc2e4\ud55c \uba3c \ubbf8\ub798\uc758 \ubcf4\uc0c1\uc740 \uc0c1\ub2f9\ud788 \ud655\uc2e0\ud560 \uc218 \uc788\ub294 \uac00\uae4c\uc6b4 \ubbf8\ub798\uc758 \ubcf4\uc0c1\ubcf4\ub2e4 \ub35c \uc911\uc694\ud574\uc9d1\ub2c8\ub2e4. \ub610\ud55c, \uc5d0\uc774\uc804\ud2b8\uac00 \uc2dc\uac04\uc801\uc73c\ub85c \uac00\uae4c\uc6b4 \uc2dc\uc810\uc758 \ubcf4\uc0c1\uc744, \ub3d9\uc77c\ud55c \uc591\uc758 \uba3c \ubbf8\ub798\uc758 \ubcf4\uc0c1\ubcf4\ub2e4 \uba3c\uc800 \uc218\uc9d1\ud558\ub3c4\ub85d \uc7a5\ub824\ud569\ub2c8\ub2e4. Q-learning\uc758 \uc8fc\uc694 \uc544\uc774\ub514\uc5b4\ub294 \ub9cc\uc77c \ud568\uc218 \\(Q^*: State \\times Action \\rightarrow \\mathbb{R}\\) \ub97c \uac00\uc9c0\uace0 \uc788\ub2e4\uba74 \ubc18\ud658\uc774 \uc5b4\ub5bb\uac8c \ub420\uc9c0 \uc54c\ub824\uc904 \uc218 \uc788\uace0, \ub9cc\uc57d \uc8fc\uc5b4\uc9c4 \uc0c1\ud0dc(state)\uc5d0\uc11c \ud589\ub3d9(action)\uc744 \ud55c\ub2e4\uba74, \ubcf4\uc0c1\uc744 \ucd5c\ub300\ud654\ud558\ub294 \uc815\ucc45\uc744 \uc27d\uac8c \uad6c\ucd95\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: \\[\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a) \\] \uadf8\ub7ec\ub098 \uc138\uacc4(world)\uc5d0 \uad00\ud55c \ubaa8\ub4e0 \uac83\uc744 \uc54c\uc9c0 \ubabb\ud558\uae30 \ub54c\ubb38\uc5d0, \\(Q^*\\) \uc5d0 \ub3c4\ub2ec\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc2e0\uacbd\ub9dd\uc740 \ubc94\uc6a9 \ud568\uc218 \uadfc\uc0ac\uc790(universal function approximator)\uc774\uae30 \ub54c\ubb38\uc5d0 \uac04\ub2e8\ud558\uac8c \uc0dd\uc131\ud558\uace0 \\(Q^*\\) \ub97c \ub2ee\ub3c4\ub85d \ud559\uc2b5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud559\uc2b5 \uc5c5\ub370\uc774\ud2b8 \uaddc\uce59\uc73c\ub85c, \uc77c\ubd80 \uc815\ucc45\uc744 \uc704\ud55c \ubaa8\ub4e0 \\(Q\\) \ud568\uc218\uac00 Bellman \ubc29\uc815\uc2dd\uc744 \uc900\uc218\ud55c\ub2e4\ub294 \uc0ac\uc2e4\uc744 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4: \\[Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s\u0027, \\pi(s\u0027)) \\] \ud3c9\ub4f1(equality)\uc758 \ub450 \uce21\uba74 \uc0ac\uc774\uc758 \ucc28\uc774\ub294 \uc2dc\uac04\ucc28 \uc624\ub958(temporal difference error), \\(\\delta\\) \uc785\ub2c8\ub2e4.: \\[\\delta = Q(s, a) - (r + \\gamma \\max_a\u0027 Q(s\u0027, a)) \\] \uc624\ub958\ub97c \ucd5c\uc18c\ud654\ud558\uae30 \uc704\ud574\uc11c Huber loss \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. Huber loss \ub294 \uc624\ub958\uac00 \uc791\uc73c\uba74 \ud3c9\uade0 \uc81c\uacf1 \uc624\ucc28( mean squared error)\uc640 \uac19\uc774 \ub3d9\uc791\ud558\uace0 \uc624\ub958\uac00 \ud074 \ub54c\ub294 \ud3c9\uade0 \uc808\ub300 \uc624\ub958\uc640 \uc720\uc0ac\ud569\ub2c8\ub2e4. - \uc774\uac83\uc740 \\(Q\\) \uc758 \ucd94\uc815\uc774 \ub9e4\uc6b0 \ud63c\ub780\uc2a4\ub7ec\uc6b8 \ub54c \uc774\uc0c1 \uac12\uc5d0 \ub354 \uac15\uac74\ud558\uac8c \ud569\ub2c8\ub2e4. \uc7ac\ud604 \uba54\ubaa8\ub9ac\uc5d0\uc11c \uc0d8\ud50c\ub9c1\ud55c \uc804\ud658 \ubc30\uce58 \\(B\\) \uc5d0\uc11c \uc774\uac83\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4: \\[\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s\u0027, r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\] \\[\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases} \\frac{1}{2}{\\delta^2} \u0026 \\text{for } |\\delta| \\le 1, \\\\ |\\delta| - \\frac{1}{2} \u0026 \\text{otherwise.} \\end{cases}\\] Q-\ub124\ud2b8\uc6cc\ud06c# \uc6b0\ub9ac \ubaa8\ub378\uc740 \ud604\uc7ac\uc640 \uc774\uc804 \uc2a4\ud06c\ub9b0 \ud328\uce58\uc758 \ucc28\uc774\ub97c \ucde8\ud558\ub294 \uc21c\uc5f0\uacb0(feed-forward) \uc2e0\uacbd\ub9dd\uc785\ub2c8\ub2e4. \ub450\uac00\uc9c0 \ucd9c\ub825 \\(Q(s, \\mathrm{left})\\) \uc640 \\(Q(s, \\mathrm{right})\\) \uac00 \uc788\uc2b5\ub2c8\ub2e4. (\uc5ec\uae30\uc11c \\(s\\) \ub294 \ub124\ud2b8\uc6cc\ud06c\uc758 \uc785\ub825\uc785\ub2c8\ub2e4) \uacb0\uacfc\uc801\uc73c\ub85c \ub124\ud2b8\uc6cc\ud06c\ub294 \uc8fc\uc5b4\uc9c4 \ud604\uc7ac \uc785\ub825\uc5d0\uc11c \uac01 \ud589\ub3d9\uc758 \uae30\ub300\uac12 \uc744 \uc608\uce21\ud558\ub824\uace0 \ud569\ub2c8\ub2e4. class DQN(nn.Module): def __init__(self, n_observations, n_actions): super(DQN, self).__init__() self.layer1 = nn.Linear(n_observations, 128) self.layer2 = nn.Linear(128, 128) self.layer3 = nn.Linear(128, n_actions) # \ucd5c\uc801\ud654 \uc911\uc5d0 \ub2e4\uc74c \ud589\ub3d9\uc744 \uacb0\uc815\ud558\uae30 \uc704\ud574\uc11c \ud558\ub098\uc758 \uc694\uc18c \ub610\ub294 \ubc30\uce58\ub97c \uc774\uc6a9\ud574 \ud638\ucd10\ub429\ub2c8\ub2e4. # ([[left0exp,right0exp]...]) \ub97c \ubc18\ud658\ud569\ub2c8\ub2e4. def forward(self, x): x = F.relu(self.layer1(x)) x = F.relu(self.layer2(x)) return self.layer3(x) \ud559\uc2b5# \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\uc640 \uc720\ud2f8\ub9ac\ud2f0# \uc774 \uc140\uc740 \ubaa8\ub378\uacfc \ucd5c\uc801\ud654\uae30\ub97c \uc778\uc2a4\ud134\uc2a4\ud654\ud558\uace0 \uc77c\ubd80 \uc720\ud2f8\ub9ac\ud2f0\ub97c \uc815\uc758\ud569\ub2c8\ub2e4: select_action - Epsilon Greedy \uc815\ucc45\uc5d0 \ub530\ub77c \ud589\ub3d9\uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4. \uac04\ub2e8\ud788 \ub9d0\ud574\uc11c, \uac00\ub054 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud589\ub3d9\uc744 \uc120\ud0dd\ud558\uace0 \ub54c\ub85c\ub294 \ub2e8\uc9c0 \ud558\ub098\ub97c \uade0\uc77c\ud558\uac8c \uc0d8\ud50c\ub9c1\ud560 \uac83\uc785\ub2c8\ub2e4. \uc784\uc758\uc758 \uc561\uc158\uc744 \uc120\ud0dd\ud560 \ud655\ub960\uc740 EPS_START \uc5d0\uc11c \uc2dc\uc791\ud574\uc11c EPS_END \ub97c \ud5a5\ud574 \uc9c0\uc218\uc801\uc73c\ub85c \uac10\uc18c\ud560 \uac83\uc785\ub2c8\ub2e4. EPS_DECAY \ub294 \uac10\uc1e0 \uc18d\ub3c4\ub97c \uc81c\uc5b4\ud569\ub2c8\ub2e4. plot_durations - \uc9c0\ub09c 100\uac1c \uc5d0\ud53c\uc18c\ub4dc\uc758 \ud3c9\uade0(\uacf5\uc2dd \ud3c9\uac00\uc5d0\uc11c \uc0ac\uc6a9 \ub41c \uc218\uce58)\uc5d0 \ub530\ub978 \uc5d0\ud53c\uc18c\ub4dc\uc758 \uc9c0\uc18d\uc744 \ub3c4\ud45c\ub85c \uadf8\ub9ac\uae30 \uc704\ud55c \ud5ec\ud37c. \ub3c4\ud45c\ub294 \uae30\ubcf8 \ud6c8\ub828 \ub8e8\ud504\uac00 \ud3ec\ud568\ub41c \uc140 \ubc11\uc5d0 \uc788\uc73c\uba70, \ub9e4 \uc5d0\ud53c\uc18c\ub4dc\ub9c8\ub2e4 \uc5c5\ub370\uc774\ud2b8\ub429\ub2c8\ub2e4. # BATCH_SIZE\ub294 \ub9ac\ud50c\ub808\uc774 \ubc84\ud37c\uc5d0\uc11c \uc0d8\ud50c\ub9c1\ub41c \ud2b8\ub79c\uc9c0\uc158\uc758 \uc218\uc785\ub2c8\ub2e4. # GAMMA\ub294 \uc774\uc804 \uc139\uc158\uc5d0\uc11c \uc5b8\uae09\ud55c \ud560\uc778 \uacc4\uc218\uc785\ub2c8\ub2e4. # EPS_START\ub294 \uc5e1\uc2e4\ub860\uc758 \uc2dc\uc791 \uac12\uc785\ub2c8\ub2e4. # EPS_END\ub294 \uc5e1\uc2e4\ub860\uc758 \ucd5c\uc885 \uac12\uc785\ub2c8\ub2e4. # EPS_DECAY\ub294 \uc5e1\uc2e4\ub860\uc758 \uc9c0\uc218 \uac10\uc1e0(exponential decay) \uc18d\ub3c4 \uc81c\uc5b4\ud558\uba70, \ub192\uc744\uc218\ub85d \uac10\uc1e0 \uc18d\ub3c4\uac00 \ub290\ub9bd\ub2c8\ub2e4. # TAU\ub294 \ubaa9\ud45c \ub124\ud2b8\uc6cc\ud06c\uc758 \uc5c5\ub370\uc774\ud2b8 \uc18d\ub3c4\uc785\ub2c8\ub2e4. # LR\uc740 ``AdamW`` \uc635\ud2f0\ub9c8\uc774\uc800\uc758 \ud559\uc2b5\uc728(learning rate)\uc785\ub2c8\ub2e4. BATCH_SIZE = 128 GAMMA = 0.99 EPS_START = 0.9 EPS_END = 0.01 EPS_DECAY = 2500 TAU = 0.005 LR = 3e-4 # gym \ud589\ub3d9 \uacf5\uac04\uc5d0\uc11c \ud589\ub3d9\uc758 \uc22b\uc790\ub97c \uc5bb\uc2b5\ub2c8\ub2e4. n_actions = env.action_space.n # \uc0c1\ud0dc \uad00\uce21 \ud69f\uc218\ub97c \uc5bb\uc2b5\ub2c8\ub2e4. state, info = env.reset() n_observations = len(state) policy_net = DQN(n_observations, n_actions).to(device) target_net = DQN(n_observations, n_actions).to(device) target_net.load_state_dict(policy_net.state_dict()) optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True) memory = ReplayMemory(10000) steps_done = 0 def select_action(state): global steps_done sample = random.random() eps_threshold = EPS_END + (EPS_START - EPS_END) * \\ math.exp(-1. * steps_done / EPS_DECAY) steps_done += 1 if sample \u003e eps_threshold: with torch.no_grad(): # t.max (1)\uc740 \uac01 \ud589\uc758 \uac00\uc7a5 \ud070 \uc5f4 \uac12\uc744 \ubc18\ud658\ud569\ub2c8\ub2e4. # \ucd5c\ub300 \uacb0\uacfc\uc758 \ub450\ubc88\uc9f8 \uc5f4\uc740 \ucd5c\ub300 \uc694\uc18c\uc758 \uc8fc\uc18c\uac12\uc774\ubbc0\ub85c, # \uae30\ub300 \ubcf4\uc0c1\uc774 \ub354 \ud070 \ud589\ub3d9\uc744 \uc120\ud0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. return policy_net(state).max(1).indices.view(1, 1) else: return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long) episode_durations = [] def plot_durations(show_result=False): plt.figure(1) durations_t = torch.tensor(episode_durations, dtype=torch.float) if show_result: plt.title(\u0027Result\u0027) else: plt.clf() plt.title(\u0027Training...\u0027) plt.xlabel(\u0027Episode\u0027) plt.ylabel(\u0027Duration\u0027) plt.plot(durations_t.numpy()) # 100\uac1c\uc758 \uc5d0\ud53c\uc18c\ub4dc \ud3c9\uade0\uc744 \uac00\uc838 \uc640\uc11c \ub3c4\ud45c \uadf8\ub9ac\uae30 if len(durations_t) \u003e= 100: means = durations_t.unfold(0, 100, 1).mean(1).view(-1) means = torch.cat((torch.zeros(99), means)) plt.plot(means.numpy()) plt.pause(0.001) # \ub3c4\ud45c\uac00 \uc5c5\ub370\uc774\ud2b8\ub418\ub3c4\ub85d \uc7a0\uc2dc \uba48\ucda4 if is_ipython: if not show_result: display.display(plt.gcf()) display.clear_output(wait=True) else: display.display(plt.gcf()) \ud559\uc2b5 \ub8e8\ud504# \ucd5c\uc885\uc801\uc73c\ub85c \ubaa8\ub378 \ud559\uc2b5\uc744 \uc704\ud55c \ucf54\ub4dc. \uc5ec\uae30\uc11c, \ucd5c\uc801\ud654\uc758 \ud55c \ub2e8\uacc4\ub97c \uc218\ud589\ud558\ub294 optimize_model \ud568\uc218\ub97c \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uba3c\uc800 \ubc30\uce58 \ud558\ub098\ub97c \uc0d8\ud50c\ub9c1\ud558\uace0 \ubaa8\ub4e0 Tensor\ub97c \ud558\ub098\ub85c \uc5f0\uacb0\ud558\uace0 \\(Q(s_t, a_t)\\) \uc640 \\(V(s_{t+1}) = \\max_a Q(s_{t+1}, a)\\) \ub97c \uacc4\uc0b0\ud558\uace0 \uadf8\uac83\ub4e4\uc744 \uc190\uc2e4\ub85c \ud569\uce69\ub2c8\ub2e4. \uc6b0\ub9ac\uac00 \uc124\uc815\ud55c \uc815\uc758\uc5d0 \ub530\ub974\uba74 \ub9cc\uc57d \\(s\\) \uac00 \ub9c8\uc9c0\ub9c9 \uc0c1\ud0dc\ub77c\uba74 \\(V(s) = 0\\) \uc785\ub2c8\ub2e4. \ub610\ud55c \uc548\uc815\uc131 \ucd94\uac00 \uc704\ud55c \\(V(s_{t+1})\\) \uacc4\uc0b0\uc744 \uc704\ud574 \ubaa9\ud45c \ub124\ud2b8\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ub300\uc0c1 \ub124\ud2b8\uc6cc\ud06c\ub294 \uc774\uc804\uc5d0 \uc815\uc758\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 TAU \uc5d0 \uc758\ud574 \uc81c\uc5b4\ub418\ub294 \uc18c\ud504\ud2b8 \uc5c5\ub370\uc774\ud2b8 \ub85c \ubaa8\ub4e0 \ub2e8\uacc4\uc5d0\uc11c \uc5c5\ub370\uc774\ud2b8\ub429\ub2c8\ub2e4. def optimize_model(): if len(memory) \u003c BATCH_SIZE: return transitions = memory.sample(BATCH_SIZE) # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for # detailed explanation). \uc774\uac83\uc740 batch-array\uc758 Transitions\uc744 Transition\uc758 batch-arrays\ub85c # \uc804\ud658\ud569\ub2c8\ub2e4. batch = Transition(*zip(*transitions)) # \ucd5c\uc885\uc774 \uc544\ub2cc \uc0c1\ud0dc\uc758 \ub9c8\uc2a4\ud06c\ub97c \uacc4\uc0b0\ud558\uace0 \ubc30\uce58 \uc694\uc18c\ub97c \uc5f0\uacb0\ud569\ub2c8\ub2e4 # (\ucd5c\uc885 \uc0c1\ud0dc\ub294 \uc2dc\ubbac\ub808\uc774\uc158\uc774 \uc885\ub8cc \ub41c \uc774\ud6c4\uc758 \uc0c1\ud0dc) non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool) non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]) state_batch = torch.cat(batch.state) action_batch = torch.cat(batch.action) reward_batch = torch.cat(batch.reward) # Q(s_t, a) \uacc4\uc0b0 - \ubaa8\ub378\uc774 Q(s_t)\ub97c \uacc4\uc0b0\ud558\uace0, \ucde8\ud55c \ud589\ub3d9\uc758 \uc5f4\uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4. # \uc774\ub4e4\uc740 policy_net\uc5d0 \ub530\ub77c \uac01 \ubc30\uce58 \uc0c1\ud0dc\uc5d0 \ub300\ud574 \uc120\ud0dd\ub41c \ud589\ub3d9\uc785\ub2c8\ub2e4. state_action_values = policy_net(state_batch).gather(1, action_batch) # \ubaa8\ub4e0 \ub2e4\uc74c \uc0c1\ud0dc\ub97c \uc704\ud55c V(s_{t+1}) \uacc4\uc0b0 # non_final_next_states\uc758 \ud589\ub3d9\ub4e4\uc5d0 \ub300\ud55c \uae30\ub300\uac12\uc740 \"\uc774\uc804\" target_net\uc744 \uae30\ubc18\uc73c\ub85c \uacc4\uc0b0\ub429\ub2c8\ub2e4. # max(1).values\ub85c \ucd5c\uace0\uc758 \ubcf4\uc0c1\uc744 \uc120\ud0dd\ud558\uc2ed\uc2dc\uc624. # \uc774\uac83\uc740 \ub9c8\uc2a4\ud06c\ub97c \uae30\ubc18\uc73c\ub85c \ubcd1\ud569\ub418\uc5b4 \uae30\ub300 \uc0c1\ud0dc \uac12\uc744 \uac16\uac70\ub098 \uc0c1\ud0dc\uac00 \ucd5c\uc885\uc778 \uacbd\uc6b0 0\uc744 \uac16\uc2b5\ub2c8\ub2e4. next_state_values = torch.zeros(BATCH_SIZE, device=device) with torch.no_grad(): next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values # \uae30\ub300 Q \uac12 \uacc4\uc0b0 expected_state_action_values = (next_state_values * GAMMA) + reward_batch # Huber \uc190\uc2e4 \uacc4\uc0b0 criterion = nn.SmoothL1Loss() loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1)) # \ubaa8\ub378 \ucd5c\uc801\ud654 optimizer.zero_grad() loss.backward() # \ubcc0\ud654\ub3c4 \ud074\ub9ac\ud551 \ubc14\uafd4\uce58\uae30 torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100) optimizer.step() \uc544\ub798\uc5d0\uc11c \uc8fc\uc694 \ud559\uc2b5 \ub8e8\ud504\ub97c \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ucc98\uc74c\uc73c\ub85c \ud658\uacbd\uc744 \uc7ac\uc124\uc815\ud558\uace0 \ucd08\uae30 state Tensor\ub97c \uc5bb\uc2b5\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c \ud589\ub3d9\uc744 \uc0d8\ud50c\ub9c1\ud558\uace0, \uadf8\uac83\uc744 \uc2e4\ud589\ud558\uace0, \ub2e4\uc74c \uc0c1\ud0dc\uc640 \ubcf4\uc0c1(\ud56d\uc0c1 1)\uc744 \uad00\ucc30\ud558\uace0, \ubaa8\ub378\uc744 \ud55c \ubc88 \ucd5c\uc801\ud654\ud569\ub2c8\ub2e4. \uc5d0\ud53c\uc18c\ub4dc\uac00 \ub05d\ub098\uba74 (\ubaa8\ub378\uc774 \uc2e4\ud328) \ub8e8\ud504\ub97c \ub2e4\uc2dc \uc2dc\uc791\ud569\ub2c8\ub2e4. \uc544\ub798\uc5d0\uc11c num_episodes \ub294 GPU\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uacbd\uc6b0 600\uc73c\ub85c, \uadf8\ub807\uc9c0 \uc54a\uc740 \uacbd\uc6b0 50\uac1c\uc758 \uc5d0\ud53c\uc18c\ub4dc\ub97c \uc124\uc815\ud558\uc5ec \ud559\uc2b5\uc774 \ub108\ubb34 \uc624\ub798 \uac78\ub9ac\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc 50\uac1c\uc758 \uc5d0\ud53c\uc18c\ub4dc\ub9cc\uc73c\ub85c\ub294 CartPole\uc5d0\uc11c \uc88b\uc740 \uc131\ub2a5\uc744 \uad00\ucc30\ud558\uae30\uc5d0\ub294 \ucda9\ubd84\uce58 \uc54a\uc2b5\ub2c8\ub2e4. 600\uac1c\uc758 \ud559\uc2b5 \uc5d0\ud53c\uc18c\ub4dc \ub0b4\uc5d0\uc11c \ubaa8\ub378\uc774 \uc9c0\uc18d\uc801\uc73c\ub85c 500\uac1c\uc758 \uc2a4\ud15d\uc744 \ub2ec\uc131\ud558\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4. RL \uc5d0\uc774\uc804\ud2b8 \ud559\uc2b5 \uacfc\uc815\uc5d0\ub294 \ub178\uc774\uc988\uac00 \ub9ce\uc744 \uc218 \uc788\uc73c\ubbc0\ub85c, \uc218\ub834(convergence)\uc774 \uad00\ucc30\ub418\uc9c0 \uc54a\uc73c\uba74 \ud559\uc2b5\uc744 \uc7ac\uc2dc\uc791\ud558\ub294 \uac83\uc774 \ub354 \ub098\uc740 \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. if torch.cuda.is_available() or torch.backends.mps.is_available(): num_episodes = 600 else: num_episodes = 50 for i_episode in range(num_episodes): # \ud658\uacbd\uacfc \uc0c1\ud0dc \ucd08\uae30\ud654 state, info = env.reset() state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) for t in count(): action = select_action(state) observation, reward, terminated, truncated, _ = env.step(action.item()) reward = torch.tensor([reward], device=device) done = terminated or truncated if terminated: next_state = None else: next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0) # \uba54\ubaa8\ub9ac\uc5d0 \ubcc0\uc774 \uc800\uc7a5 memory.push(state, action, next_state, reward) # \ub2e4\uc74c \uc0c1\ud0dc\ub85c \uc774\ub3d9 state = next_state # (\uc815\ucc45 \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c) \ucd5c\uc801\ud654 \ud55c\ub2e8\uacc4 \uc218\ud589 optimize_model() # \ubaa9\ud45c \ub124\ud2b8\uc6cc\ud06c\uc758 \uac00\uc911\uce58\ub97c \uc18c\ud504\ud2b8 \uc5c5\ub370\uc774\ud2b8 # \u03b8\u2032 \u2190 \u03c4 \u03b8 + (1 \u2212\u03c4 )\u03b8\u2032 target_net_state_dict = target_net.state_dict() policy_net_state_dict = policy_net.state_dict() for key in policy_net_state_dict: target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU) target_net.load_state_dict(target_net_state_dict) if done: episode_durations.append(t + 1) plot_durations() break print(\u0027Complete\u0027) plot_durations(show_result=True) plt.ioff() plt.show() /opt/conda/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:249: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`. (Deprecated NumPy 1.24) Complete \ub2e4\uc74c\uc740 \uc804\uccb4 \uacb0\uacfc \ub370\uc774\ud130 \ud750\ub984\uc744 \ubcf4\uc5ec\uc8fc\ub294 \ub2e4\uc774\uc5b4\uadf8\ub7a8\uc785\ub2c8\ub2e4. \ud589\ub3d9\uc740 \ubb34\uc791\uc704 \ub610\ub294 \uc815\ucc45\uc5d0 \ub530\ub77c \uc120\ud0dd\ub418\uc5b4, gym \ud658\uacbd\uc5d0\uc11c \ub2e4\uc74c \ub2e8\uacc4 \uc0d8\ud50c\uc744 \uac00\uc838\uc635\ub2c8\ub2e4. \uacb0\uacfc\ub97c \uc7ac\ud604 \uba54\ubaa8\ub9ac\uc5d0 \uc800\uc7a5\ud558\uace0 \ubaa8\ub4e0 \ubc18\ubcf5\uc5d0\uc11c \ucd5c\uc801\ud654 \ub2e8\uacc4\ub97c \uc2e4\ud589\ud569\ub2c8\ub2e4. \ucd5c\uc801\ud654\ub294 \uc7ac\ud604 \uba54\ubaa8\ub9ac\uc5d0\uc11c \ubb34\uc791\uc704 \ubc30\uce58\ub97c \uc120\ud0dd\ud558\uc5ec \uc0c8 \uc815\ucc45\uc744 \ud559\uc2b5\ud569\ub2c8\ub2e4. \u201c\uc774\uc804\u201d\uc758 target_net\uc740 \ucd5c\uc801\ud654\uc5d0\uc11c \uae30\ub300 Q \uac12\uc744 \uacc4\uc0b0\ud558\ub294 \ub370\uc5d0\ub3c4 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \ubaa9\ud45c \ub124\ud2b8\uc6cc\ud06c \uac00\uc911\uce58\uc758 \uc18c\ud504\ud2b8 \uc5c5\ub370\uc774\ud2b8\ub294 \ub9e4 \ub2e8\uacc4(step)\ub9c8\ub2e4 \uc218\ud589\ub429\ub2c8\ub2e4. Total running time of the script: (9 minutes 47.742 seconds) Download Jupyter notebook: reinforcement_q_learning.ipynb Download Python source code: reinforcement_q_learning.py Download zipped: reinforcement_q_learning.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/intermediate/reinforcement_q_learning.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>